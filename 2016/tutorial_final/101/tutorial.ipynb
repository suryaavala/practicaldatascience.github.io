{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using spaCy  for Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, we discussed about natural language processing in data science and practiced on NLTK library in Python for handling text data.\n",
    "In fact there are many other powerful libraries in python besides NLTK that we can use to process natural language.\n",
    "\n",
    "spaCy an industrial-strength natural language processing library written in Python and Cython. It aims to achieve higher processing speed and better accuracy comparing to NLTK. It also provides some features not found in NLTK library such as word vectors.\n",
    "\n",
    "In this tutorial, we will walk through some useful features in spaCy and use simple examples to show how to use those features. Then, we will use a larger data set - the tweet data to demonstrate two real-word usage of those features on natural language processing case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, to install spacy we can use the following pip command in Anaconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable the processer we should also download language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "python -m spacy.en.download all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for the language model for English. spaCy also provides a German language model, which you may try out of your interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "python -m spacy.de.download all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloads may take some time. After that, we can start playing with spaCy in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, nlp is a Language object in spaCy, which includes parser and vectors built from a huge English text set. We can also disable the parser or assign custom parser during the initialization process. Here we will stick to the default one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play with the same dataset we used in homework 3 – the tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a list of Strings. Each String is the content of one document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spacy, we can generate a Doc object to represent the language model of a document by calling on Language. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"Wake up Jack! Stop being a potato-coach.It is a good weather today. \\\n",
    "Get on your Nike shoes. Let’s go to New York and play baseball. 12345\"\n",
    "doc = nlp(text.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By iterating through a doc object, we can see the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Wake ', u'up ', u'Jack', u'! ', u'Stop ', u'being ', u'a ', u'potato', u'-', u'coach', u'.', u'It ', u'is ', u'a ', u'good ', u'weather ', u'today', u'. ', u'Get ', u'on ', u'your ', u'Nike ', u'shoes', u'. ', u'Let', u'\\u2019s ', u'go ', u'to ', u'New ', u'York ', u'and ', u'play ', u'baseball', u'. ', u'12345']\n"
     ]
    }
   ],
   "source": [
    "token_list = [token.string for token in doc]\n",
    "print token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the tokens are in their original format. But spaCy has integrated a large amount of features for token objects. For instance, we can see the lemma or lower case representation by getting lemma_ or lower_ features. We can even check if the token is url/num/email format, or check if the token is a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma: wake up jack ! stop be a potato - coach . it be a good weather today . get on your nike shoe . let ’s go to new york and play baseball . 12345\n",
      "lower: wake up jack ! stop being a potato - coach . it is a good weather today . get on your nike shoes . let ’s go to new york and play baseball . 12345\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print \"lemma:\", \" \".join(token.lemma_ for token in doc)\n",
    "print \"lower:\",\" \".join(token.lower_ for token in doc)\n",
    "print doc[-1].like_num\n",
    "print doc[1].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that \"lower\" prints each word in its lower case and \"lemma\" actually stem the words and print \"being\" and \"is\" both as be. \n",
    "\n",
    "The last two lines is check whether \"12345\" is likely a number and \"up\" is a stop word. Obviously they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'wake', u'jack', u'stop', u'potato', u'coach', u'good', u'weather', u'today', u'nike', u'shoe', u'let', u'new', u'york', u'play', u'baseball']\n"
     ]
    }
   ],
   "source": [
    "def process(doc):\n",
    "    tokens = [(token.lemma_).lower() for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return tokens\n",
    "\n",
    "token_list = process(doc)\n",
    "print token_list\n",
    "lem_doc = nlp((\" \".join(token_list).decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these features, we can process the data as we have done in homework 3 to build the token lists.\n",
    "\n",
    "Note that here we update doc as a doc with lemma representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy allows us to parse the documents in a multi-thread manner, which makes our jobs of converts documents to tokens easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = nlp.pipe([tweet.decode('UTF-8') for tweet in tweets['text']], batch_size=50, n_threads=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will play with the tweet data later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature of spaCy is that it provides named entiny recognition. The term Named Entity refers to a real-world object mentioned in the text. It can be persons, locations, orgnizations, etc. For instance, in the sentence above, \"Jack\" is a person, \"Nike\" is a company, and \"New York\" is a place. \n",
    "\n",
    "spaCy helps us identify the named entities and classify them different categories shown as follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Named Entity Recognition](img/nea.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply check the named entity type of a entity by checking entity.label feature using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PERSON': [u'Jack'], 'NORP': [], 'ORG': [u'Nike'], 'GPE': [u'New York']}\n"
     ]
    }
   ],
   "source": [
    "def extract_named_entities(doc, names=[\"PERSON\", \"NORP\", \"GPE\", \"ORG\"]):\n",
    "    name_entities = {}\n",
    "    for name in names:\n",
    "        name_entities[name] = []\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ in names:\n",
    "            name_entities[entity.label_].append(entity.text)\n",
    "    return name_entities\n",
    "\n",
    "name_entities = extract_named_entities(doc)\n",
    "print name_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previos method allows us to extract named entities of certain types. As we can see, spaCy sussessfully identify the named entities and their type as we expected.\n",
    "\n",
    "Since a named entity can represent a real-world object, it can be used to analyze relationships between objects in real world, which we will see in the later part of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech (POS) tagging is another natural language analyse feature offered by spaCy. Part of speech tagging shows whether a word in a text is noun, verb, adjactive or other types.\n",
    "\n",
    "Again, we can simply access the POS information of a token by accessing token.pos feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wake /VERB jack /NOUN stop /NOUN potato /NOUN coach /NOUN good /ADJ weather /NOUN today /NOUN nike /ADJ shoe /NOUN let /VERB new /ADJ york /NOUN play /NOUN baseball/NOUN\n"
     ]
    }
   ],
   "source": [
    "def tag_pos(doc):\n",
    "    tagged_words = [(token.string, token.pos_) for token in doc]\n",
    "    return tagged_words\n",
    "\n",
    "tagged_words = tag_pos(lem_doc)\n",
    "tagged_text = \" \".join(word[0] + \"/\" + word[1] for word in tagged_words)\n",
    "print tagged_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above shows the part of speech tag of each word with a \"/POS\" after it.\n",
    "\n",
    "Similar as before, we can define a method that extract words with certain types of POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ADV': [], 'ADJ': [u'good ', u'nike ', u'new '], 'VERB': [u'wake ', u'let '], 'NOUN': [u'jack ', u'stop ', u'potato ', u'coach ', u'weather ', u'today ', u'shoe ', u'york ', u'play ', u'baseball']}\n"
     ]
    }
   ],
   "source": [
    "def extract_pos(doc, tags=[\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]):\n",
    "    words_pos = {}\n",
    "    for tag in tags:\n",
    "        words_pos[tag] = []\n",
    "    tagged_words = tag_pos(doc)\n",
    "    for word in tagged_words:\n",
    "        if word[1] in tags:\n",
    "            words_pos[word[1]].append(word[0])\n",
    "    return words_pos\n",
    "\n",
    "words_pos = extract_pos(lem_doc)\n",
    "print words_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech tagging is an important step for machine to understand natural language. In some cases, researchers may want to discard the adverbs usually don't contains a lot of information and use only nouns to represent the document. In other cases, researchers may want to analyze on adjectives to see the emotion of a writer. However, since a word in English may be used in varies part-of-speech cases, it is also a hard task that requires many future works.\n",
    "\n",
    "We don't have the chance to use POS tagger more in this tutorial. But you can try it on your own if your are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One remarkable feature of spaCy that differentiate it from other natural language processing libraries is that it provides word vectors. A word vector is a \"word embedding\" representation in the form of numeric vector. It is usually used to analyze similarity between words. The most famous word embedding model is called word2vec. By default, spacy uses the word vectors produced by this model.\n",
    "\n",
    "The similarity between two words is simply recognized as the cosine similarity of their vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple and pineapple: 0.586574\n",
      "computer and microchip: 0.647215\n",
      "apple and computer: 0.406346\n",
      "pineapple and computer: 0.294345\n"
     ]
    }
   ],
   "source": [
    "def word_similarity(word_a, word_b):\n",
    "    if not (word_a.has_vector and word_b.has_vector):\n",
    "        return 0\n",
    "    similarity = np.dot(word_a.vector, word_b.vector) / (np.linalg.norm(word_a.vector) * np.linalg.norm(word_b.vector))\n",
    "    return similarity\n",
    "\n",
    "word_a, word_b, word_c, word_d = nlp(u'apple pineapple computer microchip');\n",
    "print \"apple and pineapple:\", word_similarity(word_a, word_b)\n",
    "print \"computer and microchip:\", word_similarity(word_c, word_d)\n",
    "print \"apple and computer:\", word_similarity(word_a, word_c)\n",
    "print \"pineapple and computer:\", word_similarity(word_b, word_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that \"apple\" and \"pineapple\", \"computer\" and \"microchip\" have higher similarity comparing to \"computer\" with those two fruits. This make sense according to our knowledge. It is also interesting to see that apple actually have higher similarity with computer than other fruits. spaCy probably also takes Apple Computer into consideration when decides those similarities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy offers simpler way for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity = word_a.similarity(word_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here word_a and word_b can be either tokens or spans.\n",
    "\n",
    "We can use this feature to search for a document that related to a topic not only by an exact word match but some similar words. We will demonstrate it through the tweet data in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with real cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play with the same dataset we used in homework 3 – the tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(pd):\n",
    "    docs = []\n",
    "    for doc in nlp.pipe([tweet.decode('UTF-8') for tweet in pd], batch_size=50, n_threads=4):\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "docs = load_data(tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @DWStweets: The choice for 2016 is clear: We need another Democrat in the White House. #DemDebate #WeAreDemocrats http://t.co/0n5g0YN46f\n"
     ]
    }
   ],
   "source": [
    "print docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above prepares a list of docs each represent a tweet. We can process those docs to generate lemmatized token lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt choice clear need democrat white house demdebate wearedemocrats\n"
     ]
    }
   ],
   "source": [
    "token_lists = [process(doc) for doc in docs]\n",
    "lem_lines = [u' '.join(tokens) for tokens in token_lists]\n",
    "lem_docs = []\n",
    "for line in lem_lines:\n",
    "    if type(line) != 'unicode':\n",
    "        try:\n",
    "            lem_docs.append(nlp(line.decode(\"UTF-8\")))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    else:\n",
    "        lem_docs.append(nlp(line))\n",
    "print lem_docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since token_lists is a list of lemmatized token list as we used in homework 3. But since here we want to fully demostrate features in spaCy, we turn the token list back to doc objects so that we can apply spaCy methods on them.\n",
    "\n",
    "As we can see the doc are now lemmatized and with stop words and non-alphabetic words removed. (Here we discard some text that can not be decoded. They are mostly non-English words.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see how tightly two entities related to each other, one easy way is to evaluate their co-occurrence in the corpus we have.\n",
    "In this case, we will extract named entities of people and group from tweets and see how strongly they are related to each other.\n",
    "\n",
    "First, we need a method that calculate the co-occurrence of different pairs of entities and the frequency of each entity.\n",
    "\n",
    "Since a capital inital letter is often used as an important factor in named entity recognition, lemmatization will affect the result of recognition. Thus, we use original docs instead of lemmatized docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_co_occurrence(docs, names):\n",
    "    freq = Counter()\n",
    "    co_occurrence = {}\n",
    "    for doc in docs:\n",
    "        named_entities = extract_named_entities(doc, names)\n",
    "        entity_set = set([entity for entities in named_entities.values() for entity in entities])\n",
    "        freq += Counter(entity_set)\n",
    "        for entity in entity_set:\n",
    "            co = entity_set.difference([entity])\n",
    "            if co_occurrence.has_key(entity):\n",
    "                co_occurrence[entity] += Counter(co)\n",
    "            else:\n",
    "                co_occurrence[entity] = Counter(co)\n",
    "    return freq, co_occurrence\n",
    "\n",
    "f, co = get_co_occurrence(docs, ['PERSON', 'NORP'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'American', 28), (u'Americans', 27), (u'Hillary', 27), (u'Muslims', 11), (u'Cruz', 9)]\n"
     ]
    }
   ],
   "source": [
    "print co['Trump'].most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of above code shows the words that show up with \"Trump\" the most often are \"American\", \"Americans\", \"Hillary\", \"Muslims\" and \"Cruz\". Obviously they are closely related to Trump. See that since we are using non-lemmatized words, \"American\" and \"Americans\" can not be recognized as same. This would be something to improve in the future work.\n",
    "\n",
    "The above method analyzes the frequency of co-occurrence but it has a limit. The entities that show up most frequently tend to have more co-occurrence with all the other words, which does not neccessarily mean there are close relationships between them. Also, the more documents we have, the higher co-occurrence we may find.\n",
    "\n",
    "Therefore, we want to take the individual frequencies of entities and corpus size into consideration and use a indicator called Pointwise Mutual Information (PMI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$PMI(x,y) = \\log \\frac{p(x,y)}{p(x)p(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $p(x.y)$ denotes the rate that $x$ and $y$ co-occur among all the documents, and $p(x)$, $p(y)$ are individual occurrence rate of $x$ and $y$, respectively.\n",
    "\n",
    "Using the formula, we can find the PMI value of entity pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815133888394\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def calculate_PMI(x, y, docs, names):\n",
    "    freq, co_occurrence = get_co_occurrence(docs, names)\n",
    "    if freq[x] == 0 or freq[y] == 0:\n",
    "        return 0\n",
    "    co = co_occurrence[x][y]\n",
    "    f_x = freq[x]\n",
    "    f_y = freq[y]\n",
    "    pmi = math.log(co) + math.log(len(docs)) - math.log(f_x) - math.log(f_y)\n",
    "    return pmi\n",
    "pmi = calculate_PMI(\"Trump\", \"American\", docs, ['PERSON', 'NORP'])\n",
    "print pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big limitation of PMI is that it has infinite range. Therefore, we have no idea how big a PMI should be to show a strong relationship. There is another indicator called Phi-Square indicator that limits the range to [0, 1], which allows us to get more sense on the relationship when look at the number.\n",
    "\n",
    "However, it's hard to evaluate which indicator performs best. Both of them have their advantage dealing with certain data. Everythin is corpus dependant. We don't have chance to show the comparison between Phi-Square and PMI here, but you can check more on https://en.wikipedia.org/wiki/Phi_coefficient\n",
    "\n",
    "Such analysis is often regard as concept occurrence analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, to illustrate whether a tweet is relevant to certain topic, one way is to check is there any relevant words or phrases in that tweet. We can utilize word vector similarity to perform the check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_relevant(tweet, topic):\n",
    "    for tweet_token in tweet:\n",
    "        for topic_token in topic:\n",
    "            if word_similarity(tweet_token, topic_token) > 0.5:\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinton take campaign cash drug industry enemy demdebate\n"
     ]
    }
   ],
   "source": [
    "def find_relevant(docs,topic):\n",
    "    relevant_doc = []\n",
    "    for doc in docs:\n",
    "        if is_relevant(doc, topic):\n",
    "            relevant_doc.append(doc)\n",
    "    return relevant_doc\n",
    "\n",
    "money = nlp(u'money')\n",
    "print find_relevant(lem_docs, money)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we identify the tweet related to \"money\" with no exact word \"money\" in the tweet. It contains the word \"cash\", which is highly similar to \"money\".\n",
    "\n",
    "This can be recognized as an example of fuzzy search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is a strong industrial level natural language library that contains a well-trained model which can boost our analysis tasks. It provides features such as tokenizing, named entity recognition, part-of-speech tagging and word verctors that will be very helpful when dealing with natural languages.\n",
    "\n",
    "In this tutorial, we walk through these basic functions in spaCy and use them in two real world cases that commonly seen in NLP problems - Concept Co-occurence and Fuzzy Search. Of course, those are just simplified examples. Real cases can be much more complicated, but you can regard it as a start and keep digging for more interesting problems.\n",
    "\n",
    "Also, spaCy has more complicated functions such as self-defined models and taggers. If you are interested, there are much more to learn and practice on https://spacy.io/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
