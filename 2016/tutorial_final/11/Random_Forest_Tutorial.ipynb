{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "### Random Forests\n",
    "This tutorial is intended as a gentle introduction to **random forests**, which are an essential machine learning tool for any data scientist. The tutorial will introduce random forests in the context of NFL two point conversions, which are a critical part of decision making for coaches. With million dollar jobs on the line, coaches cannot afford to make mistakes, and generating a model can assist coaches in making these critical decisions. By using data science, coaches can make better decisions by casting model-based predictions, and thus get better results. In addition, to show the versatility of the random forest technique, we will run our model on tweet classification as well to classify a tweet as Republican or Democrat.\n",
    "\n",
    "Random forests are considered to be a great way to predict results, since good old decision trees tend to overfit data. Random forests have also been shown to produce the best results out of the multitude of prediction algorithms out there (source: [link](http://machinelearningmastery.com/use-random-forest-testing-179-classifiers-121-datasets/)), and we will show this today by comparing the random forest classifier with SVM's on NFL and tweet data.\n",
    "\n",
    "### Outline\n",
    "Random forests can be used in practically any situation, so we will be showing aspects of the algorithm on two completely different use cases - one for predicting certain outcomes of NFL games, and the other for predicting the author of a tweet. The following topics will be covered in the extent of this tutorial:\n",
    "\n",
    "1. Setup and installation\n",
    "2. Introduction to Trees and Random Forests\n",
    "3. NFL Prediction - data processing\n",
    "4. NFL Prediction - Random Forest prediction of 2 pt conversions\n",
    "5. Why Only 72%?\n",
    "5. Another Worked Example: Tweets - data processing\n",
    "6. Tweets - Random Forest prediction\n",
    "8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "The majority of the code written in the tutorial will use the module sci-kit learn library, which has many pre-written algorithms for classifiers such as random forests. To install sci-kit learn, simply use pip as follows.\n",
    "\n",
    "`pip install -U scikit-learn`\n",
    "\n",
    "Next import the libraries to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import text_classification as tc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Trees and Random Forests\n",
    "\n",
    "In machine learning, one of the most important problems is **classification**. Over the years, researchers have developed various models that can help with classification. A very basic model for classification is the decision tree, which takes in a bunch of features, and basically goes feature by feature down a tree until an answer is reached (typically success or failure).\n",
    "\n",
    "![Decision Tree](Decision_Tree_1.png)\n",
    "\n",
    "This is a fairly simple way to clasify a test set we have never seen before - the for each data entry, traverse the decision tree based on the values of each feature in the entry, and come to a prediction at the leaf. Decision trees are really useful for a quick and simple predictor of the data set. In addition, the model is extremely easy to follow, as most users can easily visualize the tree and understand on a high level how the decision tree operates. \n",
    "\n",
    "However, plain decision trees have their Achilles' heel - overfitting of data. This happens because the tree and the data set are very closely related to each other. Thus, for the training data, decision trees will have a very high accuracy, but for a test set, a decision tree will suffer, since it is so closely related to the training set.\n",
    "\n",
    "Thus, when it is known that the test set is completely different from training (which it usually is), a decision tree will not give the best results.\n",
    "\n",
    "To solve this problem, random forests were invented! A **random forest** is a machine learning method used to counteract the major flaw with decision trees mentioned above. Random forests make many decision trees at once, using only randomly generated subsets of the data at the time. In addition, to determine splits in the tree, a random subset of features is considered at any given time. Both of these in tandem allow for greater randomization, and thus combat the problem of overfitting. \n",
    "\n",
    "![Random Forest](random_forest_new2.png)\n",
    "\n",
    "\n",
    "In this tutorial today, we will be constructing our own random forests using the sci-kit learn library, which has a pre-existing class for random forests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFL Prediction: Data Processing\n",
    "\n",
    "To show the usefulness of random forests, we will first be predicting the success or failure of a 2-point conversion in any football game. Some background: After a touchdown, a team has the option of kicking an extra point (1 point, probability 93%) [[source]](http://www.rollingstone.com/sports/features/the-nfls-new-rule-extra-points-are-no-longer-pointless-20150908), or going for a two point conversion (2 points, 46.3% success) [[source]](http://archive.advancedfootballanalytics.com/2010/12/almost-always-go-for-2-point.html). Because of this, coaches are usually reluctant to go for 2, and rather take the sure bet of 1 point (with millions of dollars on the line, this makes sense!) We will create a model that will hopefully better predict the result of a two point conversion, based on various factors, so that coaches can make a more informed decision.\n",
    "\n",
    "\n",
    "Our first data set is a set of all plays called throughout 2005-2013 (Go 49ers!!) - every single play is accounted for in this data set. Load the data set as follows. We will be using data from the 2005-2012 NFL season as the training set, and use the 2013 NFL season (through week 12) as the testing test to predict results.\n",
    "\n",
    "Finally, we will be using a metric called DVOA, which measures the efficiency of a teams offense for a given season - these metrics are in a seperate csv.\n",
    "\n",
    "Lets get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DVOA sets, converted to dictionaries for future use\n",
    "dvoa_all = dict()\n",
    "dvoa_2005 = pd.read_csv(\"2005_offdvoa.csv\").set_index('off').to_dict()\n",
    "dvoa_2006 = pd.read_csv(\"2006_offdvoa.csv\").set_index('off').to_dict()\n",
    "dvoa_2007 = pd.read_csv(\"2007_offdvoa.csv\").set_index('off').to_dict()\n",
    "dvoa_2008 = pd.read_csv(\"2008_offdvoa.csv\").set_index('off').to_dict()\n",
    "dvoa_2009 = pd.read_csv(\"2009_offdvoa.csv\").set_index('off').to_dict()\n",
    "dvoa_2010 = pd.read_csv(\"2010_offdvoa.csv\").set_index('off').to_dict()\n",
    "dvoa_2011 = pd.read_csv(\"2011_offdvoa.csv\").set_index('off').to_dict()\n",
    "dvoa_2012 = pd.read_csv(\"2012_offdvoa.csv\").set_index('off').to_dict()\n",
    "dvoa_2013 = pd.read_csv(\"2013_offdvoa.csv\").set_index('off').to_dict()\n",
    "for d in [dvoa_2005, dvoa_2006,dvoa_2007,dvoa_2008,dvoa_2009,dvoa_2010, dvoa_2011, dvoa_2012, dvoa_2013]:\n",
    "    dvoa_all.update(d)\n",
    "\n",
    "#test set\n",
    "test_set = pd.read_csv(\"2013_nfl_pbp_data_through_wk_12.csv\")\n",
    "\n",
    "\n",
    "# training set\n",
    "train2005_set = pd.read_csv(\"2005_nfl_pbp_data.csv\")\n",
    "train2006_set = pd.read_csv(\"2006_nfl_pbp_data.csv\")\n",
    "train2007_set = pd.read_csv(\"2007_nfl_pbp_data.csv\")\n",
    "train2008_set = pd.read_csv(\"2008_nfl_pbp_data.csv\")\n",
    "train2009_set = pd.read_csv(\"2009_nfl_pbp_data.csv\")\n",
    "train2010_set = pd.read_csv(\"2010_nfl_pbp_data.csv\")\n",
    "train2011_set = pd.read_csv(\"2011_nfl_pbp_data.csv\")\n",
    "train2012_set = pd.read_csv(\"2012_nfl_pbp_data.csv\")\n",
    "\n",
    "train_set = pd.concat([train2005_set, train2006_set, train2007_set, \n",
    "                       train2008_set, train2009_set, train2010_set,\n",
    "                       train2011_set, train2012_set]).dropna(how = 'all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the data sets have been loaded by printing this for each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'gameid', u'qtr', u'min', u'sec', u'off', u'def', u'down', u'togo',\n",
      "       u'ydline', u'description', u'offscore', u'defscore', u'season'],\n",
      "      dtype='object')\n",
      "            gameid  qtr   min sec  off  def  down  togo  ydline  \\\n",
      "0  20050908_OAK@NE  1.0   0.0   0   NE  OAK   NaN   NaN    22.0   \n",
      "1  20050908_OAK@NE  1.0  59.0  55  OAK   NE   1.0  10.0    72.0   \n",
      "2  20050908_OAK@NE  1.0  59.0  19  OAK   NE   2.0   3.0    65.0   \n",
      "3  20050908_OAK@NE  1.0  58.0  34  OAK   NE   1.0  10.0    61.0   \n",
      "4  20050908_OAK@NE  1.0  58.0  14  OAK   NE   1.0  10.0    32.0   \n",
      "\n",
      "                                         description  offscore  defscore  \\\n",
      "0  A.Vinatieri kicks 67 yards from NE 30 to OAK 3...       0.0       0.0   \n",
      "1  (14:55) L.Jordan left end to OAK 35 for 7 yard...       0.0       0.0   \n",
      "2  (14:19) K.Collins pass to J.Porter to OAK 39 f...       0.0       0.0   \n",
      "3  (13:34) K.Collins pass to R.Moss to NE 32 for ...       0.0       0.0   \n",
      "4  (13:14) K.Collins pass to L.Jordan pushed ob a...       0.0       0.0   \n",
      "\n",
      "   season  \n",
      "0  2005.0  \n",
      "1  2005.0  \n",
      "2  2005.0  \n",
      "3  2005.0  \n",
      "4  2005.0  \n",
      "                gameid  qtr  min sec  off def  down  togo  ydline  \\\n",
      "44848  20130203_BAL@SF  4.0  1.0  42  BAL  SF   2.0   8.0    93.0   \n",
      "44849  20130203_BAL@SF  4.0  0.0  56  BAL  SF   3.0   8.0    93.0   \n",
      "44850  20130203_BAL@SF  4.0  0.0  12  BAL  SF   4.0   7.0    92.0   \n",
      "44851  20130203_BAL@SF  4.0  0.0  12  BAL  SF   NaN   NaN    92.0   \n",
      "44852  20130203_BAL@SF  4.0  0.0  12  BAL  SF   NaN   NaN    92.0   \n",
      "\n",
      "                                             description  offscore  defscore  \\\n",
      "44848  (1:42) B.Pierce left end to BLT 7 for no gain ...      34.0      29.0   \n",
      "44849  (:56) V.Leach right guard to BLT 8 for 1 yard ...      34.0      29.0   \n",
      "44850  (:12) (Punt formation) S.Koch right end ran ob...      34.0      29.0   \n",
      "44851  S.Koch kicks 61 yards from BLT 20 to SF 19. T....      34.0      31.0   \n",
      "44852                             ����������������������      34.0      31.0   \n",
      "\n",
      "       season  \n",
      "44848  2012.0  \n",
      "44849  2012.0  \n",
      "44850  2012.0  \n",
      "44851  2012.0  \n",
      "44852  2012.0  \n"
     ]
    }
   ],
   "source": [
    "print train_set.columns\n",
    "print train_set.head()\n",
    "print train_set.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are various columns regarding different aspects of the game. In this case, though, we only need to concern ourselves with two point conversions. So, we need to filter our dataframe by the **description** column. In addition, we drop unnecessary columns like togo, since yards to go doesn't really make sense when going for a two point conversion. In fact, we for our model, we only need the description, year, offense, offense team's score, and whether or not the play was a run or a pass.\n",
    "\n",
    "Finally, for any classification algorithm, an important variable is to have is a boolean variable indicating success or failure. For each data frame, we will include such a variable which is marked zero if the try was not successful, and one if the try was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  off  offScore  season  \\\n",
      "0  TWO-POINT CONVERSION ATTEMPT. K.Collins pass t...  OAK      14.0  2005.0   \n",
      "1  TWO-POINT CONVERSION ATTEMPT. S.Jackson rushes...  STL      12.0  2005.0   \n",
      "2  TWO-POINT CONVERSION ATTEMPT. K.Warner pass to...  ARI      13.0  2005.0   \n",
      "3  (Pass formation) TWO-POINT CONVERSION ATTEMPT....  MIN       0.0  2005.0   \n",
      "4  TWO-POINT CONVERSION ATTEMPT. M.Schaub pass to...  ATL      10.0  2005.0   \n",
      "\n",
      "   success  runOrPass  \n",
      "0        0          1  \n",
      "1        0          0  \n",
      "2        0          1  \n",
      "3        1          1  \n",
      "4        1          1  \n",
      "                                           description  off  offScore  season  \\\n",
      "418  TWO-POINT CONVERSION ATTEMPT. C.Henne pass to ...  JAC      14.0  2012.0   \n",
      "419  (Run formation) TWO-POINT CONVERSION ATTEMPT. ...  DAL      10.0  2012.0   \n",
      "420  (Pass formation) TWO-POINT CONVERSION ATTEMPT....  SEA      13.0  2012.0   \n",
      "421  TWO-POINT CONVERSION ATTEMPT. M.Schaub pass to...  HOU      20.0  2012.0   \n",
      "422  (Pass formation) TWO-POINT CONVERSION ATTEMPT....   SF      23.0  2012.0   \n",
      "\n",
      "     success  runOrPass  \n",
      "418        0          1  \n",
      "419        1          1  \n",
      "420        1          1  \n",
      "421        1          1  \n",
      "422        0          1  \n",
      "423\n"
     ]
    }
   ],
   "source": [
    "def getTwoPointConv(df):\n",
    "    '''This function gets all entries in the dataframe that pertain to two-point conversions'''\n",
    "    tempDf = pd.DataFrame()\n",
    "    succ = [] #will store a indicator variable for either success or not success\n",
    "    typ = [] #stores an indicator variable for either run or pass\n",
    "    for tup in df.iterrows():\n",
    "        row = tup[1]\n",
    "        if(\"CONVERSION\" in row['description']):\n",
    "            tempDf = tempDf.append(row)\n",
    "    newDf = pd.DataFrame()\n",
    "    newDf = newDf.assign(description = tempDf['description'], off = tempDf['off'], \n",
    "                         offScore = tempDf['offscore'], defScore = tempDf['defscore'], season = tempDf['season'])\n",
    "    #get the success or failure of each two PT conversion\n",
    "    for tup in newDf.iterrows():\n",
    "        row = tup[1]\n",
    "        if(\"ATTEMPT SUCCEEDS\" in row['description']):\n",
    "            succ.append(1)\n",
    "        else:\n",
    "            succ.append(0)\n",
    "            \n",
    "    #figure out whether or not the attemp was a run or a pass\n",
    "    for tup in newDf.iterrows():\n",
    "        row = tup[1]\n",
    "        if(\"pass\" in row['description']):\n",
    "            typ.append(1)\n",
    "        else:\n",
    "            typ.append(0)\n",
    "    res = pd.Series(succ)\n",
    "    resTyp = pd.Series(typ)\n",
    "    newDf = newDf.reset_index(drop = True)\n",
    "    newDf = newDf.assign(success = res)\n",
    "    newDf = newDf.assign(runOrPass = resTyp).dropna().reset_index(drop = True)\n",
    "    return newDf\n",
    "\n",
    "two_point = getTwoPointConv(train_set)\n",
    "two_point_test = getTwoPointConv(test_set)\n",
    "print two_point.head()\n",
    "print two_point.tail()\n",
    "print len(two_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In addition, based on analysis done by other sources [[link]](http://harvardsportsanalysis.org/2016/09/predicting-two-point-conversion-success-did-the-raiders-have-a-special-edge/), DVOA (Defense-adjusted Value Over Average) is a good indicator of a team's offensive strength. We will be using offensive DVOA by year for each team, and load these variables into our dataframe as well. To read more about DVOA, check out this link [here](http://www.footballoutsiders.com/info/methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  off  offScore  season  \\\n",
      "0  TWO-POINT CONVERSION ATTEMPT. K.Collins pass t...  OAK      14.0  2005.0   \n",
      "1  TWO-POINT CONVERSION ATTEMPT. S.Jackson rushes...  STL      12.0  2005.0   \n",
      "2  TWO-POINT CONVERSION ATTEMPT. K.Warner pass to...  ARI      13.0  2005.0   \n",
      "3  (Pass formation) TWO-POINT CONVERSION ATTEMPT....  MIN       0.0  2005.0   \n",
      "4  TWO-POINT CONVERSION ATTEMPT. M.Schaub pass to...  ATL      10.0  2005.0   \n",
      "\n",
      "   success  runOrPass  offdvoa  \n",
      "0        0          1   -0.006  \n",
      "1        0          0   -0.079  \n",
      "2        0          1   -0.095  \n",
      "3        1          1   -0.139  \n",
      "4        1          1    0.033  \n",
      "422\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "def add_DVOA(df):\n",
    "    dvoa_list = []\n",
    "    for tup in df.iterrows():\n",
    "        off_team = tup[1]['off']\n",
    "        year = str(int(tup[1]['season']))\n",
    "        dvoa_list.append(dvoa_all[year][off_team])\n",
    "    res = pd.Series(dvoa_list)\n",
    "    newDf = df.assign(offdvoa = res).dropna().reset_index(drop = True)\n",
    "    return newDf\n",
    "\n",
    "two_point = add_DVOA(two_point)\n",
    "two_point_test = add_DVOA(two_point_test)\n",
    "print two_point.head()\n",
    "print len(two_point)\n",
    "print len(two_point_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we must remove any columns with strings in them, as these will confuse the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "two_point_clean = two_point.drop([\"success\", \"off\", \"description\"], axis = 1)\n",
    "two_point_test_clean = two_point_test.drop([\"success\", \"off\", \"description\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to apply our random forest method to our cleaned up data set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFL Prediction: Random Forest Prediction\n",
    "\n",
    "### Random Forest: Initial Setup\n",
    "\n",
    "At the heart of any machine learning prediction algorithm are three main functions - these are fit, predict and score. **Fit** is used on the training data, **predict** is used on the test data to predict the outcome of an indicator variable, and **score** is used to determine the accuracy of the results. The random forest method is no different - we will be calling these three methods on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.684210526316\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state = 1234567)\n",
    "clf.fit(two_point_clean, two_point[\"success\"])\n",
    "clf.score(two_point_test_clean, two_point_test[\"success\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!! This score indicates that our model predicted 67% of the 2-point conversions in the partial data for the 2013 NFL season. However, we can do better, and to do this, we need to change the parameters we input into our classifer. This process is known as **tuning** the random forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Tuning the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we initialize our RandomForestClassifier, there are many optional parameters that it can take it. Take a look [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)! We will specially look at a couple, and how they can further enhance our model.\n",
    "\n",
    "- *n_estimators*: This value determines the number of trees in the forest. More trees lead to higher accuracy, but the tradeoff is that the model will take longer to run. Finding the perfect value for this parameter is critical.\n",
    "- *max_features*: This will determine the maximum number of features to consider - options include 'sqrt' or 'log2'\n",
    "\n",
    "This tutorial will focus on n_estimators, although there are others, which you can check out on that website.\n",
    "\n",
    "I found that for my computer, it is best to set n_estimators to be a relatively large number, like 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7192982456140351"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators = 100, random_state = 1234567)\n",
    "clf2.fit(two_point_clean, two_point[\"success\"])\n",
    "clf2.score(two_point_test_clean, two_point_test[\"success\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! By setting n_estimators to be 500, the accuracy of our predictions increased by 0.03. If we increase the amount of features to consider (instead of the default sqrt(n), lets try a number > sqrt(n)), and see what happens to the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66666666666666663"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators = 100, random_state = 1234567, max_features = 4)\n",
    "clf2.fit(two_point_clean, two_point[\"success\"])\n",
    "clf2.score(two_point_test_clean, two_point_test[\"success\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moral of the Story: Sometimes, its best to leave parameters blank! I observed that the accuracy of the model peaked at the default value of sqrt(n), so in this case, I would leave that parameter untouched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some more cool things you can do with almost any classification mechanism in sklearn is extract the feature probabilities and get the actual predictions with the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0\n",
      " 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0]\n",
      "[[ 0.76        0.24      ]\n",
      " [ 0.16        0.84      ]\n",
      " [ 0.57        0.43      ]\n",
      " [ 0.61        0.39      ]\n",
      " [ 0.82        0.18      ]\n",
      " [ 0.61        0.39      ]\n",
      " [ 0.71        0.29      ]\n",
      " [ 0.47833333  0.52166667]\n",
      " [ 0.66        0.34      ]\n",
      " [ 0.54        0.46      ]\n",
      " [ 0.46        0.54      ]\n",
      " [ 0.4         0.6       ]\n",
      " [ 0.18        0.82      ]\n",
      " [ 0.46        0.54      ]\n",
      " [ 0.138       0.862     ]\n",
      " [ 0.31666667  0.68333333]\n",
      " [ 0.58        0.42      ]\n",
      " [ 0.81        0.19      ]\n",
      " [ 0.35        0.65      ]\n",
      " [ 0.52        0.48      ]\n",
      " [ 0.81        0.19      ]\n",
      " [ 0.46        0.54      ]\n",
      " [ 0.55833333  0.44166667]\n",
      " [ 0.55        0.45      ]\n",
      " [ 0.09        0.91      ]\n",
      " [ 0.32        0.68      ]\n",
      " [ 0.54833333  0.45166667]\n",
      " [ 0.54166667  0.45833333]\n",
      " [ 0.14083333  0.85916667]\n",
      " [ 0.045       0.955     ]\n",
      " [ 0.4         0.6       ]\n",
      " [ 0.32333333  0.67666667]\n",
      " [ 0.51        0.49      ]\n",
      " [ 0.26        0.74      ]\n",
      " [ 0.36416667  0.63583333]\n",
      " [ 0.75        0.25      ]\n",
      " [ 0.672       0.328     ]\n",
      " [ 0.34        0.66      ]\n",
      " [ 0.52        0.48      ]\n",
      " [ 0.29        0.71      ]\n",
      " [ 0.10583333  0.89416667]\n",
      " [ 0.74        0.26      ]\n",
      " [ 0.61        0.39      ]\n",
      " [ 0.4965      0.5035    ]\n",
      " [ 0.4755      0.5245    ]\n",
      " [ 0.702       0.298     ]\n",
      " [ 0.72        0.28      ]\n",
      " [ 0.04        0.96      ]\n",
      " [ 0.11        0.89      ]\n",
      " [ 0.1825      0.8175    ]\n",
      " [ 0.86        0.14      ]\n",
      " [ 0.84416667  0.15583333]\n",
      " [ 0.9         0.1       ]\n",
      " [ 0.065       0.935     ]\n",
      " [ 0.11        0.89      ]\n",
      " [ 0.28        0.72      ]\n",
      " [ 0.88        0.12      ]]\n"
     ]
    }
   ],
   "source": [
    "print clf2.predict(two_point_test_clean)\n",
    "print clf2.predict_proba(two_point_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the probabilities indicate that there for the first attempt, there is a 24% chance of success. Therefore, in the predictions array, 0 is given as the prediction, since the chance of a failure is 76%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision to Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49122807017543857"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf5 = LinearSVC()\n",
    "clf5.fit(two_point_clean, two_point[\"success\"])\n",
    "clf5.score(two_point_test_clean, two_point_test[\"success\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Neighbors Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49122807017543857"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf6 = KNeighborsClassifier()\n",
    "clf6.fit(two_point_clean, two_point[\"success\"])\n",
    "clf6.score(two_point_test_clean, two_point_test[\"success\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our newly learned algorithm significantly outperforms other classification methods by about .20 in terms of score. With improvement, an NFL coach could use the Random Forest model that we just wrote, as it would help them correctly classify whether or not going for two points is a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Only 72%\n",
    "\n",
    "This is a good question - even though our new model outperformed the SVM we learned in class - 72% still means our classification model has a 1 in 4 chance of being incorrect. While that is pretty accurate, there are still ways we can improve the performance of our model. \n",
    "\n",
    "One major way we can improve our model is to use more training data - sadly, only about 400 2 point attempts have been made in the NFL from 2005 to 2012. More data could be used to train our model, and thus, make it more accurate. Another way that we could make more model stronger is by adding more relevant features to our data set, such as the DVOA of the defense, or data pertaining to weather conditions, which play a big role in the NFL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Worked Example: Tweets - Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough of sports; lets dive into another hotly contested topic - politics! We will be using the tweet dataset from the third homework to run a random forest classification to determine the author of a given tweet as Republican (class label 0) or Democrat (class label 1). Load the tweets as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      screen_name                                               text\n",
      "0             GOP  RT @GOPconvention: #Oregon votes today. That m...\n",
      "1    TheDemocrats  RT @DWStweets: The choice for 2016 is clear: W...\n",
      "2  HillaryClinton  Trump's calling for trillion dollar tax cuts f...\n",
      "3  HillaryClinton  .@TimKaine's guiding principle: the belief tha...\n",
      "4        timkaine  Glad the Senate could pass a #THUD / MilCon / ...\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)\n",
    "unlabeled_tweets = pd.read_csv(\"tweets_test.csv\", na_filter=False)\n",
    "print tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pre-process the data set, we will simply call the functions from our homework (essentially, we will make a TF-IDF vector that we can then predict on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_tweets = tc.process_all(tweets)\n",
    "rare_words = tc.get_rare_words(processed_tweets)\n",
    "(tfidf, X) = tc.create_features(processed_tweets, rare_words)\n",
    "y = tc.create_labels(processed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets - Random Forest Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993236212279\n",
      "[1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0\n",
      " 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1\n",
      " 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1\n",
      " 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0\n",
      " 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1\n",
      " 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1\n",
      " 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0\n",
      " 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1\n",
      " 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1\n",
      " 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1\n",
      " 0 0 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
      " 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1\n",
      " 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1\n",
      " 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1\n",
      " 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0\n",
      " 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0\n",
      " 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1\n",
      " 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0\n",
      " 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
      " 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1\n",
      " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0\n",
      " 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "#for some reason, this method was not importing correctly, so I have included it here\n",
    "def classify_tweets(tfidf, classifier, unlabeled_tweets):\n",
    "    cpy = tc.process_all(unlabeled_tweets)\n",
    "    allWords = []\n",
    "    for thing in cpy.iterrows():\n",
    "        row = thing[1]\n",
    "        words = row['text']\n",
    "        allWords.append(\" \".join(words))\n",
    "    return classifier.predict(tfidf.transform(allWords))\n",
    "\n",
    "clf7 = RandomForestClassifier(random_state = 1234567)\n",
    "classifier = clf7.fit(X, y)\n",
    "print clf7.score(X, y)\n",
    "y_pred = classify_tweets(tfidf, classifier, unlabeled_tweets)\n",
    "print y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, our random forest model outperformed the SVC we used in homework 3 - we have 99.3% accuracy with a random forest, as opposed to the 95% obtained by our SVC with the same function calls from the homework!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, we went over the idea behind a random forest, and then we went through two worked examples of real world, appplicable problems that can be solved with a random forest. I hope this tutorial was useful, and if you need more information be sure to check out the links below!\n",
    "\n",
    "Random Forest documentation: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Original Two Point Conversion Model: http://harvardsportsanalysis.org/2016/09/predicting-two-point-conversion-success-did-the-raiders-have-a-special-edge/\n",
    "\n",
    "A very informative video about the concept of random forests: https://www.youtube.com/watch?v=loNcrMjYh64\n",
    "\n",
    "A very famous paper going into great detail about random forests (only read if very interested - its highly technical!): https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
