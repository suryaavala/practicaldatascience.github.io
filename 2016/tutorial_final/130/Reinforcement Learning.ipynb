{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "Welcome! In this tutorial, you will learn about the reinforcement learning algorithm and how to leverage it using the PyBrain Library. \n",
    "\n",
    "Reinforcement learning is characterized by the following traits:\n",
    "\n",
    "    We don't know how to classify some set of data.\n",
    "    We do know how to punish or reward a classification of the data\n",
    "\n",
    "You may wonder, when do we encounter a problem where we have these conditions? The short answer is everywhere, but if you look towards the fields of artificial intelligence and robotics, you'll find many such examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take for instance, catching a ball. What exactly makes an attempt at a catch good? We can't pinpoint the exact movements that the hand needs to do to successfully catch a ball, but we can definitively say whether or not the ball was caught. It's problems like these that are best tackled using reinforcment learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](robotball.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyBrain, reinforcement learning problems always have 3 main components, an Environment, an Agent, and a Task. On a high level, Environments define the exact space that the problem exists in, the Agent is the thing that is learning how to solve the problem, and the Task facilatates the interaction between the 2. These 3 components are bundled together and minupulated with an Experiment\n",
    "\n",
    "In this tutorial we will show how to use PyBrain to solve the maze navigation problem and then leverage PyBrain's reinforcement learning to learn the how to beat a simple fighting game AI.\n",
    "\n",
    "With that in mind, let's begin the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Installation:\n",
    "\n",
    "First, download the repository to the folder that this Jupyter notebook is in. This can be done by running the following command in your terminal\n",
    "\n",
    "\tgit clone git://github.com/pybrain/pybrain.git\n",
    "\n",
    "Now, go into the pybrain folder you just acquired and then run this command to install pybrain.\n",
    "\n",
    "\tpython setup.py install\n",
    "    \n",
    "After everything is installed, verify that the installation was successful by ensuring that the following imports work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import *\n",
    "import sys, time\n",
    "\n",
    "from pybrain.rl.environments.mazes import Maze, MDPMazeTask\n",
    "from pybrain.rl.learners.valuebased import ActionValueTable\n",
    "from pybrain.rl.agents import LearningAgent\n",
    "from pybrain.rl.learners import Q, SARSA\n",
    "from pybrain.rl.experiments import Experiment\n",
    "from pybrain.rl.environments import Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "For the maze navigation problem, we need to create the environment by making an actual maze. To do so, we need to create and then pass a 2D array into the the Maze environment that comes with the PyBrain Library.\n",
    "\n",
    "The 1's represent walls of the maze, whereas the 0's represent open spaces. The 2nd argument to the environment specifies the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "structure = array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                   [1, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "                   [1, 0, 0, 1, 0, 0, 1, 0, 1],\n",
    "                   [1, 0, 0, 1, 0, 0, 1, 0, 1],\n",
    "                   [1, 0, 0, 1, 0, 1, 1, 0, 1],\n",
    "                   [1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "                   [1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
    "                   [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "                   [1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "\n",
    "environment = Maze(structure, (7, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the environment is properly constructed by visualizing it with mathplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['power', 'arccos', 'log2', 'arctanh', 'pylab', 'fft', 'arcsin', 'sqrt', 'show_config', '__version__', 'test', 'log', 'log10']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEACAYAAACatzzfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8f4b978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#replace this visualization code in the future\n",
    "%pylab inline\n",
    "import pylab\n",
    "import numpy as np\n",
    "\n",
    "def drawMaze(mazeArray):\n",
    "    pylab.gray()\n",
    "    pylab.ion()\n",
    "\n",
    "    pylab.pcolor(mazeArray)\n",
    "    pylab.draw()\n",
    "    \n",
    "drawMaze(map(lambda x: 1-x, environment.mazeTable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "Now that we've set up the environment, we need an agent that will interact with it. The agent consists of a learner, a controller and an explorer. The learner tells the agent how to learn from the rewards it gets, the controller is essentially the \"memory\" of the agent, and the explorer informs the agent of how to explore the state space. The default agent that comes with PyBrain already has an explorer, so we'll ignore that component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner\n",
    "The learner that we'll be using in this tutorial is the Q-learner. In Q-learning, the agent keeps track of a set of states and the reward associated with each action it can take at that state. The reward of each action is updated as the agent goes through many iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner = Q()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controller:\n",
    "Because our agent uses the Q-learner, the controller we use will need to implement the ActionValueInterface. This will keep track of the reward associated with taking an action <b>A</b> in state <b>S</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "controller = ActionValueTable(81, 4) #argument 1 is the number of states that the agent can be in\n",
    "                                     #argument 2 is the number of actions that can be taken\n",
    "                                     #for a 9x9 maze, there are 81 positions the agent can be in, and the agent can \n",
    "                                     #choose to move north, south, east or west\n",
    "controller.initialize(1.)            #Initializing the table with 1 everywhere ensures that all states are \"visited\" \n",
    "                                     #at the start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine these 2 components to create the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent = LearningAgent(controller, learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "The last thing we need to do is provide a way for the agent to interact with the environment. This will be done through the task, a construct which specifies when a single learning session is over, the goal in the environment, and the reward that the agent gets for its actions.\n",
    "\n",
    "For the maze navigation problem, PyBrain already comes with a task, the MDPMazeTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task = MDPMazeTask(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task knows where the agent is in the maze and will be able to inform the agent of what actions they can take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can create the experiment, the whole construct which has the agent preform the task, and have it interact with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment = Experiment(task, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the results of the interaction by visualizing it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEACAYAAACatzzfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLGpJatz/A1y5wC1Z74XYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8e67e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment.doInteractions(500)   #the agent takes 500 \"steps\" in the environment\n",
    "agent.learn()                    #Saves the results of the last set of interactions\n",
    "agent.reset()                    #resets the position of the agent\n",
    "drawMaze(controller.params.reshape(81,4).max(1).reshape(9,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the maze become more defined if you choose to run more interactions. Because controller stores the \"memory\" of the agent, you'll have to reinitialize the controller and the agent to retrain it from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEACAYAAACatzzfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nktQ4i1qSGmdRS1LjLGpJatz/A534DPrSOpAiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8bcd048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "controller = ActionValueTable(81, 4) #argument 1 is the number of states that the agent can be in\n",
    "                                     #argument 2 is the number of actions that can be taken\n",
    "                                     #for a 9x9 maze, there are 81 positions the agent can be in, and the agent can \n",
    "                                     #choose to move north, south, east or wet\n",
    "controller.initialize(1.)  \n",
    "agent = LearningAgent(controller, learner)\n",
    "\n",
    "experiment = Experiment(task, agent)\n",
    "experiment.doInteractions(10000)\n",
    "agent.learn()\n",
    "agent.reset()\n",
    "\n",
    "pylab.pcolor(controller.params.reshape(81,4).max(1).reshape(9,9))\n",
    "pylab.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, more interactions gives the agent a better understanding of the maze environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your own RL modules\n",
    "\n",
    "We will now leverage PyBrain to solve our own RL modules. In this problem, we are simulating the experience of playing a fighting game and want to train an AI to defeat a simple, unchanging opponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Game\n",
    "\n",
    "In this simplified fighting game, the goal is to reduce the opponents health to 0 before the time runs out. The opponent for this problem is a simple static AI that will block differently depending on the attacks it has seen previously. The agent is the attacker and can choose to use a high, low, or special attack, or it can choose to delay its next attack. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the environment\n",
    "\n",
    "In order to proceed, we first have to set up our own environment for this scenario. To do so, we will first need to define important parameters that our problem requires us to keep track of, followed by satisfying PyBrain's Environment Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Parameters\n",
    "\n",
    "When defining the environment, we need to describe the actions that can be done as well as the things in the world that are affected by those actions. As described previously our actions are to attack high or low, use a resource called \"meter\" to perform a strong special attack, or to delay our next attack. As for thing affected by these actions, in the case of the fighting game, we care about the health of the enemy, the meter that you have built up, and the time remaining. Since the agent is also able to delay their attack, we also need to keep track of the time since the last attack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.utilities import Named\n",
    "from pybrain.rl.environments.environment import Environment\n",
    "\n",
    "from collections import Counter\n",
    "from random import random, choice\n",
    "from enum import Enum\n",
    "\n",
    "#Defines the kind of attack that is done\n",
    "class Attack(Enum):\n",
    "    high = 0\n",
    "    low = 1\n",
    "    unblockable = 2\n",
    "    delay = 3\n",
    "\n",
    "\"\"\" A fighting game simulation, actions are the attack you choose to do (high attack, low attack\n",
    "special attack) and observations being the damage done by your sequence of attacks. \n",
    "\n",
    "The opponent can only block high or low.\n",
    "High attacks are slower than low attacks, but build more meter and do more damage\n",
    "Low attacks are faster than high attacks.\n",
    "Special attacks do a lot of damage and can't be blocked, but you need to generate 5 meter to use them.\n",
    "You get meter by hitting the opponent\n",
    "\n",
    "There is a single agent who does the attacking\n",
    "An attack is either completely blocked or damages the defending AI\n",
    "\n",
    "Every state can have an an associated reward (the health of the enemy/the meter that you currently have/time remaining).\n",
    "\"\"\"\n",
    "class FightGame(Environment, Named):\n",
    "    # attacks. Notation is (<high/low/unblockable/misc>, damage, meter gain, startup_delay)\n",
    "    high = (Attack.high, 2, 2, 2)\n",
    "    low = (Attack.low, 1, 1, 1)\n",
    "    special = (Attack.unblockable, 10, -5, 4)\n",
    "    delay = (Attack.delay, 0, 0, 1)\n",
    "\n",
    "    allActions = [high, low, special,delay]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the opponent blocks based on the previous history of attacks, we also need to keep track of the its patterns for blocking attacks and a history of the attacks it has seen.\n",
    "\n",
    "We will story the opponent's blocking patterns as a dictionary, where a tuple of previous history is the key and the values are an array of the directions that the opponent will block given that history. The reason that its stored as an array is because the opponent varies its guard as time passes between attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # a table which determines if the opponent blocks high or low\n",
    "    # the keys are a tuple of the last n moves done by the opponent. The values are a table which determines if the \n",
    "    # opponent is blocking high or low since the last attack.\n",
    "    blockTable = None\n",
    "    \n",
    "    # a tuple of the previous n moves that were done\n",
    "    history = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we need a finite number of states to use the Q-learning algorithm, we also need to cap the health of the opponent, the meter that the agent can have, the amount of time, and the history that the opponent remembers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #Defines the maximum amounts of the various variables\n",
    "    maxHealth = 0   #the maximum health\n",
    "    maxMeter = 10   #the maximum meter\n",
    "    maxTime = 0     #the starting time\n",
    "    n = 0           #the amount of history we remember\n",
    "    maxDelay = 0    #the maximum amount of time where delay is meaningful. For simplicity this will be n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finish this section by initializing everything in the constructor and packaging key variables in a gameState tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # starting health/meter you have/time remaining/time since the last action\n",
    "    startState = None\n",
    "\n",
    "    #current state\n",
    "    gameState = None\n",
    "\n",
    "    def __init__(self, blockTable, n, startingHealth, startingMeter, startingTime, **args):\n",
    "        self.setArgs(**args)\n",
    "        self.blockTable = blockTable\n",
    "\n",
    "        self.maxHealth = startingHealth\n",
    "        self.maxMeter = 10\n",
    "        self.maxTime = startingTime\n",
    "        self.n = n\n",
    "        self.maxDelay = self.n\n",
    "\n",
    "        self.history = (Attack.delay,) * n\n",
    "        if self.startState == None:\n",
    "            self.startState = (startingHealth, startingMeter, startingTime, 0)\n",
    "        self.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Interface\n",
    "\n",
    "We now need to fill in the interface methods <b>getSensors</b>, <b>reset</b>, and <b>performAction</b>.  We'll be going through each of these on their own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getSensors()\n",
    "\n",
    "This function is used to inform the agent about the current state of the world. Seeing that we have the world state encapsulated in the gameState tuple, we just return that. Note that the interface requires the state to be packaged in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def getSensors(self):\n",
    "        \"\"\" the currently visible state of the world (the observation may be\n",
    "            stochastic - repeated calls returning different values)\n",
    "\n",
    "            :rtype: by default, this is assumed to be a numpy array of doubles\n",
    "            :note: This function is abstract and has to be implemented.\n",
    "        \"\"\"\n",
    "        obs = zeros(1)\n",
    "        obs[0] = self.gameState\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reset()\n",
    "\n",
    "This function is used to reinitialize the environment. We do this by setting our gameState to out startState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def reset(self):\n",
    "        self.gameState = self.startState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performAction(action)\n",
    "\n",
    "This function handles the most complicated part, changing the environment in response to an action.\n",
    "\n",
    "The first thing we'll do is define a function to return a random way to block. This is neccessary because if we see a pattern that is not coded into our history, we still need the opponent to block in a random direction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def randomBlock(self):\n",
    "        block = choice([Attack.high, Attack.low])\n",
    "        return block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for everything else. Essentially, we need to update the history with the last move that was used, and depending on the move used and how the opponent blocked, apply the appropriate damage, meter and time calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def performAction(self, action):\n",
    "        currentHealth = self.gameState[0]\n",
    "        currentMeter = self.gameState[1]\n",
    "        currentTime = self.gameState[2]\n",
    "        currentDelay = self.gameState[3]\n",
    "\n",
    "        attack = self.allActions[action]\n",
    "        attackType = attack[0]\n",
    "        attackDamage = attack[1]\n",
    "        attackMeter = attack[2]\n",
    "        attackTime = attack[3]\n",
    "        \n",
    "        newHealth = max(currentHealth - attackDamage, 0)\n",
    "        newMeter = min(currentMeter + attackMeter, self.maxMeter)\n",
    "        newTime = max(currentTime - attackTime,0)\n",
    "\n",
    "        #determining how the opponent blocks. This is done by finding the current pattern of blocking\n",
    "        #using the current history and then indexing into that by using the delay\n",
    "        if(self.history not in self.blockTable):\n",
    "            blockType = self.randomBlock()\n",
    "        else:\n",
    "            timeIndex = (currentDelay + attackTime - 1) % len(self.blockTable[self.history])\n",
    "            blockType = self.blockTable[self.history][timeIndex]\n",
    "        \n",
    "        #updating the history with the latest attack used\n",
    "        self.history = self.history[1:] + (attackType, )\n",
    "            \n",
    "        #Here different logic happens depending on the attack used\n",
    "        if attackType == Attack.delay:    #Here we simply increase the delay time\n",
    "            self.gameState = (newHealth, newMeter, newTime, currentDelay + attackTime)\n",
    "        if attackType == Attack.unblockable:\n",
    "            if(newMeter >= 0):            #If we have enough meter to use the special attack, we do so\n",
    "                self.gameState = (newHealth, newMeter, newTime, 0)\n",
    "            else:                         #Otherwise we are unable to and it's essentially a longer delay\n",
    "                self.gameState = (currentHealth, currentMeter, newTime, currentDelay + attackTime)\n",
    "        #here we check our block table to see what happens\n",
    "        #If an attack hits the opponent loses health and the agent gains meter\n",
    "        #otherwise nothing happens\n",
    "        if attackType == Attack.high:\n",
    "            if(blockType == Attack.high):\n",
    "                self.gameState = (currentHealth, currentMeter, newTime, 0)\n",
    "            else:\n",
    "                self.gameState = (newHealth, newMeter, newTime, 0)\n",
    "        if attackType == Attack.low:\n",
    "            if(blockType == Attack.low):\n",
    "                self.gameState = (currentHealth, currentMeter, newTime, 0)\n",
    "            else:\n",
    "                self.gameState = (newHealth, newMeter, newTime, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the FightGame\n",
    "\n",
    "Initializing the environment can be incredibly simple or very complex depending on how you want the opponent to behave. For the sake of simplicity, have the opponent that remember the previous 2 moves and only block low after seeing 2 low attacks, or after getting hit by an unblockable attack. Otherwise, it always blocks high.\n",
    "\n",
    "Our parameters for health and time will be relatively small, as the agent will learn more quickly with a smaller state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from FightGame import *\n",
    "\n",
    "structure = dict()\n",
    "structure[(Attack.delay, Attack.delay)] = [Attack.high]\n",
    "structure[(Attack.delay, Attack.low)] = [Attack.high,Attack.high]\n",
    "structure[(Attack.delay, Attack.high)] = [Attack.high,Attack.high]\n",
    "structure[(Attack.delay, Attack.unblockable)] = [Attack.low]\n",
    "\n",
    "structure[(Attack.high, Attack.delay)] = [Attack.high,Attack.high]\n",
    "structure[(Attack.high, Attack.high)] = [Attack.high,Attack.high]\n",
    "structure[(Attack.high, Attack.low)] = [Attack.high,Attack.high]\n",
    "structure[(Attack.high, Attack.unblockable)] = [Attack.low]\n",
    "\n",
    "structure[(Attack.low, Attack.low)] = [Attack.low]\n",
    "structure[(Attack.low, Attack.unblockable)] = [Attack.low]\n",
    "structure[(Attack.low, Attack.high)] = [Attack.high,Attack.high]\n",
    "structure[(Attack.low, Attack.delay)] = [Attack.high,Attack.high]\n",
    "\n",
    "structure[(Attack.unblockable, Attack.delay)] = [Attack.high,Attack.high]\n",
    "structure[(Attack.unblockable, Attack.high)] = [Attack.high,Attack.high]\n",
    "structure[(Attack.unblockable, Attack.low)] = [Attack.high,Attack.high]\n",
    "structure[(Attack.unblockable, Attack.unblockable)] = [Attack.low]\n",
    "\n",
    "health = 5\n",
    "meter = 0\n",
    "time = 25\n",
    "historyLen = 2\n",
    "\n",
    "fightEnv = FightGame(structure, historyLen, health, meter, time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating an Agent\n",
    "\n",
    "We need to create another agent for this problem. Since we're still using the Q-learner, we don't have to change much. We do however, need to modify our controller to account for the new state space and the action that we are allowed to do in this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fightLearner = Q()    \n",
    "\n",
    "fightController = ActionValueTable((fightEnv.maxHealth+1) * (fightEnv.maxMeter+1) \\\n",
    "                                   * (fightEnv.maxTime+1) * (fightEnv.maxDelay+1), 4) \n",
    "                    \n",
    "fightController.initialize(1.)      #Initializing the table with 1 everywhere ensures that all states are \"visited\" \n",
    "                                    #at the start\n",
    "fightAgent = LearningAgent(fightController, fightLearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Task\n",
    "\n",
    "The last thing we need to do is to create the task. Since the task is used to facilitate interaction between the agent and the environment, we only need to fill in each of it's interface methods, <b>getReward</b>, <b>performAction</b> and <b>getObservation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getReward()\n",
    "\n",
    "We need to determine the reward that the agent should get for doing an action. While we could simply give a positive reward if the agent defeats the opponent and a negative reward if the time runs out, we have a slightly better understanding of how to classify a sequence of moves than that. Namely, we know that an action is \"good\" if it does more damage to the opponent. Thus, we give the damage done to the opponent as a reward, and in the case where the opponent is defeated, give a massive reward in the form of the opponent's maximum health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.rl.environments import Task\n",
    "from scipy import array\n",
    "\n",
    "class FightTask(Task):\n",
    "    #metadata used for tracking wins and losses. Used primarily for showing the results of the algorithm\n",
    "    wins = 0 \n",
    "    losses = 0 \n",
    "            \n",
    "    def getReward(self):\n",
    "        \"\"\" compute and return the current reward (i.e. corresponding to the last action performed) \"\"\"\n",
    "        if(self.env.gameState[0] < self.currentHealth):\n",
    "            if self.env.gameState[0] <= 0:   #the opponent has 0 health, the agent won\n",
    "                self.env.reset()\n",
    "                reward = self.env.maxHealth\n",
    "                self.wins += 1\n",
    "            else:\n",
    "                reward = self.currentHealth - self.env.gameState[0]\n",
    "        elif self.env.gameState[2] <= 0:#    the time ran out, the agent lost\n",
    "            self.env.reset()\n",
    "            #print \"defeat\"\n",
    "            reward = -self.env.gameState[0]\n",
    "            self.losses += 1\n",
    "        else:\n",
    "            reward = 0.\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performAction(action)\n",
    "\n",
    "Here, we do the default behavior, and allow the super class to have the agent perform the action selected. By default, the action argument passed is a vector whose only element is the integer representation of the action we want to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def performAction(self, action):\n",
    "        \"\"\" The action vector is stripped and the only element is cast to integer and given\n",
    "            to the super class.\n",
    "        \"\"\"\n",
    "        Task.performAction(self, int(action[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getObservation()\n",
    "\n",
    "This function is used to map the observations we get from the game into a particular state. Since we set the number of states precisely according to the range of values that the observations can take on, we can do a direct mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def getObservation(self):\n",
    "        self.currentHealth = self.env.gameState[0]\n",
    "        self.currentMeter = self.env.gameState[1]\n",
    "        self.currentTime = self.env.gameState[2]\n",
    "        self.currentDelay = self.env.gameState[3]\n",
    "        \n",
    "        \n",
    "        #If this mapping is confusing, consider if we had a cube whose dimensions were (x,y,z).\n",
    "        #If we want to map the point (3, 5, 8), we first advance 3 units along the x axis,\n",
    "        #passing by 3 planes of area y*z. We then advance 5 units up the y axis, passing\n",
    "        #5 rows which have z elements in them. Lastly, we advance 8 units down the z axis.\n",
    "        #In total, the number of elements we passed were 3yz + 5z + 8. This methodlogy\n",
    "        #provides a 1 to 1 mapping of 3d space to 1d space. We can expand this to 4d\n",
    "        #space to correctly map our gameState in this instance.\n",
    "        index = self.currentHealth * self.env.maxMeter * self.env.maxTime * self.env.maxDelay + \\\n",
    "                self.currentMeter * self.env.maxTime * self.env.maxDelay + \\\n",
    "                self.currentTime * self.env.maxDelay + \\\n",
    "                self.currentDelay\n",
    "        obs = array([index])\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Putting it all together\n",
    "\n",
    "Now, we can combine all of the above components and run our experiment. \n",
    "\n",
    "Over 10 iterations where each iteration has 1000 interactions, we see the win rate of the agent increase dramtically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins:  41 Losses:  49\n",
      "Wins:  102 Losses:  13\n",
      "Wins:  91 Losses:  13\n",
      "Wins:  91 Losses:  14\n",
      "Wins:  95 Losses:  9\n",
      "Wins:  100 Losses:  6\n",
      "Wins:  93 Losses:  13\n",
      "Wins:  103 Losses:  6\n",
      "Wins:  97 Losses:  9\n",
      "Wins:  145 Losses:  3\n"
     ]
    }
   ],
   "source": [
    "from FightTask import *\n",
    "\n",
    "fightTask = FightTask(fightEnv)\n",
    "fightTask.verbose = True\n",
    "experiment = Experiment(fightTask, fightAgent)\n",
    "iterationCount = 10\n",
    "for i in range(iterationCount):\n",
    "    experiment.doInteractions(1000)\n",
    "    print \"Wins: \", fightTask.wins, \"Losses: \", fightTask.losses\n",
    "    fightTask.wins = 0\n",
    "    fightTask.losses = 0\n",
    "    fightAgent.learn()\n",
    "    fightAgent.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going Further\n",
    "\n",
    "In this instance, the opponent has a very simplistic behavior pattern. If we wanted to test the capabilities of the reinforcement learning algorithm further, we could give it even more complex patterns and make it remember more history. We could also make the scenario more complex, adding more tasks or more conditions for the agent to consider. All of this would dramatically increase our state space, and likely make our reinforcement learning AI perform worse, as the expanded state space would mean that it takes longer to figure out the optimal way to reach its goal.\n",
    "\n",
    "Reducing our state space would solve this! Doing things like splitting the spectrum of some of the parameters into buckets (full health, low health etc.) would greatly reduce the number of states introduced by those factors. Tweaking our reward function to include things like meter gained would also improve the performance. \n",
    "\n",
    "These optimizations vary from design space to design space, but by taking note of them, you can dramatically improve the results of your reinforcment learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Additional Resources\n",
    "\n",
    "PyBrain API: http://pybrain.org/docs/\n",
    "\n",
    "Various AI techniques for Fighting Games: http://cs229.stanford.edu/proj2008/RicciardiThill-AdaptiveAIForFightingGames.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
