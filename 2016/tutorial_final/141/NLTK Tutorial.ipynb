{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Tutorial\n",
    "nltk is one of most famous NLP text processing library in python and it provides easy-to-use interfaces to a number of different resources and algorithms. The 15688 class covers a very small protition of the powerful nltk library and in this tutorial, we will go through some of the other basic usages and functionalities of it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start\n",
    "NLTK provides various built-in text materials, for instance, books, web text, brown sents etc. We can easily import these texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download() # Download all the resource of nltk\n",
    "\n",
    "# Import all the book dataset\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the book above are stored in `nltk.Text` object. We can also build our own `Text` object from raw strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Cookie', u'Manager', u':', u'``', u'Do', u\"n't\", u'allow', u'sites', u'that', u'set', u'removed', u'cookies', u'to', u'set', u'future', u'cookies', u\"''\", u'should', u'stay', u'checked', u'When', u'in', u'full', u'screen', u'mode', u'Pressing', u'Ctrl-N', u'should', u'open', u'a', u'new', u'browser', u'when', u'only', u'download', u'dialog', u'is', u'left', u'open', u'add', u'icons', u'to', u'context', u'menu', u'So', u'called', u'``', u'tab', u'bar', u\"''\", u'should', u'be', u'made', u'a', u'proper', u'toolbar', u'or', u'given', u'the', u'ability', u'collapse', u'/', u'expand', u'.', u'[', u'XUL', u']', u'Implement', u'Cocoa-style', u'toolbar', u'customization', u'.', u'#', u'ifdefs', u'for', u'MOZ_PHOENIX', u'customize', u'dialog', u\"'s\", u'toolbar', u'has', u'small', u'icons', u'when', u'small', u'icons', u'is', u'not', u'checked', u'nightly', u'builds', u'and', u'tinderboxen', u'for', u'Phoenix', u'finish', u'tearing', u'prefs', u'UI', u'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import webtext\n",
    "firefox_txt = Text(nltk.word_tokenize(webtext.raw('firefox.txt')))\n",
    "print firefox_txt[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text search\n",
    "The `Text` object provides some useful apis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 200 matches:\n",
      " folder called New Folder [ Moz ] JavaScript error when hitting Cancel in Add \n",
      "og Choosing Expand on PT produces JavaScript error '' Opens a new window '' to\n",
      "s visible when opening a window . javascript click ( ) fails to activate links\n",
      "bled 256 colors theme Can not use Javascript to add new side bar Save Page As \n",
      " an image crashes pheonix ( 0.2 ) javascript onLoad Pageload and startup spike\n",
      " from bookmark menu ( not panel ) Javascript prefs are always blank [ cust ] D\n",
      "ot start due to script name clash Javascript should be JavaScript additional t\n",
      "t name clash Javascript should be JavaScript additional toolbars show up on po\n",
      "get cut off . XBL loading issue . JavaScript in Phoenix 0.2 different from Moz\n",
      "ses its value when selecting Text Javascript error : uncaught exception while \n",
      "oads itself Add option to prevent javascript from resizing the browser crash u\n",
      "w any information page containing javascript newsticker cause Phoenix 0.3 to a\n",
      "wap not working ( OK in n4/ie/moz Javascript based image changing does n't wor\n",
      "b Google Failed Search Causes GPF Javascript image mouseovers not working Book\n",
      "b' Toolbar editor and large fonts JavaScript `` navigator.cookieEnabled '' doe\n",
      " . Ca n't customize toolbar after javascript console keystroke compatibility w\n",
      " install ( font download ) dialog Javascript image handling Selecting one of t\n",
      ".impress.co.jp ( not in Mozilla ) Javascript : replacing images from a JS scri\n",
      "of branches in History is strange Javascript will not render rollover graphics\n",
      "st for splash screen on boot up . javascript : opens wrong window Running a fi\n",
      " the image Popup blocking ignores javascript window open commands for sites in\n",
      "mplete Yahoo Mail does n't load - javascript error # bookmarks not directly af\n",
      "ce URL while switching tabs using javascript to force users to `` right-click \n",
      "t does n't always update properly javascript prompt does n't popup a dialog bo\n",
      "ders and not in color . link with javascript does n't work until page download\n"
     ]
    }
   ],
   "source": [
    "# text concordance\n",
    "firefox_txt.concordance(\"javascript\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordance index displays the occurance of the specified word, together with the context.\n",
    "Also, we can easily analyze the context where a word is used. If two words are high frequently used within the same context, they can be regarded as synonyms.\n",
    "\n",
    "And by importing `nltk.books`, all the text of books are stored in variable `text1` -- `text9`. Let's explore around some text searching functionalities based on these books. （let's try text1, as I love the book :）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Moby Dick by Herman Melville 1851>\n",
      "Similar words to monstrous in Moby Dick:\n",
      "imperial subtly impalpable pitiable curious abundant perilous\n",
      "trustworthy untoward singular lamentable few determined maddens\n",
      "horrible tyrannical lazy mystifying christian exasperate\n",
      "\n",
      "Similar words to monstrous in Sense and Sensibility:\n",
      "so as too a not monstrous last more was how most exceedingly the only\n",
      "first her perfectly cottage will it\n",
      "\n",
      "Similar usage(context) of word monstrous and very in text2:\n",
      "a_pretty is_pretty a_lucky am_glad be_glad\n"
     ]
    }
   ],
   "source": [
    "print text1\n",
    "\n",
    "print 'Similar words to monstrous in Moby Dick:'\n",
    "text1.similar('monstrous')\n",
    "print '\\nSimilar words to monstrous in Sense and Sensibility:'\n",
    "text2.similar('very')\n",
    "print '\\nSimilar usage(context) of word monstrous and very in text2:'\n",
    "text2.common_contexts(['monstrous', 'very'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
      "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
      "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
      "mate; white whale; ivory leg; one hand\n"
     ]
    }
   ],
   "source": [
    "# Find all the collocations from the text\n",
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120f630d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = firefox_txt.dispersion_plot(['firefox', 'chrome', 'IE', 'opera', 'safari'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dispersion_plot` indicates the number of words in front of a word. The y-axis stands for words while x-axis is the whole text/book. In the figure above we see lots of occurance of whales, since this is a book about whales.\n",
    "\n",
    "We can gather useful information with the dispersion plots. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEUCAYAAAD0lTuPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110e0f150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl2 = text2.dispersion_plot(['Elinor','Marianne','Edward','Willoughby']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the name of two male leads are far more frequent than female ones, which reveals the dominance position of men in those ages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tokenization & Normalization\n",
    "NLTk provides more built-in tokenization strategies than covered in the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"this's a test.\", 'We just write some random words here?', 'with some ++ punctuations.', 'insi-de.']\n"
     ]
    }
   ],
   "source": [
    "text = \"this's a test. We just write some random words here? with some ++ punctuations. insi-de.\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentence_tokenize_list = sent_tokenize(text)\n",
    "print sentence_tokenize_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"this's a test.\", 'We just write some random words here ?', 'with some++ punctuations.', 'insi-de.']\n"
     ]
    }
   ],
   "source": [
    "text = \"this's a test. We just write some random words here ? with some++ punctuations. insi-de.\"\n",
    "print sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the sentence tokenization feature, which make use of the ending punctuations(, . ? etc.) to find the end of a sentence. In fact according to the doc, `sent_tokenize` take advantage of an instance of `PunktSentenceTokenizer` from the `nltk`, which has been well trained and is smart enough to find out the punctuation and characters marking the end of a raw sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', \"'s\", 'is', 'a', 'test', \"'\", 'with', 'punc-in-mid', 'cost', '$', '3.88', 'and', 'even', 'new', 'line']\n",
      "['This', \"'s\", 'is', 'a', 'test', \"'\", 'with', 'punc-in-mid', 'cost', '$', '3.88', 'and', 'even', 'new', 'line']\n",
      "['This', \"'\", 's', 'is', 'a', 'test', \"'\", 'with', 'punc', '-', 'in', '-', 'mid', 'cost', '$', '3', '.', '88', 'and', 'even', 'new', 'line']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print word_tokenize(\"This's is a test' with punc-in-mid cost $3.88 and even\\nnew line\")\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "print TreebankWordTokenizer().tokenize(\"This's is a test' with punc-in-mid cost $3.88 and even\\nnew line\")\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print WordPunctTokenizer().tokenize(\"This's is a test' with punc-in-mid cost $3.88 and even\\nnew line\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing a text refers to striping the prefix/affixes of the words and provide a consistence word for future porcessing. One possible processing step is stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Friends', ',', 'delegates', 'and', 'fellow', 'Americans', ':', 'I', 'humbly', 'and', 'gratefully', 'accept', 'your', 'nomination', 'for', 'the', 'presidency', 'of', 'the', 'United', 'States.Together', ',', 'we', 'will', 'lead', 'our', 'party', 'back', 'to', 'the', 'White', 'House', ',', 'and', 'we', 'will', 'lead', 'our', 'country', 'back', 'to', 'safety', ',', 'prosperity', ',', 'and', 'peace', '.', 'We', 'will', 'be', 'a', 'country', 'of', 'generosity', 'and', 'warmth', '.', 'But', 'we', 'will', 'also', 'be', 'a', 'country', 'of', 'law', 'and', 'order', '.']\n"
     ]
    }
   ],
   "source": [
    "trump_text = \"Friends, delegates and fellow Americans: I humbly and gratefully accept your nomination for the presidency of the United States.Together, we will lead our party back to the White House, and we will lead our country back to safety, prosperity, and peace. We will be a country of generosity and warmth. But we will also be a country of law and order.\"\n",
    "tokens = word_tokenize(trump_text)\n",
    "print tokens\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Friend', u',', u'deleg', u'and', u'fellow', u'American', u':', u'I', u'humbl', u'and', u'grate', u'accept', u'your', u'nomin', u'for', u'the', u'presid', u'of', u'the', u'Unit', u'States.Togeth', u',', u'we', u'will', u'lead', u'our', u'parti', u'back', u'to', u'the', u'White', u'Hous', u',', u'and', u'we', u'will', u'lead', u'our', u'countri', u'back', u'to', u'safeti', u',', u'prosper', u',', u'and', u'peac', u'.', u'We', u'will', u'be', u'a', u'countri', u'of', u'generos', u'and', u'warmth', u'.', u'But', u'we', u'will', u'also', u'be', u'a', u'countri', u'of', u'law', u'and', u'order', u'.']\n"
     ]
    }
   ],
   "source": [
    "print [porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['friend', ',', 'deleg', 'and', 'fellow', 'am', ':', 'i', 'humbl', 'and', 'grat', u'acceiv', 'yo', 'nomin', 'for', 'the', u'presid', 'of', 'the', 'unit', 'states.together', ',', 'we', 'wil', 'lead', 'our', 'party', 'back', 'to', 'the', 'whit', 'hous', ',', 'and', 'we', 'wil', 'lead', 'our', 'country', 'back', 'to', 'saf', ',', 'prosp', ',', 'and', 'peac', '.', 'we', 'wil', 'be', 'a', 'country', 'of', 'generos', 'and', 'warm', '.', 'but', 'we', 'wil', 'also', 'be', 'a', 'country', 'of', 'law', 'and', 'ord', '.']\n"
     ]
    }
   ],
   "source": [
    "print [lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another part of normalization refers to lemmatize that has already been covered in the class homework: `Lemmatization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Friends', ',', u'delegate', 'and', 'fellow', 'Americans', ':', 'I', 'humbly', 'and', 'gratefully', 'accept', 'your', 'nomination', 'for', 'the', 'presidency', 'of', 'the', 'United', 'States.Together', ',', 'we', 'will', 'lead', 'our', 'party', 'back', 'to', 'the', 'White', 'House', ',', 'and', 'we', 'will', 'lead', 'our', 'country', 'back', 'to', 'safety', ',', 'prosperity', ',', 'and', 'peace', '.', 'We', 'will', 'be', 'a', 'country', 'of', 'generosity', 'and', 'warmth', '.', 'But', 'we', 'will', 'also', 'be', 'a', 'country', 'of', 'law', 'and', 'order', '.']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "print [wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to stemming methods above, lemmatizers only removes the affixes if the resulting words in in the dictionary. In the example, we see word 'delegates' is stemmed to 'deleg' by the stemmer while in lemmatizer, it becomes 'delegate'. What's more, lemmatizer will automatically handle other cases, say: women -> woman. Generally it provides better performances. As a result, as you might have already noticed, it takes far longer time to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, for tokenization on international(non latin languages), say Chinese, nltk's support is far from satisfied. We may use some third-party tools to do the tokenization and normalization part and feed the result set to nltk afterwards for further processing. \n",
    "\n",
    "You may skip the following part if you are not intersted in segementing/tokenzing Chinese characters, but it will be very concise. Personally, I love the [jieba](https://github.com/fxsjy/jieba) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Library/Python/2.7/site-packages/jieba/dict.txt ...\n",
      "Loading model from cache /var/folders/j3/p60cssg12z31dh542s10lkx80000gn/T/jieba.cache\n",
      "Loading model cost 0.998705863953 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这/学期/我/上/了/一门/数据/科学课/，/还是/挺/有意思/的/啊/。\n",
      "苟利国家生死以/，/岂因祸福避趋之/。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "print '/'.join(jieba.cut(\"这学期我上了一门数据科学课，还是挺有意思的啊。\"))\n",
    "print '/'.join(jieba.cut(\"苟利国家生死以，岂因祸福避趋之。\", cut_all=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Analysis\n",
    "In this section, we will discuss some probability analysis using nltk. For a language set, you may want to do some probability analysis and thus making prediction/understanding context based on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19317\n",
      "906\n",
      "0.00347367331368\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import *\n",
    "\n",
    "freq = FreqDist(text1)\n",
    "print freq.B()\n",
    "print freq['whale']\n",
    "print freq.freq('whale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk.probability.FreqDist` is actually a subclass of `collections.Counter` and the difference is that the `Counter` only gives the count of each word while `FreqDist` can be used to obtain the frequency distribution. As written in the doc \n",
    "> A frequency distribution can be defined as a function mapping from each sample to the number of times that sample occurred as an outcome.\n",
    "\n",
    "This extra step of initialization will bring extra time cost on large corpus. We can see the difference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.78636097908\n",
      "4.21324896812\n"
     ]
    }
   ],
   "source": [
    "from timeit import Timer\n",
    "from collections import Counter\n",
    "\n",
    "t1 = Timer(lambda: FreqDist((text1)))\n",
    "t2 = Timer(lambda: FreqDist((text1)))\n",
    "\n",
    "print t1.timeit(number=5)\n",
    "print t2.timeit(number=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can make use of the conditional frequency distribution to do some interesting work, say find the length of all words starting with certain characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with k: length 3:0.017, length 4:0.527, length 5:0.219, length 6:0.089, length 7:0.095,\n",
      "Starting with l: length 3:0.084, length 4:0.434, length 5:0.134, length 6:0.163, length 7:0.080,\n",
      "Starting with m: length 3:0.135, length 4:0.313, length 5:0.091, length 6:0.123, length 7:0.067,\n",
      "Starting with n: length 3:0.463, length 4:0.098, length 5:0.129, length 6:0.063, length 7:0.063,\n",
      "Starting with o: length 3:0.190, length 4:0.081, length 5:0.054, length 6:0.015, length 7:0.016,\n",
      "Starting with p: length 3:0.029, length 4:0.146, length 5:0.146, length 6:0.160, length 7:0.186,\n",
      "Starting with q: length 3:0.000, length 4:0.032, length 5:0.374, length 6:0.052, length 7:0.262,\n",
      "Starting with r: length 3:0.052, length 4:0.160, length 5:0.172, length 6:0.184, length 7:0.158,\n",
      "Starting with s: length 3:0.090, length 4:0.209, length 5:0.172, length 6:0.139, length 7:0.086,\n",
      "Starting with t: length 3:0.430, length 4:0.236, length 5:0.101, length 6:0.033, length 7:0.028,\n",
      "Starting with u: length 3:0.014, length 4:0.232, length 5:0.080, length 6:0.048, length 7:0.052,\n",
      "Starting with v: length 3:0.007, length 4:0.338, length 5:0.087, length 6:0.185, length 7:0.160,\n",
      "Starting with w: length 3:0.177, length 4:0.355, length 5:0.270, length 6:0.062, length 7:0.056,\n",
      "Starting with y: length 3:0.515, length 4:0.128, length 5:0.106, length 6:0.023, length 7:0.004,\n",
      "Starting with z: length 3:0.158, length 4:0.368, length 5:0.211, length 6:0.211, length 7:0.053,\n"
     ]
    }
   ],
   "source": [
    "cf = ConditionalFreqDist()\n",
    "for word in text1:\n",
    "    cf[word[0]][len(word)] += 1\n",
    "    \n",
    "for c in sorted(cf.conditions())[-15:]:\n",
    "    print \"Starting with %s:\" % c,\n",
    "    for length in range(3,8):\n",
    "        print \"length %d:%.3f,\" % (length,cf[c].freq(length)),\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works like 2 2-D freq dist and provides the distribution considering the \"context\" of the word. With the conditional freqdist, we can do some simple predicitions, Say we predict the existence of the next word: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crashes\n",
      "directory\n",
      "favorites\n",
      "[(u'crashes', 21), (u'does', 7), (u'is', 7), (u'to', 6), (u'hangs', 4)]\n"
     ]
    }
   ],
   "source": [
    "cf = ConditionalFreqDist()\n",
    "\n",
    "last_context = None # Last word\n",
    "for token in filter(lambda t: t.isalpha(), firefox_txt):\n",
    "    cf[last_context][token] += 1 # Put the current word into the freqdict of last word\n",
    "    last_context = token\n",
    "\n",
    "print cf['firefox'].max()  # Gives crashes lol\n",
    "print cf['chrome'].max()\n",
    "print cf['IE'].max()\n",
    "\n",
    "print cf['firefox'].most_common(5) # Inherited from collections.Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do some analysis quickly by using freqDist. As shown above, it seems that firefox \"crashes\" a lot in firefox's web text XD. And we are able to construct the chain of freq words as below (machine generated texts): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firefox crashes when I try to open in the page is not work in the\n",
      "chrome directory is not work in the page is not\n"
     ]
    }
   ],
   "source": [
    "def chain(word, length=15):\n",
    "    for _ in xrange(length):\n",
    "        yield word\n",
    "        word = cf[word].max()\n",
    "\n",
    "print ' '.join(chain('firefox'))\n",
    "print ' '.join(chain('chrome', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words tagging\n",
    "We'll have a closer look at the pos-tagger in this section as I found it useful in lots of cases. First let's see two examples from the nltk docs for the tagger, without/with homonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]\n",
      "[('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# without homonyms\n",
    "text = word_tokenize(\"And now for something completely different\")\n",
    "print nltk.pos_tag(text)\n",
    "\n",
    "# with homonyms\n",
    "text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "print nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second example, refuse/permit are verbs at the first occurance while noun at the second. Pos tagger is useful in many aspects, for instance, we go back to the similar words discovery api in the first section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man king shark serpent camel job father hare bull fleece fellow\n",
      "surveyor as cobbler monks laugh kings ship times fiddler\n"
     ]
    }
   ],
   "source": [
    "text1.similar('woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the words found are also noun as the part-of-speech label is also considered together with the distribution in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'best', u'ADJ ADV NOUN VERB')\n",
      "(u'close', u'ADV ADJ VERB NOUN')\n",
      "(u'open', u'ADJ VERB NOUN ADV')\n",
      "(u'present', u'ADJ ADV NOUN VERB')\n",
      "(u'that', u'ADP DET PRON ADV')\n"
     ]
    }
   ],
   "source": [
    "# Some more detective work:\n",
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
    "data = nltk.ConditionalFreqDist((word.lower(), tag) for (word, tag) in brown_news_tagged)\n",
    "for word in sorted(data.conditions()):\n",
    "    if len(data[word]) > 3:\n",
    "        tags = [tag for (tag, _) in data[word].most_common()]\n",
    "        print(word, ' '.join(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect how \"present\" is used in different context as different pos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used as ADJ\n",
      "his election to city council in 1923 . The mayor's present term of office expires Jan. 1 . He will\n",
      "McLemore , who will retire at the close of the present school term . Dr. Clark holds an earned Doctor\n",
      "health services '' , the President called for doubling the present 10 million dollar a year federal grants for nursing\n",
      "\n",
      "\n",
      "\n",
      "Used as ADV\n",
      "1954 . In 1960 more than 6,000 Communist technicians were present in those countries . United Nations , N. Y.\n",
      ". But he was scholastically ineligible in 1959 and merely present last season . Place kicking is largely a matter\n",
      "Only the families and a dozen close friends will be present . The bride's brother , Walter D. Monroe Jr.\n",
      "\n",
      "\n",
      "\n",
      "Used as NOUN\n",
      "its annual meeting Friday noted that state taxing requirements at present are a roadblock to accepting Negroes . The statement\n",
      "\n",
      "\n",
      "\n",
      "Used as VERB\n",
      "D'Art Du Ballet '' , of Monte Carlo , will present a program of four ballets including `` Francesca Da\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tags = ('ADJ', 'ADV', 'NOUN', 'VERB')\n",
    "\n",
    "word = 'present'\n",
    "for tag in tags:\n",
    "    print \"Used as {}\".format(tag)\n",
    "    indexes = [index for index, (w, t) in enumerate(brown_news_tagged) if w==word and t==tag]\n",
    "    for i in indexes[:3]:\n",
    "        print ' '.join(w for w, t in brown_news_tagged[i-10:i+10])\n",
    "    print '\\n\\n'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And pos tagger is also useful in text classification/ opnion analysis field. ADJ/ADV/VERBs generally carry more information and thus we are able to do feature selection based on pos tags.\n",
    "\n",
    "NLTK also provides us with variuos kinds of taggers. Basically it can be divided into supervised and unsupervised ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/unidecode/__init__.py:46: RuntimeWarning: Argument <type 'str'> is not an unicode object. Passing an encoded string will likely have unexpected results.\n",
      "  _warn_if_not_unicode(string)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy using <DefaultTagger: tag=NN>: 0.124459512579\n",
      "Accuarcy using <Regexp Tagger: size=10>: 0.265968946541\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='reviews')\n",
    "\n",
    "# patterns of regex tagger\n",
    "pattern = [(r'.*ing$', 'VBG'), (r'.*ed$', 'VBD'), (r'.*es$', 'VBZ'), (r'.*ould$', 'MD'), (r'.*\\'s$', 'NN$'), (r'(The|the|A|a|An|an)$', 'AT'), (r'.*able$', 'JJ'), (r'.*s$', 'NNS'), (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), (r'.*', 'NN')]\n",
    "\n",
    "unsupervised_taggers = (nltk.DefaultTagger('NN'), nltk.RegexpTagger(pattern))\n",
    "\n",
    "for tagger in unsupervised_taggers:\n",
    "    print 'Accuarcy using {}: {}'.format(tagger, tagger.evaluate(brown_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tagger simply applies Noun tag to all words which gives unsatisfied results. We can improve it by applying the regex tagger. While we also have the supervised_taggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy using <class 'nltk.tag.sequential.UnigramTagger'>: 0.727054876588\n",
      "Accuarcy using <class 'nltk.tag.sequential.BigramTagger'>: 0.0682961897915\n",
      "Accuarcy using <class 'nltk.tag.sequential.TrigramTagger'>: 0.046489336209\n"
     ]
    }
   ],
   "source": [
    "supervised_tagger = (nltk.UnigramTagger, nltk.BigramTagger, nltk.TrigramTagger)\n",
    "\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_set, test_set = brown_tagged_sents[:size], brown_tagged_sents[size:]\n",
    "\n",
    "for Tagger in supervised_tagger:\n",
    "    tagger = Tagger(train_set)\n",
    "    print 'Accuarcy using {}: {}'.format(Tagger, tagger.evaluate(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the tagger actually decreases when we switch from unigram till trigram. It is because the bigram /trigram tagger fit all words that exists during the training stage but it is bad at new sentences. It is not able to tag the new words.\n",
    "\n",
    "Fortunatly, nltk provides an elegant way of combining all the taggers. We built the following tagger as mentioned in [4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using combined tagger is: 0.826743350108\n"
     ]
    }
   ],
   "source": [
    "def get_combine_tagger(train_set):\n",
    "    defaultTagger = nltk.DefaultTagger('NN')\n",
    "    regexpTagger = nltk.RegexpTagger(pattern, backoff = defaultTagger)\n",
    "    unigramTagger = nltk.UnigramTagger(train_set, backoff = regexpTagger)\n",
    "    bigramTagger = nltk.BigramTagger(train_set, backoff = unigramTagger)\n",
    "    return nltk.TrigramTagger(train_set, backoff = bigramTagger)\n",
    "\n",
    "print 'Accuracy using combined tagger is: {}'.format(get_combine_tagger(train_set).evaluate(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "NLTK provides a collection of classification apis that is optimized for text processing problems. Plus nltk can also take a sklearn classifier and build a classifier on top of that. In this final section, we will combine some of the features disscussed above and build a simple classifier on an email dataset. It is exactly the task we did in the contest of HW4, but we will see how nltk is better at analysising these results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3672 1500\n"
     ]
    }
   ],
   "source": [
    "# Extract the enron dataset: from http://www.aueb.gr/users/ion/data/enron-spam/\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "with zipfile.ZipFile('enron1.zip', 'r') as myzip:\n",
    "    myzip.extractall()\n",
    "\n",
    "def load_dataset(t='ham'):\n",
    "    email = []\n",
    "    for f in os.listdir('enron1/{}'.format(t)):\n",
    "        with open('enron1/{}/{}'.format(t, f)) as e:\n",
    "            email.append(e.read())\n",
    "    return email\n",
    "\n",
    "# Load all datasets\n",
    "ham, spam = [(e, 'ham') for e in load_dataset('ham')], [(e, 'spam') for e in load_dataset('spam')]\n",
    "print len(ham), len(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the feature, we simply use the count of the existence of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5172\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r'[^\\w]+')\n",
    "def extract_feature(email):\n",
    "    return Counter(re.split(pattern, email.decode('utf8', 'ignore')))\n",
    "\n",
    "features = [(extract_feature(email), label) for email, label in ham+spam]\n",
    "print len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our dataset into 4:1 and train the classifier using a simple NavieBayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam spam\n",
      "ham ham\n",
      "Accuarcy: 0.939419795222\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "import random\n",
    "random.shuffle(features)\n",
    "train_set, test_set = features[:4000], features[4000:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "# Test on test samples\n",
    "print classifier.classify(test_set[0][0]), test_set[0][1]\n",
    "print classifier.classify(test_set[15][0]), test_set[15][1]\n",
    "\n",
    "# Accuarcy\n",
    "print \"Accuarcy: {}\".format(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve high accuarcy with this simple feature(what I did in the contest HW and got around 94% precision). And then comes the best part of a nltk wrapped classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               forwarded = 1                 ham : spam   =    140.7 : 1.0\n",
      "                    2004 = 1                spam : ham    =     97.8 : 1.0\n",
      "                    pain = 1                spam : ham    =     87.7 : 1.0\n",
      "                     nom = 1                 ham : spam   =     83.4 : 1.0\n",
      "            prescription = 1                spam : ham    =     82.6 : 1.0\n",
      "                      ex = 1                spam : ham    =     70.8 : 1.0\n",
      "                    2005 = 1                spam : ham    =     67.3 : 1.0\n",
      "                creative = 1                spam : ham    =     63.9 : 1.0\n",
      "                   cheap = 1                spam : ham    =     63.9 : 1.0\n",
      "                     ibm = 1                spam : ham    =     60.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily find out the most informative features(words) with the classifier instead of caculating the coefficiency from scratch. \n",
    "\n",
    "What's more, as mentioned above, nltk provides the wrapper classifier for all sklearn classifers. With the wrappers, you are able to pass a dict as the feature set to the sklearn classifier instead of constructing and normalizing the feature matrix yourself. For instance, in the code snippet below, we take advantage of a linear SVM classifier provided by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy: 0.974402730375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "classifier = SklearnClassifier(LinearSVC()).train(train_set)\n",
    "print \"Accuarcy: {}\".format(nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SklearnClassifier` can even encasulated all the feature preprocessing step into the classifier using a `Pipeline` object. We combine a Tfidf extractor from the sklearn package with the LinearSVC to build our classifier. And we can implement our own transformer in the pipeline as long as it implements the `fit` and `transform` interface. This makes our code more close to production level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy: 0.987201365188\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipeline = Pipeline([('vectorizer', TfidfTransformer()),\n",
    "                     ('classifier', LinearSVC())])\n",
    "\n",
    "classifier = SklearnClassifier(pipeline).train(train_set)\n",
    "print \"Accuarcy: {}\".format(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. The nltk book: [Natural Language Processing with Python](http://www.nltk.org/book/) \n",
    "2. [Dive in to nltk](http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk)\n",
    "3. Slides and notes from the NLP class from my undergraduate school\n",
    "4. Yumusak, Semih, Erdogan Dogdu, and Halife Kodaz. \"Tagging Accuracy Analysis on Part-of-Speech Taggers.\" Journal of Computer and Communications 2.04 (2014): 157."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
