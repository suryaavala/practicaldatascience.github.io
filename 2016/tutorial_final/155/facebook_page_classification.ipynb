{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Public Page Classification\n",
    "\n",
    "In this tutorial, we are going to explore a little bit the biggest social graph on earth - Facebook.\n",
    "\n",
    "First, we'll need to setup the environment to be able to call Graph API. This include creating access token, downloading SDK and making some simple requests. Since the SDK is a thrid-party SDK and super limited. we also need to build a minimal Graph API implementation from scratch.\n",
    "\n",
    "Then we will use our API to scrape a small portion of public Page on Facebook and recent public Posts from each of these Pages. \n",
    "\n",
    "Finally, we could clean up these raw data to generate a well-formated data to train a SVM model to predict the genre of a Page given its Posts content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hi, Facebook\n",
    "\n",
    "In this section, we will need to get an access token from Facebook, download the third party SDK and then using that SDK to get a list of friends of yours as a practice.\n",
    "\n",
    "you can find a detailed introduction to Graph API here https://developers.facebook.com/docs/graph-api/overview\n",
    "\n",
    "\n",
    "### Create Access Token\n",
    "Since our third-party SDK don't have the authentication mechanism, we will have to use the Graph API Explorer to get our token.\n",
    "\n",
    "1. open https://developers.facebook.com/tools/explorer\n",
    "2. Click on the **Get Token button** in the top right of the Explorer.\n",
    "3. Choose the option **Get User Access Token**.\n",
    "4. In the following dialog don't check any boxes, just click the blue **Get Access Token** button.\n",
    "5. You'll see a Facebook Login Dialog, click **OK** here to proceed.\n",
    "6. Now you could see your **Access Token** filled in the Explorer.\n",
    "\n",
    "\n",
    "**NOTE: This is a short-term token. It will expire in about 2 hours.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = \"YOUR_TOKEN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the first request\n",
    "\n",
    "The second thing you need to do is to download the sdk.\n",
    "\n",
    "In your terminal, use the following command to download the **facebook-sdk** using pip\n",
    "```\n",
    "pip install facebook-sdk\n",
    "```\n",
    "once this succeeds, you should be able to import the sdk in Python.\n",
    "\n",
    "Try the following code, it should not give you any warning.\n",
    "\n",
    "**NOTE: the facebook sdk only support Graph API version up to 2.7**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import facebook\n",
    "import json, requests, time, io, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with the Graph\n",
    "\n",
    "Let's explain some basic concept here first.\n",
    "\n",
    "The Graph API is named after the idea of a 'social graph' - a representation of the information on Facebook composed of:\n",
    "\n",
    "* **nodes** - basically \"things\" such as a User, a Photo, a Page, a Comment\n",
    "* **edges** - the connections between those \"things\", such as a Page's Photos, or a Photo's Comments\n",
    "* **fields** - info about those \"things\", such as a person's birthday, or the name of a Page\n",
    "\n",
    "#### Nodes\n",
    "\n",
    "Each node has a unique **ID** which is used to access it via the Graph API. \n",
    "In the provided SDK, you could get any object by using the `get_object()` method.\n",
    "\n",
    "If you want to get multiple objects at once, you could also use `get_objects()` method. **NOTE: the maximal number of objects are 50.**\n",
    "\n",
    "Below is an example for how to get your own User Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'name': u'Yu Tianxin', u'id': u'453142421495400'}\n"
     ]
    }
   ],
   "source": [
    "graph = facebook.GraphAPI(access_token=ACCESS_TOKEN, version='2.7')\n",
    "me = graph.get_object(id='me') # NOTE, in graph api, 'me' is an alias of current token owner's id\n",
    "\n",
    "print me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edges\n",
    "\n",
    "Edges don't have an ID for it. It can only be used along with Nodes. for example, you could query for all your friends using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "{u'paging': {u'cursors': {u'after': u'QVFIUm1ZALUt6aXMxNFJHSEpqaTd4ZA0h6MThsWFZAldzEtbjJBTmhJSTBEYlRud05mVGNIc0VReUxySjFEUkhtUXJhS1NITi1GaWstdG1QVkt6TGR3WDY2ZAGRB', u'before': u'QVFIUm1ZALUt6aXMxNFJHSEpqaTd4ZA0h6MThsWFZAldzEtbjJBTmhJSTBEYlRud05mVGNIc0VReUxySjFEUkhtUXJhS1NITi1GaWstdG1QVkt6TGR3WDY2ZAGRB'}}, u'data': [{u'name': u'Muyang Li', u'id': u'1169501549760530'}], u'summary': {u'total_count': 144}}\n"
     ]
    }
   ],
   "source": [
    "friends = graph.get_connections(id='me', connection_name='friends')\n",
    "print friends['summary']['total_count']\n",
    "print friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a friends list\n",
    "\n",
    "As you can see in the above example, the output actually has pagination. So the final part of this secion is to handle that to get the full list of your friends' ids'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_connections(graph, id, connection_name, limit=100):\n",
    "    results = []\n",
    "    after = ''\n",
    "    while True:\n",
    "        response = graph.get_connections(id=id, connection_name=connection_name,after=after,limit=100)\n",
    "        data = response['data']\n",
    "        results.extend([node for node in data])\n",
    "        if len(results) >= limit:\n",
    "            break\n",
    "        if 'paging' not in response:\n",
    "            break\n",
    "        if 'next' in response['paging']:\n",
    "            print response['paging']\n",
    "            after = response['paging']['cursors']['after']\n",
    "        else:\n",
    "            break\n",
    "    return results\n",
    "            \n",
    "\n",
    "def get_friends(graph, id):\n",
    "    return get_all_connections(graph, id, 'friends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print len(get_friends(graph, 'me'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that Since Graph API V2.0, Facebook stops to provide a full list of friends of the user, along with a lot of other types data involves user privacy.\n",
    "\n",
    "Keep this in mind when collecting data via public API. The infomation you could get from a public API are highly possibly limited. So it's always a good idea to check the capability of different approaches used to collect data at the very first. This might change the methodology you choose to tackle a problem or even change your goal completely.\n",
    "(A sad story from the author about why the topic of this tutorial changed from exploring social graph to Page classifiction.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Tell Me Your Story\n",
    "\n",
    "So the goal of this turorial is to predict a public Page's genre only depending on the publicly available content.\n",
    "In order to to that, we need to get all the posts from a given Page.\n",
    "We also need to be get a decent number of different kinds of Pages to avoid underfiting the machine learning model. During this time, we should also consider adding diversity to our traning data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Posts from Page\n",
    "In this section, we need to use the SDK to get all the posts given a Page.\n",
    "As mentioned earlier, we need to handle privacy related exception here. if a Page have no public Posts, we can simply return a None here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               created_time                             id  \\\n",
      "0  2016-11-03T17:00:00+0000  19614945368_10154139688090369   \n",
      "1  2016-11-03T17:00:00+0000  19614945368_10154139689400369   \n",
      "2  2016-11-03T14:48:45+0000  19614945368_10154137185290369   \n",
      "3  2016-11-03T04:27:16+0000  19614945368_10154136263430369   \n",
      "4  2016-11-01T19:09:46+0000  19614945368_10154132476040369   \n",
      "\n",
      "                                             message  \n",
      "0  That post-show apartment hangggg\\nKelsea Balle...  \n",
      "1                                                Lol  \n",
      "2  Congratulations on CMA Vocal Group of the Year...  \n",
      "3                              HI #1 Little Big Town  \n",
      "4  Feeling really honored that a band I've loved ...  \n",
      "100\n"
     ]
    }
   ],
   "source": [
    "def get_posts(graph, page_id, limit=100):\n",
    "    \"\"\" get the most recent Posts from given a Page ID.\n",
    "    Inputs:\n",
    "        graph: Facebook SDK instance\n",
    "        page_id: str: the id of the requested Page\n",
    "        limit: total number of Posts required.\n",
    "        \n",
    "    Outputs:\n",
    "        pd.DataFrame: dataframe containing all the required posts, have fields ['created_time', 'id', 'message']\n",
    "    \"\"\"\n",
    "    posts = get_all_connections(graph, page_id, 'posts', limit=limit)\n",
    "    df = pd.DataFrame(posts)\n",
    "    if df.shape[0]:\n",
    "        return df.drop('story', axis=1, errors='ignore')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "posts_taylor_swift = get_posts(graph, 'TaylorSwift')\n",
    "print posts_taylor_swift.head()\n",
    "print len(posts_taylor_swift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search For All Public Pages\n",
    "The second step is to get a list of public Pages here. It's impossible to get a good list containing tens of thousands of Pages manually. \n",
    "\n",
    "After digging into the Graph API documentation https://developers.facebook.com/docs/graph-api/reference., one feasible solution is to use the `Search` endpoint to get a list of Pages given a keyword.\n",
    "\n",
    "However, the given sdk doesn't support `Search` endpoint, we'll have to write it from scratch.\n",
    "This also helps to get a deeper understanding how the Graph API works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'category': u'Musician/Band', u'id': u'19614945368'}\n",
      "200\n",
      "                id                                               name\n",
      "0  105799462785959                                             Artist\n",
      "1  105453336153572                                           Artistic\n",
      "2  100133933802825                                             Artist\n",
      "3  119467874799527                   Scoala de make-up Ramona Chirita\n",
      "4  221818141301837  MALI Tattoo Studio รับสักลาย แก้ลายสัก ออกแบบล...\n"
     ]
    }
   ],
   "source": [
    "class GraphAPI:\n",
    "    def __init__(self, token):\n",
    "        \"\"\" create GraphAPI instance\n",
    "        Inputs:\n",
    "            token: str: Graph API token\n",
    "        \"\"\"\n",
    "        self.BASE_URL = \"https://graph.facebook.com/v2.7/\"\n",
    "        self.token = token   \n",
    "    \n",
    "    def node_request(self, url, **payload):\n",
    "        \"\"\" Make GraphAPI node request\n",
    "        Inputs:\n",
    "            url: str: note endpoint\n",
    "            payload: dict: custom parameter passing to Graph API\n",
    "        \"\"\"\n",
    "        payload['access_token'] = self.token\n",
    "        r = requests.get(self.BASE_URL + url, params=payload)\n",
    "        return json.loads(r.text)\n",
    "    \n",
    "    def connection_request(self, url, limit=None, **payload):\n",
    "        \"\"\" Make GraphAPI connection request\n",
    "        Inputs:\n",
    "            url: str: note endpoint\n",
    "            limit: int: total nubmer of connections required\n",
    "            payload: dict: custom parameter passing to Graph API\n",
    "        \"\"\"\n",
    "        payload['access_token'] = self.token\n",
    "        payload['limit'] = limit if limit <= 100 else 100\n",
    "        r = requests.get(self.BASE_URL + url, params=payload)\n",
    "        response = json.loads(r.text)\n",
    "        \n",
    "        result = []\n",
    "        result.extend(response['data'])\n",
    "        while len(result) < limit:\n",
    "            if 'next' not in response['paging']:\n",
    "                break\n",
    "            response = json.loads(requests.get(response['paging']['next']).text)\n",
    "            result.extend(response['data'])\n",
    "        return pd.DataFrame(result)\n",
    "\n",
    "api = GraphAPI(ACCESS_TOKEN)\n",
    "taylor = api.node_request('TaylorSwift', fields=['category'])\n",
    "artists = api.connection_request('search', type='page', q='Artist', limit=200)\n",
    "print taylor\n",
    "print len(artists)\n",
    "print artists.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the training and evaluation table\n",
    "With the help of these API, we now are able to collect all the data we need.\n",
    "\n",
    "First, we will make some search queries to get a preferably random list of public pages.\n",
    "Then, we need to get all the posts from these pages. \n",
    "Also, we need to get the page's category. This will be later to used to generate the label for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id                                      name  \\\n",
      "0  105799462785959                                    Artist   \n",
      "1  354577571346409               Artist in Residence Program   \n",
      "2  105453336153572                                  Artistic   \n",
      "3  906740039387472                     Artistically Speaking   \n",
      "4  348604348593352  Associazione Artisti di Strada di Milano   \n",
      "\n",
      "                 category  \n",
      "0              Profession  \n",
      "1  Community Organization  \n",
      "2                Interest  \n",
      "3           Event Planner  \n",
      "4               Community  \n",
      "                id                                            message\n",
      "0  283319135042170  اعزاءنا الطلبة \\r\\nللاطلاع على القاعات الخاصة ...\n",
      "1  283319135042170  إعلان هام لجميع الطلبة\\r\\nننوه للطلبة الأعزاء ...\n",
      "2  283319135042170  الجامعة تستقبل الدكتور سايمون غالبين المدير ال...\n",
      "3  283319135042170  الجامعة تستقبل الدكتور سايمون غالبين المدير ال...\n",
      "4  283319135042170  الجامعة تستقبل الدكتور سايمون غالبين المدير ال...\n"
     ]
    }
   ],
   "source": [
    "keyword_list = ['Artist', 'Musician', 'Trump', 'Hillary', 'PHP', 'Data Science', 'NBA', 'Alpine Ski', 'Scuba Diving', 'Computer']\n",
    "\n",
    "def get_all_pages_and_posts(graph, api, keyword_list, limit=1000):\n",
    "    \"\"\" get all the pages and related posts from Graph API using `Search` endpoint given keyword_list\n",
    "    Inputs:\n",
    "        graph: Facebook SDK instance\n",
    "        api: custom GraphAPI instance\n",
    "        keyword_list: keywords list used to perform Search for Pages\n",
    "        limit: number of Pages required for each keyword\n",
    "    Output:\n",
    "        all_pages: array: list of tuple(page_id, page_name)\n",
    "        all_posts: dictionary(page_id, pd.Dataframe)\n",
    "    \"\"\"\n",
    "    all_pages = []\n",
    "    all_posts = {}\n",
    "    for keyword in keyword_list:\n",
    "        pages = api.connection_request('search', type='page', q=keyword, limit=limit)\n",
    "        posts = {id: get_posts(graph, id) for id in pages['id']}\n",
    "        all_pages.append(pages)\n",
    "        all_posts.update(posts)\n",
    "    return all_pages, all_posts\n",
    "\n",
    "def get_page_category(graph, page_ids):\n",
    "    \"\"\" Make GraphAPI connection request\n",
    "    Inputs:\n",
    "        graph: Facebook SDK instance\n",
    "        api: custom GraphAPI instance\n",
    "        keyword_list: keywords list used to perform Search for Pages\n",
    "        limit: number of Pages required for each keyword\n",
    "    Output:\n",
    "        all_pages: array: list of tuple(page_id, page_name)\n",
    "        all_posts: dictionary(page_id, pd.Dataframe)\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for i in range(0, len(page_ids), 50):\n",
    "        pages = graph.get_objects(ids=page_ids[i:i+50], fields='category')\n",
    "        result.update({id: page['category'] for id, page in pages.items() if 'category' in page or 'Unknown' })\n",
    "    return [result[id] for id in page_ids if id in result or 'Unknown']\n",
    "\n",
    "def collect_training_data(pages_file, posts_file, graph, api, keyword_list):\n",
    "    \"\"\" Wrap up all the previous function to collect data from Graph API. \n",
    "        If local files provided, simply load from them.\n",
    "    Input:\n",
    "        pages_file: file path to pages content\n",
    "        posts_file: file path to posts content\n",
    "        graph: Facebook SDK instance\n",
    "        api: custom GraphAPI instance\n",
    "        keyword_list: keywords list used to perform Search for Pages\n",
    "    Output:\n",
    "      df_pages: pd.Dataframe. containing fields ['id', 'name', 'category']\n",
    "      df_posts: pd.Dataframe. containing fields ['id', 'message']    \n",
    "    \"\"\"\n",
    "    if pages_file and posts_file:\n",
    "        with open('all_pages.csv', 'r') as csv:\n",
    "            df_pages = pd.DataFrame.from_csv(csv, encoding='UTF-8')\n",
    "            \n",
    "        with open('all_posts.csv', 'r') as csv:\n",
    "            df_posts = pd.DataFrame.from_csv(csv, encoding='UTF-8')\n",
    "        \n",
    "        return df_pages.reset_index(), df_posts.reset_index()\n",
    "    else:\n",
    "        all_pages, all_posts = get_all_posts(graph, api, keyword_list)\n",
    "        df_pages = pd.concat(all_pages)\n",
    "        df_pages = df_pages.assign(category=get_page_category(graph, [str(id) for id in df_pages.index]))\n",
    "        \n",
    "        posts = [[id, posts.iloc[i]['message']] for id, posts in all_posts.items() if posts is not None for i in range(len(posts)) if 'message' in posts.iloc[i]]\n",
    "        df_posts = pd.DataFrame(posts, columns=['id', 'message'])\n",
    "        return df_pages, df_posts\n",
    "\n",
    "df_pages, df_posts = collect_training_data('all_pages.csv', 'all_posts.csv', graph, api, keyword_list)\n",
    "\n",
    "print df_pages.head()\n",
    "print df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Know It!\n",
    "In this section, we could use the collected data to train a model to predict the genre of a Page given its public Posts content. Since there are lots of possible categories in the data. It's too much work for us to predict each of them. To simply the problem, we instead just predict if a given Page is IT related.\n",
    "\n",
    "After a quick investigation of our collected data, we decide Page with provided category in the following list at IT-related:\n",
    "* Computers/Technology\n",
    "* Computer Company\n",
    "* Internet/Software\n",
    "* Computers\n",
    "* Software\n",
    "* Internet Company\n",
    "* Computers/Internet Website\n",
    "\n",
    "\n",
    "### Data pre processing\n",
    "The first step is to pre-process the data to remove unusable data. That include:\n",
    "1. remove all the pages with no posts\n",
    "2. remove all the non-English posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305967\n",
      "                 id                                            message\n",
      "0  1507446969543009            GAME TIME!! Houston Rockets - NY Knicks\n",
      "1  1507446969543009  Dal caldo al freddo....da Manu Ginobili al Bar...\n",
      "2  1507446969543009   Tutto pronto per San Antonio Spurs - Miami Heat!\n",
      "3  1507446969543009  Vista la giornata nuvolosa, relax e shopping s...\n",
      "4  1507446969543009                                       EXTRA GAME!!\n",
      "150326\n"
     ]
    }
   ],
   "source": [
    "def pre_process_data(df_posts):\n",
    "    \"\"\" remove posts can't be used for training\n",
    "    Input:\n",
    "        df_posts: df.DataFrame. posts dataset containing field `message`\n",
    "    Output:\n",
    "        df.DataFrame: cleaned dataset with same schema\n",
    "    \"\"\"\n",
    "    def is_ascii(message):\n",
    "        try:\n",
    "            message.decode('ascii')\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "    df = df_posts[pd.notnull(df_posts['message'])]\n",
    "    df = df[df['message'].apply(is_ascii)]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "print len(df_posts)\n",
    "posts = pre_process_data(df_posts)\n",
    "print posts.head()\n",
    "print len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features\n",
    "Now that we have all our row data with some pre-process. we could use Nature Language Process(NLP) technique to create our features to train the Machine Learning model.\n",
    "\n",
    "In this section, we need to do convert all the post content into a set of tokens for each page. for each token, we need to:\n",
    "1. convert all the token to lowercase\n",
    "2. convert all the token to their lemmatized form. This will remove all the Non-English content.\n",
    "3. all words with punctuation should be processed as follows: (a) Apostrophe of the form `'s` should be ignored. (b)All other apostrophe should be ignored. (c) Break the word at all other punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'time', 'houston', u'rocket', 'ny', 'knicks']\n"
     ]
    }
   ],
   "source": [
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'s\", '').replace(\"'\", '')\n",
    "    replace_punctuation = string.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text = text.translate(replace_punctuation)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            res = lemmatizer.lemmatize(token)\n",
    "            result.append(res)\n",
    "        except:\n",
    "            continue\n",
    "    return result\n",
    "\n",
    "print process('GAME TIME!! Houston Rockets - NY Knicks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of this function, we could now generate a list of tokens for each Page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                            message  \\\n",
      "0   542952449048579  [by, matt, beck, a, todos, nuestros, clientes,...   \n",
      "1      129650712580  [the, ieee, computer, society, and, acm, have,...   \n",
      "2   361468313944069  [from, eric, trump, twitter, erictrump, riding...   \n",
      "3   178787235479558  [we, are, really, thrilled, to, introduce, kni...   \n",
      "4  1472341039718413  [admiring, that, contour, yo, bts, of, this, b...   \n",
      "\n",
      "                                    name                   category  \n",
      "0         Hillary Salon Peluqueria & Spa  Spas/Beauty/Personal Care  \n",
      "1  Computing Now - IEEE Computer Society                    Website  \n",
      "2            Trump Tower at Century City                Real Estate  \n",
      "3                             JoomShaper       Computers/Technology  \n",
      "4                              Makeuport       Professional Service  \n"
     ]
    }
   ],
   "source": [
    "def generate_tokens(df_pages, df_posts):\n",
    "    \"\"\" Generate tokens for each page.\n",
    "    Input:\n",
    "        df_pages: pd.DataFrame. pages dataset containing 'id' field\n",
    "        df_posts: pd.DataFrame. Posts dataset containing 'id' and 'message' field.\n",
    "    Output:\n",
    "        pd.DataFrame. dataset with row per page 'id'\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for id, indices in df_posts.groupby('id').groups.items():\n",
    "        token = [process(str(df_posts.iloc[index]['message'])) for index in indices]\n",
    "        token = np.concatenate(token)\n",
    "        result[id] = token\n",
    "    \n",
    "    data = pd.merge(pd.DataFrame().assign(id=result.keys(), message=result.values()), df_pages, on='id', how='left')\n",
    "    return data\n",
    "\n",
    "data = generate_tokens(df_pages, posts)\n",
    "print data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When doing prediction based on natural languange, it's almost certain that you don't need that many possible words.\n",
    "Some of them are very popular which add no value to our model, like stopwords. Others are too rare that are most likely to be typos.\n",
    "In the NLTK package, they provide a list of stopwords we could borrow. In the following section, we need to get a list of rare words. Rare words are defined as words only occured once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91644\n"
     ]
    }
   ],
   "source": [
    "def get_rare_words(data):\n",
    "    \"\"\" use the word count information across all posts in training data to come up with a feature list\n",
    "    Inputs:\n",
    "        data: pd.DataFrame: the output of generate_tokens() function\n",
    "    Outputs:\n",
    "        list(str): list of rare words\n",
    "    \"\"\"\n",
    "    token_list = [token for tokens in data['message'] for token in tokens]\n",
    "    counter = Counter(token_list)\n",
    "    return [k for k,v in counter.iteritems() if v == 1]\n",
    "\n",
    "rare_words = get_rare_words(data)\n",
    "print len(rare_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could create a feature matrix for each page using `sklearn.feature_extraction.text.TfidfVectorizer`.\n",
    "The `sklearn` package contains lots of useful APIs used by Machine Learning.\n",
    "The `Tfidf` technique is used to convert natural languange words into machine understandable numbers.\n",
    "It also describes the **Importance** of a word.\n",
    "\n",
    "you can find more infomation about TF-IDF here https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3619, 72898)\n"
     ]
    }
   ],
   "source": [
    "def create_features(data, rare_words):\n",
    "    \"\"\" creates the feature matrix using the Page posts\n",
    "    Inputs:\n",
    "        data: pd.DataFrame: Page posts collected above\n",
    "        rare_words: list(str): the output of get_rare_words() function\n",
    "    Outputs:\n",
    "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "    \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.extend(rare_words)\n",
    "    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=stopwords)\n",
    "    transformer = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "\n",
    "    # get frequency counts (sparse) matrix\n",
    "    all_tokens = [' '.join(tokens) for tokens in data['message']]\n",
    "    freq_matrix = vectorizer.fit_transform(all_tokens)\n",
    "    return (vectorizer, freq_matrix)\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "(tfidf, X) = create_features(data, rare_words)\n",
    "print X.shape\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Labels\n",
    "The previous section converted all the posts content into numbers we could use to train the model.\n",
    "But we are missing one important part here - The Label.\n",
    "\n",
    "As we are solving a classficition problem, we need to assign each sample a Label as the class as the training target.\n",
    "It's a simple process here. For each sample, if it's category is in the IT_related cateogry defined above, it should have a label with value 1, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_labels(data):\n",
    "    \"\"\" create label data.\n",
    "    Input:\n",
    "        data: pd.Dataframe. containing field 'category'\n",
    "    Output:\n",
    "        arraylike: a list of 1 or 0. 1 indicating the corresponding sample in data is IT-related\n",
    "    \"\"\"\n",
    "    it_category = ['Computers/Technology', 'Computer Company', 'Internet/Software', 'Computers', 'Software',\n",
    "                  'Internet Company', 'Computers/Internet Website']\n",
    "    return np.array([1 if category in it_category else 0 for category in data['category']])\n",
    "\n",
    "y = create_labels(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, Learning!\n",
    "\n",
    "Finally we could learn our Machine Learning model now. \n",
    "The model we choose to use the is Support Vector Machine (SVM), you can find a descriptive introduction here https://en.wikipedia.org/wiki/Support_vector_machine.\n",
    "\n",
    "The implementation of SVM is not very complicated, but we don't need to recreate the wheel for now.\n",
    "With the help of `sklearn`, we could easily train a SVM classifier using\n",
    "```\n",
    "clf = sklearn.svm.SVC(kernel=kernel)\n",
    "clf.fit(X,y)\n",
    "```\n",
    "\n",
    "However, you need to keep onething in mind: **The data we collected have a heavy sequential correlation**.\n",
    "This is because we use keywords to Search for Pages, so neighboring Pages are mostly about the same topic.\n",
    "In order to solve this problem, we need to shuffle our data before using it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn_classifier(X, y, kernel='linear'):\n",
    "    \"\"\" learns a classifier from the input features and labels using the kernel function supplied\n",
    "    Inputs:\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels\n",
    "        kernel: str: kernel function to be used with classifier. [linear|poly|rbf|sigmoid]\n",
    "    Outputs:\n",
    "        sklearn.svm.classes.SVC: classifier learnt from data\n",
    "    \"\"\"\n",
    "    clf = sklearn.svm.SVC(kernel=kernel)\n",
    "    clf.fit(X, y)\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could evalute the accuracy of our model.\n",
    "Note that we split our data into two part - train and validation.\n",
    "\n",
    "This is because it's possible to overfiting a model with too much data, that it will be able to perfectly do prediction on the traininig data. But this will result in the loss of generic of that model.\n",
    "\n",
    "So it's always a good idea to keep a portion of your data from training and only use that for evaluting the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968221070812\n",
      "0.911602209945\n"
     ]
    }
   ],
   "source": [
    "def evaluate_classifier(classifier, X_validation, y_validation):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "    Inputs:\n",
    "        classifier: sklearn.svm.classes.SVC: classifer to evaluate\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels\n",
    "    Outputs:\n",
    "        double: accuracy of classifier on the validation data\n",
    "    \"\"\"\n",
    "    result = [classifier.predict(X_validation[i]) == y_validation[i] for i in range(X_validation.shape[0])]\n",
    "    return float(sum(result)) / X_validation.shape[0]\n",
    "\n",
    "\n",
    "N = len(y)\n",
    "perm = np.random.permutation(range(N))\n",
    "training_data_indices = perm[:int(N * 0.8)]\n",
    "validation_data_indices = perm[int(N * 0.8):]\n",
    "X_train = X[training_data_indices]\n",
    "y_train = y[training_data_indices]\n",
    "X_validation = X[validation_data_indices]\n",
    "y_validation = y[validation_data_indices]\n",
    "\n",
    "classifier = learn_classifier(X_train, y_train, 'linear')\n",
    "accuracy = evaluate_classifier(classifier, X_train, y_train)\n",
    "print accuracy\n",
    "\n",
    "accuracy = evaluate_classifier(classifier, X_validation, y_validation)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output, the accuracy of the model is very high predicting on the training data, but dropped a little bit on the validation data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last but Not Least\n",
    "After training the first model, it's always a good idea to play around with different parameters. \n",
    "For example, in this model, we could change the kernal function used to see what's the best for us.https://en.wikipedia.org/wiki/Kernel_method\n",
    "\n",
    "Also, we could use technique called cross validation to get a more accurate validation of the accuracy of our classifier. https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "Best of all, it's highly recommended to take the 15-688 class if you could to get a much better understand of all the content mentioned in this tutorial :P\n",
    "\n",
    "Due to time limitation, we could not dig into furthur on these topics. \n",
    "\n",
    "**Disclaimer**: some of the ideas from this tutorial is referenced from the 15-688 class homework. But all the content and code are original.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
