{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRODUCTION\n",
    "\n",
    "This tutorial is on Neural Networks, a fundamental concept in Machine Learning. In the recent past, Neural Networks (or Artificial Neural Networks) have been succesfully applied to problems such as speech recognition (thank Neural Networks for the Siri on your phone!), computer visions (Uber definitely makes use of multi hidden layer neural networks for developing driverless cars) etc. \n",
    "\n",
    "Neural networks are appropriate for problems with the following characteristics:\n",
    "\n",
    "1) Training Example With Errors: Neural network training methods are very robust to noise in the training data\n",
    "\n",
    "2) The ability for people to understand the learned function is not important: The weights learned by neural networks are often difficult for humans to interpret. Learned neural networks are not easily communicated to humans.\n",
    "\n",
    "3) Long training times are acceptable: Neural Network training algorithms typically require longer training times than other Machine Learning algorithms such as for example Decision Tree Algorithms. Training times can range from a few seconds to many hours depending on factors such as the number of weights in the network, the number of training examples considered, the number of hidden layers etc.\n",
    "\n",
    "Through this tutorial, I will introduce you to the key concepts used to build Neural Networks, and will also build a simple neural network for a simplified dataset as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple different types of units that can be used to make up Neural Networks. We look at 2 for now:\n",
    "\n",
    "1) A perceptron takes a vector of real-valued inputs, calculates a linear combination of these inputs, then outputs a 1 if the result is greater than some threshold and-1 otherwise\n",
    "\n",
    "2) To develop a multilayer neural network, we unfrotunately cannot just use perceptrons, as it has a discontinuous theshhold and thus is not differentiable and not suitable for gradient descent. Thus, we look at another type of unit called the sigmoid unit, similar to the percepptron but having a smooth, differentiable threshhold function. Like the perceptron, the sigmoid unit first computes a linear combination of its inputs, then applies a threshold to the result. We can see the sigmoid function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "t = 0\n",
    "for i in xrange(w): #w is list of weights\n",
    "    t+= w[i] * x[i] #x is inputs\n",
    "\n",
    "#here, t is a linear combination of inputs\n",
    "    \n",
    "def sigmoid(t):\n",
    "    return (float(1)/float(1+ math.exp(-t)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we understand the core of how a neural network works:\n",
    "    \n",
    "We essentially have some inputs (the input layer) and an output layer (let us take the number of hidden layers to be 0 for now).\n",
    "\n",
    "Now, for every interconnection between the input layer and the output layer, we have some weight assigned (initially we randomize the weight). Thus, we get a linear combination of the inputs (sum (Wi * Xi for all i)) and then apply the sigmoid function (definted in the cell above) to that to give a predicted output. (NOTE: Linear combination of inputs python code is in cell below)\n",
    "\n",
    "Of course, it will not be very accurate since we initially randomized the weights. So the question arises, how do we change the weights every time so that our predicted output is closer to the actual output. A very common technique to so this is known as the backpropagation algorithm, which is explained and shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearCombination(X, W):\n",
    "    #here, X is a list of all inputs, W is a list of all weights\n",
    "    total = 0\n",
    "    for i in xrange(len(X)):\n",
    "        total += X[i] * W[i]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BackPropagation Algorithm: This algorithm learns the weights for a multilayer network, given a network with a fixed set of units and interconnections. It employs gradient descent to attempt to minimize the squared error between the network output values and the target values for these outputs. \n",
    "\n",
    "A step by step (and easy to implement!) implementation is as follows:\n",
    "\n",
    "1) Initialize all weights to small random numbers\n",
    "\n",
    "2) For each training example (X, t) (where t is the actual output): \n",
    "\n",
    "        (a) For each output unit k: calculate DeltaK = O_k * (1 - O_k)  * (T_k - O_k)\n",
    "        \n",
    "        (b)  For each hidden unit k: calculate DeltaH = O_h * (1 - O_h) * sum(W_h,k * DeltaK for all outputs k)\n",
    "        \n",
    "        (c) Update each network weight W_i,j = W_i,j  + learningRate * DeltaJ * X_i,j\n",
    "\n",
    "(Note: O_k is the predicted  output K at every step, O_h is the predicted out for each hidden unit h, W_h,k is the weight of the interconnection between hidden unit h and output unit k, deltaJ is either the DeltaK or DeltaH depending on whether J is a hidden or output unit, W_i,j is the weight of the interconnection between i and j)\n",
    "\n",
    "Some helper functions that we will use of the backpropagation algorithm to implement a neural network are written below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateWeight(learningRate, deltaJ, X_ij, currentWeight):\n",
    "    return currentWeight + learningRate * deltaJ * X_ij\n",
    "\n",
    "def getDeltaK(predictedOutput, actualOutput):\n",
    "    return predictedOutput  * (1-predictedOutput) * (actualOutput - predictedOutput)\n",
    "\n",
    "\n",
    "def getDeltaH(val, w_k, delta_k):\n",
    "    # getting delta H if there is only one output unit (this is the case we will handle in our sample dataset)\n",
    "    return val * (1-val) * (w_k  * delta_k)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we actually build our own neural network and use it for a particular dataset. For this tutorial, we use a simple music songs dataset with attributes as follows:\n",
    "\n",
    "1) Year of release of the song : Discrete (any year between 1900 and 2000)\n",
    "2) Length of recording (in minutes): Continuous\n",
    "3) Jazz (Yes or No): Binary Categorical\n",
    "4) Rock & Roll (Yes or No): Binary Categorical\n",
    "5) Ouput Variable: Hit (Yes or No): Whether the song made it to the Billboard Top 50 songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a neural network with 4 input units (one for each of the inputs), one hidden layer with 3 units, and one output unit (the output).\n",
    "We start by initializing the weights for all inter-connections (we use the random function to randomly initialize them).\n",
    "Next, we go through every line of the training dataset and for the variables 'Jazz' and 'Rock & roll', we convert 'yes' and 'no' to 1 and 0. \n",
    "Then, we calculate the sigmoid function for each of the hidden units , and subsequently for the output unit (which essentially the predicted output).\n",
    "We then continuously update our weights by the BackPropagation algorithm described above, and then calculate the mean squared error.\n",
    "We do this for 500 iterations of the training data (I chose 500 at random to gain more accuracy but still remain efficient).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28333395964841607, 0.1858211752359032)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main():\n",
    "    inputX = []\n",
    "    w11, w12,  w13 ,w21 , w22 , w23 , w31 = random.random(), random.random(),random.random(),random.random(),random.random(),random.random(),random.random()\n",
    "    w32 ,w33 , w41 , w42 , w43 , w5 , w6 , w7 = random.random(),random.random(),random.random(),random.random(),random.random(),random.random(),random.random(),random.random()\n",
    "    count = 0\n",
    "    startMSE = 0\n",
    "    finalMSE = 0\n",
    "    totalLines = 0\n",
    "    while (count< 500): \n",
    "        count+=1\n",
    "        totalSquaredError = 0\n",
    "        for line in open(\"music_train.csv\", 'r').readlines()[1:]:\n",
    "            if (count ==1):\n",
    "                totalLines+=1\n",
    "            temp1 = 0 #value of X3\n",
    "            temp2 = 0 #value of X4\n",
    "            temp3 = 0 #actual output\n",
    "            values = line.split(\",\")\n",
    "            inputX = values[0:4]  #X1,X2,X3,X4\n",
    "            x1 = float(float(inputX[0]) - 1900)/float(100) #need to normalize x1\n",
    "            x2 = float(inputX[1])/float(7) #need for normalization\n",
    "            if (inputX[2]==\"yes\"): #converting \"yes and no's to 1 and 0\n",
    "                temp1 = 1 #value of thirs input\n",
    "            if (inputX[3]==\"yes\"): #converting yes and no to 1 and 0\n",
    "                temp2 = 1 #value of fourth input\n",
    "\n",
    "\n",
    "            t1 = 0\n",
    "            t2 = 0\n",
    "            t3 = 0\n",
    "            \n",
    "            #hidden layer unit one\n",
    "            #t1 = (x1* w11) + (x2 * w21) + (temp1 * w31 )+ (temp2 * w41)  \n",
    "            t1 = linearCombination([x1,x2,temp1,temp2], [w11,w21,w31,w41])\n",
    "            Y5 =  sigmoid(t1) #output for hidden unit 1\n",
    "\n",
    "            #hidden layer unit two \n",
    "            #t2 = (x1* w12) + (x2 * w22) + (temp1 * w32 )+ (temp2 * w42) \n",
    "            t2 = linearCombination([x1,x2,temp1,temp2], [w12,w22,w32,w42])\n",
    "            Y6 = sigmoid(t2) #output for hidden unit 2\n",
    "\n",
    "            #hidden layer unit three\n",
    "            #t3 = (x1* w13) + (x2 * w23) + (temp1 * w33 )+ (temp2 * w43)\n",
    "            t3 = linearCombination([x1,x2,temp1,temp2], [w13,w23,w33,w43])\n",
    "            Y7 = sigmoid(t3) #output for hidden unit 3\n",
    "\n",
    "\n",
    "            t4 = 0\n",
    "            #t4 = Y5 * w5 + Y6 * w6 + Y7 * w7\n",
    "            t4 = linearCombination([Y5,Y6,Y7], [w5,w6,w7])\n",
    "            predictedOutput = sigmoid(t4) \n",
    "\n",
    "            #step 2\n",
    "            deltaK = 0\n",
    "            if (values[4]==\"yes\\r\\n\"):\n",
    "                temp3 = 1 # temp 3 is the actual output \n",
    "            deltaK = getDeltaK(predictedOutput, temp3)\n",
    "            w5 = (0.01 * deltaK * Y5) + w5\n",
    "            w6 = (0.01 * deltaK * Y6) + w6\n",
    "            w7  = (0.01 * deltaK * Y7 ) + w7\n",
    "\n",
    "\n",
    "            #step3 \n",
    "            deltaH1 , deltaH2 , deltaH3 = 0,0,0\n",
    "\n",
    "            deltaH1 = getDeltaH(Y5, w5, deltaK)\n",
    "\n",
    "            deltaH2 = getDeltaH(Y6, w6, deltaK)\n",
    "\n",
    "            deltaH3 = getDeltaH(Y7, w7, deltaK)\n",
    "\n",
    "\n",
    "            #update weights\n",
    "            w11 = updateWeight(0.01, deltaH1, x1,w11)\n",
    "            w12 = updateWeight(0.01, deltaH2, x1,w12)\n",
    "            w13 = updateWeight(0.01, deltaH3, x1,w13)\n",
    "            w21 = updateWeight(0.01, deltaH1, x2,w21)\n",
    "            w22 = updateWeight(0.01, deltaH2, x2,w22)\n",
    "            w23 = updateWeight(0.01, deltaH3, x2,w23)\n",
    "            w31 = updateWeight(0.01, deltaH1, float(temp1),w31)\n",
    "            w32 = updateWeight(0.01, deltaH2, float(temp1),w32)\n",
    "            w33 = updateWeight(0.01, deltaH3, float(temp1),w33)\n",
    "            w41 = updateWeight(0.01, deltaH1, float(temp2),w41)\n",
    "            w42 = updateWeight(0.01, deltaH2, float(temp2),w42)\n",
    "            w43 = updateWeight(0.01, deltaH3, float(temp2),w43)\n",
    "            totalSquaredError += (predictedOutput - float(temp3)) * (predictedOutput - float(temp3))\n",
    "        MSE = float(totalSquaredError) / float(totalLines)\n",
    "        if (count==1):\n",
    "            startMSE = MSE\n",
    "        #sys.stdout.write(\"%f\\n\"%MSE)\n",
    "    #sys.stdout.write(\"TRAINING COMPLETED! NOW PREDICTING.\\n\")\n",
    "    finalMSE = MSE\n",
    "\n",
    "    W = (w11,w12,w13,w21,w22,w23,w31,w32,w33,w41,w42,w43,w5,w6,w7) #final weights from our model\n",
    "    return (startMSE, finalMSE, W)\n",
    "\n",
    "main()[0],main()[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, comparing the MSE for the first iteration of the training set compared to the last iteration of the training set, our MSE has decreased from 0.283 to 0.185. This shows that the greater the number of times we run our training dataset, the better our weights get trained and lesser the MSE (i.e. backpropagation algorithm does work).\n",
    "\n",
    "Next, we check how accurate our model is for a test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes no\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "no no\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "yes no\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "yes no\r\n",
      "\n",
      "yes no\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "yes no\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "no no\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "no no\r\n",
      "\n",
      "yes yes\t\r\n",
      "\n",
      "no no\r\n",
      "\n",
      "yes no\r\n",
      "\n",
      "no no\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "yes yes\r\n",
      "\n",
      "yes yes\r\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2692307692307692"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getErrorRateForTestSet(W):\n",
    "    (w11,w12,w13,w21,w22,w23,w31,w32,w33,w41,w42,w43,w5,w6,w7) = W\n",
    "    #do on training data\n",
    "    predictions  = []\n",
    "    error = 0\n",
    "    total = 0\n",
    "    for line in open(\"music_dev.csv\", 'r').readlines()[1:]:\n",
    "        temp1 = 0 #value of X3\n",
    "        temp2 = 0 #value of X4\n",
    "        temp3 = 0 #actual output\n",
    "        values = line.split(\",\")\n",
    "        inputX = values[0:4]  #X1,X2,X3,X4\n",
    "        output = values[4]\n",
    "        x1 = float(float(inputX[0]) - 1900)/float(100)\n",
    "        x2 = float(inputX[1])/float(7)\n",
    "        if (inputX[2]==\"yes\"):\n",
    "            temp1 = 1 #value of thirs input\n",
    "        if (inputX[3]==\"yes\"):\n",
    "            temp2 = 1 #value of fourth input\n",
    "        t1 = 0\n",
    "        t2 = 0\n",
    "        t3 = 0\n",
    "        \n",
    "        #hidden layer unit one\n",
    "        t1 = linearCombination([x1,x2,temp1,temp2], [w11,w21,w31,w41])\n",
    "        Y5 =  sigmoid(t1)\n",
    "\n",
    "        #hidden layer unit two \n",
    "\n",
    "        t2 = linearCombination([x1,x2,temp1,temp2], [w12,w22,w32,w42])\n",
    "        Y6 = sigmoid(t2)\n",
    "\n",
    "        #hidden layer unit three\n",
    "        t3 = linearCombination([x1,x2,temp1,temp2], [w13,w23,w33,w43])\n",
    "        Y7 = sigmoid(t3)\n",
    "\n",
    "\n",
    "        t4 = 0\n",
    "        #t4 = Y5 * w5 + Y6 * w6 + Y7 * w7\n",
    "        t4 = linearCombination([Y5,Y6,Y7], [w5,w6,w7])\n",
    "        predictedOutput = sigmoid(t4) \n",
    "        if (predictedOutput>=0.5):\n",
    "            predictions += [\"yes\"]\n",
    "        else:\n",
    "            predictions += [\"no\"]\n",
    "        print predictions[-1], output\n",
    "        if (predictions[-1] + \"\\r\\n\" != output):\n",
    "            error+=1\n",
    "        total +=1\n",
    "    errorRate = float(error)/total\n",
    "    return errorRate\n",
    "    \n",
    "weights = main()[2]\n",
    "getErrorRateForTestSet(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first print the prediction vs. the actual output, and at the end we show the error rate.\n",
    "\n",
    "We see our error rate is ~27%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, through this tutorial, we were able to understand some funamental concepts associated with Neural Networks. \n",
    "We learnt the types of units that make up neural networks, how to train a neural network (i.e. how to train your weights via the BackPropagation Algorithm), and we even made a neural network for a simple dataset.\n",
    "I hope you find this useful and will try to implement a neural network yourself now!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
