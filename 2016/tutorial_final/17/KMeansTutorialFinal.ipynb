{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will introduce you to K-means clustering including utilizing different similarity metrics, and useful initialization techniques. In particular we will be focusing on how to apply this algorithm to text classification. This algorithm is a simple and powerful algorthim in machine learning that has improtant implications in text mining and data science.\n",
    "\n",
    "Below is an example of clustering documents with k=3: \n",
    "[<img src=\"http://www.codeproject.com/KB/recipes/439890/clustering-process.png\">](http://www.codeproject.com/KB/recipes/439890/clustering-process.png)\n",
    "(click for full-size version).  This is an example of the first iteration of the k-means clustering algorithm where we begin by selecting k=3 random 'centroids'. After selecting the centroids we must then assign the rest of the documents based upon their 'similarity' to the current centroids. Here we assign colors, red, green, or blue depending upon whether a given document is in a certain centroid's 'cluster'. The algorithm will then update the centroids, and recalculate the clusters for a given number of iterations or after a certain stopping criterion is met. For the rest of this tutorial we will assume that the stopping criterion is simply the number of update iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "\n",
    "In this tutorial, we will show how to do K-means clustering in Python, using only native python libraries like [sys](https://docs.python.org/2/library/sys.html), [math](https://docs.python.org/2/library/math.html), and [random](https://docs.python.org/2/library/random.html).\n",
    "\n",
    "We will use a small subset of a data set containing about 300 documents. We will not be providing the raw data but instead will present the data as two document vector text files. These files are in the form where each line corresponds to a new document and each line has word indices (\"index\" referring to a specific word in a document) and the frequency or how often that word appears in the given document. This is presented in the form \n",
    "\n",
    "$wordIndex_{0}$:$frequency_{0}$ $wordIndex_{1}$:$frequency_{1}$ ... $wordIndex_{n}$:$frequency_{n}$\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Extracting Word Vectors](#Extracting-Word-Vectors)\n",
    "- [Data Structure Initialization](#Data-Structure-Initialization)\n",
    "- [Baseline K-Means Algorithm](#Baseline-K-Means-Algorithm)\n",
    "- [Diagnostics](#Diagnostics)\n",
    "- [Improving The Model: Similarity Metrics](#Improving The Model: Similarity Metrics)\n",
    "- [Improving The Model: K-Means++ Initialization](#Improving-The-Model:-K-Means++-Initialization)\n",
    "- [Improving The Model: Miscellaneous Approaches](#Improving-The-Model:-Miscellaneous Approaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first begin by extracting the data we want from our document text file. Note, as mentioned above that we will not be calculating the word frequency itself from the raw text but instead assume that the data is already in the form mentioned above.\n",
    "\n",
    "Therefore the first step is to simply extract the data from our text file, \"train-document-vectors.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_file = \"train-document-vectors.txt\"\n",
    "with open(doc_file, \"r\") as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code simply reads every line of the file as a string, where content is an array of strings corresponding to a given document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this raw vector, can we clean it up for use in our clustering algorithm? In fact we can, we will do this using two data structures. \n",
    "\n",
    "The first being similar to our \"content\" array, is an array of arrays of wordIndex/frequency tuples we will call our \"vectors\". We can convert this easy enough from the strings by using the \"split\" function to seperate the words in each line and then splitting the wordIndex from its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializes vectors in the form of (wordIdx,frequency) array.\n",
    "def initVectors ():\n",
    "    vectors = [[]] * len(content)\n",
    "    for i in range(0,len(content)):\n",
    "        words = content[i].split(\" \")\n",
    "        words = words[0:(len(words)-1)]\n",
    "        vector = [0] * len(words)\n",
    "        for j in range(0,len(words)):\n",
    "            wFreq = words[j].split(\":\")\n",
    "            wFreq = (int(wFreq[0]),float(wFreq[1]))\n",
    "            vector[j] = wFreq\n",
    "        vectors[i] = map(tuple,sorted(vector))\n",
    "    return(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define a more efficient structure that will help speed up our clustering algorithm. We will define a python dictionary of dictionaries where instead of each element being the document, we will have each key be a wordIndex that will refer to a dictionary of documents (where said word appears) as keys with the frequency in which it appears in said document as its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializes dictionary for use cosine similarity calculation;\n",
    "# Of the form Dictionary {wordIdx : Dictionary (Doc : Frequency}}.\n",
    "def initDict ():\n",
    "    D = {}\n",
    "    for i in range(0,len(content)):\n",
    "        words = content[i].split(\" \")\n",
    "        words = words[0:(len(words)-1)]\n",
    "        for j in range(0,len(words)):\n",
    "            word = words[j].split(\":\")\n",
    "            word = (int(word[0]),float(word[1]))\n",
    "            if(word[0] not in D):\n",
    "                D[word[0]] = {i:word[1]}\n",
    "            else:\n",
    "                D[word[0]][i] = word[1]\n",
    "    return(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with these two we can now initialize the data structures we will need to implement our K-means clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors = initVectors()\n",
    "dictionary = initDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline K-Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data initialized we can now begin to think about K-Means Clustering. First of all, what is K-means clustering?\n",
    "\n",
    "In K-means we will define a number, K, of clusters or categories we reasonably believe the documents can be clasified into. Knowing this number, K, we then have a simple set of tasks to perform.\n",
    "\n",
    "1. We must initialize K centroids. We begin by choosing K random documents as our centroids, later we will discuss a more efficient means of initialization.\n",
    "2. Next, we must initialize the cluster membership of every document. We do this by determing the \"similarity\" each document has to a given centroids and then assign membership to the centroid that has the higher \"similarity\". We first define similarity as the centroid that maximizes the cosine similarity, however we will later show other methods for determining similarity.\n",
    "3. Next, we must update K new centroids for our cluster by finding centroids that represent the mean word frequency for all words present in the current cluster.\n",
    "4. We then, with our new clusters calculate the new cluster membership of all documents.\n",
    "5. We continue steps 3-4 until a certain \"stopping criterion\" is met. For our purposes we will simply define a set number of iterations we wish for our algorithm to run.\n",
    "\n",
    "Let's begin by defining the methods that we will need for our algorithm. Because we are dealing with a sparse matrix in the form of a dictionary and an array we have to think of different ways to calculate things like mangitude and the cosine similarity.\n",
    "\n",
    "We begin by calculating the magnitude of a document vector and dot product of two vectors below. The dot product notice uses the dictionary data structure we defined above with indices to denote a specific vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculates magnitude of a document vector.\n",
    "def magnitude (doc):\n",
    "    return(math.sqrt(sum(map(lambda x: x[1]*x[1],doc))))\n",
    "\n",
    "# Calculates the dot prodict of two vectors given by their docIdxs, L0 and L1. \n",
    "def dotProduct (L0,L1,V,D):\n",
    "    return(sum(map(lambda x: D[x[0]][L0]*D[x[0]][L1] if L1 in D[x[0]] else 0,V[L0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define the similarity metric we will use for this implementation. This will involve defining two cosine similarity functions, one for the centroid initilization step (where we define a specific document as our centroid) and one for the update step (where we may not have a specific document centroid, i.e. our centroid is the mean of all words in the cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given 2 docIdx's doc0 and doc1 calculates Cosine Similarity.\n",
    "def cosineSim (doc0,doc1,V,D):\n",
    "    num = dotProduct(doc0,doc1,V,D)\n",
    "    denom = magnitude(V[doc0]) * magnitude(V[doc1])\n",
    "    return(num/denom)\n",
    "\n",
    "# Given a docIdx doc0 and a centroid D1, calculates Cosine Similarity.\n",
    "def cosineSim2 (doc0,D1,V,D):\n",
    "    num = sum(map(lambda x: D1[x]*D[x][doc0] if doc0 in D[x] else 0,D1))\n",
    "    denom = magnitude(V[doc0]) * math.sqrt(sum(map(lambda x: D1[x]*D1[x],D1)))\n",
    "    return(num/denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing all this we can now implement our K-means algorithm for clustering our documents as noted above. This will return both an array of the final K centroids, and an array determing the cluster membership of every document in the corpus.\n",
    "\n",
    "We also included print statements (verbose) for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculates K-Means Clusters and centroids.\n",
    "\n",
    "def baselineKMeans(D,V,k,iters,sim,sim2,init,verbose=False):\n",
    "    \n",
    "    # Step 1: Random initial centroids\n",
    "    centroidIdxs = init(D,V,k)\n",
    "    centroids = map(lambda vi: dict(V[vi]),centroidIdxs)\n",
    "    \n",
    "    # Step 2: Initialize clusters\n",
    "    clusters = [0] * len(V)\n",
    "    for i in range(0,len(V)):\n",
    "        cs = map(lambda c: sim (i,c,V,D),centroidIdxs)\n",
    "        clusters[i] = cs.index(max(cs))\n",
    "        \n",
    "    # Step 3-5: Update Step\n",
    "    for i in range(0,iters):\n",
    "        if (verbose):\n",
    "            print \"Iteration: \", i\n",
    "        \n",
    "        # Step 3: Update Centroids\n",
    "        for c in range(0,k):\n",
    "            cluster = [index for index,value in enumerate(clusters) if value == c]\n",
    "            for doc in cluster:\n",
    "                for word in V[doc]:\n",
    "                    if(word[0] in centroids[c]):\n",
    "                        centroids[c][word[0]] += D[word[0]][doc]\n",
    "                    else:\n",
    "                        centroids[c][word[0]] = D[word[0]][doc]\n",
    "            centroids[c] = dict(map(lambda (k,v): (k, v/len(clusters)), centroids[c].iteritems()))           \n",
    "        if (verbose):\n",
    "            print \"Centroids Updated\"\n",
    "        \n",
    "        # Step 4: Update Clusters\n",
    "        def f (j):\n",
    "            cs = map(lambda C: sim2 (j,C,V,D),centroids)\n",
    "            return(cs.index(max(cs)))\n",
    "        clusters = map(lambda j: f(j),range(0,len(V)))\n",
    "        if (verbose):\n",
    "            print \"Cluster Membership Updated\"\n",
    "        \n",
    "    return(centroids,clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with all this we can finally begin classify our corpus of documents. We will begin by defining K=30 and have our stopping criterion be 10 iterations of the algorithm. (Notice we input a function for randomly initializing our centroids, later we will show a different way to initialize our centroids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Centroids Updated\n",
      "Cluster Membership Updated\n",
      "Iteration:  1\n",
      "Centroids Updated\n",
      "Cluster Membership Updated\n",
      "Iteration:  2\n",
      "Centroids Updated\n",
      "Cluster Membership Updated\n",
      "Iteration:  3\n",
      "Centroids Updated\n",
      "Cluster Membership Updated\n",
      "Iteration:  4\n",
      "Centroids Updated\n",
      "Cluster Membership Updated\n",
      "Iteration:  5\n",
      "Centroids Updated\n",
      "Cluster Membership Updated\n",
      "Iteration:  6\n",
      "Centroids Updated\n",
      "Cluster Membership Updated\n",
      "Iteration:  7\n",
      "Centroids Updated\n",
      "Cluster Membership Updated\n",
      "Iteration:  8\n",
      "Centroids Updated\n",
      "Cluster Membership Updated\n",
      "Iteration:  9\n",
      "Centroids Updated\n",
      "Cluster Membership Updated\n"
     ]
    }
   ],
   "source": [
    "K = 30\n",
    "stopping_criterion = 10\n",
    "def randomInit (D,V,k):\n",
    "    return(random.sample(range(0,len(V)),k))\n",
    "(centers,clusters) = baselineKMeans(dictionary,vectors,K,stopping_criterion,cosineSim,cosineSim2,randomInit,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our centers and cluster memberships defined we can now perform diagnostics to assess the accuracy of our classification algorithm by comparing to our text file, and assesing the macro F1 scores of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have run our algorithm, how can we assess the accuracy of our model? One way we can do this is by using a F1 score compared to file with the correct clusters (https://en.wikipedia.org/wiki/F1_score).\n",
    "\n",
    "We will do this by first reading in our text file \"verify-document-vectors.txt\" file and then calculating the F1 macro score as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verify_file = \"verify-document-vectors.txt\"\n",
    "with open(verify_file, \"r\") as f:\n",
    "    content2 = f.readlines()\n",
    "    \n",
    "# Initialize cluster membership list and verify .txt file\n",
    "# to be used in calculating F1 score.\n",
    "def initializeComparison(estimated,actual):\n",
    "    verifyDict = dict()\n",
    "    eventCounter = 0\n",
    "    docCounter = 0\n",
    "    verifyClusters = []\n",
    "    trainClusters = []\n",
    "    \n",
    "    doc = 0\n",
    "    for cluster in estimated:\n",
    "        cluster = cluster\n",
    "        while len(trainClusters) <= cluster:\n",
    "            trainClusters.append([])\n",
    "        trainClusters[cluster].append(doc)\n",
    "        doc += 1\n",
    "    \n",
    "    for line in actual:\n",
    "        cluster = line.strip()\n",
    "\n",
    "        if cluster != \"unlabeled\":\n",
    "            if cluster not in verifyDict:\n",
    "                verifyDict[cluster] = eventCounter\n",
    "                eventCounter += 1\n",
    "            clusterID = verifyDict[cluster]\n",
    "            while len(verifyClusters) <= clusterID:\n",
    "                verifyClusters.append([])\n",
    "            verifyClusters[clusterID].append(docCounter)\n",
    "\n",
    "        docCounter += 1\n",
    "    return(trainClusters,verifyClusters)\n",
    "\n",
    "# Calculate F1 Score between Training clusters and Verified clusters\n",
    "def findMacroF1 (trainClusters,verifyClusters):\n",
    "    clusterF1s = []\n",
    "    \n",
    "    for verifyCluster in verifyClusters:\n",
    "        bestF1 = -1\n",
    "\n",
    "        for trainCluster in trainClusters:\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            fn = 0\n",
    "            for item in verifyCluster:\n",
    "                if item in trainCluster:\n",
    "                    tp += 1.0\n",
    "                else:\n",
    "                    fn += 1.0\n",
    "            for item in trainCluster:\n",
    "                if item not in verifyCluster:\n",
    "                    fp += 1.0\n",
    "            # if none match, just ignore\n",
    "            if tp == 0:\n",
    "                continue\n",
    "            precision = tp / (tp+fp)\n",
    "            recall = tp / (tp+fn)\n",
    "            f1 = 2*precision*recall/(precision+recall)\n",
    "            if f1 > bestF1:\n",
    "                bestF1 = f1\n",
    "        clusterF1s.append(bestF1)\n",
    "        \n",
    "    macroF1 = 0\n",
    "    for item in clusterF1s:\n",
    "        macroF1 += item\n",
    "    macroF1 = macroF1 / len(clusterF1s)\n",
    "    return(macroF1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have read the actual membership, initialized the clusters, and defined a function for finding the macro F1 score we can now test the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3714560011648377"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train,verify) = initializeComparison(clusters,content2)\n",
    "findMacroF1 (train,verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now doing this we get a decent F1 Macro score, but can we do better? There are many different methods, including adjusting the cluster size, K, and other techniques. Below we will go through some simple ways to improve upon our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving The Model: Similarity Metrics\n",
    "\n",
    "Above we utilized the cosine similarity between two documents to determine the similarity, but this need not be our only means. Below we will look at using the dot product similarity and the more typical euclidean distance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start with the dot product similarity. This, and the euclidean distance metric, are both fairly similar to calculate as the cosine similarity so we will use a similar functions as we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given 2 docIdx's doc0 and doc1 and a boolean determining whether to\n",
    "# calculate the dot product or euclidean distance, calculates similarity.\n",
    "def dotSim (doc0,doc1,V,D):\n",
    "    sim = dotProduct(doc0,doc1,V,D)\n",
    "    return(sim)\n",
    "\n",
    "def euclideanSim (doc0,doc1,V,D):\n",
    "    sim = math.sqrt((sum(map(lambda x: (D[x[0]][doc0]-D[x[0]][doc1])**2 if doc1 in D[x[0]] else 0,V[doc0]))))\n",
    "    return(sim)\n",
    "\n",
    "# Given a docIdx doc0 and a centroid D1, and a boolean determining whether to\n",
    "# calculate the dot product or euclidean distance, calculates similarity.\n",
    "def dotSim2 (doc0,D1,V,D):\n",
    "    sim = sum(map(lambda x: D1[x]*D[x][doc0] if doc0 in D[x] else 0,D1))\n",
    "    return(sim)\n",
    "\n",
    "def euclideanSim2 (doc0,D1,V,D):\n",
    "    sim = math.sqrt(sum(map(lambda x: (D1[x]-D[x][doc0])**2 if doc0 in D[x] else 0,D1)))\n",
    "    return(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with these two different metrics defined we can now test the accuracy of the model with these two metrics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03330337240988559"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(centers,clusters) = baselineKMeans(dictionary,vectors,K,stopping_criterion,dotSim,dotSim2,randomInit)\n",
    "(train,verify) = initializeComparison(clusters,content2)\n",
    "findMacroF1 (train,verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2665283175465976"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(centers,clusters) = baselineKMeans(dictionary,vectors,K,stopping_criterion,euclideanSim,euclideanSim2,randomInit)\n",
    "(train,verify) = initializeComparison(clusters,content2)\n",
    "findMacroF1 (train,verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these two methods, we see that we have not significantly improved our performance, in fact in both cases we performed significantly worse, is there any other hope to improve our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving The Model: K-Means++ Initialization\n",
    "\n",
    "Before we mentioned that in setting our initial clusters we simply randomly assign documents to be a centroid. However, if we wish to have initial centroids that are significantly different from each other at the beginning, it could go a ways to improving the accuracy.\n",
    "\n",
    "This is the intuition behind the K-Means++ initialization approach (https://en.wikipedia.org/wiki/K-means%2B%2B). Here we modify Step 1 of our previous baseline approach using this new initialization technique as outlined below.\n",
    "\n",
    "1. Choose one center uniformly at random from among the data points.\n",
    "2. For each data point x, compute D(x), the distance between x and the nearest center that has already been chosen.\n",
    "3. Choose one new data point at random as a new center, using a weighted probability distribution where a point x is chosen with probability proportional to D(x)^2.\n",
    "4. Repeat Steps 2 and 3 until k centers have been chosen.\n",
    "\n",
    "After these steps we then work through the standard baseline k-means algorithm outlined previously. Below we define a function for the initialization step. Notice because of the relative poor performance of our other similarity metrics, we will be using cosine similarity as the \"distance\" or similarity metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performs K-Means++ initialization step. Calculates next centroid by\n",
    "# choosing from a random distribution where each centroid has probability\n",
    "# (1-CosineSimilarity)^2 of being chosen.\n",
    "def initKPlus (D,V,k):\n",
    "    centers = []\n",
    "    nearestDists = [0] * len(V)\n",
    "    centers = centers + [random.randrange(0,len(V))]\n",
    "    for i in range(1,k):\n",
    "        #print \"Calculating Centroid: \", i\n",
    "        for j in range(0,len(V)):\n",
    "            cs = map(lambda c: 1 - cosineSim (j,c,V,D),centers)\n",
    "            nearestDists[j] = max(cs)\n",
    "        r = random.uniform(0.0,sum(map(lambda x: x*x,nearestDists)))\n",
    "        p = 0\n",
    "        #print nearestDists\n",
    "        for d in range(0,len(nearestDists)):\n",
    "            #print r, p, nearestDists[d]\n",
    "            if r>= p and r <= p+(nearestDists[d]*nearestDists[d]):\n",
    "                centers = centers + [d]\n",
    "            p += nearestDists[d]*nearestDists[d]\n",
    "    return(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the above funtion we simply need to call our original algorithm with the new initialization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4669066843784131"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(centers,clusters) = baselineKMeans(dictionary,vectors,K,stopping_criterion,cosineSim,cosineSim2,initKPlus)\n",
    "(train,verify) = initializeComparison(clusters,content2)\n",
    "findMacroF1 (train,verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method thus, while adding some time for initialization as shown above, greatly increases the accuracy of our model at K=30 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Model: Miscellaneous Approaches\n",
    "\n",
    "We have now seen a few main approaches to increase accuracy in k-means clustering. Lastly we will look at some general miscellaneous approaches.\n",
    "\n",
    "One approach to improving accuracy can be to instead of looking at the raw frequency of words, as we have been doing in this text classification approach, we instead look at the term frequency–inverse document frequency, or TF-IDF (https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Using this approach, we hoped to normalize the data and thus have features that more accurately reflect the ratio and quality of different documents.\n",
    "\n",
    "We can do this by representing the new weights using the formula below,\n",
    "\n",
    "$d_{tf-idf} = (tf_{1} \u0003 log(N/df_{1}), tf_{2} \u0003 log(N/df_{2}), ..., tf_{n} \u0003 log(N/df_{n})$\n",
    "\n",
    "Where $tf_{i}$ represents the current weights, $N$ represents the number of total documents, and df_{i} represents the document frequency of a certain word.\n",
    "\n",
    "Knowing this we can begin by replacing the weights in the dictionary and vector data structures we defined previously to this tf-idf representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculates new TF-IDF weights to replace raw frequency weights in the\n",
    "# dictionary D and array of vectors, V.\n",
    "def newWeights(D,V):\n",
    "    (nuV,nuD) = (initVectors (),initDict ())\n",
    "    for w in D:\n",
    "        for d in D[w]:\n",
    "            weight = D[w][d] * math.log(len(V) / len(D[w]))\n",
    "            nuD[w][d] = weight\n",
    "            i = [index for index,value in enumerate(V[d]) if value[0] == w][0]\n",
    "            nuV[d][i] = (nuV[d][i][0],weight)\n",
    "    return(nuV,nuD)\n",
    "\n",
    "(weightedVectors,weightedDictionary) = newWeights(dictionary,vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After replacing the weights as defined above we can now assess the accuracy of our new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4422692040188081"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(centers,clusters) = baselineKMeans(weightedDictionary,weightedVectors,K,stopping_criterion,cosineSim,cosineSim2,randomInit)\n",
    "(train,verify) = initializeComparison(clusters,content2)\n",
    "findMacroF1 (train,verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an improvement to our original model and when coupled with our initialization technique can be very useful.\n",
    "\n",
    "Some other methods we could use to improve accuracy can be to increase the amount of iterations we perform in our clustering algorithm, or better yet, change the stopping criterion from being an arbitrary amount of iterations to be instead reflecting something we may want to minimize (such as the distance centroids move, for example).\n",
    "\n",
    "Another obvious technique is to experiment with the size of K, clusters. Here we have been using the same K=30 clusters for all methods. We will end off with the results we get from modifying the size of K using the baseline approach\n",
    "\n",
    "![title](Kclusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and references\n",
    "\n",
    "This tutorial highlighted just a few methods for K-Means clustering in python for text classification, for further resources on the material above check the following links!\n",
    "\n",
    "1. KMeans Clustering: \n",
    "https://en.wikipedia.org/wiki/K-means_clustering,\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "2. Diagnostics: https://en.wikipedia.org/wiki/F1_score\n",
    "3. Similarity Metrics:\n",
    "https://en.wikipedia.org/wiki/Euclidean_distance\n",
    "https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "4. KMeans++: https://en.wikipedia.org/wiki/K-means%2B%2B\n",
    "5. Tf-Idf Representation:\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.7900&rep=rep1&type=pdf\n",
    "6. Clustering Algorithms for Text Mining: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.7900&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
