{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS OF MOVIE REVIEWS USING THE NLTK PACKAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION\n",
    "\n",
    "This tutorial will introduce to you some basic methods for sentiment analysis of documents. Sentiment analysis is the process of determining whether a peice of text is positive, negative or neutral. It is often also called opinion mining. It can be particularly useful to find out the opinion of people about certain topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NATURAL LANGUAGE TOOLKIT\n",
    "\n",
    "Natural Language Toolkit is an extremely useful library for dealing with human language data. This is most useful for text processing and text analysis and would  form the basis of my program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMINING THE DATA  \n",
    "\n",
    "The dataset called the sentence polarity dataset v1.0, has been taken from the website of Professor Bo Pang of Cornell University. It essentially consists of 5331 positive movie reviews and 5331 negative movie reviews. He has obtained the dataset from the movie review website Rotten Tomatoes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE SENTIMENT ANALYSIS PROCESS\n",
    "\n",
    "The primary package that will be used for this process will be the nltk Package.\n",
    "\n",
    "The end goal of this process is to assign a positive or negative sentiment to each movie review while evaulating multiple feature selection mechanisms and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re, math, collections, itertools\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment.util import * \n",
    "import math\n",
    "import string\n",
    "from nltk import precision\n",
    "from nltk import recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posReviews = open('rt-polarity-pos.txt', 'rU')\n",
    "negReviews = open('rt-polarity-neg.txt', 'rU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better understanding of what the dataset looks like, I'll print out the first 5 reviews for each of the positive\n",
    "reviews file and the negative reviews file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Reviews\n",
      "\n",
      "['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \\n', 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth . \\n', 'effective but too-tepid biopic\\n', 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \\n', \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \\n\"] \n",
      "\n",
      "Negative Reviews \n",
      "\n",
      "['simplistic , silly and tedious . \\n', \"it's so laddish and juvenile , only teenage boys could possibly find it funny . \\n\", 'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \\n', '[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \\n', 'a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \\n']\n"
     ]
    }
   ],
   "source": [
    "with open(\"rt-polarity-pos.txt\") as myfile:\n",
    "    head = [next(myfile) for x in xrange(5)]\n",
    "print (\"Positive Reviews\\n\")\n",
    "print head,'\\n'\n",
    "with open(\"rt-polarity-neg.txt\") as myfile:\n",
    "    head = [next(myfile) for x in xrange(5)]\n",
    "print (\"Negative Reviews \\n\")\n",
    "print head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the reviews to make them suitable for sentiment analysis. I'll  pass the reviews through the process  function, in which multiple operations would be applied to the text passed to it. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "In the process function, several operaitons are applied to the text fed to the function. Apart from removing the stopwords, all the punctuations are removed from the reviews. All the text is converted to the lower case. Also, the text is first lemmatized and then tokenized.  Lemmatization is the process of bringing the words to their root form. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    doc = text\n",
    "    doc = doc.lower()\n",
    "    doc = doc.replace(\"\\'s\",'')\n",
    "    doc = doc.replace(\"\\'\",'')\n",
    "    processed =''\n",
    "    tokens1 = []\n",
    "    \n",
    "    exclude = set(string.punctuation)\n",
    "    for ch in doc:\n",
    "        if ch in exclude:\n",
    "            processed = processed+ ' '\n",
    "        else:\n",
    "            processed = processed + ch\n",
    "\n",
    "    tokens = nltk.word_tokenize(processed)\n",
    "    new = stopwords.words('english')\n",
    "    \n",
    "    tokens = list(set(tokens) - set(new))\n",
    "    \n",
    "    for i in tokens:\n",
    "        try:\n",
    "            tokens1.append(str(lemmatizer.lemmatize(i)))\n",
    "        except:\n",
    "            pass\n",
    "    return tokens1       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can start the process of feature evaluation.'Features' in the sentiment analysis context refer to the text that you are analyzing that would correlate to the labels. Features in this case are the words of the movie reviews. The first task after processing the reviews is to evaluate different feature selection mechanisms using different methods to get different subset of the features from the reviews and then evaluating those features. I will build up two variables with positive and negative features each in the next part. Those would contain the output of the feature selection mechanism depending on whether the review it would draw is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below is a function written for feature evaluation. It essentially first builds up two lists, each with postive features of the positive reviews and negative features of the negative reviews while appending positive and negative to each review according to the dataset they belong to. Then training and test data sets are\n",
    "built with 75 percent and 25 percent of the data respectively and a Naive Bayes classifier is trained. The classifier is then used to predict labels for the test dataset.  \n",
    "\n",
    "In the latter part of the function, I iterate through the test dataset with i being an arbitrary identifier, feat being the features and labels(positive or negative), building up two sets, the labelset(actual labels) and testSet(predicted labels). This is done to assist in calculating various metrics for the classification which would help me in measuring how well the trained model did. Various metrics that are published are accuracy, precision and recall. \n",
    "Accuracy gives the percentage ratio of the number of reviews correctly labeled.  Precision gives the measure of positive predictions that are correct whereas recall gives a measure of how good a classifier is at detecting the positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_evaluation(features):\n",
    "    \n",
    "    posReviews = open('rt-polarity-pos.txt', 'rU')\n",
    "    negReviews = open('rt-polarity-neg.txt', 'rU')\n",
    "    \n",
    "    positiveFeatures = []\n",
    "    negativeFeatures = []\n",
    "    \n",
    "    for i in posReviews:\n",
    "        pWord = process(i)\n",
    "        positiveWords = [features(pWord), 'positive']\n",
    "        positiveFeatures.append(positiveWords)\n",
    "    for i in negReviews:\n",
    "        nWord = process(i)\n",
    "        negativeWords = [features(nWord), 'negative']\n",
    "        negativeFeatures.append(negativeWords)\n",
    "\n",
    "    pCut = int(math.floor(len(positiveFeatures)*3/4))\n",
    "    nCut = int(math.floor(len(negativeFeatures)*3/4))\n",
    "    \n",
    "    trainData = positiveFeatures[:pCut] + negativeFeatures[:nCut]\n",
    "    testData = positiveFeatures[pCut:] + negativeFeatures[nCut:]\n",
    "\n",
    "    #training a Naive Bayes Classifier\n",
    "    nBClassifier = NaiveBayesClassifier.train(trainData)\n",
    "\n",
    "    #initiating the label set and the test Set\n",
    "    labelSet = collections.defaultdict(set)\n",
    "    testSet = collections.defaultdict(set)\n",
    "\n",
    "    #puts sentences with correct labels in the labeled Set and the predicted values in the testset\n",
    "    for i, (feat, lab) in enumerate(testData):\n",
    "        labelSet[lab].add(i)\n",
    "        predictedValues = nBClassifier.classify(feat)\n",
    "        testSet[predictedValues].add(i)\t\n",
    "\n",
    "    print 'training done on %d instances, testing done on %d instances' % (len(trainData), len(testData))\n",
    "    print 'accuracy = ', nltk.classify.util.accuracy(nBClassifier, testData)\n",
    "    print 'positive precision = ', precision(labelSet['positive'], testSet['positive'])\n",
    "    print 'positive recall = ', recall(labelSet['positive'], testSet['positive'])\n",
    "    print 'negative precision =', precision(labelSet['negative'], testSet['negative'])\n",
    "    print 'negative recall = ', recall(labelSet['negative'], testSet['negative'])\n",
    "    nBClassifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The add_check function below is a basic feature selection mechanism which just builds up a dictionary adds a true boolean check as the value and words passed to it as keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_check(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now check how effective this feature selection mechanism is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done on 7996 instances, testing done on 2666 instances\n",
      "accuracy =  0.763315828957\n",
      "positive precision =  0.765105740181\n",
      "positive recall =  0.759939984996\n",
      "negative precision = 0.761549925484\n",
      "negative recall =  0.766691672918\n",
      "Most Informative Features\n",
      "              engrossing = True           positi : negati =     17.0 : 1.0\n",
      "                   quiet = True           positi : negati =     15.7 : 1.0\n",
      "                mediocre = True           negati : positi =     13.7 : 1.0\n",
      "               absorbing = True           positi : negati =     13.0 : 1.0\n",
      "              refreshing = True           positi : negati =     13.0 : 1.0\n",
      "                 triumph = True           positi : negati =     13.0 : 1.0\n",
      "                portrait = True           positi : negati =     12.4 : 1.0\n",
      "               inventive = True           positi : negati =     12.3 : 1.0\n",
      "            refreshingly = True           positi : negati =     11.7 : 1.0\n",
      "                    flat = True           negati : positi =     11.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "feature_evaluation(add_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The add_check mechanism gives us an accuracy of 76.33% which is a good measure. The precision and recall values are close to each other which means the model is predicting everything pretty evenly. \n",
    "\n",
    "The most informative features gives us what are the chances of a review being positive or negative with a particular word being in it. For example, if there is the word engrossing in a reviews, there is a 17 to 1 chance of the review being postive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, another way of feature generation is attempted. In this attempted only N most informative features are generated. The N features that convey the most information. Hence, we need to calculate the information gain of each word. The scores_words function will do precisely just that. \n",
    "\n",
    "Two frequency distributions are used in the below function. One is a Frequency distribution which is used to store the frequency of all the words and the other is conditional frequency distribution which is used to separately store the frequencies of the positive and negative words.\n",
    "Chi Squared tests are used to calculate the information gain. More can be read about Chi Squared tests here (http://www2.lv.psu.edu/jxm57/irp/chisquar.html). The postive information gain and negative information gain for  is calculated, added up and stored in a dictionary linked to the word which is then returned by this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scores_words():\n",
    "    posReviews = open('rt-polarity-pos.txt', 'rU')\n",
    "    negReviews = open('rt-polarity-neg.txt', 'rU')\n",
    "    \n",
    "    positiveWords = []\n",
    "    negativeWords = []\n",
    "\n",
    "    for i in posReviews:\n",
    "\n",
    "        pWord = process(i)\n",
    "        positiveWords.append(pWord)\n",
    "\n",
    "    for i in negReviews:\n",
    "        nWord = process(i)\n",
    "        negativeWords.append(nWord)\n",
    "\n",
    "    #making the positive word list and the negative word list iterable\n",
    "    positiveWords = list(itertools.chain(*positiveWords))\n",
    "    negativeWords = list(itertools.chain(*negativeWords))\n",
    "\n",
    "    wordsFD = FreqDist()\n",
    "    pnWordsFD = ConditionalFreqDist()\n",
    "\n",
    "    for k in positiveWords:\n",
    "        wordsFD[k] += 1\n",
    "        pnWordsFD['positive'][k] += 1\n",
    "    for word in negativeWords:\n",
    "        wordsFD[k] += 1\n",
    "        pnWordsFD['negative'][k] += 1\n",
    "\n",
    "    #finding the count of negative and positive words and total word count \n",
    "    pWCount = pnWordsFD['positive'].N()\n",
    "    nWCount = pnWordsFD['negative'].N()\n",
    "    tWCount = pWCount + nWCount\n",
    "\n",
    "    scores = {}\n",
    "    for wd, frequency in wordsFD.iteritems():\n",
    "\n",
    "        positiveScore = BigramAssocMeasures.chi_sq(pnWordsFD['positive'][wd], (frequency, pWCount), tWCount)\n",
    "        negativeScore = BigramAssocMeasures.chi_sq(pnWordsFD['negative'][wd], (frequency, nWCount), tWCount)\n",
    "        scores[wd] = positiveScore + negativeScore\n",
    "\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "The find best words function below finds the best N features, given the set of all the words and scores and the value of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = scores_words()\n",
    "\n",
    "def findBestWords(scores, no):\n",
    "    bestValues = sorted(scores.iteritems(), key=lambda (m, n): n, reverse=True)[:no]\n",
    "    bestWords = set([m for m, n in bestValues])\n",
    "    return bestWords\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best features function below will return true only for words that are in the best words list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates mechanism for feature selection that will only use the best words\n",
    "def bestFeatures(wds):\n",
    "    return dict([(wd, True) for wd in wds if wd in bestWords])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block of code, I will attempt different number of features ranging from 2000 to 15000 to notice how does the accuracy vary with the number of top N informative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating the top 2000 word features\n",
      "training done on 7996 instances, testing done on 2666 instances\n",
      "accuracy =  0.708552138035\n",
      "positive precision =  0.664887307236\n",
      "positive recall =  0.84096024006\n",
      "negative precision = 0.783673469388\n",
      "negative recall =  0.576144036009\n",
      "Most Informative Features\n",
      "              engrossing = True           positi : negati =     17.0 : 1.0\n",
      "                   quiet = True           positi : negati =     15.7 : 1.0\n",
      "               absorbing = True           positi : negati =     13.0 : 1.0\n",
      "              refreshing = True           positi : negati =     13.0 : 1.0\n",
      "                 triumph = True           positi : negati =     13.0 : 1.0\n",
      "                portrait = True           positi : negati =     12.4 : 1.0\n",
      "               inventive = True           positi : negati =     12.3 : 1.0\n",
      "            refreshingly = True           positi : negati =     11.7 : 1.0\n",
      "                 culture = True           positi : negati =     11.0 : 1.0\n",
      "               affecting = True           positi : negati =     11.0 : 1.0\n",
      "evaluating the top 5000 word features\n",
      "training done on 7996 instances, testing done on 2666 instances\n",
      "accuracy =  0.720555138785\n",
      "positive precision =  0.681481481481\n",
      "positive recall =  0.828207051763\n",
      "negative precision = 0.781070745698\n",
      "negative recall =  0.612903225806\n",
      "Most Informative Features\n",
      "              engrossing = True           positi : negati =     17.0 : 1.0\n",
      "                   quiet = True           positi : negati =     15.7 : 1.0\n",
      "               absorbing = True           positi : negati =     13.0 : 1.0\n",
      "                 triumph = True           positi : negati =     13.0 : 1.0\n",
      "              refreshing = True           positi : negati =     13.0 : 1.0\n",
      "                portrait = True           positi : negati =     12.4 : 1.0\n",
      "               inventive = True           positi : negati =     12.3 : 1.0\n",
      "            refreshingly = True           positi : negati =     11.7 : 1.0\n",
      "                    flat = True           negati : positi =     11.4 : 1.0\n",
      "                  boring = True           negati : positi =     11.3 : 1.0\n",
      "evaluating the top 10000 word features\n",
      "training done on 7996 instances, testing done on 2666 instances\n",
      "accuracy =  0.718304576144\n",
      "positive precision =  0.696091644205\n",
      "positive recall =  0.774943735934\n",
      "negative precision = 0.746192893401\n",
      "negative recall =  0.661665416354\n",
      "Most Informative Features\n",
      "              engrossing = True           positi : negati =     17.0 : 1.0\n",
      "                   quiet = True           positi : negati =     15.7 : 1.0\n",
      "                mediocre = True           negati : positi =     13.7 : 1.0\n",
      "               absorbing = True           positi : negati =     13.0 : 1.0\n",
      "                 triumph = True           positi : negati =     13.0 : 1.0\n",
      "              refreshing = True           positi : negati =     13.0 : 1.0\n",
      "                portrait = True           positi : negati =     12.4 : 1.0\n",
      "               inventive = True           positi : negati =     12.3 : 1.0\n",
      "            refreshingly = True           positi : negati =     11.7 : 1.0\n",
      "                    flat = True           negati : positi =     11.4 : 1.0\n",
      "evaluating the top 15000 word features\n",
      "training done on 7996 instances, testing done on 2666 instances\n",
      "accuracy =  0.712678169542\n",
      "positive precision =  0.694311172036\n",
      "positive recall =  0.759939984996\n",
      "negative precision = 0.73487986744\n",
      "negative recall =  0.665416354089\n",
      "Most Informative Features\n",
      "              engrossing = True           positi : negati =     17.0 : 1.0\n",
      "                   quiet = True           positi : negati =     15.7 : 1.0\n",
      "                mediocre = True           negati : positi =     13.7 : 1.0\n",
      "               absorbing = True           positi : negati =     13.0 : 1.0\n",
      "                 triumph = True           positi : negati =     13.0 : 1.0\n",
      "              refreshing = True           positi : negati =     13.0 : 1.0\n",
      "                portrait = True           positi : negati =     12.4 : 1.0\n",
      "               inventive = True           positi : negati =     12.3 : 1.0\n",
      "            refreshingly = True           positi : negati =     11.7 : 1.0\n",
      "                    flat = True           negati : positi =     11.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#no. of features to select\n",
    "\n",
    "numbers = [2000, 5000, 10000, 15000]\n",
    "\n",
    "for no in numbers:\n",
    "  \n",
    "    print 'evaluating the top %d word features' % (no)\n",
    "    \n",
    "    bestWords = findBestWords(scores, no)\n",
    "    feature_evaluation(bestFeatures)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It is observed that accuracy for all the iterations of number of features tested is less than the initial accuracy that was achieved previously in our basic feature generation method where I used all the words. Though unexpected but this is possibly due to the fact that in the process function, the processing that is done removes the stop-words and punctuations in the cleaning up of the data and thus most un-important words have already been removed.\n",
    "\n",
    "There are other possible ways of increasing the accuracy by better feature generation. There are various operations \n",
    "that can be done through within the NLTK. Other feature selection mechanisms can be used and other classification algorithms may be attempted.  There are other operations such as chunking, chinking, n-grams etc which might prove to be useful depending upon the use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "One such powerful feature of the NLTK library is the part of speech tagging of words. Part of speech tagging refers to assigning tags to the words whether they are nouns, verbs, adjectives etc. In this next part, part of speech tags will be assigned to the words in the reviews. Assuming that only verbs, nouns, adjectives and adverbs are significant for analyzing sentiment, the remaining parts of speech would be removed from the text to make \n",
    "sentiment score more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESOURCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "Various such NLTK operations can be referred to from StreamHacker which is a great source of information and has lots \n",
    "of relevant information especially related to Sentiment analysis. Another great resource is https://pythonprogramming.net/sentiment-analysis-module-nltk-tutorial/ which has mutiple useful tutorials and examples for using different features of the NLTK."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
