{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will introduce you to theano library. Theano is a Python library that lets you to define, optimize, and evaluate mathematical expressions, especially ones with multi-dimensional arrays (numpy.ndarray). \n",
    "\n",
    "In the machine learning field, you were introduced by far the most popular library -- scikit-learn. Why do I introduce you to a seemly less popular library in the same field? Well, what sets Theano apart is that it takes advantage of the computer’s GPU in order to attain speeds rivaling hand-crafted C implementations for problems involving large amounts of data. It can also surpass C on a CPU by many orders of magnitude by taking advantage of recent GPUs. The key point is that Theano allows you to write model specifications rather than the model implementations. This is particularly useful as Theano is very well integrated into the GPU, which provides substantial speed-ups for deep learning training.\n",
    "\n",
    "How does theano relate to other mathematical libraries? Theano sits somewhere between NumPy and the Python symbolic mathematics library SymPy.\n",
    "\n",
    "To get a flavor of how to use Theano, let's have a look at simple example. It doesn’t show off many of Theano’s features, but it illustrates concretely what Theano is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = T.dscalar('x')\n",
    "y = T.dscalar('y')\n",
    "z = x + y\n",
    "f = theano.function([x, y], z)\n",
    "print np.allclose(f(15.9, 6.7), 22.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial content\n",
    "\n",
    "In this tutorial, first we will go through some basic theano operations -- algebra, derivatives, conditions and loop. Then we will apply them in an application example. Further resources are listed at the end.\n",
    "<ul>\n",
    "<li><a href='#dest1'>Algebra</a></li>\n",
    "<li><a href='#dest2'>Derivatives in Theano</a></li>\n",
    "<li><a href='#dest3'>Conditions -- ifelse and switch</a></li>\n",
    "<li><a href='#dest4'>Looping using scan</a></li>\n",
    "<li><a href='#dest5'>Application example: classifying MNIST digits using logistic regression</a></li>\n",
    "<li><a href='#dest6'>Further resources</a></li>\n",
    "<li><a href='#dest7'>Reference</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dest1'></a>\n",
    "## Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding two Scalars\n",
    "\n",
    "The simple example in the beginning shows the essentials of adding two scalars. Let's go through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1 **\n",
    "Using <i style=\"background-color:#D5F5E3\">dscalar</i>, we declare two double scalar symbols. What is tricky is that x, y are instances of TensorVariable. While their 'type' field are made TensorType.\n",
    "\n",
    "**Step 2 **\n",
    "Combine x and y into their sum z.\n",
    "\n",
    "**Step 3 **\n",
    "Create a function taking x and y as inputs and giving z as output. The first argument to function is a list of Variables that will be provided as inputs to the function. The second argument is a single Variable or a list of Variables. f may then be used like a normal Python function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding two Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar to adding two scalars. The only change from the previous example is that you need to instantiate x and y using the matrix Types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = T.dmatrix('x')\n",
    "y = T.dmatrix('y')\n",
    "z = x + y\n",
    "f = theano.function([x, y], z)\n",
    "print np.allclose(f(np.array([[1, 2], [3, 4]]), np.array([[10, 20], [30, 40]])), np.array([[11, 22], [33, 44]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dest2'></a>\n",
    "## Derivatives in theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we will use the macro <i style=\"background-color:#D5F5E3\">T.grad</i>. For instance, we can compute this complicated function:\n",
    "\\begin{equation*}\n",
    "d(s(x))/dx\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "s(x)=sum(1 / (1 + exp(-x)))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25      ,  0.19661193],\n",
       "       [ 0.19661193,  0.10499359]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = T.dmatrix('x')\n",
    "s = T.sum(1 / (1 + T.exp(-x)))\n",
    "gs = T.grad(s, x)\n",
    "dlogistic = theano.function([x], gs)\n",
    "dlogistic([[0, 1], [-1, -2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Jacobian\n",
    "\n",
    "In vector calculus, the Jacobian matrix is the matrix of all first-order partial derivatives of a vector-valued function. Check out https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant. Theano implements the theano.gradient.jacobian() macro that does all that is needed to compute the Jacobian. The following text explains how to do it manually.\n",
    "\n",
    "In order to manually compute the Jacobian of some function y with respect to some parameter x we need to use <i style=\"background-color:#D5F5E3\">scan</i>. What we do is to loop over the entries in y and compute the gradient of y[i] with respect to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.  0.]\n",
      " [ 0.  8.]]\n"
     ]
    }
   ],
   "source": [
    "x = T.dvector('x')\n",
    "y = x ** 2\n",
    "J, updates = theano.scan(lambda i, y,x : T.grad(y[i], x), sequences=T.arange(y.shape[0]), non_sequences=[y,x])\n",
    "f = theano.function([x], J, updates=updates)\n",
    "print f([2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Hessian\n",
    "\n",
    "In mathematics, the Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. Check out https://en.wikipedia.org/wiki/Hessian_matrix. Theano implements theano.gradient.hessian() macro that does all that is needed to compute the Hessian. \n",
    "\n",
    "You can compute the Hessian manually similarly to the Jacobian. The only difference is that now, instead of computing the Jacobian of some expression y, we compute the Jacobian of T.grad(cost,x), where cost is some scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  0.]\n",
      " [ 0.  2.]]\n"
     ]
    }
   ],
   "source": [
    "x = T.dvector('x')\n",
    "y = x ** 2\n",
    "cost = y.sum()\n",
    "gy = T.grad(cost, x)\n",
    "H, updates = theano.scan(lambda i, gy,x : T.grad(gy[i], x), sequences=T.arange(gy.shape[0]), non_sequences=[gy, x])\n",
    "f = theano.function([x], H, updates=updates)\n",
    "print f([2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='dest3'></a>\n",
    "## Conditions -- ifelse and switch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano provides ifelse and switch as condition statements. \n",
    "<p style=\"background-color:#D5F5E3\">ifelse(condition, var1, var2) -- if condition is true, return var1, otherwise return var2</p>\n",
    "<p style=\"background-color:#D5F5E3\">switch(tensor, var1, var2) -- if tensor is true, return var1, otherwise return var2</p>\n",
    "<p>Whereas switch evaluates both output variables, ifelse is lazy and only evaluates one variable with respect to the condition. In other words, if condition is true, ifelse only calculates var1, while switch calculates both var1 and var2.</p>\n",
    "<p>Suppose we have two scalar variables $a, $b, and two matrices $\\mathbf{x, y}$. Define a function:\n",
    "$$ \n",
    "\\mathbf z = f(a, b,\\mathbf{x, y}) = \\left\\{ \n",
    "\\begin{aligned}\n",
    "    \\mathbf x & ,\\ a <= b\\\\\n",
    "    \\mathbf y & ,\\ a > b\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "Declare variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b = T.scalars('a', 'b')\n",
    "x,y = T.matrices('x', 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build with ifelse. Use T.lt() as \"less than or equal\", T.gt() as \"greater than or equal\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_lazy = theano.ifelse.ifelse(T.lt(a, b), T.mean(x), T.mean(y))\n",
    "f_lazyifelse = theano.function([a, b, x, y], z_lazy, mode=theano.Mode(linker='vm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build with switch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_switch = T.switch(T.lt(a, b), T.mean(x), T.mean(y))\n",
    "f_switch = theano.function([a, b, x, y], z_switch, mode=theano.Mode(linker='vm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val1 = 0.\n",
    "val2 = 1.\n",
    "big_mat1 = np.ones((10000, 1000), dtype=theano.config.floatX)\n",
    "big_mat2 = np.ones((10000, 1000), dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switch time = 0.229577\n",
      "ifelse time = 0.126928\n"
     ]
    }
   ],
   "source": [
    "n_times = 10\n",
    "\n",
    "tic = time.clock()\n",
    "for i in xrange(n_times):\n",
    "    f_switch(val1, val2, big_mat1, big_mat2)\n",
    "print 'switch time =', time.clock() - tic\n",
    "\n",
    "tic = time.clock()\n",
    "for i in xrange(n_times):\n",
    "    f_lazyifelse(val1, val2, big_mat1, big_mat2)\n",
    "print 'ifelse time =',time.clock() - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the IfElse op spends less time (about half as much) than Switch since it computes only one variable out of the two. Unless linker='vm' or linker='cvm' are used, ifelse will compute both variables and take the same computation time as switch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dest4'></a>\n",
    "## Looping using scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing A^k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that A is a tensor and you want to compute A^k elementwise. The python/numpy code might look like:\n",
    "<br>result = 1</br>\n",
    "<br>for i in range(k):</br>\n",
    "<br>....result = result * A</br>\n",
    "<p>The equivalent Theano code is:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   4.   9.  16.  25.  36.  49.  64.  81.]\n",
      "[  0.00000000e+00   1.00000000e+00   1.60000000e+01   8.10000000e+01\n",
      "   2.56000000e+02   6.25000000e+02   1.29600000e+03   2.40100000e+03\n",
      "   4.09600000e+03   6.56100000e+03]\n"
     ]
    }
   ],
   "source": [
    "k = T.iscalar(\"k\")\n",
    "A = T.vector(\"A\")\n",
    "\n",
    "# Symbolic description of the result\n",
    "result, updates = theano.scan(fn=lambda prior_result, A: prior_result * A,\n",
    "                              outputs_info=T.ones_like(A),\n",
    "                              non_sequences=A,\n",
    "                              n_steps=k)\n",
    "final_result = result[-1]\n",
    "power = theano.function(inputs=[A,k], outputs=final_result, updates=updates)\n",
    "\n",
    "print(power(range(10),2))\n",
    "print(power(range(10),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Scan returns a tuple containing our result (result) and a dictionary of updates (empty in this case). Note that the result is not a matrix, but a 3D tensor containing the value of A^k for each step. We want the last value (after k steps) so we compile a function to return just that. Note that there is an optimization, that at compile time will detect that you are using just the last value of the result and ensure that scan does not store all the intermediate values that are used. So do not worry if A and k are large.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listed below are three things that we need to handle in the A^k example:\n",
    "<table>\n",
    "<tr>\n",
    "<th>for loop</th>\n",
    "<th>scan</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Initial value assigned to result</td>\n",
    "<td>Initialization occurs in outputs_info</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Accumulation of results in result</td>\n",
    "<td>Accumulation happens automatically</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Unchanging variable A</td>\n",
    "<td>Unchanging variables are passed to scan as non_sequences</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dest5'></a>\n",
    "## Application example: classifying MNIST digits using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass classification is the task of predicting some discrete-valued output\n",
    "$$y^{(i)} \\in \\{1,2,\\ldots,k\\}.$$\n",
    "To accomplish this task, we're going to expand our notion of a hypothesis function a bit.  Instead of having a scalar-valued hypothesis function (e.g., for binary classification, where it determines the confidence level in our prediction), in multi-class classification we have a _vector valued_ hypothesis function\n",
    "$$h_\\theta : \\mathbb{R}^n \\rightarrow \\mathbb{R}^k$$\n",
    "Fortunately, everything else remains pretty similar as with normal classification, we just need to specify the three ingredients of any machine learning algorithm\n",
    "1. The hypothesis class\n",
    "2. The loss function\n",
    "3. The optimization appoach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hypothesis class\n",
    "<p>Logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix W and a bias vector b. The probability that an input vector x is a member of a class i can be written as:\n",
    "\\begin{eqnarray}\n",
    "P(Y=k \\mid x) = \\frac{\\text{exp}(\\beta_{k0} + \\beta^{T}_k x)}{1 + \\sum^{K-1}_{l=1} \\text{exp}(\\beta_{l0} + \\beta^{T}_{l} x)}\n",
    "\\end{eqnarray}\n",
    "where $k \\in \\{1,\\ldots,K-1 \\}$.</p>\n",
    "<p>Written in matrix form:\n",
    "\\begin{eqnarray}\n",
    "P(Y=k \\mid x; W,b) = \\frac{\\text{exp}(W_k x + b_k)}{\\sum^{K}_{l=1} \\text{exp}(W_l x + b_l)}\n",
    "\\end{eqnarray}</p>\n",
    "<p>Thus we classify the image to a particular digit by taking the highest probability digit across all digits 0...9. The argmax function is:\n",
    "\\begin{eqnarray}\n",
    "y_{\\text{pred}} = \\text{argmax}_k P(Y=k \\mid x; W,b)\n",
    "\\end{eqnarray}</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function\n",
    "\n",
    "For logistic regression the negative log-likelihood for N observations of training data is given by:\n",
    "\\begin{eqnarray}\n",
    "\\ell (\\theta = \\{ W, d \\} \\mid \\mathcal{D}) = \\sum_{i=1}^N \\log (P(Y=k \\mid x_i; \\theta))\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The optimization appoach\n",
    "\n",
    "<p>We will use stochastic gradient descent to evaluate our loss function. In machine learning and other statistical estimation examples objective functions often have the form:\n",
    "\\begin{eqnarray}\n",
    "f(x) = \\sum_{i=1}^n f_i (x)\n",
    "\\end{eqnarray}</p>\n",
    "<p>That is, the objective function is a sum of functions fi, which are often associated with the i-th observation of the training/feature data set.</p>\n",
    "<p>Stochastic gradient descent is similar to gradient descent except that instead of evaluating all partial derivatives of the summands fi, ∂fi/∂xj at each step, a random subset of partials is evaluated at every step. This leads to huge savings in computational cost.</p>\n",
    "<p>What the following code will do are:</p>\n",
    "1. Create a class that encapsulates logistic regression model. It contains the weight matrix W, the bias vector b, a function of calculating P(Y=k∣x;W,b) and class prediction y_pred. This class also defines the negative log-likelihood loss function in a symbolic manner and how to calculae the error rate for a particular batch.\n",
    "2. Decompress up the MNIST gzip file and build the model.\n",
    "3. Train the model during which print iteration and error information.\n",
    "4. Save the trained model.\n",
    "5. Load the saved model and predict labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, minibatch 83/83, validation error 12.458333 %\n",
      "     epoch 1, minibatch 83/83, test error of best model 12.375000 %\n",
      "epoch 2, minibatch 83/83, validation error 11.010417 %\n",
      "     epoch 2, minibatch 83/83, test error of best model 10.958333 %\n",
      "epoch 3, minibatch 83/83, validation error 10.312500 %\n",
      "     epoch 3, minibatch 83/83, test error of best model 10.312500 %\n",
      "epoch 4, minibatch 83/83, validation error 9.875000 %\n",
      "     epoch 4, minibatch 83/83, test error of best model 9.833333 %\n",
      "epoch 5, minibatch 83/83, validation error 9.562500 %\n",
      "     epoch 5, minibatch 83/83, test error of best model 9.479167 %\n",
      "epoch 6, minibatch 83/83, validation error 9.322917 %\n",
      "     epoch 6, minibatch 83/83, test error of best model 9.291667 %\n",
      "epoch 7, minibatch 83/83, validation error 9.187500 %\n",
      "     epoch 7, minibatch 83/83, test error of best model 9.000000 %\n",
      "epoch 8, minibatch 83/83, validation error 8.989583 %\n",
      "     epoch 8, minibatch 83/83, test error of best model 8.958333 %\n",
      "epoch 9, minibatch 83/83, validation error 8.937500 %\n",
      "     epoch 9, minibatch 83/83, test error of best model 8.812500 %\n",
      "epoch 10, minibatch 83/83, validation error 8.750000 %\n",
      "     epoch 10, minibatch 83/83, test error of best model 8.666667 %\n",
      "epoch 11, minibatch 83/83, validation error 8.666667 %\n",
      "     epoch 11, minibatch 83/83, test error of best model 8.520833 %\n",
      "epoch 12, minibatch 83/83, validation error 8.583333 %\n",
      "     epoch 12, minibatch 83/83, test error of best model 8.416667 %\n",
      "epoch 13, minibatch 83/83, validation error 8.489583 %\n",
      "     epoch 13, minibatch 83/83, test error of best model 8.291667 %\n",
      "epoch 14, minibatch 83/83, validation error 8.427083 %\n",
      "     epoch 14, minibatch 83/83, test error of best model 8.281250 %\n",
      "epoch 15, minibatch 83/83, validation error 8.354167 %\n",
      "     epoch 15, minibatch 83/83, test error of best model 8.270833 %\n",
      "epoch 16, minibatch 83/83, validation error 8.302083 %\n",
      "     epoch 16, minibatch 83/83, test error of best model 8.239583 %\n",
      "epoch 17, minibatch 83/83, validation error 8.250000 %\n",
      "     epoch 17, minibatch 83/83, test error of best model 8.177083 %\n",
      "epoch 18, minibatch 83/83, validation error 8.229167 %\n",
      "     epoch 18, minibatch 83/83, test error of best model 8.062500 %\n",
      "epoch 19, minibatch 83/83, validation error 8.260417 %\n",
      "epoch 20, minibatch 83/83, validation error 8.260417 %\n",
      "epoch 21, minibatch 83/83, validation error 8.208333 %\n",
      "     epoch 21, minibatch 83/83, test error of best model 7.947917 %\n",
      "epoch 22, minibatch 83/83, validation error 8.187500 %\n",
      "     epoch 22, minibatch 83/83, test error of best model 7.927083 %\n",
      "epoch 23, minibatch 83/83, validation error 8.156250 %\n",
      "     epoch 23, minibatch 83/83, test error of best model 7.958333 %\n",
      "epoch 24, minibatch 83/83, validation error 8.114583 %\n",
      "     epoch 24, minibatch 83/83, test error of best model 7.947917 %\n",
      "epoch 25, minibatch 83/83, validation error 8.093750 %\n",
      "     epoch 25, minibatch 83/83, test error of best model 7.947917 %\n",
      "epoch 26, minibatch 83/83, validation error 8.104167 %\n",
      "epoch 27, minibatch 83/83, validation error 8.104167 %\n",
      "epoch 28, minibatch 83/83, validation error 8.052083 %\n",
      "     epoch 28, minibatch 83/83, test error of best model 7.843750 %\n",
      "epoch 29, minibatch 83/83, validation error 8.052083 %\n",
      "epoch 30, minibatch 83/83, validation error 8.031250 %\n",
      "     epoch 30, minibatch 83/83, test error of best model 7.843750 %\n",
      "epoch 31, minibatch 83/83, validation error 8.010417 %\n",
      "     epoch 31, minibatch 83/83, test error of best model 7.833333 %\n",
      "epoch 32, minibatch 83/83, validation error 7.979167 %\n",
      "     epoch 32, minibatch 83/83, test error of best model 7.812500 %\n",
      "epoch 33, minibatch 83/83, validation error 7.947917 %\n",
      "     epoch 33, minibatch 83/83, test error of best model 7.739583 %\n",
      "epoch 34, minibatch 83/83, validation error 7.875000 %\n",
      "     epoch 34, minibatch 83/83, test error of best model 7.729167 %\n",
      "epoch 35, minibatch 83/83, validation error 7.885417 %\n",
      "epoch 36, minibatch 83/83, validation error 7.843750 %\n",
      "     epoch 36, minibatch 83/83, test error of best model 7.697917 %\n",
      "epoch 37, minibatch 83/83, validation error 7.802083 %\n",
      "     epoch 37, minibatch 83/83, test error of best model 7.635417 %\n",
      "epoch 38, minibatch 83/83, validation error 7.812500 %\n",
      "epoch 39, minibatch 83/83, validation error 7.812500 %\n",
      "epoch 40, minibatch 83/83, validation error 7.822917 %\n",
      "epoch 41, minibatch 83/83, validation error 7.791667 %\n",
      "     epoch 41, minibatch 83/83, test error of best model 7.625000 %\n",
      "epoch 42, minibatch 83/83, validation error 7.770833 %\n",
      "     epoch 42, minibatch 83/83, test error of best model 7.614583 %\n",
      "epoch 43, minibatch 83/83, validation error 7.750000 %\n",
      "     epoch 43, minibatch 83/83, test error of best model 7.593750 %\n",
      "epoch 44, minibatch 83/83, validation error 7.739583 %\n",
      "     epoch 44, minibatch 83/83, test error of best model 7.593750 %\n",
      "epoch 45, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 46, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 47, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 48, minibatch 83/83, validation error 7.708333 %\n",
      "     epoch 48, minibatch 83/83, test error of best model 7.583333 %\n",
      "epoch 49, minibatch 83/83, validation error 7.677083 %\n",
      "     epoch 49, minibatch 83/83, test error of best model 7.572917 %\n",
      "epoch 50, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 51, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 52, minibatch 83/83, validation error 7.656250 %\n",
      "     epoch 52, minibatch 83/83, test error of best model 7.541667 %\n",
      "epoch 53, minibatch 83/83, validation error 7.656250 %\n",
      "epoch 54, minibatch 83/83, validation error 7.635417 %\n",
      "     epoch 54, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 55, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 56, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 57, minibatch 83/83, validation error 7.604167 %\n",
      "     epoch 57, minibatch 83/83, test error of best model 7.489583 %\n",
      "epoch 58, minibatch 83/83, validation error 7.583333 %\n",
      "     epoch 58, minibatch 83/83, test error of best model 7.458333 %\n",
      "epoch 59, minibatch 83/83, validation error 7.572917 %\n",
      "     epoch 59, minibatch 83/83, test error of best model 7.468750 %\n",
      "epoch 60, minibatch 83/83, validation error 7.572917 %\n",
      "epoch 61, minibatch 83/83, validation error 7.583333 %\n",
      "epoch 62, minibatch 83/83, validation error 7.572917 %\n",
      "     epoch 62, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 63, minibatch 83/83, validation error 7.562500 %\n",
      "     epoch 63, minibatch 83/83, test error of best model 7.510417 %\n",
      "epoch 64, minibatch 83/83, validation error 7.572917 %\n",
      "epoch 65, minibatch 83/83, validation error 7.562500 %\n",
      "epoch 66, minibatch 83/83, validation error 7.552083 %\n",
      "     epoch 66, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 67, minibatch 83/83, validation error 7.552083 %\n",
      "epoch 68, minibatch 83/83, validation error 7.531250 %\n",
      "     epoch 68, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 69, minibatch 83/83, validation error 7.531250 %\n",
      "epoch 70, minibatch 83/83, validation error 7.510417 %\n",
      "     epoch 70, minibatch 83/83, test error of best model 7.500000 %\n",
      "epoch 71, minibatch 83/83, validation error 7.520833 %\n",
      "epoch 72, minibatch 83/83, validation error 7.510417 %\n",
      "epoch 73, minibatch 83/83, validation error 7.500000 %\n",
      "     epoch 73, minibatch 83/83, test error of best model 7.489583 %\n",
      "Optimization complete with best validation score of 7.500000 %,with test performance 7.489583 %\n",
      "total time = 32.0737941265\n",
      "Predicted values for the first 20 examples in test set:\n",
      "[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4]\n",
      "Correct labels for the first 20 examples in test set:\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "\n",
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" \n",
    "        input (theano.tensor.TensorType): symbolic variable describing the input of the architecture (one minibatch)\n",
    "        n_in (int): number of features - n\n",
    "        n_out (int): number of classes - k\n",
    "        \"\"\"\n",
    "        # Use borrow=True to avoid costly deep copying. This is similar to passing by \n",
    "        # reference in C++. However this parameter will have no effect on a GPU.\n",
    "        self.W = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression of P(Y=k∣x;W,B)\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic expression of maximizing probability of classes\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "\n",
    "        self.params = [self.W, self.b]\n",
    "        self.input = input\n",
    "\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution. We use the mean instead of the sum so that the \n",
    "        learning rate is less dependent on the batch size.\n",
    "\n",
    "        y (theano.tensor.TensorType): corresponds to a vector that gives for each example the correct label\n",
    "        \"\"\"\n",
    "        # y.shape[0]: number of records\n",
    "        # T.arange(y.shape[0]): a symbolic vector containing [0,1,2,... y.shape[0]-1] \n",
    "        # T.log(self.p_y_given_x): a matrix of Log-Probabilities\n",
    "        # T.log(self.p_y_x)[T.arange(y.shape[0]), y]: a vector containing the log likelihoods of each training \n",
    "        # example/class pair\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch.\n",
    "        \n",
    "        y (theano.tensor.TensorType): a vector that gives for each example the correct label\n",
    "        \"\"\"\n",
    "        # check if y has the same dimension as y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1 represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "\n",
    "def load_data(dataset):\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        try:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        except:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    "\n",
    "            \n",
    "    def shared_dataset(data_xy, borrow=True):\n",
    "        \"\"\" Loads the dataset into shared variables.\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code runs on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "        \"\"\"\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(np.asarray(data_x, dtype=theano.config.floatX), borrow=borrow)\n",
    "        shared_y = theano.shared(np.asarray(data_y, dtype=theano.config.floatX), borrow=borrow)\n",
    "        # When storing data on the GPU it has to be stored as floats. But during our computations\n",
    "        # we need them as ints (we use labels as index, and if they are floats it doesn't make sense) \n",
    "        # therefore instead of returning ``shared_y`` we will have to cast it to int.\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "    return rval\n",
    "\n",
    "\n",
    "def sgd_optimization_mnist(learning_rate=0.13, n_epochs=1000,\n",
    "                           dataset='mnist.pkl.gz',\n",
    "                           batch_size=600):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent optimization\n",
    "\n",
    "    learning_rate (float)\n",
    "    n_epochs (int): maximal iterations to run the optimizer\n",
    "    dataset (string)\n",
    "    \"\"\"\n",
    "    datasets = load_data(dataset)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    #####build model######\n",
    "    index = T.lscalar()\n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "\n",
    "    # each MNIST image are 28*28\n",
    "    classifier = LogisticRegression(input=x, n_in=28 * 28, n_out=10)\n",
    "\n",
    "    # the cost we minimize during training is the negative log likelihood\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes of a minibatch\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute the gradient of cost\n",
    "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "    # specify how to update the parameters\n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, and updates the parameter \n",
    "    # of the model based on the rules defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    #######train model########\n",
    "    # the initial minimum number of examples to look at in each minibatch\n",
    "    # As classification error decreases, patience value increases, as more samples per minibatch are needed in order \n",
    "    # to decrease classification error.\n",
    "    patience = 5000  \n",
    "    patience_increase = 2  # wait this much longer when a new best is found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is considered significant\n",
    "    # how often to assess the classification performance on the validation set\n",
    "    validation_frequency = min(n_train_batches, patience // 2)                            \n",
    "    best_validation_loss = np.inf\n",
    "    test_score = 0.\n",
    "    start_time = time.time()\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            # If number of iterations is a multiple of validation frequency the validation, loss is calculated \n",
    "            # and printed.\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = [validate_model(i) for i in range(n_valid_batches)]\n",
    "                this_validation_loss = np.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    # test it on the test set\n",
    "                    test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "                    test_score = np.mean(test_losses)\n",
    "\n",
    "                    print(\n",
    "                        (\n",
    "                            '     epoch %i, minibatch %i/%i, test error of'\n",
    "                            ' best model %f %%'\n",
    "                        ) %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            test_score * 100.\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # save the best model\n",
    "                    with open('best_model.pkl', 'wb') as f:\n",
    "                        pickle.dump(classifier, f)\n",
    "            # If we exceed the \"patience\" for this minibatch, then we skip to the next minibatch.\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%,'\n",
    "            'with test performance %f %%'\n",
    "        )\n",
    "        % (best_validation_loss * 100., test_score * 100.)\n",
    "    )\n",
    "    print 'total time =', end_time - start_time\n",
    "\n",
    "\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    Load a trained model and use it to predict labels.\n",
    "    \"\"\"\n",
    "    # load the saved model\n",
    "    classifier = pickle.load(open('best_model.pkl'))\n",
    "\n",
    "    # compile a predictor function\n",
    "    predict_model = theano.function(\n",
    "        inputs=[classifier.input],\n",
    "        outputs=classifier.y_pred)\n",
    "\n",
    "    # test it on part of test test\n",
    "    dataset='mnist.pkl.gz'\n",
    "    datasets = load_data(dataset)\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    test_set_x = test_set_x.get_value()\n",
    "\n",
    "    predicted_values = predict_model(test_set_x[:20])\n",
    "    print(\"Predicted values for the first 20 examples in test set:\")\n",
    "    print(predicted_values)\n",
    "    print (\"Correct labels for the first 20 examples in test set:\")\n",
    "    print(test_set_y.eval()[:20])\n",
    "\n",
    "\n",
    "sgd_optimization_mnist()\n",
    "predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dest6'></a>\n",
    "## Further resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a href=\"http://deeplearning.net/software/theano/tutorial/index.html#advanced\">Theano advanced</a>\n",
    "2. <a href=\"http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/\">Logistic Regression</a>\n",
    "3. <a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\">Wikipedia: Stochastic Gradient Descent</a>\n",
    "4. <a href=\"https://www.udacity.com/course/intro-to-parallel-programming--cs344\">Udacity: Intro to Parallel Programming - Using CUDA to Harness the Power of GPUs</a>\n",
    "5. <a href=\"https://www.coursera.org/learn/machine-learning\">Coursera: Machine Learning by Andrew Ng</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dest7'></a>\n",
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.quantstart.com/articles/Deep-Learning-with-Theano-Part-1-Logistic-Regression\n",
    "2. http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/\n",
    "3. http://deeplearning.net/software/theano\n",
    "4. homework4 mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset and best model in this tutorial are available at https://drive.google.com/drive/folders/0B5-WcG2D1SViMjRqSmpEMVZERzQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
