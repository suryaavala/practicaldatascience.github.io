{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Correction Using Google's Ngram Dataset\n",
    "\n",
    "In this tutorial, we will implement a simple spell correction API based on Google's Ngram dataset. The API should be able to give a list of suggested words for a mistyped word inputed. And the list can also be sorted by the correctness probability of the words.\n",
    "\n",
    "We will use two datasets as input for this problem: Google's Ngram dataset and a list of English dictionary words. Google's Ngram dataset collected the Ngram words in the published books from 1505 to 2008 in Google Books. The words are collected up to 5Gram. And all the Ngrams' occurences are also collected in the dataset. Since most data is collected by OCR, which casues an unnegligible amount of misspelled words in the dataset. We will also use a list of English dictionary words as another method of correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google_ngram_downloader import readline_google_store\n",
    "import string\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from Queue import PriorityQueue\n",
    "from math import log\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Collection\n",
    "\n",
    "In this part, we will be collecting the 1-gram words from Google Ngram. Although we only use the smallest 1-gram data, the size of the dataset is still too huge to be downloaded to local storage. To our rescue, google-ngram-downloader allow us to conveniently fetch each entry in the dataset once each time. Using this API, we can space efficiently filter and process all the data we need.\n",
    "\n",
    "Following is the example usage of google-ngram-downloader:\n",
    "\n",
    "```python\n",
    ">>> fname, url, records = next(readline_google_store(ngram_len=1))\n",
    ">>> fname\n",
    "'googlebooks-eng-all-1gram-20120701-0.gz'\n",
    ">>> url\n",
    "'http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-1gram-20120701-0.gz'\n",
    ">>> next(records)\n",
    "Record(ngram=u\"0'9\", year=1797, match_count=1, volume_count=1)\n",
    "```\n",
    "After initialized the records, you can continuely call `next(records)` to fetch the next entry of data. When the iteration reach the end, `next()` will raise a `StopIteration` exception so that you will know when to stop. Each entry of ngram is returned as type `Record` storing the occurences of a ngram in a certain year of publications. The `Record` type has 4 attributes:\n",
    "- ngram: The ngram string. It is constructed by the format of `\"word_type ...\"`. In the ngram string, there may have underscores in it. Before the first underscore is the word of ngram we need. After that is the type of the word.\n",
    "- year: The year of publications this ngram exists in.\n",
    "- match_count: The occurence of the ngram in the books of this year.\n",
    "- volume_count: The number of books that the ngram exists in.\n",
    "\n",
    "Becasue we are fetching the ngrams from internet, the data collection process will be extremely slow. For the convenience of the work afterwards, we will firstly process and filter the raw data from Google Ngram and store the result to a sqlite database.\n",
    "\n",
    "**Specifications:**\n",
    "- Create a database and a table named words. The table should include the ngram string, match count and volume count.\n",
    "- Calculate all the match count and volume count of each 1-gram word from year 2000 (including year 2000). The word string should be stored in database in unicode and should not include the type information in it.\n",
    "- Since fetching all the 1-grams will take a huge amount of time, you only need to fetch 1-grams starting with letter 'k'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_words(dbfile):\n",
    "    \"\"\" Fetch 1-gram words from Google Ngram dataset and store to a sqlite table\n",
    "    Inputs:\n",
    "        dbfile (str): path to the database file\n",
    "    Outputs:\n",
    "        None\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(dbfile)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''drop table if exists words''')\n",
    "    cursor.execute('''create table words (word text, match_count long, volume_count long)''')\n",
    "\n",
    "    curr_word = None\n",
    "    match_cnt = 0\n",
    "    volume_cnt = 0\n",
    "    word_dict = dict()\n",
    "    \n",
    "    # To fetch all the 1-grams, use the following code\n",
    "    # _, _, records = next(readline_google_store(ngram_len=1, lang='eng', indices=list(string.ascii_lowercase)))\n",
    "    \n",
    "    _, _, records = next(readline_google_store(ngram_len=1, lang='eng', indices='k'))\n",
    "\n",
    "    # word count\n",
    "    while True:\n",
    "        try:\n",
    "            record = next(records)\n",
    "            # skip the books 20 years ago\n",
    "            if record.year < 2000:\n",
    "                continue\n",
    "            ngram = record.ngram\n",
    "            word = ngram[:ngram.find('_')]\n",
    "\n",
    "            if word == curr_word:\n",
    "                # add match and volume count to counters\n",
    "                match_cnt += record.match_count\n",
    "                volume_cnt += record.volume_count\n",
    "            else:\n",
    "                # finished traversing current word\n",
    "                if curr_word is not None and match_cnt > 10000 and volume_cnt > 1000:\n",
    "                    # word_dict[curr_word] = Word(curr_word, match_cnt, volume_cnt)\n",
    "                    cursor.execute('''insert into words values (?, ?, ?)''', (curr_word, match_cnt, volume_cnt))\n",
    "                    print curr_word, match_cnt, volume_cnt\n",
    "                curr_word = word\n",
    "                match_cnt = 0\n",
    "                volume_cnt = 0\n",
    "\n",
    "        except StopIteration:\n",
    "            # add last word to retval\n",
    "            if curr_word is not None and match_cnt > 10 and volume_cnt > 5:\n",
    "                # word_dict[curr_word] = Word(curr_word, match_cnt, volume_cnt)\n",
    "                cursor.execute('''insert into words values (?, ?, ?)''', (curr_word, match_cnt, volume_cnt))\n",
    "\n",
    "                print curr_word, match_cnt, volume_cnt\n",
    "            break\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_filename = 'sc.db'\n",
    "# fetch_words(db_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have already insert all 1-grams we need in the database. For the next step, we need to filter out the misspelled words. To do that, you will need to check each 1-gram in the database if it exists in the English dictionary and toss the ones that aren't in the dictionary. The dictionary word list is given in the file `'wordsEn.txt'`.\n",
    "\n",
    "**Specifications:**\n",
    "- Count a 1-gram string in the dictionary if the lower case of the string is a valid word in the dictionary.\n",
    "- return a python dictionary which has the 1-gram string as key and a `Word` type structure as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Structure for a 1-gram word\n",
    "class Word:\n",
    "    def __init__(self, value, match_count, volume_count):\n",
    "        assert isinstance(match_count, int)\n",
    "        assert isinstance(volume_count, int)\n",
    "\n",
    "        self.value = value\n",
    "        self.match_count = match_count\n",
    "        self.volume_count = volume_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dictionary(dict_filename):\n",
    "    \"\"\" Load the dictionary file to memory\n",
    "    Inputs:\n",
    "        dict_filename (str): path to the dictionary file\n",
    "    Outputs:\n",
    "        set: A set contains all the dictionary words\n",
    "    \"\"\"\n",
    "    file = open(dict_filename, 'r')\n",
    "    dict_set = set()\n",
    "    for word in file:\n",
    "        word = word.replace('\\n', '').replace('\\r', '')\n",
    "        dict_set.add(word)\n",
    "\n",
    "    return dict_set\n",
    "\n",
    "\n",
    "def filter_words(dbfile):\n",
    "    \"\"\" Filter the misspelled words in Google 1-gram\n",
    "    Inputs:\n",
    "        dbfile: path to the database file\n",
    "    Outputs:\n",
    "        dict: A python dictionary with the key of str and value of Word\n",
    "    \"\"\"\n",
    "    dict_set = get_dictionary('wordsEn.txt')\n",
    "    conn = sqlite3.connect(dbfile)\n",
    "    raw_words = pd.read_sql_query('''select * from words''', conn)\n",
    "    words_dict = dict()\n",
    "    raw_words[['match_count', 'volume_count']] = raw_words[['match_count', 'volume_count']].astype(int)\n",
    "    for index, row in raw_words.iterrows():\n",
    "        raw_word = row['word']\n",
    "        if raw_word in dict_set:\n",
    "            words_dict[raw_word] = Word(raw_word, row['match_count'], row['volume_count'])\n",
    "\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n"
     ]
    }
   ],
   "source": [
    "words = filter_words(db_filename)\n",
    "print len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Edit Distance\n",
    "Now that we have collected the vocabulary words from Google Ngram, we will start to perform the approximate search in the vocabulary. For this search problem, we firstly define a similarity function between two words - edit distance. The edit distance between two words $w_1, w_2$ is defined as the the least number of operations needed to modify $w_1$ into $w_2$. There are 4 kinds of edit operation included: (1) insert a letter at any place in a word, (2) delete a letter from a word, (3) change a letter in a word to another letter, and (4) swap two consecutive letters in a word.\n",
    "A popular algorithm to calculate edit distance is using dynamic programming. The core idea of the algorithm is to solve the bigger problem by solving several subproblems. Specificly, we maintain a matrix $D \\in R^{(|w_1|+1)\\cdot (|w_2|+1)}$, in which, every entry $D_{i,j}$ denotes the edit distance between the first $i$ letters of $w_1$ and the first $j$ letters of $w_2$. And the value function of $D_{i,j}$ is: \n",
    "$$\n",
    "    D_{i,j}=\n",
    "\\begin{cases}\n",
    "    min(D_{i-2,j-2}+1, D_{i-1,j-1} + (w_1^{i-1}=w_2^{j-1}), D_{i-1,j}+1, D_{i, j-1}+1),& \\text{if } w_1^{i-1}=w_2^{j-2}, w_1^{i-2}=w_2^{j-1}\\\\\n",
    "    min(D_{i-1,j-1} + (w_1^{i-1}=w_2^{j-1}), D_{i-1,j}+1, D_{i, j-1}+1),               & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "As shown in the value function, the value of $D_{i,j}$ can be calculated only using entries on its top-left. Therefore, updating $D_{i,j}$ from left to right, top to bottom will garantee a valid solution at $D_{|w_1|,|w_2|}$.\n",
    "This dynamic programming approach takes $O(|w_1|\\cdot |w_2|)$ time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_cost(letter):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def delete_cost(letter):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def change_cost(letter1, letter2):\n",
    "    if letter1 == letter2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def swap_cost(letter1, letter2):\n",
    "    if letter1 == letter2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def edit_distance(word1, word2, insert_func, delete_func, change_func, swap_func):\n",
    "    \"\"\" Calculate the edit distance between two words\n",
    "    Inputs:\n",
    "        word1(str), word2(str): The two words to be compared\n",
    "        insert_func: The insertion cost function\n",
    "        delete_func: The deletion cost function\n",
    "        change_func: The modification cost function\n",
    "        swap_func: The swapping cost function\n",
    "    Outputs:\n",
    "        int: The edit distance\n",
    "    \"\"\"\n",
    "    m = len(word1)\n",
    "    n = len(word2)\n",
    "    dp = [[0 for j in range(n + 1)] for i in range(m + 1)]\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        dp[i][0] = dp[i - 1][0] + delete_cost(word1[i - 1])\n",
    "    for j in range(1, n + 1):\n",
    "        dp[0][j] = dp[0][j - 1] + insert_cost(word2[j - 1])\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            letter1 = word1[i - 1]\n",
    "            letter2 = word2[j - 1]\n",
    "            dp[i][j] = min(dp[i - 1][j - 1] + change_func(letter1, letter2),\n",
    "                           dp[i - 1][j] + delete_func(letter1),\n",
    "                           dp[i][j - 1] + insert_func(letter2))\n",
    "            if i > 1 and j > 1 and letter1 == word2[j - 2] and letter2 == word1[\n",
    "                        i - 2]:\n",
    "                dp[i][j] = min(dp[i][j],\n",
    "                               dp[i - 2][j - 2] + swap_func(letter1, letter2))\n",
    "\n",
    "    return dp[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('applepen', 'pineappleone', insert_cost, delete_cost, change_cost, swap_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct implementation of `edit_distance` should return 6.\n",
    "\n",
    "Now we can calaulate the edit distance between a word and another, an obviously simple solution to do the spell correction is exhaustively calculate the edit distance between the query word $w$ and every word $v_i$ in the vocabulary $V$. And choose $v_i$ which minimizes `edit_distacne`$(w, v_i)$ as the suggestion. However, this method is expensive. Since we have to calcualte the edit distance between $w$ and every word in $V$. To the rescue, we will implement the `k-gram indexes` in the next part to eliminate most of the vocabulary for compareation.\n",
    "Note: In our implementation, we seperately wrote 4 trivial cost functions (`insert_cost`, `delete_cost`, `change_cost` and `swap_cost`), and pass them as parameters to `edit_distance`. It may seem trivial in this case, but by writing these functions outside `edit_distance` makes the code highly extensible for advance features on `edit_distance`. For example, we may define that the cost of changing one letter to another is dependent on the locations of the two letters on the keyboard, or dependent on the similarity in pronunciation. For these cases, only the corresponding cost functions need to be modified, all other code can keep the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. K-gram Indexes\n",
    "In this part, we will build k-gram indexes to reduce the size of vocabulary for compareation with the query word. \n",
    "As mentioned in part 2, exhausitively comparing the query word to all the words in the vocabulary set is very time consuming. To make the spell correction algorithm faster, we need another algorithm to quickly and accurately limit the vocabulary set to be compared in a small amount. After the smaller set is determined, we then can perform `edit_distance` to further search the nearest accurate word. \n",
    "We now show how we can do that using k-gram indexes. As example, we will explain how bigram(2-gram) index works. Differ to the Google Ngram which is the ngram dataset in units of word, the bigrams here are in units of letter. For every bigram (two consecutive letters), we store all the vocabulary words that conatins it. So that once the bigram index is built, given a certain bigram, the corresponding vocabulary set that contains the bigram can be fetched in $O(1)$ time. \n",
    "To find the limited vocabulary using the bigram index, we firstly enumerate all bigrams in the query word. Then we use the calculated bigram index to find vocabulary words that contains each of the bigrams. The more bigrams a word contains, the more similar this word will be to the query word.\n",
    "\n",
    "**Specifications:**\n",
    "- As the same with the example, we will only implement the bigram index\n",
    "- To reduce complexity, we will build the index using the vocabulary words in lowercase\n",
    "- As the same, `bigram_search` also use the query word in lowercase for the search in bigram index\n",
    "- Punctuations and non-letter characters should be included in the bigram index\n",
    "- Keep vocabulary words that differ by at most 3 bigrams to the query word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_add(bigram_dict, key, value):\n",
    "    if key not in bigram_dict:\n",
    "        bigram_dict[key] = set()\n",
    "    bigram_dict[key].add(value)\n",
    "\n",
    "\n",
    "def bigram_index(words_dict):\n",
    "    \"\"\" Build the bigram index from the vocabulary words\n",
    "    Inputs:\n",
    "        words_dict(dict): The vocabulary words\n",
    "    Outputs:\n",
    "        dict: A python dictionary with the key of str, value of set of str\n",
    "    \"\"\"\n",
    "    bigram = dict()\n",
    "    for w in words_dict:\n",
    "        w_lower = w.lower()\n",
    "        if len(w) < 2:\n",
    "            bigram_add(bigram, w_lower, w)\n",
    "            continue\n",
    "        for i in range(len(w_lower) - 1):\n",
    "            bi = w_lower[i: i + 2]\n",
    "            bigram_add(bigram, bi, w)\n",
    "    return bigram\n",
    "\n",
    "\n",
    "def bigram_search(bigram_dict, word):\n",
    "    \"\"\" Vague search the query word in the bigram index\n",
    "    Inputs:\n",
    "        bigram_dict(dict): The bigram index\n",
    "        word(str): The query word\n",
    "    Outputs:\n",
    "        list: A list of potential matching vocabulary\n",
    "    \"\"\"\n",
    "    word_lower = word.lower()\n",
    "    bi_list = list()\n",
    "    for i in range(len(word_lower) - 1):\n",
    "        bi = word_lower[i: i + 2]\n",
    "        if bi in bigram_dict:\n",
    "            bi_list += list(bigram_dict[bi])\n",
    "    bi_cnt = Counter(bi_list)\n",
    "    return [w for w in bi_cnt if bi_cnt[w] > len(word_lower) - 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "bigram = bigram_index(words)\n",
    "vocabulary = bigram_search(bigram, 'kewnel')\n",
    "print len(words)\n",
    "print len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the code above should be:\n",
    "```python\n",
    ">>> len(words)\n",
    "294\n",
    ">>> len(vocabulary)\n",
    "4\n",
    "```\n",
    "After searching in the prebuilt bigram index, the size of vocabulary set reduced from 294 to 4 for the query word 'kernwl'. About $98\\%$ of the vocabulary has been eliminated in $O(|w|)$ time.\n",
    "We can then run `edit_distance` between the query word and the reduced vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Put Everything Together\n",
    "Now we have the elements required to build a simple spell corrector. The only job left is to assemble them together.\n",
    "Using the previous features we developed, we can eliminate the vocabulary and calculate the edit distance between the query word and words in the vocabulary $V$. We also knows the occurence of each word in the vocabulary. To find the best suggestion word, we define:\n",
    "$$score(q, w) = \\frac{log(w.\\text{volume_count}) + log(w.\\text{match_count})}{d(q, w) + \\epsilon}$$\n",
    "where $q$ is the query word, $w$ is the vocabulary word, $d(q, w)$ is the edit distance between $q$ and $w$ and $\\epsilon$ is the smoothing variable.(In the implementation, we take $\\epsilon=0.1$)\n",
    "For each $w$ in $V$, we calculate the score. The vocabulary word with the maximum score will be the best suggestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'kernel', u'kennel', u'kernels']\n"
     ]
    }
   ],
   "source": [
    "def word_suggestion(words_dict, bigram_dict, word, n):\n",
    "    \"\"\" Give correct word suggestion for an input word\n",
    "    Inputs:\n",
    "        words_dict(dict): The word dictionary (fetched from Google 1-gram with the occurence)\n",
    "        bigram_dict(dict): The bigram index\n",
    "        word(str): The query word\n",
    "        n(int): Number of suggestion words to return\n",
    "    Outputs:\n",
    "        list: A list of suggestion words\n",
    "    \"\"\"\n",
    "    bigram_list = bigram_search(bigram_dict, word)\n",
    "    top_words = PriorityQueue()\n",
    "    for bw in bigram_list:\n",
    "        cost = edit_distance(word, bw, insert_cost, delete_cost, change_cost, swap_cost)\n",
    "        score = 1 / (cost + 0.1) * (log(words_dict[bw].volume_count) + log(words_dict[bw].match_count))\n",
    "        top_words.put((-score, bw))\n",
    "    retval = list()\n",
    "    cnt = 0\n",
    "    while not top_words.empty() and cnt < n:\n",
    "        retval.append(top_words.get()[1])\n",
    "        cnt += 1\n",
    "    return retval\n",
    "\n",
    "suggestion = word_suggestion(words, bigram, 'kewnel', 3)\n",
    "print suggestion"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
