{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will introduce you to a straight-forward yet powerful classification algorithm, the k-Nearest Neighbor(kNN). When asked to predict the class for an unseen data instance, kNN will search the entire training dataset for the nearest k data points. It then takes a majority vote from the classes of those k neighbors, and assign that class to the new data instance. Despite it's simplicity, kNN works incredibly well in practice, and has a wide application ranging from computer vision to gene expression, and many other fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "\n",
    "This tutorial first walks you through a toy kNN classification example with two numeric attributes and binary classification to get you familiarize with the basics. The tutorial uses Matplotlib to plot the data points for a better visual aid. After you get some sense of how kNN works, more properties about kNN will be discussed, as well as some general performance tuning techniques. And finally, we'll put everything together, and show you a much more comprehensive and practical application of kNN on letter recognition. Data for the application will be collected from UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Town of Cats and Dogs\n",
    "\n",
    "In a faraway land there is this little town, whose residents are cats and dogs only. Generally, cats tend to live closer with other cats and dogs likewise. However, there are some of them who embrace the diversity don't mind living close to the other species. The town is welcoming a new resident, and we already know the location in town where he/she chooses to build a house. And now, we would like to predict whether our new resident is a cat or a dog using kNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand and Load the data\n",
    "\n",
    "Every instance in the training set is consisted of the house location and the species(label) of a resident, while the location is represented by coordinates (two floats representing x- and y- coordinate respectively) and the label would be either 'c' (for cat) or 'd'(for dog).\n",
    "\n",
    "The training set has 20 data points, 10 of them labeled 'c' and the other 10 labeled 'd'. Data is pre-generated and stored in a CSV file. We will read the training data into a list of dictionaries for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'coordinates': (2.5, 2.5), 'label': 'c'}, {'coordinates': (3.0, 5.0), 'label': 'c'}, {'coordinates': (2.0, 17.0), 'label': 'd'}, {'coordinates': (6.5, 8.0), 'label': 'c'}, {'coordinates': (9.0, 9.0), 'label': 'd'}, {'coordinates': (6.5, 11.0), 'label': 'd'}, {'coordinates': (9.0, 12.0), 'label': 'c'}, {'coordinates': (6.0, 13.0), 'label': 'c'}, {'coordinates': (7.0, 15.0), 'label': 'c'}, {'coordinates': (11.0, 1.0), 'label': 'd'}, {'coordinates': (14.0, 6.0), 'label': 'd'}, {'coordinates': (11.0, 10.0), 'label': 'c'}, {'coordinates': (13.5, 12.0), 'label': 'd'}, {'coordinates': (10.5, 17.0), 'label': 'c'}, {'coordinates': (16.0, 4.0), 'label': 'd'}, {'coordinates': (16.5, 6.0), 'label': 'c'}, {'coordinates': (17.0, 8.0), 'label': 'd'}, {'coordinates': (17.5, 12.0), 'label': 'd'}, {'coordinates': (17.0, 13.0), 'label': 'd'}, {'coordinates': (18.0, 17.0), 'label': 'c'}]\n",
      "\n",
      "Number of examples in training data: 20\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "with open('example1_training.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        d = {'coordinates' : (float(row['c1']), float(row['c2'])), 'label' : row['label']}\n",
    "        training_data.append(d)\n",
    "print training_data\n",
    "print (\"\\nNumber of examples in training data: \" + str(len(training_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute distances and get nearest k neighbors\n",
    "\n",
    "In this example, we'll be using k=5. Normally, k takes the value of sqrt(n) (the reason will be discussed later) and should be an odd number for binary classification problems to avoid ties.\n",
    "\n",
    "In order to identify the 5 nearest neighbors, we will need to calculate the distance between our test data point and any single point in training data. Since both x- and y- coordinates are represented as numeric value, we can simply use the Euclidean distance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly generate the x- and y- coordinates for the test data\n",
    "test_c1 = random.uniform(0, 20)\n",
    "test_c2 = random.uniform(0, 20)\n",
    "\n",
    "# Helper function for calculating the Euclidean distance of two points\n",
    "def euclideanDistance(p1, p2):\n",
    "    distance = pow(p1[0]-p2[0],2) + pow(p1[1]-p2[1],2)\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "def getKNearestNeighbors(trainingSet, testInstance, k):\n",
    "    distances = []\n",
    "    for row in trainingSet:\n",
    "        distance = euclideanDistance(row['coordinates'], testInstance)\n",
    "        distances.append((distance, row['label']))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_neighbors = []\n",
    "    for i in range(k):\n",
    "        k_neighbors.append(distances[i][1])\n",
    "    return k_neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6.225220035038335, 16.066385214848065)\n",
      "['c', 'c', 'd', 'c', 'c']\n"
     ]
    }
   ],
   "source": [
    "k_neighbors = getKNearestNeighbors(training_data, (test_c1, test_c2), 5)\n",
    "print (test_c1, test_c2)\n",
    "print k_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the majority vote \n",
    "\n",
    "We are finally ready to make the prediction. Given the labels (aka classes) of the k=5 nearest neighbors, we will count the occurence of each label (in our case there are only two of them). The label that has got the most counts will be the label that kNN assign to the test data instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def getMajorityVote(neighbors):\n",
    "    votes = {}\n",
    "    for i in neighbors:\n",
    "        if i in votes.keys():\n",
    "            votes[i] += 1\n",
    "        else:\n",
    "            votes[i] = 1\n",
    "    sorted_votes = sorted(votes.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_votes, sorted_votes[0][0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', 4), ('d', 1)]\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "sorted_votes, predicted_label = getMajorityVote(k_neighbors)\n",
    "print sorted_votes\n",
    "print predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "Up to this point, we have seen all the essential parts of kNN algorithm. Since we do not have a known label for our test data, we are not able to verify the accuracy of the classification by kNN. However, we can visualize the data to have a more direct feeling of the layout of training data, and therefore, whether classification \"makes sense\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# Use svg backend for better quality\n",
    "matplotlib.use(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 5.0) # you should adjust this to fit your screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_training (training_data, test_c1, test_c2):\n",
    "    training_x_d = []\n",
    "    training_y_d = []\n",
    "    training_x_c = []\n",
    "    training_y_c = []\n",
    "    for instance in training_data:\n",
    "        if instance['label'] == 'd':\n",
    "            training_x_d.append(instance['coordinates'][0])\n",
    "            training_y_d.append(instance['coordinates'][1])\n",
    "        else:\n",
    "            training_x_c.append(instance['coordinates'][0])\n",
    "            training_y_c.append(instance['coordinates'][1])\n",
    "\n",
    "    plt.scatter(np.asarray(training_x_d), np.asarray(training_y_d), 100,'r','^')\n",
    "    plt.scatter(np.asarray(training_x_c), np.asarray(training_y_c), 50,'b','s')\n",
    "    plt.scatter(np.asarray([test_c1]), np.asarray([test_c2]), 500, 'yellow', '*')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAE8CAYAAADt4JSAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10df54e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = plot_training(training_data, test_c1, test_c2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scatter plot, red triangles denote the cat families, blue squares the dog families, and yellow star the new resident. If we are to make a guess of whether the new resident is cat or dog by simply looking at the plot, 'dog' would probably be a reasonable answer, which is exactly what the kNN algorithm has shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some observations\n",
    "\n",
    "After seeing the above example, now is a good time to discuss some important properties of kNN.\n",
    "\n",
    "#### 1. kNN is a lazy learning algorithm\n",
    "As you might have noticed, there is no explicit \"training phase\" when using kNN algorithm. It keeps all training data and does not try to learn any generalized rules (models) from those samples. This could be a useful property when you don't want to make any assumptions about the data (for example, data could be linearly parameterized as in linear regression).\n",
    "\n",
    "#### 2. Choice of k\n",
    "Suppose the algorithm tries to find the k nearest neighbors after computing all distances. When k=1, we only need to traverse all distances once, therefore O(n). As k increases, sorting the distances tends to take longer. In the worst case when k equals size of training data, n, we will have to sort all distances, therefore O(n*logn). However, when k is relatively big, the algorithm becomes more resistant to noises since it takes into consideration a larger subset of training data when predicting the new instance. \n",
    "\n",
    "#### 3. Calculating the distance\n",
    "It's reasonable to use Euclidean distance when all features contain only numerical data. For other types of data such as categorical, Hamming distance could be useful (the most straight-forward case would be counting the number of features that are the same). You could assign various weights to different features when calculating the distance. More often, distance itself is used as a weight when gathering the votes from the k nearest neighbors. The weight in the vote is generally inversely proportional to the distance, if we consider closer neighbors more important than further neighbors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter Recognition - A more comprehensive example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you are probably ready for a real-world example that uses moderate amount of training and test data. This tutorial will show you how kNN algorithm could be applied for letter recognition.\n",
    "\n",
    "The dataset we use is obtained from UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Letter+Recognition. There are 20,000 labeled datapoints in total, and we will split those into 10,000 + 10,000 (training + testing). Each datapoint contains 1 label (capital A-Z) and 16 integer attributes of a given letter. For more information regarding the metrics, please refer to the above link.\n",
    "\n",
    "We first read in and process the data to create our training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LR_training_X = []\n",
    "LR_training_y = []\n",
    "LR_test_X = []\n",
    "LR_test_y = []\n",
    "with open('letter-recognition.data') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        LR_training_X.append(np.asarray(map(int, row[1:])))\n",
    "        LR_training_y.append(row[0])\n",
    "LR_test_X = LR_training_X[10000:]\n",
    "LR_test_y = LR_training_y[10000:]\n",
    "LR_training_y = LR_training_y[:10000]\n",
    "LR_training_X = LR_training_X[:10000]\n",
    "print LR_training_X[:5]\n",
    "print LR_training_y[:5]\n",
    "print (len(LR_training_X), len(LR_training_y), len(LR_test_X), len(LR_test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the put-together kNN algorithm that takes in training and test data, predicts the label for each instance in test set, and gives the error rate of the prediction. Since the features are consisted of numerical values only, we compute the distances using 2-norm. The rest of the steps are very similar to the previous example, as the concept of the algorithm holds the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "def kNN(training_X, training_y, test_X, test_y, k):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    training_X: a list of arrays that each contain a sample data instance\n",
    "    training_y: a list of labels of training data\n",
    "    test_X: a list of arrays each contains a test data instance\n",
    "    test_y: a list of true labels of test data\n",
    "    \n",
    "    Output:\n",
    "    prediction: a list of predicted labels for test data test_X\n",
    "    error: the error rate of the prediction (#incorrectly-classified instances/total # test instances)\n",
    "    \n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    error = 0\n",
    "    for i, test_instance in enumerate(test_X):\n",
    "        # Calculate and sort distances\n",
    "        distances = []\n",
    "        for j, training_instance in enumerate(training_X):\n",
    "            d = LA.norm(test_instance - training_instance)\n",
    "            distances.append((d, training_y[j]))\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # find the k nearest neighbors\n",
    "        k_neighbors = []\n",
    "        for j in range(k):\n",
    "            k_neighbors.append(distances[j][1])\n",
    "        \n",
    "        # get the majority vote\n",
    "        sorted_votes, predicted_label = getMajorityVote(k_neighbors)\n",
    "        predictions.append(predicted_label) \n",
    "        error += 0 if test_y[i] == predicted_label else 1\n",
    "    return predictions, float(error)/len(test_X)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'J', 'D', 'O', 'Q', 'H', 'R', 'T', 'U', 'K'] 0.0585\n",
      "['W', 'J', 'D', 'O', 'Q', 'H', 'R', 'T', 'U', 'K'] 0.0755\n",
      "['W', 'J', 'D', 'O', 'Q', 'H', 'R', 'T', 'U', 'K'] 0.0638\n",
      "['W', 'J', 'D', 'O', 'Q', 'H', 'R', 'T', 'U', 'K'] 0.0682\n",
      "['W', 'J', 'D', 'O', 'Q', 'H', 'R', 'T', 'U', 'K'] 0.0694\n",
      "['W', 'J', 'D', 'O', 'Q', 'H', 'R', 'T', 'U', 'K'] 0.0701\n",
      "['W', 'J', 'D', 'O', 'Q', 'H', 'R', 'T', 'U', 'K'] 0.0707\n",
      "['W', 'J', 'D', 'O', 'Q', 'H', 'R', 'T', 'U', 'K'] 0.0744\n",
      "['W', 'J', 'D', 'O', 'Q', 'H', 'R', 'T', 'U', 'K'] 0.0757\n",
      "['W', 'J', 'D', 'O', 'Q', 'H', 'R', 'T', 'U', 'K'] 0.0795\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Note: The following code might take very long. I didn't time it but most likely more than 30 minutes.\"\"\"\n",
    "for k in range(1,11):\n",
    "    predictions, error_rate = kNN(LR_training_X, LR_training_y, LR_test_X, LR_test_y, k)\n",
    "    print predictions[:10], error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except the sharp increase and decrease in error rate when k=1->2 and k=2->3, we see a gradual decrease of accuracy when k grows larger. We may infer from the result that, data points that belong to different clusters (i.e. have different labels) tend not to have a clear separation. This is to say that, if a data instance is not in the center of a cluster, its neighbors might contain data points that have other labels, which might interfere with the prediction. Do you have any other guesses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References: \n",
    "\n",
    "https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/\n",
    "\n",
    "http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_ml/py_knn/py_knn_opencv/py_knn_opencv.html#knn-opencv\n",
    "\n",
    "http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
