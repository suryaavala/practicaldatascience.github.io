{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer - A Deep Learning Approach\n",
    "\n",
    "## Why use deep learning?\n",
    "\n",
    "Deep learning has substantially pushed the state-of-the-art in a wide range of tasks, including speech recognition, image recognition, and object detection. In 2012, deep neural networks (DNNs) shocked the world by halving the error rate in the ImageNet contest, and precipitated the rapid adoption of deep learning by the whole community. From then on, practitioners just don't get any result if they don't use deep learning. Therefore, let's take a look at this hottest technique.\n",
    "\n",
    "\n",
    "## Why recognize digits?\n",
    "\n",
    "Recognizing digits with MNIST dataset is the hello world example in the machine learning world. It is simple, but illustrative enough. For those who take 688, you must have got familiar with it during lab4, and we reuse some code to maintain the consistency. For those who don't, the MNIST database is a large database of 28×28 pixel handwritten digits, with 60,000 training images and 10,000 testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "# the code for loading the database into numpy array.\n",
    "def parse_images(filename):\n",
    "    f = open(filename,\"rb\");\n",
    "    magic,size = struct.unpack('>ii', f.read(8))\n",
    "    sx,sy = struct.unpack('>ii', f.read(8))\n",
    "    X = []\n",
    "    for i in range(size):\n",
    "        im =  struct.unpack('B'*(sx*sy), f.read(sx*sy))\n",
    "        X.append([float(x)/255.0 for x in im]);\n",
    "    return np.array(X).transpose();\n",
    "\n",
    "def parse_labels(filename):\n",
    "    f = open(filename,\"rb\");\n",
    "    magic,size = struct.unpack('>ii', f.read(8))\n",
    "    return np.array(struct.unpack('B'*size, f.read(size)))\n",
    "\n",
    "# dataset can be downloaded from http://yann.lecun.com/exdb/mnist/\n",
    "train_images = parse_images(\"train-images-idx3-ubyte\")\n",
    "train_labels = parse_labels(\"train-labels-idx1-ubyte\")\n",
    "test_images = parse_images(\"t10k-images-idx3-ubyte\")\n",
    "test_labels = parse_labels(\"t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# shows some samplings of the digits from the MNIST database\n",
    "# such that we have a feeling of the database\n",
    "M,N = 10,20\n",
    "fig, ax = plt.subplots(figsize=(N,M))\n",
    "digits = np.vstack([np.hstack([np.reshape(train_images[:, i*N+j],(28,28)) \n",
    "                               for j in range(N)]) for i in range(M)])\n",
    "ax.imshow(255-digits, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![MNIST.png](https://s11.postimg.org/azqm5mg9f/MNIST.png)](https://postimg.org/image/8v694jemn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is deep learning?\n",
    "\n",
    "Deep, in its appearance, means that the networks have great depth, i.e. have many layers of hidden units. 20 years ago a typical neural network (NN) had 3-5 layers of hidden units, while recently Microsoft researchers won the ImageNet contest with 152 layers NNs!$$$$However, deep learning is not just stacking layer over layer. Without modern architectures and techniques, a deeper network, with millions of parameters, may suffer from:\n",
    "1. Hard to train.\n",
    "2. Overfitting.\n",
    "\n",
    "To solve these issues, scientists introduced new architectures, like convolutional networks, and new training techniques, like dropout. Therefore, DNNs in essence are NNs with modern architectures, and are trained with modern techniques. In this tutorial, we will first introduce a simple NN, and then proceed to some cool deep techniques.\n",
    "\n",
    "## Multiclass classification and one-hot representation\n",
    "\n",
    "Classification of the digits is a multiclass (10 classes) classification. In binary classification problem, we use 0 to represent one class, while use 1 to represent another. This can be easily extended to the multiclass scenario. Specifically, assuming we have $C$ classes, we use a binary vector of length $C$, all except one of which are zeros, hence named one-hot. Say the original label is $k$, then only the kth element of the one-hot vector is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converts dense labels to one-hot labels.\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return np.transpose(labels_one_hot)\n",
    "\n",
    "train_labels = dense_to_one_hot(train_labels)\n",
    "test_labels = dense_to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Neural networks\n",
    "\n",
    "Neural networks consist of a collection of neurons, which is a logistic unit. Mathematically, the output $y$ satisfies the following formula:$$y=\\sigma(w_{1}x_{1}+w_{2}x_{2}+b)$$where $\\sigma(x)$ is the sigmoid function, and is defined as $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$. The sigmoid is also called activation function, which limits the output between 0 and 1. If $y>0.5$, the neuron is activated; otherwise, it is deactivated. Below shows a single neuron.[![single_neuron.png](https://s22.postimg.org/kvfm6qhc1/single_neuron.png)](https://postimg.org/image/p4kc8wkl9/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x)) # compute sigmoid in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most powerful ability of NNs is that they can, rather than need to be fed with manually crafted features, directly learn abstract representations from the raw input. Therefore, all we need to do is to construct a network and throw all inputs to it. In our case, we have 28×28 pixel images, and thus the size of input is 784. The output size is 10, as we have 10 classes in one-hot representation. Also, we need to have some neurons in between input and output to model the underlying mapping between pixels and classes. As a first attempt, let's use one layer of neurons and set the number of neurons as 256. (the number does not matter once it is reasonable)\n",
    "\n",
    "For simplicity of mathematical notation, we introduce the matrix formulation of the previous equation. Now, the input $\\mathbf{x}$ is a vector of 784 elements, the weight matrix $\\mathbf{W_1}$ is 256×784, $\\mathbf{W_2}$ is 10×256, and the output $\\mathbf{y}$ is a vector of 10 elements.$$h(\\mathbf{x})=\\sigma(\\mathbf{W_1}\\mathbf{x}+\\mathbf{b_1})$$$$\\mathbf{y}=\\sigma(\\mathbf{W_2}h(\\mathbf{x})+\\mathbf{b_2})$$Finally, we should classify the image as the digit that is most probable. Hence, we should use maximum. However, we instead use soft maximum function here, as it is easier to compute its gradient, which is defined as:$$p(y=i)=\\exp(y_i)/\\sum_{j=0}^{C}{\\exp(y_j)}$$Intuitively, the softmax function outputs a number between 0 and 1, which can be viewed as the probability of the image belonging to a certain class. For better understanding, the architecture of the network is illustrated below (bias is omitted).[![one_layer_network.png](https://s22.postimg.org/4yt40k4ap/one_layer_network.png)](https://postimg.org/image/uucujr64d/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax layer\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    sum_exp_x = np.sum(exp_x, 0)\n",
    "    return np.divide(exp_x, sum_exp_x)\n",
    "\n",
    "# feed-forward phase of neural networks\n",
    "def feed_forward(x, W1, b1, W2, b2):\n",
    "    h1 = sigmoid(np.add(np.matmul(W1, x), b1))\n",
    "    y = sigmoid(np.add(np.matmul(W2, h1), b2))\n",
    "    p = softmax(y)\n",
    "    return h1, y, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have officially finished the so-called feed-forward process of NNs. Now, we have many adjustable parameters, $\\mathbf{W_1}$, $\\mathbf{W_2}$, $\\mathbf{b_1}$, and $\\mathbf{b_2}$, it is time to learn these parameters. \n",
    "\n",
    "Imagine that the network outputs a vector (0.1, 0.1, ..., 0.1) for a certain image, while the label for this image is 0. Thus, the one-hot representation is (1, 0, 0, ..., 0). Certainly, we want the network's output to match the one-hot representation, i.e. increase the probability of 0, and make other digits less likely. Thus, we hope to change the output $\\mathbf{y}$ by $(\\Delta{y},-\\Delta{y},-\\Delta{y},...,-\\Delta{y})$, where $\\Delta{y}$ is a small positive number, by changing individual parameters, say $b_1$. Apparently, this is where gradient should come into the scene. Specifically, the new $b_1$ should be set as something like:$$b_1=b_1+\\alpha\\frac{\\partial{\\Delta\\mathbf{y}}}{\\partial{b_1}}$$where $\\alpha$ is a hyper-parameter we should decide later. Also, $\\alpha$ controls how far each step is, which is known as the learning rate. \n",
    "\n",
    "Until now the $\\Delta{y}$ part is just intuition, we want to formalize it mathematically. As we mentioned before, the output of softmax function is the probability that one image belongs to a certain class. Therefore, we can construct a MLE estimator, and we hope to maximize the likelihood. Given a certain image, we obtain:$$p_{MLE}=\\prod_{i=0}^{C}{p_i}^{t_i}$$where $t_{k}=1$ if $k$ is the true label. Note that when $t_i=0$, ${p_i}^{t_i}$ gives a one, and when $t_{i}=1$, it gives $p_i$. Thus, this is equivalent to maximize $p_k$, where $k$ is the correct label. However, this mathematical notation makes the manipulation simpler. Moreover, equivalently, it is usually more convenient to minimize the negative log-likelihood, and we call it loss function. Also, this famous loss function is named as cross-entropy loss.$$L=-\\sum_{i=0}^{C}{t_i}\\log{p_i}$$Therefore, instead of using $\\Delta\\mathbf{y}$, formally, we have the update rule as:$$\\mathbf{W}=\\mathbf{W}-\\alpha\\frac{\\partial{L}}{\\partial{\\mathbf{W}}}$$$$\\mathbf{b}=\\mathbf{b}-\\alpha\\frac{\\partial{L}}{\\partial{\\mathbf{b}}}$$Since $L$ is a function of $\\mathbf{p}$ and $\\mathbf{p}$ is function of $\\mathbf{y}$, we denote as $L=o(\\mathbf{y})$. According to the chain rule in calculus, it is easy to get$$\\frac{\\partial{L}}{\\partial{\\mathbf{W_2}}}=\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{W_2}}}$$$$\\frac{\\partial{L}}{\\partial{\\mathbf{b_2}}}=\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{b_2}}}$$$$\\frac{\\partial{L}}{\\partial{\\mathbf{W_1}}}=\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}\\frac{\\partial{\\mathbf{y}}}{\\partial{h(\\mathbf{x})}}\\frac{\\partial{h(\\mathbf{x})}}{\\partial{\\mathbf{W_1}}}$$$$\\frac{\\partial{L}}{\\partial{\\mathbf{b_1}}}=\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}\\frac{\\partial{\\mathbf{y}}}{\\partial{h(\\mathbf{x})}}\\frac{\\partial{h(\\mathbf{x})}}{\\partial{\\mathbf{b_1}}}$$Think of this procedure: we first have the loss w.r.t $\\mathbf{y}$, which is $\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}$. Then, we can calculate the loss w.r.t. $h(\\mathbf{x})$, which is $\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}\\frac{\\partial{\\mathbf{y}}}{\\partial{h(\\mathbf{x})}}$. Finally, based on the loss w.r.t. $h(\\mathbf{x})$, we can compute the loss w.r.t. $\\mathbf{W_1}$ and $\\mathbf{b_1}$. The process looks like we are propagating the loss from $L$ to $\\mathbf{y}$ to $h(\\mathbf{x})$ and finally to every parameter, and this is exactly why this learning process is named back-propagation.\n",
    "\n",
    "With simple calculus and some mathematical manipulation, we can get the value of each component for the above rules. To save some space, here list some important results as an example:$$\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}=\\mathbf{p}-\\mathbf{t}$$\n",
    "$$\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{b_1}}}=\\mathbf{y}(1-\\mathbf{y})$$\n",
    "$$\\frac{\\partial{\\mathbf{y}}}{\\partial{h(\\mathbf{x})}}=\\mathbf{W_2}^T\\mathbf{y}(1-\\mathbf{y})$$Interestingly, the loss w.r.t. $\\mathbf{y}$ is $\\mathbf{p}-\\mathbf{t}$, which matches our intuition. Thus, we can view loss functions as a mathematical formulation of our intuition.\n",
    "\n",
    "The above is the famous optimization technique gradient descent (GD). One thing left is that we need to repeatedly apply these update rules until the loss function reaches a local minimum (Optimization of NNs is non-convex, and this is partly why we should use GD). Basically, the loss on the training set will keep going down, but we actually desire to have a higher accuracy on the test set. They are not equivalent. One thing is overfitting. Therefore, we hope to stop before the error rate on the test set go up. A common approach is to specify a maximum amount of steps for taking gradient, which can be selected based on cross-validation. \n",
    "\n",
    "Before we proceed to training, we first need to figure out some ways to evaluate our approach: usually we use the loss and error rate on both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the evaluation code\n",
    "def cross_entropy(y, truth):\n",
    "    entropy = -np.add(np.multiply(truth, np.log(y)), \\\n",
    "                  np.multiply(-np.subtract(truth, 1), np.log(-np.subtract(y, 1))))\n",
    "    mean_entropy = np.sum(entropy) / entropy.shape[1]\n",
    "    return mean_entropy\n",
    "\n",
    "def error_rate(y, truth):\n",
    "    return np.mean(np.argmax(truth, 0) != np.argmax(y, 0))\n",
    "\n",
    "def evaluate(y, truth):\n",
    "    loss = cross_entropy(p, truth)\n",
    "    error = error_rate(p, truth)\n",
    "    return loss, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start our training code. Note that, instead of computing gradient for each example and summing them, we concatenate the examples together, such that $\\mathbf{x}$ becomes 784×60,000, and we can use matrix multiplication, which is much faster. For now, we have two hyper-parameters, learning rate and epochs, and we, somewhat randomly, set them as 1e-4 and 100 in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def weight_variables(dim1, dim2, variance):\n",
    "    matrix = np.multiply(2, np.random.rand(dim1, dim2))\n",
    "    matrix = np.multiply(np.subtract(matrix, 1), variance)\n",
    "    return matrix\n",
    "\n",
    "def bias_variables(dim1, dim2, variance):\n",
    "    matrix = np.ones((dim1, dim2))\n",
    "    matrix = np.multiply(matrix, variance)\n",
    "    return matrix\n",
    "\n",
    "import math\n",
    "\n",
    "num_hidden_units = 256\n",
    "learn_rate = 1e-4\n",
    "epochs = 200\n",
    "\n",
    "images = train_images\n",
    "labels = train_labels\n",
    "\n",
    "variance1 = math.sqrt(6.0) / (784.0 + num_hidden_units)\n",
    "W1 = weight_variables(num_hidden_units, 784, variance1)\n",
    "b1 = bias_variables(num_hidden_units, 1, 0)\n",
    "\n",
    "variance2 = math.sqrt(6.0) / (10.0 + num_hidden_units)\n",
    "W2 = weight_variables(10, num_hidden_units, variance2)\n",
    "b2 = bias_variables(10, 1, 0)\n",
    "\n",
    "images_transposed = np.transpose(images)\n",
    "images_ones = np.ones((images_transposed.shape[0], 1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    h1, y, p = feed_forward(images, W1, b1, W2, b2)\n",
    "\n",
    "    # every 20 steps print train loss and error rate\n",
    "    if epoch % 20 == 0:\n",
    "        train_loss, train_error = evaluate(p, train_labels)\n",
    "        print \"training loss\", train_loss, \"training error:\", train_error\n",
    "    \n",
    "    h1_tranposed = np.transpose(h1)\n",
    "    \n",
    "    loss_derivative = np.subtract(p, labels)\n",
    "    output = np.multiply(loss_derivative, np.multiply(y, (1-y)))\n",
    "    hidden = np.multiply(np.matmul(np.transpose(W2), output), np.multiply(h1, (1-h1)))\n",
    "\n",
    "    W2_gradient = np.matmul(output, h1_tranposed)\n",
    "    b2_gradient = np.matmul(output, np.ones((h1_tranposed.shape[0], 1)))\n",
    "    W2 = np.subtract(W2, np.multiply(learn_rate, W2_gradient))\n",
    "    b2 = np.subtract(b2, np.multiply(learn_rate, b2_gradient))\n",
    "\n",
    "    W1_gradient = np.matmul(hidden, images_transposed)\n",
    "    b1_gradient = np.matmul(hidden, images_ones)\n",
    "    W1 = np.subtract(W1, np.multiply(learn_rate, W1_gradient))\n",
    "    b1 = np.subtract(b1, np.multiply(learn_rate, b1_gradient))\n",
    "\n",
    "_, _, p = feed_forward(train_images, W1, b1, W2, b2)\n",
    "train_loss, train_error = evaluate(p, train_labels)\n",
    "print \"training loss\", train_loss, \"training error:\", train_error\n",
    "_, _, p = feed_forward(test_images, W1, b1, W2, b2)\n",
    "test_loss, test_error = evaluate(p, test_labels)\n",
    "print \"test loss\", test_loss, \"test error:\", test_error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After running the code, you should see something similar to:\n",
    "  training loss 3.25064678292 training error: 0.887633333333\n",
    "  training loss 3.20428458326 training error: 0.846083333333\n",
    "  training loss 2.76691405222 training error: 0.325966666667\n",
    "  training loss 2.63884964838 training error: 0.309883333333\n",
    "  training loss 2.56982048941 training error: 0.2524\n",
    "  training loss 2.52559156841 training error: 0.2132\n",
    "  training loss 2.49819515957 training error: 0.188183333333\n",
    "  training loss 2.47981450903 training error: 0.1759\n",
    "  training loss 2.46614326558 training error: 0.16835\n",
    "  training loss 2.45483191902 training error: 0.1654\n",
    "  training loss 2.44510834604 training error: 0.16915\n",
    "  test loss 2.43895755975 test error: 0.166"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Going deeper\n",
    "\n",
    "Frankly, the above result is pretty crappy in NNs' world. To make it better, we will introduce several more advanced techniques in this section.\n",
    "\n",
    "### Stochastic gradient descent\n",
    "\n",
    "Gradient descent calculates perfect gradient of the training set, but the gradient may not model the test set well. Stochastic gradient descent (SGD), as its name suggests, introduces some stochasticity to the model, which potentially lessens the overfitting. Here, we will use a mini-batch SGD, i.e. every time we use a mini-batch of size $N$ to calculate the gradient. This can utilize the matrix multiplication, which is one of the most optimized code in the world, and thus the computation is much more efficient. Also, mini-batch SGD gives less stochasticity than SGD, but more than GD. This strikes a balance between the two methods, which is proven to be very effective in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a random batch from training set of batch size\n",
    "def get_batch(batch_size=50):\n",
    "    indexes = np.random.randint(0, 60000, batch_size)\n",
    "    return train_images[:, indexes], train_labels[:, indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Sigmoid function is a S-shape function, and has a dynamic range. Typicall, the dynamic range is set as $[-\\sqrt{3},\\sqrt{3}]$. Outside the range, the neuron may always output 1 or 0, which causes the loss of capacity. Thus, we should initialize the weights in this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Illustration of the dynamic range of sigmoid function\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(8,1.5))\n",
    "x = np.arange(-4,4,0.05)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "ones = np.ones((11, 1))\n",
    "straight = np.arange(0,1.1,0.1)\n",
    "plt.plot(ones * np.sqrt(3), straight, 'r--')\n",
    "plt.plot(ones * -np.sqrt(3), straight, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![sigmoid_range.png](https://s22.postimg.org/3xk53bx75/sigmoid_range.png)](https://postimg.org/image/5clps1ya5/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight decay and maximum norm\n",
    "\n",
    "Similar reasons as above, we don't want the weight to become too large. The first way is to add a weight decay term to the loss function, i.e.$$L=-\\sum_{i=0}^{C}{t_i}\\log{p_i}+\\lambda\\sum{\\mathbf{W}^2}$$where $\\lambda$ controls the extent of weight decay. From this loss function, we know that we want to push the weights towards zero. Note that, formally, weight decay can be modelled as a MAP estimator, where the weight decay is a Gaussian prior. Specifically,$$p_{MAP}=p(\\mathbf{W})\\prod_{i=0}^{C}{p_i}^{t_i}$$where $p(\\mathbf{W})$ is a zero mean Gaussian distribution. The second way is to constraint the maximum value directly, i.e. $\\Vert\\mathbf{W}\\Vert\\leq2$. A justification is that constraining weight vectors to lie inside a ball of fixed radius makes it possible to use a huge learning rate without the possibility of weights blowing up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code to limit maximum norm of weights as C\n",
    "def max_norm(W, C):\n",
    "    W[W > C] = C\n",
    "    W[W < -C] = -C\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "\n",
    "Optimization is prone to get stuck in local optima. The momentum solve this problem by the idea that we can escape the local optimum by a push if it is a shallow optimum, and momentum is just that push. Instead of using only current gradient, we also take into account the gradient in the previous steps. Formally, the gradient considering momentum $\\hat{\\Delta}^{(t)}$ is$$\\hat{\\Delta}^{(t)}=\\Delta^{(t)}+\\gamma\\hat{\\Delta}^{(t-1)}$$where $\\gamma$ is the strength of momentum and $\\Delta^{(t)}$ is the current gradient of loss function $L$.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Another shortcoming of current back-propagation algorithm is that all neurons are updated together. One way to break the symmetry is that we use random initialization. Another is the dropout. The idea is that, during training, we randomly disable some neurons. Specifically, we treat those disabled neurons as non-existent, and some steps remain exact the same. Therefore, instead of training one NN, we are actually training a bunch of NNs, and a subset of neurons should be able to independently predict the result. Finally, in testing, we multiply the output of all neurons with a constant 1 - dropout to offset the dropout in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_mask(dim, dropout, train=True):\n",
    "    # the mask for dropout during train and test, respectively\n",
    "    if train:\n",
    "        mask = np.random.rand(dim, 1)\n",
    "        mask[mask < dropout] = 0\n",
    "        mask[mask >= dropout] = 1\n",
    "    else:\n",
    "        mask = (1 - dropout) * np.ones((dim, 1)) \n",
    "    return mask\n",
    "\n",
    "def feed_forward_with_dropout(x, W1, b1, W2, b2, W3, b3, dropout, dim, train=True):\n",
    "    mask = gen_mask(dim, dropout, train)\n",
    "    h1 = np.multiply(mask, sigmoid(np.add(np.matmul(W1, x), b1)))\n",
    "    h2 = np.multiply(mask, sigmoid(np.add(np.matmul(W2, h1), b2)))\n",
    "    y = sigmoid(np.add(np.matmul(W3, h2), b3))\n",
    "    p = softmax(y)\n",
    "    return h1, h2, y, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these advanced methods in hand, we can run a much larger network. Now, let's increase hidden layers to 2 layers with 256 neurons per layer. The hyper-parameters are chosen based on our experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(15388) # fix the seed\n",
    "\n",
    "num_hidden_units1 = 256\n",
    "num_hidden_units2 = 256\n",
    "\n",
    "variance1 = math.sqrt(6.0) / (784.0 + num_hidden_units1)\n",
    "W1 = weight_variables(num_hidden_units1, 784, variance1)\n",
    "b1 = bias_variables(num_hidden_units1, 1, variance1)\n",
    "\n",
    "variance2 = math.sqrt(6.0) / (num_hidden_units1 + num_hidden_units2)\n",
    "W2 = weight_variables(num_hidden_units2, num_hidden_units1, variance2)\n",
    "b2 = bias_variables(num_hidden_units2, 1, variance2)\n",
    "\n",
    "variance3 = math.sqrt(6.0) / (10.0 + num_hidden_units2)\n",
    "W3 = weight_variables(10, num_hidden_units2, variance3)\n",
    "b3 = bias_variables(10, 1, variance3)\n",
    "\n",
    "learn_rate = 0.01\n",
    "steps = 60000 # train on one mini-batch is one step\n",
    "momentum = 0.95\n",
    "dropout = 0.5\n",
    "weight_decay = 0.0\n",
    "C = 2\n",
    "\n",
    "last_W3 = np.zeros(W3.shape)\n",
    "last_b3 = np.zeros(b3.shape)\n",
    "last_W2 = np.zeros(W2.shape)\n",
    "last_b2 = np.zeros(b2.shape)\n",
    "last_W1 = np.zeros(W1.shape)\n",
    "last_b1 = np.zeros(b1.shape)\n",
    "\n",
    "for step in range(0, steps):\n",
    "    images, labels = get_batch()\n",
    "    h1, h2, y, p = feed_forward_with_dropout(\n",
    "        images, W1, b1, W2, b2, W3, b3, dropout, num_hidden_units1)\n",
    "\n",
    "    images_transposed = np.transpose(images)\n",
    "    images_ones = np.ones((images_transposed.shape[0], 1))\n",
    "    \n",
    "    # every 5000 steps print train loss and error rate\n",
    "    if step % 20000 == 0:\n",
    "        train_loss, train_error = evaluate(p, labels)\n",
    "        print \"step\", step, \"training loss\", train_loss, \"training error:\", train_error\n",
    "    \n",
    "    h1_tranposed = np.transpose(h1)\n",
    "    h2_tranposed = np.transpose(h2)\n",
    "    \n",
    "    loss_derivative = np.subtract(p, labels)\n",
    "    output = np.multiply(loss_derivative, np.multiply(y, (1-y)))\n",
    "    hidden2 = np.multiply(np.matmul(np.transpose(W3), output), np.multiply(h2, (1-h2)))\n",
    "    hidden1 = np.multiply(np.matmul(np.transpose(W2), hidden2), np.multiply(h1, (1-h1)))\n",
    "\n",
    "    W3_gradient = np.matmul(output, h2_tranposed) \\\n",
    "        + (momentum * last_W3) + weight_decay * W3\n",
    "    b3_gradient = np.matmul(output, np.ones((h2_tranposed.shape[0], 1))) \\\n",
    "        + (momentum * last_b3)\n",
    "    W3 = np.subtract(W3, np.multiply(learn_rate, W3_gradient))\n",
    "    b3 = np.subtract(b3, np.multiply(learn_rate, b3_gradient))\n",
    "\n",
    "    W2_gradient = np.matmul(hidden2, h1_tranposed) \\\n",
    "        + (momentum * last_W2) + weight_decay * W2\n",
    "    b2_gradient = np.matmul(hidden2, np.ones((h1_tranposed.shape[0], 1))) \\\n",
    "        + (momentum * last_b2)\n",
    "    W2 = np.subtract(W2, np.multiply(learn_rate, W2_gradient))\n",
    "    b2 = np.subtract(b2, np.multiply(learn_rate, b2_gradient))\n",
    "\n",
    "    W1_gradient = np.matmul(hidden1, images_transposed) \\\n",
    "        + (momentum * last_W1) + weight_decay * W1\n",
    "    b1_gradient = np.matmul(hidden1, images_ones) \\\n",
    "        + (momentum * last_b1)\n",
    "    W1 = np.subtract(W1, np.multiply(learn_rate, W1_gradient))\n",
    "    b1 = np.subtract(b1, np.multiply(learn_rate, b1_gradient))\n",
    "    \n",
    "    W3 = max_norm(W3, C)\n",
    "    W2 = max_norm(W2, C)\n",
    "    W1 = max_norm(W1, C)\n",
    "    \n",
    "    last_W3 = W3_gradient\n",
    "    last_b3 = b3_gradient\n",
    "    last_W2 = W2_gradient\n",
    "    last_b2 = b2_gradient\n",
    "    last_W1 = W1_gradient\n",
    "    last_b1 = b1_gradient\n",
    "    \n",
    "_, _, _, p = feed_forward_with_dropout(\n",
    "    train_images, W1, b1, W2, b2, W3, b3, dropout, num_hidden_units1, False)\n",
    "train_loss, train_error = evaluate(p, train_labels)\n",
    "print \"training loss\", train_loss, \"training error:\", train_error\n",
    "_, _, _, p = feed_forward_with_dropout(\n",
    "    test_images, W1, b1, W2, b2, W3, b3, dropout, num_hidden_units1, False)\n",
    "test_loss, test_error = evaluate(p, test_labels)\n",
    "print \"test loss\", test_loss, \"test error:\", test_error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you run the code, you will see the result as follows:\n",
    "step 0 training loss 3.24968681136 training error: 0.86\n",
    "step 5000 training loss 2.40279346963 training error: 0.16\n",
    "step 10000 training loss 2.32252722373 training error: 0.04\n",
    "step 15000 training loss 2.32432520667 training error: 0.06\n",
    "step 20000 training loss 2.32351511378 training error: 0.06\n",
    "step 25000 training loss 2.29420776586 training error: 0.04\n",
    "training loss 2.28214673939 training error: 0.0176666666667\n",
    "test loss 2.29084446693 test error: 0.027"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a significant increase of accuracy from 84% to 98%, and the code runs faster. However, the result is far from the state-of-the-art (Nowadays, 1% error rate is very common), as this is only a tutorial and running a too large network consumes too much time. If we increase the network size and keep training, we probably will get much better result. Besides, there are many more interesting techniques in deep learning, but they are out of this simple tutorial's scope.\n",
    "\n",
    "Finally, Hugo Larochelle hosts a great introductory course about feed-forward NNs and training NN on the website http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html. You can watch some videos if you want to learn more. Hope you have a sense of the deep learning now, and thank you all for reading."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
