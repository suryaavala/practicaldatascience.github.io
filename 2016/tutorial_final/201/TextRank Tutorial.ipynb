{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "This tutorial will introduce you to the TextRank algorithm that is commonly used to summarize a document of text. In the modern day, data is being collected and generated at an unprecedented rate. To make use of this data, we need to be able to extract features and important information quickly and accurately. For text data, it is very important to summarize large collections or documents of text to understand what the collection or document is about. Summarization is useful everywhere, since almost every professional at some point will read large amounts of text. There are many approaches for this problem, such as supervised machine learning or maximum entropy. In this tutorial, however, we will go over an unsupervised algorithm called TextRank(or LexRank but they're the same concept). TextRank is an algorithm that retrieves the important parts of the document via a method similar to PageRank, but using different vertices and edges. \n",
    "\n",
    "# Tutorial Content\n",
    "\n",
    "We will go over how the TextRank algorithm works by starting with a bag of words approach. We will be using data copied from a Wikipedia page. After covering the basics of TextRank and its use on document summarization, we will go over how TextRank can be applied to keyword extraction. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation \n",
    "Please just use the anaconda package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import networkx as nx\n",
    "import numpy as np  \n",
    "import nltk\n",
    "import string\n",
    "import operator\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Rank\n",
    "\n",
    "Text Rank is an algorithm introduced in 2004 from researchers at University of Michigan. It uses the same concept as PageRank: do a random walk on a graph where the sentences are vertices and the similarity between each sentence are the edges. This means that the algorithm is extracting the sentences that are most similar to other sentences, which indicate that this sentence is important and covers the information in a lot of other sentences. To do that, we start by tokenizing all the sentences. \n",
    "\n",
    "# Sentence splitting\n",
    "\n",
    "Since we are doing summarization of the document by finding the most important sentences, we need to split the document by sentences. \n",
    "We do this through NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_tokens(document):\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    sentences = tokenizer.tokenize(document)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Another keyphrase extraction algorithm is TextRank.', u'While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data.', u'Many documents with known keyphrases are needed.']\n"
     ]
    }
   ],
   "source": [
    "document = \"\"\"\n",
    "Another keyphrase extraction algorithm is TextRank. While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data. Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate. Unsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[3] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages. Recall this is based on the notion of \"prestige\" or \"recommendation\" from social networks. In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties. Thus the algorithm is easily portable to new domains and languages.\n",
    "\n",
    "TextRank is a general purpose graph-based ranking algorithm for NLP. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or lexical similarity between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n",
    "\n",
    "The vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together. Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step.\n",
    "\n",
    "Edges are created based on word co-occurrence in this application of TextRank. Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text. N is typically around 2â€“10. Thus, \"natural\" and \"language\" might be linked in a text about NLP. \"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words. These edges build on the notion of \"text cohesion\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.\n",
    "\n",
    "Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.\n",
    "\n",
    "It is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences. Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words. Running PageRank/TextRank on the graph is likely to rank \"learning\" highly. Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\". If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\". If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\". In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".\n",
    "\n",
    "In short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below.\n",
    "\"\"\"\n",
    "document = unicode(document, 'ascii', 'ignore')\n",
    "document1 = ' '.join(document.strip().split('\\n'))\n",
    "sentences = sentence_tokens(document1)\n",
    "print(sentences[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating Bag of Words\n",
    "\n",
    "To use page rank, we need to create a similarity graph of some kind to do a random walk on. To do that, we create a bag of words for each individual sentence. We could use Python's default Counter library, but that returns a dictionary of counts while we want a sparse matrix of the word occurrences in each matrix; basically a matrix of unique words as columns and sentences as rows and each entry is whether a word occurs in a sentence. Luckily, CountVectorizer from sklearn does exactly that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: array of sentences\n",
    "# output: sparse matrix of word occurences\n",
    "def counts(tokens):\n",
    "    counter = CountVectorizer()\n",
    "    matrix = counter.fit_transform(tokens)\n",
    "    return matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49x359 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 802 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_matrix = counts(sentences)\n",
    "word_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Graph\n",
    "\n",
    "Now, we have a sparse matrix of sentences by words, but we want a mirror matrix of sentences by sentences because that represents the graph we want. First, we should normalize our graph so zeroes don't ruin the calculations. We do this using TfidfTransformer, which normalizes a count matrix into a tf-idf matrix which better represent the importance of a word in a set of documents. We now multiply the normalized matrix by its transpose, which creates a mirror matrix where each entry is the result of multiplying every tfidf of each word in a sentence by another sentences' and adding them together. The result is a number from 0 to 1, where 1 means the sentences are exactly the same, and 0 means the sentences are completely different. This is an adjancency matrix that represents the graph of sentences and edges that represent similarities. This specific approach is done by the LexRank algorithm. The TextRank algorithm simply uses a different similarity measure that isn't tfidf. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49x49 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1929 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def graph(word_matrix):\n",
    "    normalized_matrix = TfidfTransformer().fit_transform(word_matrix)\n",
    "    similarity_graph = normalized_matrix * normalized_matrix.T\n",
    "    return similarity_graph\n",
    "\n",
    "similarity_graph = graph(word_matrix)\n",
    "similarity_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Algorithm\n",
    "\n",
    "Now, we use networkx's page rank algorithm on this sparse matrix. The PageRank algorithm does a random walk on this graph, our sentences, and terminates after a fixed number and produces the rank of each sentence, which is how similar this sentence is to every other sentence. The higher the rank, means it is similar to a lot of sentences in this article, implying it must be important in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[3] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph). The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. These edges build on the notion of \"text cohesion\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def summary(similarity_graph, n):\n",
    "\n",
    "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    ranked = sorted(((scores[i],s) for i,s in enumerate(sentences)),\n",
    "                    reverse=True)\n",
    "    summary = \"\"\n",
    "    for i in range(n):\n",
    "        summary += ranked[i][1] + \" \"\n",
    "    return summary\n",
    "summ = summary(similarity_graph, 4)\n",
    "print summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TextRank for keyword extraction\n",
    "\n",
    "Imagine a scenario where you're given a large document and you need to figure out what are the key things this document talks about. You want to use an algorithm that extracts the most important words and phrases from the text. But what determines importance of a word or a phrase? For this problem, we can also use the TextRank approach, but varied slightly. \n",
    "\n",
    "# Process text\n",
    "\n",
    "For a keyword extraction algorithm, a natural intuition is to find the words that are occur the most. To effectively do that, removing the punctuation and then lemmatizing the words will let the algorithm better count the words that occur in different contexts. The same reasoning applies to the TextRank algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# taken from homework3 \n",
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    b = text.lower()\n",
    "    b = b.replace(\"'s\",\"\")\n",
    "\n",
    "    def applyFunc(s):\n",
    "\n",
    "        if s is \"'\":\n",
    "            return \"\"\n",
    "        elif s in string.punctuation:\n",
    "            return \" \"\n",
    "        else:\n",
    "            return s\n",
    "                \n",
    "    \n",
    "        \n",
    "    newB = ''.join(map(applyFunc, b))\n",
    "    \n",
    "    tokens = nltk.word_tokenize(newB)\n",
    "    newTokens = []\n",
    "    for tok in tokens :\n",
    "        \n",
    "        try: \n",
    "            word = lemmatizer.lemmatize(tok)\n",
    "            newTokens.append(word)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    return newTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sample', 'test', 'input', 'for', 'processing']\n",
      "['i', 'dont', 'know', 'how', 'this', u'work']\n",
      "['im', 'doing', 'well', 'how', 'about', 'you']\n",
      "['are', 'those', 'john', 'bahsd', u'dish']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample test input for processing.\"\n",
    "print process(text) \n",
    "# lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "# print(lemmatizer.lemmatize(\"processes\"))\n",
    "process(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\")\n",
    "print(process(\"I don't know how this works\"))\n",
    "print(process(\"I'm doing well! How about you?\"))\n",
    "print(process(\"Are #those John-Bahsd's dishes?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "\n",
    "Extremely common words, or stop words, can also ruin the algorithm. We get rid of all the stopwords that come with nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stopwords=nltk.corpus.stopwords.words('english')\n",
    "    new_tokens = []\n",
    "    for tok in tokens:\n",
    "        if not tok in stopwords:\n",
    "            new_tokens.append(tok)\n",
    "    return new_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'another', u'keyphrase', u'extraction', u'algorithm', u'textrank', u'supervised', u'method', u'nice', u'property', u'like', u'able', u'produce', u'interpretable', u'rule', u'feature', u'characterize', u'keyphrase', u'also', u'require', u'large', u'amount', u'training', u'data', u'many', u'document', u'known', u'keyphrases', u'needed', u'furthermore', u'training', u'specific', u'domain', u'tends', u'customize', u'extraction', u'process', u'domain', u'resulting', u'classifier', u'necessarily', u'portable', u'turney', u'result', u'demonstrate', u'unsupervised', u'keyphrase', u'extraction', u'remove', u'need', u'training', u'data', u'approach', u'problem', u'different', u'angle', u'instead', u'trying', u'learn', u'explicit', u'feature', u'characterize', u'keyphrases', u'textrank', u'algorithm', u'3', u'exploit', u'structure', u'text', u'determine', u'keyphrases', u'appear', u'central', u'text', u'way', u'pagerank', u'selects', u'important', u'web', u'page', u'recall', u'based', u'notion', u'prestige', u'recommendation', u'social', u'network', u'way', u'textrank', u'doe', u'rely', u'previous', u'training', u'data', u'rather', u'run', u'arbitrary', u'piece', u'text', u'produce', u'output']\n"
     ]
    }
   ],
   "source": [
    "tokens = remove_stopwords(process(document))\n",
    "print tokens[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurence\n",
    "\n",
    "Now that we've finished processing the text, how does TextRank actually work on keywords? The key concept behind TextRank is creating a graph with unigrams as vertices and the co-occurence between 2 words as edges. A co-occurence is when a word is within a window n of another word. For example, if a co-occurence window is 2, that means only words that are next to each other are counted by the algorithm. A co-occurence window of 3 means that words 2 ahead of the word will be added as edges to the graph. \n",
    "\n",
    "# Making a Graph\n",
    "\n",
    "TextRank uses the PageRank algorithm to rank the nodes. To use PageRank, we first need to make a graph. Conveniently, networkx provides a great graph data structure. To simplify the algorithm, we will use a co-occurence window of 2, and use unweighted edges instead of weighted edges. Usually, the edge is weighted by the amount of times the co-occurence happened. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'networkx.classes.graph.Graph'>\n"
     ]
    }
   ],
   "source": [
    "def make_graph(tokens):\n",
    "    graph = nx.Graph()\n",
    "#     set(tokens) generates the unique unigrams of from tokens\n",
    "    graph.add_nodes_from(set(tokens))\n",
    "    \n",
    "#     add edges for every adjacent word (co-occurence window of 2)\n",
    "    for i in range(len(tokens) - 2 + 1):\n",
    "        t1, t2 = tokens[i], tokens[i+1]\n",
    "        graph.add_edge(*sorted([t1,t2]))\n",
    "    return graph\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "graph = make_graph(tokens)\n",
    "print type(graph)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction\n",
    "\n",
    "Now, we will rank the nodes in the graph via the PageRank algorithm in networkx. The function will take in a parameter n for the number of keywords that needs to be extracted from the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'word', u'would', u'text', u'vertex', u'unigrams', u'pagerank', u'supervised', u'graph', u'based', u'keyphrases'])\n"
     ]
    }
   ],
   "source": [
    "def extract_n_keywords(graph, n=10):\n",
    "    ranks = nx.pagerank(graph)\n",
    "    keywords = {rank[0]: rank[1] for rank in sorted(ranks.items(), key=operator.itemgetter(1),reverse=True)[:n]}\n",
    "    words = keywords.keys()\n",
    "    return set(words), ranks\n",
    "keywords, word_ranks = extract_n_keywords(graph)\n",
    "print keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Phrases\n",
    "\n",
    "So far, we've only extracted key unigrams from the text, but we want are not just unigrams, we want phrases along with unigrams. Forturnately, we don't need to go back and create a new graph based on ngrams to find important phrases. After getting the top n keywords, all we need to do is check all the times the keywords occur in the document, and see if other keywords are adjacent to it. Then we average the pagerank scores so we don't overweight longer phrases and rerank the keywords with key phrases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def key_phrases(keywords, tokens):\n",
    "    from itertools import takewhile, tee, izip\n",
    "    keyphrases = {}\n",
    "    j = 0\n",
    "    for i, word in enumerate(tokens):\n",
    "        if i < j:\n",
    "            continue\n",
    "        if word in keywords:\n",
    "            temp = []\n",
    "#             if its adjacent to the keyword, add it as a phrase\n",
    "            for x in tokens[i:i+10] :\n",
    "                if x in keywords:\n",
    "                    temp.append(x)\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            kp_words = temp\n",
    "            sum_ranks = 0\n",
    "            for w in kp_words:\n",
    "                sum_ranks += word_ranks[w]\n",
    "            \n",
    "            avg_pagerank = sum_ranks / float(len(kp_words))\n",
    "            \n",
    "#             insert it back into the keyphrases, and rerank later\n",
    "            keyphrases[' '.join(kp_words)] = avg_pagerank\n",
    "            \n",
    "            j = i + len(kp_words)\n",
    "    ranked_phrases = sorted(keyphrases.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    phrases = map((lambda x: x[0]), ranked_phrases)\n",
    "    return phrases, ranked_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 0.021334247901\n",
      "graph 0.0180259236916\n",
      "vertex graph 0.0163042600728\n",
      "graph vertex 0.0163042600728\n",
      "based text 0.0156433251488\n",
      "graph would 0.0151572236273\n",
      "vertex 0.014582596454\n",
      "pagerank graph 0.0140475086452\n",
      "graph based 0.0139891630441\n",
      "vertex unigrams 0.0139198268555\n",
      "keyphrases 0.0134768320763\n",
      "vertex would 0.0134355600085\n",
      "unigrams 0.0132570572571\n",
      "keyphrases supervised 0.0124746149791\n",
      "would 0.012288523563\n",
      "supervised 0.0114723978819\n",
      "word 0.0111125170695\n",
      "based word 0.010532459733\n",
      "pagerank 0.0100690935987\n",
      "based 0.00995240239659\n"
     ]
    }
   ],
   "source": [
    "keywords_and_phrases, ranks = key_phrases(keywords, tokens)\n",
    "for i in ranks:\n",
    "    print i[0], i[1]\n",
    "# print ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Now that we know how TextRank works, how do we know how well it works? This is actually a very difficult question to answer. Due to the difficulty in determining what a good summary is, there isn't an absolute measure that determines how good a summarization algorithm is. However, typical benchmark to use is the ROUGE(Recall-Oriented Understudy for Gisting Evaluation) measure. It is a recall-based measure, which encourages an algorithm to cover as many topics as it can. The measure compares the generated summary against a reference summary and computes the recall based on any ngram. For the purposes of this tutorial, we will not go over the evaluation because the ROUGE system requires a registration application. \n",
    "\n",
    "# Further Reading\n",
    "\n",
    "The original TextRank paper: \n",
    "http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
    "\n",
    "Other, more advanced and better versions of TextRank: \n",
    "\n",
    "DivRank:\n",
    "http://clair.si.umich.edu/~radev/papers/SIGKDD2010.pdf\n",
    "\n",
    "CollabRank:\n",
    "http://www.aclweb.org/anthology/C/C08/C08-1122.pdf\n",
    "\n",
    "ExpandRank:\n",
    "http://www.aaai.org/Papers/AAAI/2008/AAAI08-136.pdf\n",
    "\n",
    "Wikipedia:\n",
    "https://en.wikipedia.org/wiki/Automatic_summarization\n",
    "\n",
    "More on Evaluation:\n",
    "ROUGE for python\n",
    "https://pypi.python.org/pypi/pyrouge/0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
