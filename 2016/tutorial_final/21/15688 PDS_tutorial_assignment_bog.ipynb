{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15688 Practical Data Science: Student Tutorial_Assignment Checkout\n",
    "\n",
    "## Word2vec in TensorFlow\n",
    "\n",
    "Carnegie Mellon University\n",
    "\n",
    "Gilbert Gao\n",
    "\n",
    "*bog@andrew.cmu.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conception\n",
    "\n",
    "#### What is Word Embedding ?\n",
    "\n",
    ">Word Embedding or Distributed representation, a Vector space models (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other'). \n",
    "\n",
    "[source](https://www.tensorflow.org/versions/r0.11/tutorials/word2vec/index.html)\n",
    "\n",
    "In Natural Language Processing, the traditional and classical method is to represent a word to a discrete signal, for example, [cat] -> [id537], and [dog] -> [id142]. The most shortage of the method is that it is lacking of relationship of these two words (they are both animals). And this One-hot Representation also make the vectors of words too sparse, so we need much training to get a satisficed model. Word Embedding is developed to solve such problem. \n",
    "\n",
    "Word embedding(Vector Representations of Words) is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much lower dimension.[source](https://en.wikipedia.org/wiki/Word_embedding)\n",
    "\n",
    "\n",
    "A word embedding $W:words→ℝ_n$ is a paramaterized function mapping words in some language to high-dimensional vectors (perhaps 200 to 500 dimensions). For example, we might find:\n",
    "\n",
    "$$W(\"cat\")=(0.2, -0.4, 0.7, ...)$$\n",
    "$$W(\"cap\")=(0.1, -0.4, 0.7, ...)$$\n",
    "\n",
    "$$W(\"mat\")=(0.0, 0.6, -0.1, ...)$$\n",
    "$$W(\"map\")=(0.0, 0.6, -0.1, ...)$$\n",
    "\n",
    "Typically, the function is a lookup table, parameterized by a matrix, with a row for each word: \n",
    "$$W_θ(w_n)=θ_n$$\n",
    "\n",
    "VSMs have a long, rich history in NLP, but all methods depend in some way or another on the Distributional Hypothesis, which states that words that appear in the same contexts share semantic meaning. The different approaches that leverage this principle can be divided into two categories: count-based methods (e.g. Latent Semantic Analysis), and predictive methods (e.g. neural probabilistic language models).\n",
    "\n",
    "source: [Distributed Representations of Words and Phrases and Their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is TensorFlow ?\n",
    "\n",
    "TensorFlow™ is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.\n",
    "\n",
    "[source](https://www.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### What is word2vec ?\n",
    "\n",
    "Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text. word2vec is an efficient implementation of the continuous bag-of-words(CBOW) or skip-gram architectures(Word Embedding) for computing vector representations of words(These two architectures are similar in algorithm.), literary translating words (strings) to vectors (lists of floats). These representations can be subsequently used in many natural language processing applications and for further research.\n",
    "\n",
    "word2vec organizes word by semantic meaning, and turns text into a numerical form that Deep Learning Nets and machine learning algorithms can in-turn use.\n",
    "\n",
    "\n",
    "#### word2vec is a simple neural networks\n",
    "\n",
    "Word2vec is a two-layer neural network that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus.\n",
    "\n",
    "![Figure. word2vec nerual network ](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
    "\n",
    "\n",
    "#### Usage of Workflow \n",
    "\n",
    "The word2vec model takes a text corpus as input and produces the word vectors as output. At first it constructs a vocabulary from the training text data corpus and then learns vector representation of words.\n",
    "\n",
    "Reference\n",
    "https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "\n",
    "#### Two architectures of word2vec\n",
    "\n",
    "1.Continuous bag of words(CBOW): Predict a missing word in a sentence based on the surrounding context\n",
    "2.Skip-gram: Each current word as an input to a log-linear classifier to predict words within a certain range before and after that current word\n",
    "![Figure. CBOW vs.Skip-gram](https://silvrback.s3.amazonaws.com/uploads/60a81cd5-5189-4550-9709-523b3feef3d1/sentiment_01_large.png)\n",
    "\n",
    "Skip-gram architecture could be viewed as the inverse of Continuous bag of words architecture. Given the context (surronding words) to CBOW, predict the current word.  Given the current word to Skip-gram architecture , predict the context (surrounding words).\n",
    "\n",
    "Compare these two architectures, CBOW is several times faster to train than the skip-gram and has slightly better accuracy for frequent words. Skip-gram works well with a small amount of the training data and well represents rare words. And Skip-gram is the most common architecture.\n",
    "\n",
    "Referred from a Google implementation of word2vec,\n",
    "Performance:\n",
    "The training speed can be significantly improved by using parallel training on multiple-CPU machine (use the switch '-threads N'). The hyper-parameter choice is crucial for performance (both speed and accuracy), however varies for different applications. The main choices to make are:\n",
    "- architecture: skip-gram (slower, better for infrequent words) vs CBOW (fast)\n",
    "- the training algorithm: hierarchical softmax (better for infrequent words) vs negative sampling (better for frequent words, better with low dimensional vectors)\n",
    "- sub-sampling of frequent words: can improve both accuracy and speed for large data sets (useful values are in range 1e-3 to 1e-5)\n",
    "- dimensionality of the word vectors: usually more is better, but not always\n",
    "- context (window) size: for skip-gram usually around 10, for CBOW around 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Training\n",
    "\n",
    "Neural probabilistic language models are traditionally trained using the maximum likelihood (ML) principle to maximize the probability of the next word $w_t$ (for \"target\") given the previous words $h$ (for \"history\") in terms of a softmax function,\n",
    "\n",
    "However, feature learning in word2vec we do not need a full probabilistic model, The CBOW and skip-gram models are instead trained using a binary classification objective (logistic regression) to discriminate the real target words $w_t$ from $k$ imaginary (noise) words, in the same context.\n",
    "\n",
    "For detail of softmax function, there is a [paper](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf) as reference. \n",
    "\n",
    "\n",
    "#### Evaluating Embeddings: Analogical Reasoning\n",
    "\n",
    "Embeddings are useful for a wide variety of prediction tasks in NLP. Short of training a full-blown part-of-speech model or named-entity model, one simple way to evaluate embeddings is to directly use them to predict syntactic and semantic relationships like king is to queen as father is to ?. This is called analogical reasoning and the task was introduced by Mikolov and colleagues. TensorFlow has build_eval_graph() and eval() functions in tensorflow/models/embedding/word2vec.py.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## word2vec Workout\n",
    "\n",
    "The goal of this part is to implement simple word2vec using TensorFlow and to train a Word2Vec skip-gram model on given data\n",
    "\n",
    "Referrence: TensorFlow Official word2vec implementation, following the steps.\n",
    "[tensorflow/models/embedding/word2vec.py](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/models/embedding/word2vec.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from six.moves import urllib\n",
    "import zipfile\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "[source](http://mattmahoney.net/dc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "URL = 'http://mattmahoney.net/dc/'\n",
    "EXPECTED_BYTES = 31344016\n",
    "FILENAME = 'text8.zip'\n",
    "\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(URL + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "    \n",
    "\n",
    "# Read the data into a string.\n",
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    \n",
    "    for name in f.namelist():        \n",
    "        return f.read(name).split()\n",
    "    f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Found and verified', 'text8.zip')\n",
      "('type(words): ', <type 'list'>)\n",
      "('len(words): ', 17005207)\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    filename = maybe_download(FILENAME, EXPECTED_BYTES)  \n",
    "    words = read_data(FILENAME)\n",
    "    print(\"type(words): \", type(words))\n",
    "    print(\"len(words): \", len(words)) \n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"ERROR: \", e.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build the dictionary and replace rare words with UNK token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 50000\n",
    "\n",
    "# words : type is of list, total size = 17005207\n",
    "def build_dataset(words):\n",
    "    # word histogram\n",
    "    count = [['UNK', -1]]   \n",
    "\n",
    "    # container without overlapping of words \n",
    "    # (word, counts)\n",
    "    count.extend(collections.Counter(words).most_common(VOCABULARY_SIZE - 1))\n",
    "\n",
    "    # word -> id\n",
    "    dictionary = dict()\n",
    "    \n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) \n",
    "\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    \n",
    "    # convert a sequence of words into a sequence of  id numbers.\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "#             count[0][1] = unk_count\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    \n",
    "    # id -> word\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "build_dataset(words)\n",
    "del words  # to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words and UNK [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430)]\n",
      "Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156, 128, 742, 477, 10572, 134]\n",
      "len(data) 17005207\n"
     ]
    }
   ],
   "source": [
    "print 'Most common words and UNK', count[:10]\n",
    "print 'Sample data', data[:15]\n",
    "print 'len(data)', len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate a training batch for the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    " \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32) # batch (8,)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # labels (8,1)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ] # 3\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    for _ in range(span): # 0,1,2\n",
    "        buffer.append(data[data_index]) # 0 -> 1 -> 2\n",
    "        data_index = (data_index + 1) % len(data) # 1 -> 2 -> 3\n",
    "        \n",
    "    for i in range(batch_size // num_skips): # 0,1,2,3\n",
    "        target = skip_window  # target label at the center of the buffer, 1 \n",
    "        targets_to_avoid = [ skip_window ] \n",
    "        \n",
    "        # now, target is 1\n",
    "        for j in range(num_skips): # 0, 1\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1) # either of 0,1, or 2\n",
    "            targets_to_avoid.append(target) \n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]                       \n",
    "        buffer.append(data[data_index])        \n",
    "        data_index = (data_index + 1) % len(data)        \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3084, '->', 12)\n",
      "(3084, '->', 5239)\n",
      "(12, '->', 6)\n",
      "(12, '->', 3084)\n",
      "(6, '->', 12)\n",
      "(6, '->', 195)\n",
      "(195, '->', 2)\n",
      "(195, '->', 6)\n",
      "5239\n",
      "3084\n",
      "12\n",
      "6\n",
      "195\n",
      "2\n",
      "3137\n",
      "46\n",
      "59\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_SKIPS = 2 # 4 How many times to reuse an input to generate a label\n",
    "SKIP_WINDOW = 1 # 2 How many words to consider left and right.\n",
    "\n",
    "batch, labels = generate_batch(batch_size=BATCH_SIZE, num_skips=NUM_SKIPS, skip_window=SKIP_WINDOW)\n",
    "for i in range(8):\n",
    "    print(batch[i], '->', labels[i, 0])\n",
    "for i in data[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train a skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('valid_examples', array([ 3, 99, 11, 73, 84, 30, 38, 63, 52, 96, 67, 98, 28, 50, 61, 89]))\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 128  # Dimension of the embedding vector.\n",
    "BATCH_SIZE = 128\n",
    "# pick a random validation set to sample nearest neighbors, but limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "VALID_SIZE = 16     # Random set of words to evaluate similarity on.\n",
    "VALID_WINDOW = 100  # Only pick dev samples in the head of the distribution.\n",
    "NUM_SAMPLED = 64    # Number of negative examples to sample.\n",
    "\n",
    "valid_examples = np.array(random.sample(np.arange(VALID_WINDOW), VALID_SIZE))\n",
    "print(\"valid_examples\", valid_examples)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with graph.device(\"/cpu:0\"):\n",
    "        # Input data.\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        \n",
    "        # Construct the variables.\n",
    "        \n",
    "        # input embeddings: W_I\n",
    "        # define Embedding Matrix and random initialized\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform( [VOCABULARY_SIZE, EMBEDDING_SIZE], -1.0, 1.0)\n",
    "        )\n",
    "        \n",
    "        # output weights: W_O\n",
    "        # Noise-Contrastive softmax function, need to set weight for every word\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [VOCABULARY_SIZE, EMBEDDING_SIZE],\n",
    "                stddev=1.0 / math.sqrt(EMBEDDING_SIZE)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        nce_biases = tf.Variable(tf.zeros([VOCABULARY_SIZE]))\n",
    "            \n",
    "        # Look up embeddings(vector) for inputs in batch. v_t = W_I x_t\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                nce_weights, # W_O\n",
    "                nce_biases,  # b_O\n",
    "                embed,  # v_t\n",
    "                train_labels,\n",
    "                NUM_SAMPLED, # the number of classes to randomly sample per batch: Negative sampling \n",
    "                VOCABULARY_SIZE, # the number of possible classes.\n",
    "                num_true=1 # the number of target classes per training example\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Now we have the loss node, but we still\n",
    "        # but we still need to construct the Stochastic gradient descent optimizer \n",
    "        # using a learning rate of 1.0.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "        \n",
    "        # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm # make v_t unit vector\n",
    "        \n",
    "        # Calculate unit vector v_t = W_I x_t\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "            normalized_embeddings, \n",
    "            valid_dataset\n",
    "        )\n",
    "        \n",
    "        # |v_t|^2\n",
    "        similarity = tf.matmul(\n",
    "            valid_embeddings, \n",
    "            normalized_embeddings, \n",
    "            transpose_b = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('valid_embeddings._shape: ', TensorShape([Dimension(16), Dimension(128)]))\n",
      "('normalized_embeddings._shape: ', TensorShape([Dimension(50000), Dimension(128)]))\n",
      "('similarity._shape: ', TensorShape([Dimension(16), Dimension(50000)]))\n"
     ]
    }
   ],
   "source": [
    "print(\"valid_embeddings._shape: \", valid_embeddings._shape)\n",
    "print(\"normalized_embeddings._shape: \", normalized_embeddings._shape)\n",
    "print(\"similarity._shape: \", similarity._shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Variables Initialized\n",
      "('Average loss at step ', 0, ': ', 278.0208740234375)\n",
      "Nearest to and: allusions, buzzard, temptation, commodities, ordaining, migrate, recourse, obsessed,\n",
      "Nearest to while: jingles, hellenism, enclosure, troy, sociale, essayists, watches, fished,\n",
      "Nearest to is: duff, confound, outskirts, subtleties, frege, fantasia, deprivation, effectiveness,\n",
      "Nearest to b: beers, counterfactual, darling, crystal, bodhisattvas, adp, medicine, appendicitis,\n",
      "Nearest to war: bleed, tls, pakistani, chiropractors, cultivars, jure, minya, phonemic,\n",
      "Nearest to his: audition, jammed, hulls, excitation, brahma, oppressive, insurrections, modernist,\n",
      "Nearest to not: pullback, apsu, nubia, lets, ostensible, bastard, mucous, bryozoans,\n",
      "Nearest to into: caldera, durst, kellermann, granddaughter, collided, rodgers, mixing, prayed,\n",
      "Nearest to most: firemen, vigilante, patel, pony, martini, muppet, polg, inhospitable,\n",
      "Nearest to history: rome, accelerating, holistic, slid, mohawk, balkan, tibet, intervening,\n",
      "Nearest to only: lemons, consecrations, reflections, ebbinghaus, avatar, ag, descending, slams,\n",
      "Nearest to up: workshops, admitting, nov, slurs, himmler, turbine, newsletters, erudition,\n",
      "Nearest to from: authorize, encroaching, denominations, cages, motorcycle, steadily, disadvantages, mythographers,\n",
      "Nearest to all: notification, flame, illusions, piet, codecs, binomial, respiration, culminating,\n",
      "Nearest to after: ballistics, vignette, smelling, tradesmen, prey, asset, ethnographers, proposing,\n",
      "Nearest to called: academia, migne, arid, kla, greenberg, flies, acheson, dictate,\n"
     ]
    }
   ],
   "source": [
    "NUM_STEPS = 100 #001\n",
    "DISPLAY_STEPS = 2000\n",
    "SIMILARITY_STEPS = 10000\n",
    "MODEL_PATH = \"./model.ckpt\"\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # must initialize all variables before we use them.\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"All Variables Initialized\")\n",
    "    \n",
    "    average_loss = 0\n",
    "    \n",
    "    # The training of model is simple\n",
    "    # Just start session, using feed_dict, putting into training data\n",
    "    for step in xrange(NUM_STEPS):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size=BATCH_SIZE, num_skips=NUM_SKIPS, skip_window=SKIP_WINDOW)\n",
    "        feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
    "        \n",
    "        # One update step is evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "    \n",
    "        if step % DISPLAY_STEPS == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / DISPLAY_STEPS\n",
    "    \n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "    \n",
    "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % SIMILARITY_STEPS == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(VALID_SIZE):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log_str = \"Nearest to %s:\" % valid_word\n",
    "            \n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)\n",
    "                \n",
    "    # equivalent to session.run(normalized_embeddings)             \n",
    "    final_embeddings = normalized_embeddings.eval()     \n",
    "    \n",
    "    # save a model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# restore the model\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Python27]",
   "language": "python",
   "name": "Python [Python27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
