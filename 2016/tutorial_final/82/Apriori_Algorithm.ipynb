{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "This tutorial will introduce you to the method of Association Rules Mining and a seminal algoithm known as Apriori Algorithm, for mining assosciation rules. Association rules mining builds upon the broader concept of mining frequent patterns. Frequent patterns are patterns that appear in datasets recurrently and frequently. The motivation for frequent patterns mining comes from Rakesh Agrawals concept of strong rules for disconvering associations between products in a transactions records at point-of-sale systems of supermarkets. This example of mining for frequent itemsets is widely known as market-basket analysis.\n",
    "\n",
    "#### Market Basket Analysis:\n",
    "Market basket analysis is the process of analyzing customer buying habits by discovering associations between different products  or items that customers place in their shopping basket. The associations, when discovered, help the retailers to manage their shelf space, develop marketing strategies, engage in selective marketing and bundling of the products together. For example, if a customer buys a toothbrush, what is the likelihood of the customer buying a mouthwash like Listerine. \n",
    "\n",
    "\n",
    "\n",
    "The association rules mining also finds its applications in recommendation systems in e-commerce websites, video streaming websites, borrower defaulter prediction in capital lending firms, web-page usage mining, intrusion detection and so on. \n",
    "\n",
    "Although there are many association rules mining algorithms, we would be exploring apriori algorithm. In doing so, we will define the constituents of association rules viz, itemsets, frequent itemsets etc. \n",
    "\n",
    "#### Tutorial Content:\n",
    "\n",
    "In our build-up to implementing apriori algorithm, we will learn about what itemsets are, how are they represented, the measures that quantify interestingness of sossciation rules. Theorotically, we will use the basic concepts of probability to define the measures that quantify interestingness in association rules. In python, we would be using following libraries to implement to algorithm :\n",
    "\n",
    "    numpy\n",
    "    pandas\n",
    "    itertools.chain\n",
    "    itertools.combinations\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import collections as cp\n",
    "from itertools import chain\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures of Rule Interestingness in dataset:\n",
    "\n",
    "There are two measures of rule interestingess of data that lays foundation to mine frequent patterns. They are known as rule support and confidence. \n",
    "\n",
    "                                 toothbrush => mouthwash [support = 5%, confidence = 80%]\n",
    "\n",
    "A support of 5% of association rule is equal to saying that 5% of all the transactions that are being considered for analysis have toothbrush and mouthwash purchased together.\n",
    "\n",
    "A confidence of 80% of association rule is equivalent to saying that 80% of the customers who bought toothbrush also bought mouthwash.\n",
    "\n",
    "Association rules are considered to be interesting if they satisfy a minimum support threshold and a minimum confidence threshold. These thresholds can be set by the users of the system, decision managers of the organization, or domain experts. \n",
    "\n",
    "\n",
    "# Itemsets and Association Rules\n",
    "\n",
    "Let $$I = {I_1, I_2,..., I_m}$$ be a set of items and D be the dataset under consideration. Each transaction T is a set of items such that T ⊆ I and has an identifier, TID. Let A be a set of items. A transaction T is said to contain A if and only if A ⊆ T. An association rule is an implication of the form A ⇒ B, where A ⊂ I, B ⊂ I, and A∩B = φ.\n",
    "\n",
    "The association rule A ⇒ B holds in the transaction set D, with support *s* (percentage of transactions in D that contain A U B ) \n",
    "\n",
    "                                support( A ⇒ B ) = P(A U B)\n",
    "                                \n",
    "and with confidence *c*, where c is the percentage of transactions in D containing A that also contain B, which is equal to the conditional probability *P(B*|*A)*. \n",
    "                                \n",
    "                                confidence( A ⇒ B ) = P(B|A)\n",
    "                             \n",
    "                             \n",
    "Rules that satistfy the requirement of minimum support threshold and minimum confidence threshold are considered as strong rules. \n",
    "\n",
    "At this point we can introduce the association rule mining, generally, a two step process :\n",
    "\n",
    "1. Find all the itemsets that are frequent by selecting the itemsets that occur at least as frequently as a predetermined minimum support count, min_sup.\n",
    "\n",
    "2. Generate strong association rules from the frequent itemsets obtained from the step 1. In addition to the min_sup requirement, these rules must satisfy the mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fileExtract(filename):\n",
    "    with open(filename, 'rU') as file_iter:\n",
    "        for line in file_iter:\n",
    "                line = line.strip().rstrip(',') #Removing the the comma at the end of the line\n",
    "                record = frozenset(line.split(','))\n",
    "                yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The data of each set is stored in frozenset object which is immutable \n",
    "filename = \"\"\"C:\\Users\\Ketan\\Documents\\CMU\\Fall Semester\\Practical Data Science\\Tutorial\\INTEGRATED-DATASET.csv\"\"\"\n",
    "loadedData = fileExtract(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>>print list(loadedData)\n",
    "[frozenset(['Brooklyn', 'LBE', '11204']), frozenset(['Cambria Heights', 'MBE', 'WBE', 'BLACK', '11411']), \n",
    " frozenset(['MBE', '10598', 'BLACK', 'Yorktown Heights']), frozenset(['11561', 'MBE', 'BLACK', 'Long Beach']), \n",
    " frozenset(['MBE', 'Brooklyn', 'ASIAN', '11235']), frozenset(['MBE', '10010', 'WBE', 'ASIAN', 'New York']), frozenset(['10026', 'MBE', 'New York', 'ASIAN']), \n",
    " frozenset(['10026', 'MBE', 'New York', 'BLACK'])\n",
    " ....\n",
    " ....\n",
    " ....\n",
    " frozenset(['NON-MINORITY', 'WBE', '10025', 'New York']), frozenset(['MBE', '11554', 'WBE', 'ASIAN', 'East Meadow']), \n",
    " frozenset(['MBE', 'Brooklyn', 'WBE', 'BLACK', '11208']), frozenset(['NON-MINORITY', 'WBE', '7717', 'Avon by the Sea']), \n",
    " frozenset(['MBE', '11417', 'LBE', 'ASIAN', 'Ozone Park']), frozenset(['NON-MINORITY', '10010', 'WBE', 'New York']), \n",
    " frozenset(['NON-MINORITY', 'Teaneck', 'WBE', '7666']), frozenset(['Bronx', 'MBE', 'WBE', 'BLACK', '10456']), \n",
    " frozenset(['MBE', '7514', 'BLACK', 'Paterson']), frozenset(['NON-MINORITY', 'WBE', '10023', 'New York']), \n",
    " frozenset(['MBE', 'Valley Stream', 'ASIAN', '11580']), frozenset(['MBE', 'Brooklyn', 'BLACK', '11214']), \n",
    " frozenset(['New York', 'LBE', '10016']), frozenset(['MBE', 'New York', 'ASIAN', '10002'])]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getItemsetsTransactionsList(loadedData):\n",
    "    transactionList = list()                #Create list of transactions\n",
    "    itemSet = set()             \n",
    "    for record in loadedData:\n",
    "        transaction = frozenset(record)\n",
    "        transactionList.append(transaction)\n",
    "        for item in transaction:\n",
    "            itemSet.add(frozenset([item]))  # Generating 1-itemSets\n",
    "    return itemSet, transactionList\n",
    "itemSet, transactionList = getItemsetsTransactionsList(loadedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> print itemSet\n",
    "frozenset(['Brooklyn', 'LBE', '11204'])\n",
    "frozenset(['Cambria Heights', 'MBE', 'WBE', 'BLACK', '11411'])\n",
    "frozenset(['MBE', '10598', 'BLACK', 'Yorktown Heights'])\n",
    "frozenset(['11561', 'MBE', 'BLACK', 'Long Beach'])\n",
    "frozenset(['MBE', 'Brooklyn', 'ASIAN', '11235'])\n",
    "frozenset(['MBE', '10010', 'WBE', 'ASIAN', 'New York'])\n",
    "frozenset(['10026', 'MBE', 'New York', 'ASIAN'])\n",
    "frozenset(['10026', 'MBE', 'New York', 'BLACK'])\n",
    ".....\n",
    ".....\n",
    "frozenset(['NON-MINORITY', 'WBE', 'Mineola', '11501'])\n",
    "frozenset(['MBE', 'ASIAN', '10550', 'Mount Vernon'])\n",
    "frozenset(['MBE', 'Port Chester', '10573', 'HISPANIC'])\n",
    "frozenset(['NON-MINORITY', 'Merrick', 'WBE', '11566'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have generated unique itemsets and the transaction list of all transactions, the next step is to process them by applying the Apriori ALgorithm.\n",
    "\n",
    "Apriori Algorithm uses the prior knowledge of the frequently occurring itemsets. It employs an iterative approach also known as level-wise search, where k-itemsets are used to explore (k+1) itemsets.     \n",
    "        \n",
    " \n",
    "Apriori Algorithm can be divided into two steps:\n",
    "\n",
    " 1. Join Step\n",
    " 2. Prune Step\n",
    "        \n",
    "###### 1. Join Step: \n",
    "In this step, a set of candidate k-itemsets is generating by joining $L_{k-1}$ with itself to find $L_k$\n",
    "###### 2. Prune Step: \n",
    "A superset of $L_k$ called $C_k$ is maintained, which has members that may or may not be frequent. To determine the items that become part of $L_k$, a scan of a transaction list is made to check of counts of the items greater than the minimum support count. All the items that have count greater than minimum support count become part of $L_k$. However, $C_k$ can be very huge and result in too many scans to the transactionList (which would be itself huge) and a lot of computation. To avoid this, the algorithm makes use of what is called the *Apriori Property*, which is described below. \n",
    "\n",
    "Any (k − 1)-itemset that is not frequent cannot be a subset of a frequent k-itemset. Hence, if any (k − 1)-subset of a candidate k-itemset is not in $L_{k−1}$, then the candidate cannot be frequent either and so can be removed from $C_k$. This subset testing can be done quickly by maintaining a hash tree of all frequent itemsets.\n",
    "\n",
    "#### Illustration by example :\n",
    "\n",
    "[<img src=\"https://s18.postimg.org/eujogosbt/Transactions.png\">](https://s18.postimg.org/eujogosbt/Transactions.png)\n",
    "\n",
    "                                        [Sourced from Data Mining: Concepts and Techniques]\n",
    "\n",
    "Suppose we have a database of transactions as shown above. It has 9 transactions. Each transaction has one or many items that were purchased together. We will apply Apriori Algorithm the transaction dataset to find the frequent itemsets.\n",
    "\n",
    "**Step : 1.**    In the first iteration of the algorithm, each item that appears in the transaction set, is one of the members of the candidate 1-itemsets, $C_1$. As such, we scan the dataset to get counts of occurences of all the items.\n",
    "\n",
    "**Step : 2.**    We will assume that the minimum support count is of 2 counts. $therefore$ Relative support would be $2/9 = 22%$. Now we can identify the set of frequent itemsets, $L_1$. It would be all the candidate 1-itemsets in $C_1$ that satisfy the minimum support condition. In our case, all candidates satisfy this condition.\n",
    "\n",
    "[<img src=\"https://s14.postimg.org/qv5g284w1/image.png\">](https://s14.postimg.org/qv5g284w1/image.png)\n",
    "\n",
    "**Step : 3.**    Now comes the **join step**. We would now join $L_1$ with itself to generate candidate set of 2-itemsets, $C_2$. It is to be noted that each subset of the candidates in $C_2$ is also frequent, hence, the **prune step** would not remove any candidates.\n",
    "\n",
    "**Step : 4.**    Again, the transaction dataset is scanned to get the support counts of all the candidates in $C_2$. The candidates that have support count greater than *min_sup* make up the frequent 2-itemsets, $L_2$\n",
    "\n",
    "[<img src=\"https://s12.postimg.org/cw3wvqxsd/image.png\">](https://s12.postimg.org/cw3wvqxsd/image.png)\n",
    "\n",
    "**Step : 5.**    Now, for generation of candidate 3-itemsets, $C_3$, we join $L_2 x L_2$, from which we obtain : {{$I_1$, $I_2$, $I_3$}, {$I_1$, $I_2$, $I_5$}, {$I_1$, $I_3$, $I_5$}, {$I_2$, $I_3$, $I_4$}, {$I_2$, $I_3$, $I_5$}, {$I_2$, $I_4$, $I_5$}}. We can apply the **prune step** here. We know that Apriori Property says that for a itemset to be frequent, all of its subsets must also be frequent. If we take the $4^th$ itemset, {$I_2$, $I_3$, $I_4$}, the subset {$I_3$, $I_4$} is not a frequent 2-itemset (Please refer the picture for $L_2$). And hence, {$I_2$, $I_3$, $I_4$} is not a frequent 3-itemset. Same can be deduced about the other three candidate 3-itemsets and hence would be pruned. This saved the effort of retrieving the counts of these itemsets during the subsequent scan to the transaction dataset. \n",
    "\n",
    "**Step : 6**     The transactions in the dataset are scanned to determine the counts of the remaining and those have counts greater than the min_sup are selected as frequent 3-itemset, $L_3$.\n",
    "\n",
    "[<img src=\"https://s13.postimg.org/d1w0nw05j/image.png\">](https://s13.postimg.org/d1w0nw05j/image.png)\n",
    "\n",
    "**Step : 7**     Further, the algorithm performs, $L_3 x L_3$ to get the candidate set of 4-itemsets, $C_4$. The join results in {{$I_1$,$I_2$,$I_3$,$I_5$}}, however, is pruned because its subset {$I_2$,$I_3$,$I_5$} is not frequent. And hence we reach a point where $C_4 = \\phi$ and the algorithm terminates, having found all the frequent itemsets.\n",
    "\n",
    "\n",
    "## Generating Association Rules from Frequent Itemsets:\n",
    "\n",
    "Now that we have all the possible frequent itemsets, we proceed to find the association rules, (which is the ultimate goal of the activity). The strong association rules satisfy both the minimum support threshold and minimum confidence threshold. We can find the confidence using the following equation for two items A and B :\n",
    "\n",
    "\n",
    "                        confidence (A => B) = P(B/A) = support_count(A U B)/support_count(A)\n",
    "                        \n",
    "Based on this equation, the association rules can be formed as follows :\n",
    "\n",
    "- For each frequent itemset $l$, generate all nonempty subsets of $l$.\n",
    "                        \n",
    "- For every nonempty subset $s$ of $l$, output the rule “$s ⇒ (l − s)$” if \n",
    "                      \n",
    " $\\frac{(support count(l))}{(support count(s))}$ ≥ $min-conf$\n",
    "              \n",
    "   where min_conf is the minimum confidence threshold.\n",
    "   \n",
    "From our example, one of the frequent 3-itemset was $l$ = {{$I_1$,$I_2$,$I_5$}}. The non-empty subsets that can be generated from this itemset are {$I_1$}, {$I_2$}, {$I_5$}, {$I_1$,$I_2$}, {$I_2$,$I_5$}, {$I_1$,$I_5$}. The resulting association rules, by applying the formula above are :\n",
    "\n",
    "$I_1$ ^ $I_2$   $=>$   $I_5$   $:$  $confidence = 2/4 = 50\\% $\n",
    "\n",
    "$I_1$ ^ $I_5$   $=>$   $I_2$   $:$  $confidence = 2/2 = 100\\% $\n",
    "\n",
    "$I_5$ ^ $I_2$   $=>$   $I_1$   $:$  $confidence = 2/2 = 100\\% $\n",
    "\n",
    "$I_1$ $=>$ $I_2$ ^ $I_5$       $:$  $confidence = 2/6 = 100\\% $\n",
    "\n",
    "$I_2$ $=>$ $I_1$ ^ $I_5$       $:$  $confidence = 2/7 = 29\\% $\n",
    "\n",
    "$I_5$ $=>$ $I_2$ ^ $I_1$       $:$  $confidence = 2/2 = 100\\% $\n",
    "\n",
    "By fixing the minimum confidence threshold, we can select or reject the rules that satisfy or don' satisfy the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequencySet = cp.defaultdict(int) \n",
    "largeSet = dict()\n",
    "assocRules = dict()\n",
    "if(assocRules == largeSet){\n",
    "    print \"Should not happen\"\n",
    "}\n",
    "else {\n",
    "    print \"OK\"\n",
    "} #Vanity check, not relevant for calculation\n",
    "minSupport = 0.17\n",
    "minConfidence = 0.5\n",
    "\n",
    "def getMinimumSupportItems(itemSet, transactionList, minSupport, freqSet):\n",
    "        \"\"\"Function to calculate the support of items of itemset in the transaction. The support is checked against minimum support.\n",
    "        Returns the itemset with those  items that satisfy the minimum threshold requirement\"\"\"\n",
    "        newItemSet = set()\n",
    "        localSet = cp.defaultdict(int) #local dictionary to count the items in the itemset that are part of the transaction\n",
    "\n",
    "        for item in itemSet:\n",
    "                for transaction in transactionList:\n",
    "                        if item.issubset(transaction):\n",
    "                                frequencySet[item] += 1\n",
    "                                localSet[item] += 1\n",
    "        print itemSet\n",
    "        for item, count in localSet.items():\n",
    "                support = float(count)/len(transactionList)\n",
    "\n",
    "                if support >= minSupport:\n",
    "                        newItemSet.add(item)\n",
    "\n",
    "        return newItemSet\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing and confirming the contents of the qualified newItemSet\n",
    "supportOnlySet = getMinimumSupportItems(itemSet, transactionList, minSupport, frequencySet)\n",
    "print supportOnlySet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the frequent 1-itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> print supportOnlySet\n",
    "set([frozenset(['BLACK']), frozenset(['ASIAN']), frozenset(['New York']), frozenset(['MBE']), frozenset(['NON-MINORITY']), frozenset(['WBE'])])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def joinSet(itemSet, length):\n",
    "    \"\"\"Function to perform the join step of the Apriori Algorithm\"\"\"\n",
    "    return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subsets(arr):\n",
    "    \"\"\" Returns non empty subsets of arr\"\"\"\n",
    "    return chain(*[combinations(arr, i + 1) for i, a in enumerate(arr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We canlculate the k-itemsets by iterating level-wise will there \n",
    "# are no frequent itemsets as illustrated in the example above\n",
    "toBeProcessedSet = supportOnlySet\n",
    "k = 2 \n",
    "while(toBeProcessedSet != set([])):\n",
    "    largeSet[k-1] = toBeProcessedSet\n",
    "    toBeProcessedSet = joinSet(toBeProcessedSet, k)\n",
    "    toBeProcessedSet_c = getMinimumSupportItems(toBeProcessedSet,transactionList,minSupport,frequencySet)\n",
    "    toBeProcessedSet = toBeProcessedSet_c\n",
    "    k = k + 1\n",
    "\n",
    "def getSupport(item):\n",
    "    \"Local function to get the support of k-itemsets\"\n",
    "    return float(frequencySet[item])/len(transactionList)\n",
    "\n",
    "finalItems = []\n",
    "for key, value in largeSet.items():\n",
    "    finalItems.extend([(tuple(item), getSupport(item)) for item in value])\n",
    "print finalItems\n",
    "\n",
    "finalRules = []\n",
    "for key, value in largeSet.items()[1:]:\n",
    "    for item in value:\n",
    "        _subsets = map(frozenset, [x for x in subsets(item)])\n",
    "        for element in _subsets:\n",
    "            remain = item.difference(element)\n",
    "            if len(remain) > 0:\n",
    "                confidence = getSupport(item)/getSupport(element)\n",
    "                if confidence >= minConfidence:\n",
    "                    finalRules.append(((tuple(element), tuple(remain)), confidence))\n",
    "print finalRules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printResults(items, rules):\n",
    "    \"\"\"prints the generated itemsets sorted by support and the confidence rules sorted by confidence\"\"\"\n",
    "    for item, support in sorted(items, key=lambda (item, support): support):\n",
    "        print \"item: %s , %.3f\" % (str(item), support)\n",
    "    print \"\\n------------------------ RULES:\"\n",
    "    for rule, confidence in sorted(rules, key=lambda (rule, confidence): confidence):\n",
    "        pre, post = rule\n",
    "        print \"Rule: %s ==> %s , %.3f\" % (str(pre), str(post), confidence)\n",
    "printResults(finalItems, finalRules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> printResults(finalItems, finalRules)\n",
    "item: ('MBE', 'New York') , 0.170\n",
    "item: ('New York', 'WBE') , 0.175\n",
    "item: ('MBE', 'ASIAN') , 0.200\n",
    "item: ('ASIAN',) , 0.202\n",
    "item: ('New York',) , 0.295\n",
    "item: ('NON-MINORITY',) , 0.300\n",
    "item: ('NON-MINORITY', 'WBE') , 0.300\n",
    "item: ('BLACK',) , 0.301\n",
    "item: ('MBE', 'BLACK') , 0.301\n",
    "item: ('WBE',) , 0.477\n",
    "item: ('MBE',) , 0.671\n",
    "\n",
    "------------------------ RULES:\n",
    "Rule: ('New York',) ==> ('MBE',) , 0.578\n",
    "Rule: ('New York',) ==> ('WBE',) , 0.594\n",
    "Rule: ('WBE',) ==> ('NON-MINORITY',) , 0.628\n",
    "Rule: ('ASIAN',) ==> ('MBE',) , 0.990\n",
    "Rule: ('BLACK',) ==> ('MBE',) , 1.000\n",
    "Rule: ('NON-MINORITY',) ==> ('WBE',) , 1.000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### If the code is to be converted in a python file, please copy this pease of code at the top of the blocks. \n",
    "#You will be able to pass arguments of min_sup and min_conf from command line\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    optparser = OptionParser()\n",
    "    optparser.add_option('-f', '--inputFile',\n",
    "                         dest='input',\n",
    "                         help='filename containing csv',\n",
    "                         default=None)\n",
    "    optparser.add_option('-s', '--minSupport',\n",
    "                         dest='minS',\n",
    "                         help='minimum support value',\n",
    "                         default=0.15,\n",
    "                         type='float')\n",
    "    optparser.add_option('-c', '--minConfidence',\n",
    "                         dest='minC',\n",
    "                         help='minimum confidence value',\n",
    "                         default=0.6,\n",
    "                         type='float')\n",
    "\n",
    "    (options, args) = optparser.parse_args()\n",
    "\n",
    "    inFile = None\n",
    "    if options.input is None:\n",
    "            inFile = sys.stdin\n",
    "    elif options.input is not None:\n",
    "            inFile = dataFromFile(options.input)\n",
    "    else:\n",
    "            print 'No dataset filename specified, system with exit\\n'\n",
    "            sys.exit('System will exit')\n",
    "\n",
    "    minSupport = options.minS\n",
    "    minConfidence = options.minC\n",
    "\n",
    "    items, rules = runApriori(inFile, minSupport, minConfidence)\n",
    "\n",
    "    printResults(items, rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and references\n",
    "\n",
    "1. Han and Kamber : Data Mining, Concepts and Techniques\n",
    "2. https://github.com/asaini/Apriori/ - Reference for implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
