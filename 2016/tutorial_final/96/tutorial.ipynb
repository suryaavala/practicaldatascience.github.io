{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import asyncio         # for handling asynchronous tasks\n",
    "import aiohttp         # for making asynchronous http requests (requires asyncio)\n",
    "import requests        # for making synchronous requests\n",
    "import scraper         # from homework 1\n",
    "import csv             # for csv parsing\n",
    "import time            # for timing our implementations\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor # for parallelizing work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "This tutorial requires Python3, since `async` and `await` aren't present in Python2.\n",
    "\n",
    "To run Jupyter with Python3, enter into your terminal:\n",
    "```\n",
    "> pip3 install jupyter\n",
    "> jupyter kernelspec install\n",
    "```\n",
    "\n",
    "To run locally, you'll also need a modified `scraper.py` from Homework 1 (some of the Yelp HTML has changed) and `restaurants.csv`, a list of Yelp business urls.\n",
    "\n",
    "## Generating the data\n",
    "\n",
    "If you'd like, you can generate `restaurants.csv` yourself; just change `config_filepath` to point to your Yelp API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'yelp.obj.business.Business'>\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# change to the filepath containing your API keys\n",
    "config_filepath = \"yelp_api.json\"\n",
    "\n",
    "# feel free to change to your liking:\n",
    "output_filepath = \"restaurants.csv\"\n",
    "restaurant_queries = [\"San Francisco\"]\n",
    "restaurants = []\n",
    "\n",
    "# make the calls to Yelp API\n",
    "yelp_client = scraper.authenticate(config_filepath)\n",
    "for query in restaurant_queries:\n",
    "    restaurants += scraper.all_restaurants(yelp_client, query)\n",
    "    \n",
    "print(type(restaurants[0]))\n",
    "print(len(restaurants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to write the data to a file, so you don't have to regenerate every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract urls from each business object\n",
    "urls = [r.url for r in restaurants]\n",
    "\n",
    "# write the results to a csv file\n",
    "with open(output_filepath, \"w\") as f:\n",
    "    w = csv.writer(f, lineterminator='\\n')\n",
    "    for url in urls:\n",
    "        w.writerow([url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data from file\n",
    "\n",
    "If you'd prefer not to generate the data, run the cell below to read from `restaurants.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.yelp.com/biz/dakshin-san-francisco-6?adjust_creative=xkxIFbCrCF3OVf-SDbQRWA&utm_campaign=yelp_api&utm_medium=api_v2_search&utm_source=xkxIFbCrCF3OVf-SDbQRWA\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "with open(\"restaurants.csv\", \"r\") as f:\n",
    "    r = csv.reader(f)\n",
    "    urls = [row[0] for row in r]\n",
    "    \n",
    "print(urls[0])\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "One of the challenges with data science is dealing with large amounts of data efficiently. Suppose, for example, that you are given a list of 1,000 different Yelp restaurants, and are tasked with scraping all of their reviews and ratings. How would you approach this problem?\n",
    "\n",
    "## Naive approach\n",
    "\n",
    "Based on what we've covered thus far, a reasonable approach might be:\n",
    "\n",
    "- loop through urls:\n",
    "    - while there's a next page:\n",
    "        - use `requests` to grab the HTML\n",
    "        - run the HTML through `beautifulsoup`, process the data, etc.\n",
    "        - update the `url` to point to the next page\n",
    "\n",
    "The following demonstrates this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_page (html):\n",
    "    \"\"\"\n",
    "    Given the HTML for a yelp page, do some processing like parsing the reviews on that page, \n",
    "    finding the next page of reviews, etc.\n",
    "    \n",
    "    Inputs:\n",
    "        html (string): HTML of a yelp page\n",
    "    Outputs:\n",
    "        (list, string): list of reviews, url of next page\n",
    "    \"\"\"\n",
    "    # simulate long-running processing\n",
    "    time.sleep(0.2)    \n",
    "    return scraper.parse_page(html)\n",
    "\n",
    "def fetch_url (url):\n",
    "    \"\"\"\n",
    "    Given a yelp business url, aggregate all the reviews for that business.\n",
    "    \n",
    "    Inputs:\n",
    "        url (string): the yelp url\n",
    "    Outputs:\n",
    "        (list): all the reviews for that business\n",
    "    \"\"\"\n",
    "    parsed_reviews = []\n",
    "    \n",
    "    # 2. while we still have url to parse\n",
    "    while url:\n",
    "        # 3. use requests to grab the HTML\n",
    "        res = requests.get(url)\n",
    "        html = res.text\n",
    "        \n",
    "        # 4,5. parse with beautifulsoup, update url\n",
    "        reviews, url = parse_page(html)\n",
    "        parsed_reviews += reviews\n",
    "        \n",
    "    return parsed_reviews\n",
    "        \n",
    "\n",
    "def get_reviews (urls):\n",
    "    \"\"\"\n",
    "    Given a list of yelp business urls, aggregate the reviews of all the businesses.\n",
    "    \n",
    "    Inputs:\n",
    "        urls (list): list of valid yelp business urls\n",
    "    Outputs:\n",
    "        (list): list of reviews\n",
    "    \"\"\"\n",
    "    parsed_reviews = []\n",
    "    \n",
    "    # 1. loop through websites\n",
    "    for url in urls:\n",
    "        reviews = fetch_url(url)\n",
    "        parsed_reviews += reviews\n",
    "\n",
    "    return parsed_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, if you were to use only the API exposed by Homework 1, this would be precisely how you would do it.\n",
    "\n",
    "## What's the problem?\n",
    "\n",
    "To get a rough idea of how it behaves, let's try to time how long it takes to process just 5 urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3650\n",
      "Took: 548.1295688152313s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "results = get_reviews(urls[:5])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(len(results))\n",
    "print(\"Took: {}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For just 5 urls, we spent something in the neighborhood of 550s, or 9 minutes. This is because `requests.get` is synchronous, meaning a second request cannot happen until the previous request has finished. Further, the way we're handling our 200ms computation (simulated by `time.sleep`) blocks our program on every request, which obviously isn't ideal.\n",
    "\n",
    "What if data processing took more than 200ms per page? What if we had 10,000 instead of 1,000 urls? Hopefully, it's becoming clear that doing work sequentially is unviable as we begin to scale the system.\n",
    "\n",
    "## What's the fix?\n",
    "\n",
    "Looking back, the main culprits are:\n",
    "\n",
    "**1.** requests are blocking, so we spend a lot of time waiting  \n",
    "**2.** requesting and processing happen sequentially, so we're not parallelizing work at all\n",
    "\n",
    "We spend a lot of time waiting, when there's little reason to do the work in-order: if the 100th request responds before the 1st, we could begin processing the response immediately, since we don't actually need the 1st response to know how to parse the 100th. In other words, we should decouple *requesting* and *processing* data.\n",
    "\n",
    "In the tutorial, we'll discuss two general ideas:\n",
    "\n",
    "**1.** asynchronous IO  \n",
    "**2.** multiprocessing to decouple requesting and processing\n",
    "\n",
    "Let's walk through each in order.\n",
    "\n",
    "## Idea 1 - Asynchronous IO\n",
    "\n",
    "\"Asynchronous request\" is just a fancy name for a request that doesn't block while it waits for a response. Instead, when an asynchronous function waits for IO, your program can temporarily run some other code, and later resume execution of the function once the response arrives.\n",
    "\n",
    "To illustrate, observe the results of sending 3 synchronous vs. asynchronous requests (don't worry about the code just yet; just note the order of the printed messages):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synchronous example:\n",
      "Starting 0\n",
      "Got back 0\n",
      "Starting 1\n",
      "Got back 1\n",
      "Starting 2\n",
      "Got back 2\n",
      "Asynchronous example:\n",
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n",
      "Got back 0\n",
      "Got back 2\n",
      "Got back 1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "url = \"https://google.com\"\n",
    "num_requests = 3\n",
    "\n",
    "# synchronous example\n",
    "def synchronous_requests ():\n",
    "    for i in range(num_requests):\n",
    "        print(\"Starting {}\".format(i))\n",
    "        res = requests.get(url)\n",
    "        print(\"Got back {}\".format(i))\n",
    "\n",
    "# asynchronous example\n",
    "async def fetch_async (session, i):\n",
    "    print(\"Starting {}\".format(i))\n",
    "    async with session.get(url) as resp:\n",
    "        res = await resp.read()\n",
    "        print(\"Got back {}\".format(i))\n",
    "        return res\n",
    "\n",
    "async def asynchronous_requests ():\n",
    "    tasks = []\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for i in range(num_requests):\n",
    "            task = asyncio.ensure_future(fetch_async(session, i))\n",
    "            tasks.append(task)\n",
    "    \n",
    "        return await asyncio.gather(*tasks)\n",
    "    \n",
    "print(\"Synchronous example:\")\n",
    "synchronous_requests()\n",
    "\n",
    "print(\"Asynchronous example:\")\n",
    "loop = asyncio.get_event_loop()\n",
    "fut = asyncio.ensure_future(asynchronous_requests())\n",
    "res = loop.run_until_complete(fut)\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how asynchronous requests are made back-to-back, and responses are processed in order of arrival (which might not match the order that they were sent).\n",
    "\n",
    "What `asyncio` is doing underneath the hood is quite interesting, but in order to understand how it works, we have to first define some terms.\n",
    "\n",
    "### Event loop\n",
    "\n",
    "In order to run functions asynchronously, the `asyncio` library actually maintains a loop that repeatedly executes tasks in an internal queue. This is known as the event loop.\n",
    "\n",
    "Whenever a task needs to wait for IO, it tells the scheduler to \"pause\" it and schedule another task in the meantime. More technically, the task \"yields\" execution and is taken out of the queue. Once the IO finishes, the scheduler reschedules it (puts it back in the queue) and eventually \"resumes\" execution of the task. At any point in time, only one task gets executed, but because each task yields instead of blocks, wait-time is minimized.\n",
    "\n",
    "This cooperative yielding between tasks allows for execution of non-blocking IO on a single thread. Due to the nature of cooperative multitasking, even if tasks are initiated in a certain order, the order in which they are rescheduled depends on how other tasks yield, which explains the seemingly arbitrary ordering of the \"Got back\" messages.\n",
    "\n",
    "### Coroutines\n",
    "\n",
    "The aforementioned functions that \"yield\" execution are called coroutines (it might be helpful to think of them as \"cooperative\" routines). Everytime a coroutine calls `await` on some IO, it yields execution until the IO finishes, after which it can resume execution where it left off. Note that `await` can only be used in coroutines (declared with `async def`), since regular functions have no concept of yielding.\n",
    "\n",
    "For example, one might translate `await asyncio.gather(*tasks)` as: \"yield execution of this function until `asyncio.gather` resolves to a value, after which continue running this function where we left off\".\n",
    "\n",
    "### Futures\n",
    "\n",
    "In order for a coroutine to know when IO has finished, it uses something called Futures. Futures are objects that can \"resolve\" to a value at a later point in time. Before a Future is resolved, it acts as a temporary placeholder for the eventual result.\n",
    "\n",
    "In the example above, `asyncio.gather(*tasks)` returns a Future object that later resolves to the real return value of `asyncio.gather`. When we `await` this Future, we yield until it resolves to a value, and only then do we return.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "A Task is just a subclass of Future that waits for the result of a coroutine. In other words, a Task resolves to a value only when the coroutine it wraps finally returns. In this case, `asynchronous_requests` is a coroutine which we wrap as a Task using `ensure_future` (discussed later).\n",
    "\n",
    "### Step 1.1 - Event loop\n",
    "\n",
    "We first define two coroutines called `fetch_async` and `asynchronous_requests` with the `async def` keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def fetch_async (session, i):\n",
    "    # omitted for brevity\n",
    "    pass\n",
    "    \n",
    "async def asynchronous_requests ():\n",
    "    # omitted for brevity\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we run our `asynchronous_requests` coroutine by putting it on the event loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the default event loop\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "# make sure that we have a future-like object. anything that goes into the \n",
    "# event loop has to know how to be suspended and resolved at a later time.\n",
    "fut = asyncio.ensure_future(asynchronous_requests())\n",
    "\n",
    "# run our task until it resolves to a value (i.e the coroutine finally returns)\n",
    "loop.run_until_complete(fut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ensure_future` function can be a bit obscure. Essentially, it takes a Future or coroutine and ensures that a Future-like object is returned. If it receives a Future, it returns that directly; if it receives a coroutine, it wraps it in a Task (which, as you'll recall, is a Future object), schedules it on the event loop, and returns the Task.\n",
    "\n",
    "In our case, `asynchronous_requests()` returns a coroutine, which we then schedule using `ensure_future`. By calling `loop.run_until_complete`, we run the event loop until `asynchronous_requests()` resolves to a value, i.e `await asyncio.gather(*tasks)` resolves to a value, i.e our tasks complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2 - Asynchronous HTTP\n",
    "\n",
    "The `fetch_async` function looks confusing mainly because of syntax. It essentially waits for two Future-like objects: first, for the Future returned by `session.get` to resolve to a `response` object; then, for the Future returned by `response.read` to resolve to the actual response body.\n",
    "\n",
    "This two-step process exists for lazy evaluation: if we didn't need the response body (e.g we only cared about the status), we could access `response.status` without having to wait for the body to be read.\n",
    "\n",
    "The `with` keyword is just convenient syntax to make sure that the `response` object is properly closed once execution exits the `with` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def fetch_async (session, i):\n",
    "    # wait for the response object from our GET request\n",
    "    async with session.get(url) as response:\n",
    "        # if we just needed e.g status, could have returned response.status without waiting\n",
    "        \n",
    "        # yield until the body has been read, and then return the value\n",
    "        return await response.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`asynchronous_requests` is a little more involved. First, we obtain a `ClientSession` object, which is an object through which `aiohttp` exposes its HTTP API. Don't worry much about `ClientSession` -- just know that you should reuse the same `ClientSession` object when making multiple requests (since `aiohttp` does some optimizations internally).\n",
    "\n",
    "More interesting is the loop, which stores into a list the Future objects representing the eventual return values of our `fetch_async` calls. Note that the loop itself doesn't `await` anything, so it completes nearly instantly. Only when we `await asyncio.gather` do we \"pause\" the function and yield to other tasks until the Futures resolve. Indeed, all `asyncio.gather` does is return a Future object that resolves only when every task in `tasks` has resolved. (The `*tasks` syntax is just a Python way to unpack a list as individual arguments to a function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def asynchronous_requests ():\n",
    "    tasks = []\n",
    "    \n",
    "    # obtain the session to make HTTP requests\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        \n",
    "        # accumulate a bunch of Future objects, one for each `fetch_async` call\n",
    "        for i in range(num_requests):\n",
    "            task = asyncio.ensure_future(fetch_async(session, i))\n",
    "            tasks.append(task)\n",
    "    \n",
    "        # yield execution (i.e don't return) until all of the gathered Future objects have resolved\n",
    "        return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be tempted to draw an analogy between `asyncio.gather` and a function that loops through the tasks, `await`ing and accumulating the result to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "async def asynchronous_requests_alt ():\n",
    "    results = []\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        \n",
    "        # instead of gathering them all at once, what if we wait for responses one at a time?\n",
    "        for i in range(num_requests):\n",
    "            page = await fetch_async(session, i)\n",
    "            results.append(page)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike `asyncio.gather`, this does *not* take advantage of the event loop. Calling `await` inside the loop causes the function to yield, thereby blocking subsequent iterations of the loop. So, the second request will only be sent once the first request finishes, i.e the requests run synchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0\n",
      "Got back 0\n",
      "Starting 1\n",
      "Got back 1\n",
      "Starting 2\n",
      "Got back 2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "fut = asyncio.ensure_future(asynchronous_requests_alt())\n",
    "loop.run_until_complete(fut)\n",
    "print(len(fut.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3 - Putting it together\n",
    "\n",
    "Now that we hopefully make more sense of `asyncio`, we can adapt our `get_reviews` function to run asynchronously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "async def fetch_url_async (session, url, fut):\n",
    "    \"\"\"\n",
    "    Same as fetch_url, except with async requests.\n",
    "    \"\"\"\n",
    "    parsed_reviews = []\n",
    "    while url:\n",
    "        # make the request asynchronously like above\n",
    "        async with session.get(url) as response:\n",
    "            html = await response.read()\n",
    "\n",
    "            # for non-async functions like parse_page, no need to await\n",
    "            reviews, url = parse_page(html)\n",
    "            parsed_reviews += reviews\n",
    "            \n",
    "    # indicate we are done by resolving the future\n",
    "    fut.set_result(parsed_reviews)\n",
    "\n",
    "async def get_reviews_async (urls):\n",
    "    \"\"\"\n",
    "    Same as get_reviews, except with `fetch_url_async` for making async HTTP requests,\n",
    "    and `asyncio.gather` for waiting for all Tasks in `tasks` to resolve.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for url in urls:\n",
    "            # create a Future that will be resolved in `fetch_url_async`\n",
    "            fut = asyncio.Future()\n",
    "            \n",
    "            # schedule `fetch_url_async`\n",
    "            asyncio.ensure_future(fetch_url_async(session, url, fut))\n",
    "            \n",
    "            # accumulate these Futures\n",
    "            tasks.append(fut)\n",
    "        \n",
    "        # wait for all the accumulated Futures to be resolved\n",
    "        res = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # res is a 2d list where 0th element is results from 0th url, 1st element from 1st url, etc.\n",
    "        # all we're doing here is flattening the 2d list so we have one big list of results.\n",
    "        return [r for url_results in res for r in url_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to see if there's improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3652\n",
      "Took: 411.61619210243225s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "fut = asyncio.ensure_future(get_reviews_async(urls[:5]))\n",
    "res = loop.run_until_complete(fut)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(len(res))\n",
    "print(\"Took: {}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We just saved 100s with minimal changes. However, we still haven't decoupled requesting and processing. If the processing step takes a while, the event loop still gets blocked and other requests can't be sent.\n",
    "\n",
    "## Idea 2 - Multiprocessing\n",
    "\n",
    "In `asyncio`, the event loop should never block. With cooperative multitasking, if one task decides not to yield, there's nothing stopping it from running forever. This is why CPU-bound tasks don't mix well `asyncio`: on a single thread, a CPU-intensive task will completely hog the processor. But! If we multi-thread, the OS can schedule each thread to run for a proportionate amount of time.\n",
    "\n",
    "### GIL\n",
    "\n",
    "Multithreading exists in Python, with a caveat: Python threads can only switch when waiting on IO. This is due to Python's [Global Interpreter Lock (GIL)](https://en.wikipedia.org/wiki/Global_interpreter_lock), which trades concurrent execution of multiple threads for efficient single-threaded processing. We won't go into it here; the main takeaway is that if your thread is doing heavy computation, it will block the event loop regardless of multithreading. \n",
    "\n",
    "### Step 2.1 - Multiprocessing\n",
    "\n",
    "The workaround is to substitute threads for processes. This sidesteps the GIL because a multi-core machine can just run multiple processes in parallel, with a separate GIL in each. Luckily, `asyncio` already has a function for delegating computation to a pool of separate processes.\n",
    "\n",
    "First, the code (only the commented lines have changed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "async def fetch_url_async (pool, session, url, fut):\n",
    "    \"\"\"\n",
    "    An implementation of `fetch_url_async` that delegates long-running computation to a process pool\n",
    "    \"\"\"\n",
    "    parsed_reviews = []\n",
    "    while url:\n",
    "        async with session.get(url) as response:\n",
    "            html = await response.read()\n",
    "\n",
    "            # do the heavy computation in a separate process\n",
    "            reviews, url = await loop.run_in_executor(pool, parse_page, html)\n",
    "            \n",
    "            parsed_reviews += reviews\n",
    "\n",
    "    fut.set_result(parsed_reviews)\n",
    "\n",
    "async def get_reviews_async (pool, urls):\n",
    "    \"\"\"\n",
    "    Pretty much identical to previous `get_reviews_async`, except now we pass the given process pool\n",
    "    `pool` to `fetch_url_async`.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:    \n",
    "        for url in urls:\n",
    "            task = asyncio.Future()\n",
    "            \n",
    "            # the only difference is that we pass the pool executor\n",
    "            asyncio.ensure_future(fetch_url_async(pool, session, url, task))\n",
    "            \n",
    "            tasks.append(task)\n",
    "        \n",
    "        res = await asyncio.gather(*tasks)\n",
    "        return [r for url_results in res for r in url_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, we defer the heavy computation in `parse_page` to a pool of processes using `run_in_executor`. The `run_in_executor` function schedules the provided function to be run in the pool, returning a Future that resolves once the function returns. Note that `run_in_executor` works for non-async functions like `parse_page` because the function is not actually yielding to the event loop, but running in a separate process, where it can block without affecting the original process.\n",
    "\n",
    "To create the pool, we use `ProcessPoolExecutor`. Finding a value for `num_processes` requires some intuition and trial and error, but you can always omit the argument: `ProcessPoolExecutor` will just use as many workers as you have processors on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3652\n",
      "Took 379.3171589374542s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# initialize process pool\n",
    "num_processes = 3\n",
    "pool = ProcessPoolExecutor(num_processes)\n",
    "\n",
    "# kick off the event loop\n",
    "loop = asyncio.get_event_loop()\n",
    "fut = asyncio.ensure_future(get_reviews_async(pool, urls[:5]))\n",
    "res = loop.run_until_complete(fut)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(len(res))\n",
    "print(\"Took {}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome: we've reduced 550 to 380s. If the `parse_page` process were even more CPU-intensive, and if the machine had more cores, the advantages would perhaps be even more pronounced. \n",
    "\n",
    "## Summary\n",
    "\n",
    "- prefer `asyncio` when working with IO-bound tasks (waiting on network, disk, etc.)\n",
    "- use multiprocessing to parallelize CPU-bound tasks\n",
    "\n",
    "## References\n",
    "\n",
    "**Stuff we covered**  \n",
    "- [Event loops, coroutines, futures, tasks](http://masnun.com/2015/11/20/python-asyncio-future-task-and-the-event-loop.html)\n",
    "- [Using coroutines with processes](https://pymotw.com/3/asyncio/executors.html)\n",
    "- [Challenges with multiprocessing + asyncio](http://stackoverflow.com/questions/21159103/what-kind-of-problems-if-any-would-there-be-combining-asyncio-with-multiproces)\n",
    "- [Threads vs processes vs asyncio](https://www.youtube.com/watch?v=B0Qfe3U_hKU)\n",
    "\n",
    "**Stuff to explore (Pipelining with multiprocessing)**  \n",
    "- [Brief examples of Process, Queue, and Lock in multiprocessing](http://toastdriven.com/blog/2008/nov/11/brief-introduction-multiprocessing/)\n",
    "- [Basics of multiprocessing](https://pymotw.com/2/multiprocessing/basics.html)\n",
    "- [Communicating between processes with Queues](https://pymotw.com/2/multiprocessing/communication.html)\n",
    "- [Pipelining (and also doing async stuff with other async libraries)](https://www.youtube.com/watch?v=jq2IFUQRbGo&index=37&list=PL2k6bbM_wgju204mCEyw3bmDH62dp_sLu)\n",
    "\n",
    "**API References**  \n",
    "- [ProcessPoolExecutor](https://docs.python.org/3.2/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor)\n",
    "- [Tasks and coroutines](https://docs.python.org/3/library/asyncio-task.html)\n",
    "- [aiohttp ClientSession and ClientResponse](http://aiohttp.readthedocs.io/en/stable/client_reference.html)\n",
    "- [ensure_future](https://github.com/python/asyncio/blob/f9b0d516fa60f9da35e87da344e365f604281ccf/asyncio/tasks.py#L548)\n",
    "\n",
    "**Related tutorials**\n",
    "- [Building a web crawler with asyncio](http://aosabook.org/en/500L/a-web-crawler-with-asyncio-coroutines.html)\n",
    "- [Building a multiplayer game in Python (Queues, Processes, asyncio)](https://7webpages.com/blog/writing-online-multiplayer-game-with-python-and-asyncio-writing-game-loop/)\n",
    "- [Making a million requests with asyncio](https://pawelmhm.github.io/asyncio/python/aiohttp/2016/04/22/asyncio-aiohttp.html)\n",
    "- [Asynchronous Python](http://ntoll.org/article/asyncio)\n",
    "- [How await/async actually works](http://www.snarky.ca/how-the-heck-does-async-await-work-in-python-3-5)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
