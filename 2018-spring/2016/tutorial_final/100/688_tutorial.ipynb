{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy Tutorial\n",
    "##### by Malvin De Nunez Estevez - mdenunez@andrew.cmu.edu\n",
    "\n",
    "Scrapy is primarily a web scraping library that allows the extraction of structured data from websites for different applicatons. It can also be used to collect data using APIs and as web crawler.\n",
    "\n",
    "Technically one could do all scraping using simply libraries such as Requests and Beatiful Soup. But Scrapy provides lots of built-in features that would be tedious to implement and a great deal of modularity for extensive scraping tasks. You can read some arguments of why one would use Scrapy over an HTTP library plus parsing library in this post: \n",
    "https://www.quora.com/Why-would-some-use-scrapy-instead-of-just-crawling-with-requests-or-urllib2\n",
    "\n",
    "This tutorial will go over the basics of how to set up Scrapy and then will illustrate its usage with two examples.\n",
    "\n",
    "##### Disclaimer\n",
    "\n",
    "The examples below include links of different websites that may change over time. Also, the code that parses the websites may become useless if the HTML layouts were to change. The examples are current as of 11/02/2016. Please update as necessary to prevent errors or just have as reference. Make sure logging is enabled to use it to debug any errors.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Scrapy can be easily installed through a pip install with the command below. It can be also be donwloaded at: https://scrapy.org/download/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "The traditional way of using Scrapy would be through a \"Scrapy Project\", which contains different directories and files for every step of the scraping pipeline. The modularity and neatness of this approach requires to create and modify different files and run things using the command prompt. Instead of this, this tutorial will use the Scrapy API methodology trying to keep everything within the notebook. \n",
    "\n",
    "The official documentation of the API can be found here:\n",
    "https://doc.scrapy.org/en/latest/topics/api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy Spider\n",
    "A Scrapy spider is a class that will dictate how we are going to crawl through the websites and scrape the data in them. All spider objects will subclass scrapy.Spider. Below is the general structure of a spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OurSpider(scrapy.Spider):\n",
    "    name = \"\"\n",
    "    start_urls = [\n",
    "        '',\n",
    "        '',\n",
    "    ]\n",
    "    def start_requests(self):\n",
    "        pass\n",
    "    def parse(self, response):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure Overview\n",
    "* name: identifier for the Spider. \n",
    "* start_urls: list of urls to be scraped. \n",
    "* start_requests(): specifies how to perform HTTP requests. When the request is done here one specifies how to handle the response (e.g., parse() method for successful requests).\n",
    "* parse(): method that will handle the parsing of the response and will also define how to crawl the websites, if applicable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy Item\n",
    "A Scrapy item is a class where the data collected from websites can be stored. This class effectively recreates dictionaries. The biggest difference is that an Item by default does not allow you to assign a value to a key that was not declared as a field. It follows the following format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class First_scrapyItem(scrapy.Item):\n",
    "    name = scrapy.Field() #attribute \"name\" to be extracted \n",
    "    address = scrapy.Field() #attribute \"address\" to be extracted\n",
    "    #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of defining an item class is that it allows to easily modify or validate the collected items (as shown in the second application) and to export them to files such as JSON and CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Yelp restaurant reviews\n",
    "\n",
    "To start off let's create a spider that will crawl through all pages of a specific Yelp restaurant and parse the review comments. We are interested in the author, date, rating, and content (text) of the reviews. For this example, we will choose the restaurant Gaucho Parrila Argentina: https://www.yelp.com/biz/gaucho-parrilla-argentina-pittsburgh\n",
    "\n",
    "Before building the spider we need to know the HTML structure of the website, and especially of the comments to be parsed. The individual reviews in the Yelp websites are within 'div' tags that have attribute itemprop=\"review\". Inside these tags the features of interest can be found by searching for the right \"itemprop\" attribute, as done in the code below.\n",
    "\n",
    "Although Scrapy has its own built in methodology to browse through HTML trees with the scrapy.selector class, the library Beatiful Soup will be used in this tutorial instead. Beautiful Soup is fairly user-friendly and intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review_Item (scrapy.item object)\n",
    "The Review_Item class will indicate the fields that we are interested in collecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Review_Item(scrapy.Item):\n",
    "    author = scrapy.Field() #author of review\n",
    "    date = scrapy.Field() #date published\n",
    "    rating = scrapy.Field() #rating (1-5)\n",
    "    text = scrapy.Field() #content of review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YelpSpider (scrapy.spider object)\n",
    "\n",
    "The YelpSpider class will follow the structure above for the most part. Here are some changes and pointers: \n",
    "* errback_httpbin method: method that will report in the log requests that fail and the reason\n",
    "* In the HTTP requests the 'dont_obey_robotstxt' attribute is set to true. This will prevent the scrapy bot from being blocked. Only information publicly displayed is being extracted so this should be fine. \n",
    "* The parse method will scrape and parse the reviews as well as collect the link to the next page. After getting the following link, if any, the parse method will call itself. Once there are no more pages left in that restaurant, the following link in the start_urls list would be scraped (but here there is only one).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#needed for errback_httpbin method\n",
    "from scrapy.spidermiddlewares.httperror import HttpError\n",
    "from twisted.internet.error import DNSLookupError\n",
    "from twisted.internet.error import TimeoutError, TCPTimedOutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class YelpSpider(scrapy.Spider):\n",
    "    name = \"Yelp_Spider\"\n",
    "    start_urls = ['https://www.yelp.com/biz/gaucho-parrilla-argentina-pittsburgh']\n",
    "  \n",
    "    #get HTTP request for every link in start_urls\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield scrapy.Request(url, callback=self.parse, errback=self.errback_httpbin, meta={'dont_obey_robotstxt':True})\n",
    "    \n",
    "    #parse HTTP response\n",
    "    def parse(self, response):\n",
    "        #create soup and find all reviews\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        raw_reviews = soup.findAll(itemprop=\"review\")\n",
    "        #initiliaze review item and find desired information\n",
    "        review = Review_Item()\n",
    "        for each_review in raw_reviews:\n",
    "            review['author'] = each_review.find(itemprop='author')['content']\n",
    "            review['date'] = each_review.find(itemprop='datePublished')['content']\n",
    "            review['rating'] = each_review.find(itemprop='ratingValue')['content']\n",
    "            review['text'] = each_review.find(itemprop='description').text\n",
    "            yield review\n",
    "\n",
    "        # find link to next page. If exists, call parse method again. \n",
    "        # Otherwise, make request for next link in start_urls, if any.\n",
    "        next_page = soup.find(\"a\", class_=\"u-decoration-none next pagination-links_anchor\")\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page[\"href\"])\n",
    "            yield scrapy.Request(next_page, callback=self.parse,meta={'dont_obey_robotstxt':True})\n",
    "\n",
    "    #handle HTTP request failures\n",
    "    def errback_httpbin(self, failure):\n",
    "        # logs failures\n",
    "        self.logger.error(repr(failure))\n",
    "        if failure.check(HttpError):\n",
    "            response = failure.value.response\n",
    "            self.logger.error(\"HttpError occurred on %s\", response.url)\n",
    "        elif failure.check(DNSLookupError):\n",
    "            request = failure.request\n",
    "            self.logger.error(\"DNSLookupError occurred on %s\", request.url)    \n",
    "        elif failure.check(TimeoutError, TCPTimedOutError):\n",
    "            request = failure.request\n",
    "            self.logger.error(\"TimeoutError occurred on %s\", request.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the spider is created, let's set up the API to perform the crawl. A few pointers:\n",
    "\n",
    "* We are initilizing a CrawlerProcess object that will manage the crawls. Although CrawlerProcess could directly take a Scrapy Spider, we are first creating a Crawler object from our YelpSpider that will help us to save the items without externally reading them. Read more about the API here: https://doc.scrapy.org/en/latest/topics/api.html#scrapy.crawler\n",
    "* Settings: settings of the crawl. In this case the output file will be written into a .json file \"result.json\", the log is disabled (otherwise would have shown in the output), and a download delay is introduced to put less pressure on websites. Here one can see a list of all the settings to play around with: https://doc.scrapy.org/en/latest/topics/settings.html#topics-settings-ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#needed to run Scrapy API\n",
    "from scrapy.settings import Settings\n",
    "from scrapy.crawler import CrawlerProcess, Crawler\n",
    "from scrapy import signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#store reviews in this list\n",
    "crawled_reviews = []\n",
    "\n",
    "#add item to items list when signaled\n",
    "def add_item(item):\n",
    "    crawled_reviews.append(item)\n",
    "\n",
    "#intialize settings\n",
    "settings = Settings()\n",
    "settings.set(\"FEED_FORMAT\",'json')\n",
    "settings.set(\"FEED_URI\",'result.json')\n",
    "settings.set(\"LOG_ENABLED\",False)\n",
    "settings.set(\"DOWNLOAD_DELAY\",1)\n",
    "#settings.set(\"LOG_FILE\",'logfile.txt')\n",
    "\n",
    "#initiliaze process\n",
    "process = CrawlerProcess(settings)\n",
    "#create crawler. Needed to collect items in code\n",
    "crawler = Crawler(YelpSpider, settings)\n",
    "#when item is collected, add it to list\n",
    "crawler.signals.connect(add_item, signals.item_scraped)\n",
    "#add crawler to process\n",
    "process.crawl(crawler)\n",
    "#start process\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for illustration the reviews were exported into a .json file. But let's take a look at the first review that was stored in the crawled_reviews list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'Ana G.',\n",
      " 'date': '2016-09-01',\n",
      " 'rating': '5.0',\n",
      " 'text': u\"So after waiting so long to come to this wonderful establishment I am glad to say I finally made it!  I ended up paying this place a visit over the weekend at around 3 pm when the lunch crowds had dissipated and the dinner crowd was yet to show up.   Came here with hubby and the in-laws when they were visiting from GA. Anyway I was initially overwhelmed by their menu but we started with the empanadas and fungos (mushrooms). For mains, my husband had the seafood soup and I had the Argentinian salad with filet. \\n\\nThe empanadas were delicious, and were highly complimented by the various chimichurris they had. Chicken went really great with ajo (garlic sauce)) and the beef went great with the regular chimichurri. \\n\\nMy salad was excellent, it had greens, tomatos, mushrooms, onion, bell peppers, and some other stuff I dont remember along with a perfectly cooked filet. It was beautiful, it was tasty, and it was a great bang for your buck. I will have that again.\\n\\nAs for hubbys soup, it had all sorts of sea monsters: clams, muscles, and calamari in a delicious broth which had a bit of a kick to it. \\n\\nAlll dishes had impeccable presentation, the whole establishment had an extremely clean feel and the wait staff was very helpful and attentive. I honestly could not ask more. \\n\\nI can't wait to go back.\"}\n"
     ]
    }
   ],
   "source": [
    "print crawled_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy runs inside the Twisted asynchronous networking library, which is a bit tricky to deal with especially when running Scrapy from a script. Even following the official documentation and different posts on Stackoverflow, stopping the reactor for later crawls was not accomplished. It's worth saying that if one were to start the crawls simultaneously this would not be a problem. \n",
    "\n",
    "To keep things simple, please **restart** the kernel and output to shut off the Twisted reactor and avoid errors in the following part. Also, make sure to **run the cells that do imports**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Online Shopping - Nike Shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far Scrapy really hasn't brought much to the table. Let's see how we can use Scrapy for more demanding tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say one is looking for shoes in the Nike.com website. It is painful having  to click at every link to see if one may like the shoe and at the end they may not even have the right size. This example will create a spider that will search through every shoe in the Men Basketball Shoes section and export the results to a .csv file. Using Beatiful Soup as before, the following information will be collected from every shoe:\n",
    "* Name\n",
    "* Size (the desired size is specified by user inside spider)\n",
    "* Price\n",
    "* Rating\n",
    "* Number of reviews\n",
    "* Reviews\n",
    "* Colors available\n",
    "* Link\n",
    "\n",
    "Just for fun, the shoes in the Men Running Shoes section will be scraped as well to make some illustrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shoe Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Shoe_Item(scrapy.Item):\n",
    "    name = scrapy.Field() \n",
    "    size = scrapy.Field() \n",
    "    price = scrapy.Field()\n",
    "    colors = scrapy.Field()\n",
    "    rating = scrapy.Field() #rating (1-5)\n",
    "    reviews_count = scrapy.Field() #number of reviews\n",
    "    reviews = scrapy.Field() #individual reviews\n",
    "    link = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShoeSpider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShoeSpider(scrapy.Spider):\n",
    "    name = \"Shoe_Spider\"\n",
    "    shoe_size = 10  #int or floating point ending in .5\n",
    "    start_urls = [\n",
    "    'http://store.nike.com/us/en_us/pw/mens-basketball-shoes/7puZ8r1Zoi3',\n",
    "    'http://store.nike.com/us/en_us/pw/mens-running-shoes/7puZ8yzZoi3?ipp=120'\n",
    "]\n",
    "     \n",
    "    #get HTTP request for every link in start_urls\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield scrapy.Request(url, callback=self.parse, errback=self.errback_httpbin, meta={'dont_obey_robotstxt':True})\n",
    "\n",
    "    #obtain link from all shoes and pass them one at a time for parsing\n",
    "    def parse(self, response):\n",
    "        #create soup and get all links\n",
    "        main_soup = BeautifulSoup(response.text, 'lxml')        \n",
    "        all_shoes = main_soup.find_all('div',class_=\"grid-item-box\")\n",
    "        for element in all_shoes:\n",
    "            shoe_link = element.a['href']\n",
    "            yield scrapy.Request(shoe_link, callback=self.parse_shoe, errback=self.errback_httpbin,meta={'dont_obey_robotstxt':True}, dont_filter=True)\n",
    "    \n",
    "    #parse individual shoes\n",
    "    def parse_shoe(self, response):\n",
    "        #create soup of individual shoe\n",
    "        soup = BeautifulSoup(response.text, 'lxml') \n",
    "        #create shoe item\n",
    "        shoe = Shoe_Item()\n",
    "        \n",
    "        #see if shoe size is unavailable. If unavailable, don't check anything else\n",
    "        not_avail = soup.find_all(\"option\", class_=\"exp-pdp-size-not-in-stock selectBox-disabled\")\n",
    "        if(float(self.shoe_size) in (float(element.text) for element in not_avail)):\n",
    "            shoe['size'] = None\n",
    "        else: \n",
    "            #see if shoe size exist for the shoe\n",
    "            if(type(self.shoe_size) is float and soup.find(attrs={\"data-label\": \"(%f)\"%self.shoe_size}) is not None):\n",
    "                shoe['size'] = self.shoe_size\n",
    "            elif(type(self.shoe_size) is int and soup.find(attrs={\"data-label\": \"(%d)\"%self.shoe_size}) is not None):\n",
    "                shoe['size'] = self.shoe_size\n",
    "            #get name\n",
    "            try:\n",
    "                shoe['name'] = soup.h1.text\n",
    "            except:\n",
    "                shoe['name'] = None\n",
    "\n",
    "            #see if colors are available\n",
    "            try:\n",
    "                shoe['colors'] = [element.get('alt') for element in soup.find(\"div\",class_=\"color-chips\").find_all(\"img\")]\n",
    "            except:\n",
    "                shoe['colors'] = None\n",
    "\n",
    "            #get price\n",
    "            try:\n",
    "                shoe['price'] = soup.find(itemprop='price').text \n",
    "            except:\n",
    "                shoe['price'] = None\n",
    "\n",
    "            #check if shoe has yet been rated. Collect rating and reviews, 0 otherwise\n",
    "            try:\n",
    "                shoe['rating'] = float(soup.find(itemprop='ratingValue')['content'][:4])\n",
    "                shoe['reviews_count'] = int(soup.find(itemprop='reviewCount')['content'])\n",
    "                shoe['reviews'] = [review.text for review in soup.find_all(\"div\", class_=\"reviewText\")]\n",
    "            except:\n",
    "                shoe['rating'] = 0\n",
    "                shoe['reviews_count'] = 0\n",
    "                shoe['reviews'] = 0\n",
    "            #get shoe link\n",
    "            shoe['link'] = response.url\n",
    "        yield shoe\n",
    "\n",
    "    def errback_httpbin(self, failure):\n",
    "        # logs failures\n",
    "        self.logger.error(repr(failure))\n",
    "        if failure.check(HttpError):\n",
    "            response = failure.value.response\n",
    "            self.logger.error(\"HttpError occurred on %s\", response.url)\n",
    "        elif failure.check(DNSLookupError):\n",
    "            request = failure.request\n",
    "            self.logger.error(\"DNSLookupError occurred on %s\", request.url)\n",
    "        elif failure.check(TimeoutError, TCPTimedOutError):\n",
    "            request = failure.request\n",
    "            self.logger.error(\"TimeoutError occurred on %s\", request.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spider Item Pipeline\n",
    "For this task we will introduce the Scrapy Item Pipeline. This functionality will help to process the collected items. For example, in the pipeline one may decide to keep only the keywords of the reviews. In this example, we will use the pipeline to simply drop shoes without enough reviews (min of 10) or rated too low (below 4/5), duplicates, and shoes that don't have the desired size available.\n",
    "\n",
    "Typically you would add the pipeline to a file in the project directory. Here, instead, the settings in the crawler will be adjusted accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#supports dropping items\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class ShoePipeline(object):\n",
    "    #set to keep track of unique elements\n",
    "    def __init__(self):\n",
    "        self.ids_seen = set()\n",
    "        self.reviews_count_min = 10\n",
    "        self.rating_min = 4\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        #removes element if already seen\n",
    "        if item['name'] in self.ids_seen:\n",
    "            raise DropItem(\"Duplicate item found: %s\" % item)\n",
    "        self.ids_seen.add(item['name']) #add to set if not seen\n",
    "        #drops item if conditions not met\n",
    "        if(item['size'] is None):\n",
    "            raise DropItem(\"Shoe %s does not have your size\" % item)\n",
    "        elif(item['rating'] < self.rating_min):\n",
    "            raise DropItem(\"Shoe %s is rated below 4\" % item)\n",
    "        elif(item['reviews_count'] < self.reviews_count_min):\n",
    "            raise DropItem(\"Shoe %s does not enough reviews\" % item)\n",
    "        else:\n",
    "            return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, the settings will be adjusted to create a text file \"logfile.txt\" for logging and to export the results in the file \"shoes.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#intialize settings\n",
    "settings = Settings()\n",
    "#output to csv file\n",
    "settings.set(\"FEED_FORMAT\",'csv')\n",
    "settings.set(\"FEED_URI\",'shoes.csv')\n",
    "settings.set(\"DOWNLOAD_DELAY\",1)\n",
    "#log in text file\n",
    "settings.set(\"LOG_FILE\",'logfile.txt')\n",
    "\n",
    "#add ShoePipeline. The number (100 in this case) just indicates order but there\n",
    "#are no more pipelines in this case. Find more info here: https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "settings.set('ITEM_PIPELINES', {\n",
    "    '__main__.ShoePipeline': 100\n",
    "})\n",
    "\n",
    "#initiliaze process and crawler\n",
    "process = CrawlerProcess(settings)\n",
    "crawler = Crawler(ShoeSpider, settings)\n",
    "#add crawler to process\n",
    "process.crawl(crawler)\n",
    "#start process\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the summary of statistics of the crawl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_count/INFO 14\n",
      "downloader/response_count 224\n",
      "downloader/response_bytes 7514424\n",
      "finish_reason finished\n",
      "item_dropped_count 101\n",
      "item_dropped_reasons_count/DropItem 101\n",
      "log_count/ERROR 54\n",
      "spider_exceptions/ValueError 23\n",
      "log_count/DEBUG 250\n",
      "scheduler/dequeued 224\n",
      "log_count/WARNING 101\n",
      "request_depth_max 1\n",
      "start_time 2016-11-02 23:19:28.326000\n",
      "downloader/request_method_count/GET 224\n",
      "downloader/request_bytes 104462\n",
      "downloader/response_status_count/200 180\n",
      "response_received_count 180\n",
      "scheduler/enqueued/memory 224\n",
      "finish_time 2016-11-02 23:23:59.615000\n",
      "item_scraped_count 25\n",
      "scheduler/dequeued/memory 224\n",
      "scheduler/enqueued 224\n",
      "downloader/request_count 224\n",
      "downloader/response_status_count/301 44\n"
     ]
    }
   ],
   "source": [
    "for k, v in crawler.stats.get_stats().iteritems():\n",
    "    print k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is why there are some errors (found by looking through the log file): \n",
    "* There are some shoes linked that are \"customizeable\" and have different HTML layout. These shoes come in defined colors so they are accounted for ultimately.\n",
    "* The Men Running Shoes that we added to scrape has shoes that are \"unisex\", which again vary in HTML layout. These shoes have list sizes for both Men and Women in a different format. \n",
    "\n",
    "It took less than 5 minutes to go through 182 websites for a final count of 25 shoes. The results now are conveniently stored in a CSV file that could simply be opened for inspection. If the results would have been more and of more complexity, one could have imported them for further analysis through the Pandas library, for example.\n",
    "\n",
    "Below is a screenshot of the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Srapy provides cool features that are great if one has to repeatedly scrape a large number of websites, such as a summary of stats and error handling capabilities. It would be a great tool for someone who does business off of resaling Amazon or Ebay articles, for example. However, for someone who has does not scrape often, setting up the whole Scrapy environment is probably not worth the hassle. Instead, this person should consider simply using the Requests and Beatiful Soup libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "\n",
    "The Scrapy Documentation: https://doc.scrapy.org/en/latest/index.html\n",
    "\n",
    "Stack Overflow: http://stackoverflow.com/questions/13437402/how-to-run-scrapy-from-within-a-python-script \n",
    "\n",
    "Scrapy Tutorial - Tutorials Point: https://www.tutorialspoint.com/scrapy/index.htm"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
