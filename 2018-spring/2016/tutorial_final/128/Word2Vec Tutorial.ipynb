{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Word2Vec is a widely used pipeline that can map words into a high dimensional semantic space. Given a training text corpus, which is in the form of a list of sentences, it creates a vocabulary, mapping a word to a vector in this semantic space. It does this by using a set of models, specifically two-layer neural networks.\n",
    "\n",
    "Because of its relative ease of use, it is a powerful tool that can be used to make interesting insights of complicated text data sets. This projection onto the sematic space can be used to achieve very interesting results, such as solving logical analogies and classifying types of blog articles.\n",
    "\n",
    "This tutorial will teach you how to use Word2Vec, and will show some practical applications of it, including some interesting insights we can derive from a few datasets. It will use it in an innovative way that is not as commonly used: to determine the similarity between sentences. It will also compare this approach to simpler approaches, namely TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "\n",
    "This tutorial will show how to use Word2Vec in Python, specifically using [genism](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Installing the libraries](#Installing-the-libraries)\n",
    "- [Getting the datasets](#Getting-the-datasets)\n",
    "- [Parsing data](#Parsing-data)\n",
    "- [Model training](#Model-training)\n",
    "- [Compute sentence similarities](#Compute-sentence-similarities)\n",
    "- [Comparison with TF-IDF Method](#Comparison-with-TF-IDF-Method)\n",
    "- [Further Resources](#Further-Resources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, you'll need to install and import the various libraries that we will use.  Assuming you have anaconda fully installed, you can install genism and nltk using `conda`:\n",
    "\n",
    "    $ conda install -c anaconda genism\n",
    "\n",
    "    $ conda install -c anaconda nltk\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import WikiCorpus\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "import multiprocessing\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, where do we even start?\n",
    "\n",
    "The first step to using Word2Vec is having a dataset to train on. This choice of text corpus will depend on your intended use of the tranied model. In this tutorial, we will explore how the different data sets differ, and compare the vocabularies generated by each. Though both data sets are from Amazon Reviews, one contains only sarcastic reviews while the other contains only reviews about Video Games.\n",
    "\n",
    "The data sets used in this tutorial come from the following places:\n",
    "- [Amazon Review Data (Video Game Reviews)](http://jmcauley.ucsd.edu/data/amazon/)\n",
    "- [Amazon Review Data (Reviews marked as sarcastic)](http://storm.cis.fordham.edu/~filatova/SarcasmCorpus.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the dataset (which may take a while), we need to convert the text into a standard format that the Word2Vec model can train on. This may involve decompressing files, removing punctuation, etc., and is dependent on the input data format. We will look at how to do so for the Amazon Review data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Amazon Review Files:\n",
    "raw_sarcasm_file_name = 'sarcasm/sarcasm_lines.txt'\n",
    "sarcasm_file_name = 'corpuses/sarcasm.txt'\n",
    "\n",
    "raw_amazon_video_file_name = 'amazon_review/reviews_Video_Games_5.json.gz'\n",
    "amazon_video_file_name = 'corpuses/amazon_vg.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The sarcasm data came in a different format than the rest, so we need to write code to parse that file individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create sarcasm corpus\n",
    "with open(sarcasm_file_name, 'w') as sarcasm:\n",
    "    with open(raw_sarcasm_file_name, 'r') as raw_sarcasm:\n",
    "        for line in raw_sarcasm.readlines():\n",
    "            columns = line.split('\\t')\n",
    "            if len(columns) == 2:\n",
    "                utt = columns[1]\n",
    "                utt = re.sub(\"[^a-zA-Z']\",\" \", utt)\n",
    "                utt = utt.lower()\n",
    "                sarcasm.write(unicode(utt + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the Amazon Review data came in a .json.gz format, and we will obtain the review texts in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse gzip of Amazon Review file\n",
    "def parse_gzip(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    \n",
    "    #to limit the number of input lines\n",
    "    linecount = 0\n",
    "    for l in g:\n",
    "        linecount += 1\n",
    "        if linecount>10000:\n",
    "            break\n",
    "        yield eval(l)\n",
    "\n",
    "# parse reviews from gzip file and write to corpus\n",
    "def parse_amazon(in_gz, out_txt):\n",
    "    with open(out_txt, 'w') as amazon:\n",
    "        for d in parse_gzip(in_gz):\n",
    "            utt = d['reviewText']\n",
    "            utt = re.sub(\"[^a-zA-Z']\",\" \", utt)\n",
    "            utt = utt.lower()\n",
    "            amazon.write(unicode(utt + '\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Amazon Video Game Review parse\n"
     ]
    }
   ],
   "source": [
    "parse_amazon(raw_amazon_video_file_name, amazon_video_file_name)\n",
    "print \"Completed Amazon Video Game Review parse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the raw Amazon Review data has parsed into a text corpus that matches the format that the Word2Vec model requires, which is simply a list of sentence strings separated by spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have the data in the format we want it in, how do we actually train our model so we can start predicting the similarities?\n",
    "\n",
    "The first step is to separate the text into sentences, represented by a list of words. We do that here using regular expressions as it faster than using the python split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a list of sentencse from an input text file\n",
    "def tokenize_data(df):\n",
    "    df_iter = df.iterrows()\n",
    "    str2lst = []\n",
    "    for i, row in df_iter:\n",
    "        s = str(row[0])\n",
    "        tokenizer = RegexpTokenizer('\\w+')        \n",
    "        lst = tokenizer.tokenize(s)\n",
    "        for e in lst:\n",
    "            e.decode(encoding='utf-8', errors='ignore')\n",
    "        str2lst.append(lst)\n",
    "    return str2lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can finally pass it into the Word2Vec model for training! However, before doing that we first refine the data even more. Often in the sentences, conjuction words, like \"don't\" or \"can't\", appear and they should be considered as one word. Also, two and three word phrases, such as \"in the\", \"of course\", and \"I am sorry\", occur, and since they are thought of as single phrases they should be treated as such. So, we calculate and combine these two and three word phrases into bigrams and trigrams respectively, using the Phrases class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a model from a given text file\n",
    "def build_model(data_file):\n",
    "    freq = 10\n",
    "    size_NN = 80\n",
    "    n_threads = 4\n",
    "\n",
    "    train_df = pd.read_csv(data_file,header=None)\n",
    "    sentences = tokenize_data(train_df)\n",
    "\n",
    "    l = []\n",
    "    for lst in sentences:\n",
    "        lst_u = [s.decode('utf-8','ignore') for s in lst]\n",
    "        l.append(lst_u)\n",
    "\n",
    "    bigram = phrases.Phrases(l)\n",
    "    trigram = phrases.Phrases(bigram[l])\n",
    "\n",
    "    model = Word2Vec(min_count=freq,size=size_NN, workers=n_threads, alpha=0.025, min_alpha=0.025,\n",
    "                     max_vocab_size=50000000)\n",
    "    model.build_vocab(trigram[bigram[l]])\n",
    "    print \"created initial model\"\n",
    "\n",
    "    for epoch in range(5):\n",
    "        random.shuffle(l)\n",
    "        model.train(trigram[bigram[l]])\n",
    "        print \"epoch #\" + str(epoch) + \" completed\"\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then, we create the models and indices with the commands below. Careful, this may take a long time, depending on how big your text corpus is, and how long you selected your semantic vectors to be (in our case, size_NN=80, which is small enough). Unless running with a lot of memmory, it is advised to not a use large number of sentences for each corpus. In this case, I limited the number of sentences extracted to 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created initial model\n",
      "epoch #0 completed\n",
      "epoch #1 completed\n",
      "epoch #2 completed\n",
      "epoch #3 completed\n",
      "epoch #4 completed\n",
      "completed training the sarcasm model\n",
      "created initial model\n",
      "epoch #0 completed\n",
      "epoch #1 completed\n",
      "epoch #2 completed\n",
      "epoch #3 completed\n",
      "epoch #4 completed\n",
      "completed training the video game reviews model\n"
     ]
    }
   ],
   "source": [
    "sarcasm = build_model(sarcasm_file_name)\n",
    "sarcasm_index = set(sarcasm.index2word)\n",
    "print \"completed training the sarcasm model\"\n",
    "amazon_vg = build_model(amazon_video_file_name)\n",
    "amazon_vg_index = set(amazon_vg.index2word)\n",
    "print \"completed training the video game reviews model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute sentence similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will determine a way to find the similarities between sentences. We do this by mapping each word in each sentence to the semantic space, and then averaging all of these vectors to create a final sentence vector. Then, we find the cosine difference between these sentence vectors, and that becomes the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Averages all word vectors in a given paragraph\n",
    "def avg_vectors(words, model, num_features, index2word_set):\n",
    "        featureVec = np.zeros(num_features, dtype=\"float32\")\n",
    "        nwords = 0\n",
    "        \n",
    "        for word in words:\n",
    "            if word in index2word_set:\n",
    "                nwords = nwords+1\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "        if nwords > 0:\n",
    "            featureVec = np.divide(featureVec, nwords)\n",
    "\n",
    "        return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computes sentence similarities\n",
    "def compute_similarities(model, sentences, index, best=2, worst=0):\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "        vectors.append(avg_feature_vector(sentence.lower().split(\" \"), model, 80, index))\n",
    "        \n",
    "    similar = []\n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(len(vectors)):\n",
    "            if i>j:\n",
    "                sentence = [sentences[i], sentences[j]]\n",
    "                similarity = 1 - cosine(vectors[i], vectors[j])\n",
    "                if not np.isnan(similarity):\n",
    "                    similar.append((similarity, sentence))\n",
    "                \n",
    "    similar.sort(reverse=True)\n",
    "    if len(similar)<max(best, worst):\n",
    "        print \"not enough data\"\n",
    "    else:\n",
    "        if best:\n",
    "            print \"best \" + str(best)\n",
    "            for x in similar[:best]:\n",
    "                print x\n",
    "        if worst:\n",
    "            print \"worst \" + str(worst)\n",
    "            for x in similar[-1*worst:]:\n",
    "                print x\n",
    "\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setence1 = \"do not buy\"\n",
    "setence2 = \"terrible product\"\n",
    "setence3 = \"best product ever\"\n",
    "setence4 = \"i love this product\"\n",
    "setence5 = \"i use this product every day\"\n",
    "sentences = [setence1, setence2, setence3, setence4, setence5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcasm similarities\n",
      "best 2\n",
      "(0.92048052816809933, ['i use this product every day', 'i love this product'])\n",
      "(0.80848912011328022, ['best product ever', 'terrible product'])\n",
      "\n",
      "Video Game data similarities\n",
      "best 2\n",
      "(0.67825017781110841, ['i use this product every day', 'i love this product'])\n",
      "(0.50801964822782431, ['i love this product', 'best product ever'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Sarcasm similarities\"\n",
    "compute_similarities(sarcasm, sentences, sarcasm_index)\n",
    "\n",
    "print \"Video Game data similarities\"\n",
    "compute_similarities(amazon_vg, sentences, amazon_vg_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show some interesting insights about the Amazon Review data in comparison to the Sarcasm data. Both of them correctly found similarities in 'i use this product every day' and 'i love this product'. Also, the sarcasm corpus links 'best product ever' and 'terrible product', which seem to be opposites, to be the next most popular, while the video game corpus links 'i love this product' and 'best product ever' to be the next most popular. This is because the data with sarcastic sentences may often have conflicting sentences, while the video game reviews are more normal in tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Comparison with TF-IDF Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare how well word2vec performs with how well the TF-IDF algorithm works. The TF-IDF algorithm essentially counts the number of each word occurence in each sentence, mapping each pair of words into a vector representing this count, and then computes the cosine similarity between the two sentences. This is a much more naive way of doing things, and performs much worse, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_tfidf_similarities(sentences, best=2, worst=0):\n",
    "    similar = []\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i>j:\n",
    "                sentence = [sentences[i], sentences[j]]\n",
    "                vect = TfidfVectorizer(min_df=1)\n",
    "                tfidf = vect.fit_transform(sentence)\n",
    "                similarity = (tfidf * tfidf.T).A[0,1]\n",
    "                \n",
    "                if not np.isnan(similarity):\n",
    "                    similar.append((similarity, sentence))\n",
    "    similar.sort(reverse=True)\n",
    "    if len(similar)<max(best, worst):\n",
    "        print \"not enough data\"\n",
    "    else:\n",
    "        if best:\n",
    "            print \"best \" + str(best)\n",
    "            for x in similar[:best]:\n",
    "                print x\n",
    "        if worst:\n",
    "            print \"worst \" + str(worst)\n",
    "            for x in similar[-1*worst:]:\n",
    "                print x\n",
    "\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF similarities:\n",
      "best 3\n",
      "(0.3563004293331381, ['i use this product every day', 'i love this product'])\n",
      "(0.26055567105626237, ['i love this product', 'terrible product'])\n",
      "(0.26055567105626237, ['best product ever', 'terrible product'])\n",
      "\n",
      "Video Game data similarities\n",
      "best 2\n",
      "(0.67825017781110841, ['i use this product every day', 'i love this product'])\n",
      "(0.50801964822782431, ['i love this product', 'best product ever'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"TF-IDF similarities:\"\n",
    "compute_tfidf_similarities(sentences,3)\n",
    "\n",
    "print \"Video Game data similarities\"\n",
    "compute_similarities(amazon_vg, sentences, amazon_vg_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The model trained on the Video Game corpus accurately predicted the similar sentences, while the TF-IDF algorithm did not because it did not take the semantics of each word into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If you would like to learn more about word2vec, and the technologies discussed in this tutorial, you may view the following online resources:\n",
    "\n",
    "- [The original word2vec paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- [The TF-IDF word relavence paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf)\n",
    "- [Other interesting uses of word2vec, including using it for solving analogies](https://quomodocumque.wordpress.com/2016/01/15/messing-around-with-word2vec/)\n",
    "- [Fun and interactive website to solve analogies created using word2vec](http://deeplearner.fz-qqq.net/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
