{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This tutorial introduces Pattern, which is a web mining module for the Python programming language.\n",
    "\n",
    "It has independent modules for :<br/>\n",
    "1) data mining:\n",
    "Google, Twitter, Bing, Yahoo, Facebook and Wikipedia APIs that support varying degrees of search functionality, including image search on some sources; a web crawler, a HTML DOM parser<br/>\n",
    "2) natural language processing:\n",
    "part-of-speech taggers, n-gram search, sentiment analysis, WordNet<br/>\n",
    "3) machine learning:\n",
    "vector space model, clustering, SVM, network analysis and visualization.<br/>\n",
    "\n",
    "With the extensive APIs provided by the web module, Pattern can help to extract data from a variety of sources, and its support for several different data processing and visualization tools in the language processing and machine learning modules makes it a very useful module for data science purposes. Although many of these functionalities can be extracted from different libraries introduced in class, such as Beautifulsoup for web scraping, Scikit-learn for machine learning algorithms or NLTK for NLP purposes, Pattern not only offers a single unified source of all such information, but it also adds some interesting features not found in those libraries, such as a web crawler in the Web module that can make data mining much easier, integrated APIs for mining from popular networking sites, and a Genetic Algorithm implementation and Latent Semantic Analysis (LSA) capability in the Vector module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "\n",
    "In this tutorial, we will show how to use the [Pattern](http://www.clips.ua.ac.be/pattern) module for data mining and processing, specifically using its [web module](http://www.clips.ua.ac.be/pages/pattern-web/), its [NLP module](http://www.clips.ua.ac.be/pages/pattern-en), and its [Machine Learning module](https://geopy.readthedocs.io).\n",
    "\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Installing the library and acquiring licenses](#Installing-the-libraries-and-acquiring-licenses)\n",
    "- [Data Mining using the APIs](#Data-mining-using-the-APIs)\n",
    "- [Using the NLP tooklit](#NLP-module)\n",
    "- [Using the Vector module](#Vector-module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the libraries and acquiring licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, you'll need to install the Pattern library, which is written for Python 2.5+ (no support for Python 3 yet). It has no external dependencies, except LSA in the pattern.vector module, which requires NumPy. \n",
    "You can install Pattern using `pip`:\n",
    "\n",
    "    $ pip install pattern\n",
    "    \n",
    "In case the pip installation causes some problems, a slightly older version is available on Conda as well, which can accessed as:\n",
    "\n",
    "    $ conda install pattern\n",
    "\n",
    "If neither of these methods work, then the file can be downloaded from the [web site](http://www.clips.ua.ac.be/pattern). \n",
    "To install Pattern so that the module is available in all Python scripts, from the command line do:\n",
    "\n",
    "        $ cd pattern-2.6\n",
    "    $ python setup.py install \n",
    "\n",
    "After you run the installs, make sure the following commands work for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pattern.web import URL, SEARCH\n",
    "from pattern.en import parse, parsetree\n",
    "from pattern.vector import Document, Model\n",
    "from pattern.graph import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the search APIs, you will need to acquire the licenses from their respectives sites, as listed here: \n",
    "[Google](https://code.google.com/apis/console/), [Facebook](http://www.clips.ua.ac.be/pattern-facebook),[Bing](https://datamarket.azure.com/dataset/5BA839F1-12CE-4CCE-BF57-A49D98D29A44),[Twitter](https://apps.twitter.com/app/new) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data mining using the APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Having acquired the licenses for the various APIs, we can use them depending on our data needs. In general, Pattern's SearchEngine object has a number of subclasses that can be used to query different web services (e.g., Google, Wikipedia). SearchEngine.search() returns a list of Result objects for a given query string, just like a search field and a results page in a browser.\n",
    "Each search engine has different settings for the search() method, and the different APIs provide varying features. In this section, we will look at some of the tasks that we can carry out using the Google, Facebook and Twitter APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook\n",
    "Facebook offers the option to search for publicly-available posts that contain the data that we specify in our query using the SEARCH type. Once a user has authorized Pattern to access their profile, it can be used to search for posts from a specfic user using the NEWS type. The COMMENTS type also allows to access the comments data associated with a particular post, idenified by its post ID (possibly obtained from the NEWS or SEARCH queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile name: Ritwik Rajendra\n"
     ]
    }
   ],
   "source": [
    "#Here, I am printing the list of posts that I have made, along the likes and comments associated with each post\n",
    "from pattern.web import Facebook, SEARCH, NEWS, COMMENTS, LIKES\n",
    "fb = Facebook(license=None) #Enter license here\n",
    "me = fb.profile(id=None)\n",
    "print \"Profile name:\",me[1]\n",
    "posts=fb.search(me[0], type=NEWS, count=100)\n",
    "for post in posts:\n",
    "    print repr(post.id)\n",
    "    print repr(post.text)\n",
    "    print repr(post.url)\n",
    "    if post.comments > 0:\n",
    "        print '%i comments' % post.comments \n",
    "        print [(r.text, r.author) for r in fb.search(post.id, type=COMMENTS)]\n",
    "    if post.likes > 0:\n",
    "        print '%i likes' % post.likes \n",
    "        print [r.author for r in fb.search(post.id, type=LIKES)]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter\n",
    "Twitter provides a couple of methods that allow us to keep track of the newly available data, as well as the currently popular topics. <br/>\n",
    "Twitter.stream() returns an endless, live stream of Result objects. A Stream is a Python list that accumulates each time Stream.update() is called; while the trends() function returns the list of the most popular topics on Twitter at that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top trending Twitter topics as of 19/10/2016 21:05:58:\n",
      "#debatenight\n",
      "World Series\n",
      "Messi\n",
      "#افلاس_السعوديه_بعد_3سنوات\n",
      "Enem\n",
      "Juventude\n",
      "#MTVCatfishBR\n",
      "#LaVoz5\n",
      "#RejectedHillarySlogans\n",
      "Jara\n",
      "Pedro Rocha\n",
      "Arrascaeta\n",
      "VAI CORINTHIANS\n",
      "Charlie Brown\n",
      "São Victor\n",
      "Derrick Rose\n",
      "Borja\n",
      "I'M DIRECTIONER\n",
      "Urias\n",
      "Thiago Martins\n",
      "Polic\n",
      "#TylerEndedBellaParty\n",
      "#من_ملوثات_تويتر\n",
      "#EuFicariaFelizSe\n",
      "#Desaforados\n",
      "#RRYMUDRodaronFeo\n",
      "#GBBO\n",
      "#sddsInvernoDetremuraSdv\n",
      "#VivasLasQueremos\n",
      "#SimoneeSimariaNoTVZ\n",
      "#برشلونه_السيتي\n",
      "#TodoCambia\n",
      "#SignsYoureLying\n",
      "#DinahAppreciationDay\n",
      "#BeRomanticIn4Words\n",
      "#POGOENMTVHITS\n",
      "#TeQuedasOTevas\n",
      "#MiercolesIntratable\n",
      "#PoyrazKarayel\n",
      "#لو_تدري_انك\n",
      "#このタグ見た人は今欲しいものを言う\n",
      "#WWENXT\n",
      "#عايش_ليه_في_مصر\n",
      "#SaiaJusta\n",
      "#PorLaNochePrefiero\n",
      "#NaucalpanSinLey\n",
      "#Beşiktaş\n",
      "#Top100DJs\n",
      "#FicaRavena\n",
      "#Velvet3\n",
      "\n",
      "Tweets related to the most trending topic,  #debatenight  are\n",
      "\n",
      "\n",
      "I feel like blood pressure medications must be sponsoring this election #debatenight #PresidentialDebate\n",
      "I feel like blood pressure medications must be sponsoring this election #debatenight #PresidentialDebate\n",
      "I feel like blood pressure medications must be sponsoring this election #debatenight #PresidentialDebate\n",
      "I feel like blood pressure medications must be sponsoring this election #debatenight #PresidentialDebate\n",
      "I feel like blood pressure medications must be sponsoring this election #debatenight #PresidentialDebate\n",
      "RT @jchaltiwanger: Here’s a no-nonsense guide to the positions and proposed policies of Hillary Clinton and Donald Trump. #debatenight http…\n",
      "RT @jchaltiwanger: Here’s a no-nonsense guide to the positions and proposed policies of Hillary Clinton and Donald Trump. #debatenight http…\n",
      "#DebateNight\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import Twitter\n",
    "print \"Top trending Twitter topics as of \"+time.strftime(\"%d/%m/%Y %H:%M:%S\")+\":\"\n",
    "trends= Twitter().trends(cached=False, count=10)\n",
    "for trend in trends:\n",
    "    print trend\n",
    "\n",
    "s = Twitter().stream(trends[0])\n",
    "print \"\\nTweets related to the most trending topic, \",trends[0],\" are\"\n",
    "for i in range(10):\n",
    "    time.sleep(1) \n",
    "    s.update()\n",
    "    print s[-1].text if s else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so, every time i go out for a ride during an #Arsenal match and miss it, they boss it. I hear you, universe. I hear you. #coyg\n",
      "RT @Stuart_PhotoAFC: Dennis Bergkamp and @MesutOzil1088 #arsenal https://t.co/NxKcB66lYs\n",
      "RT @yara_lb: Selfie with The Legend of #Arsenal and #Barcelona @thierryhenry\n",
      "#TierryHenry\n",
      "#يارا #يارا_فانز #تيري_هنري #اسبانيا #برشلونة htt…\n",
      "RT @ArsenalsRelated: Alexis Sánchez golazo vs. Ludogorets Razgrad. 🔥🔥🔥 #Arsenal https://t.co/D07AHsWwG4\n",
      "⚽🏃 #Arsenal #foreverArsenal\n",
      "RT @soccerdotcom: Mesut Ozil turning into a ruthless goal scorer now 👀👀 #Arsenal #UCL\n",
      "Hoping that #Hillary delivers a performance as dominant as #Arsenal did today #ImWithHer #debate\n",
      "#Arsenal Arsene Wenger: \"Our confidence is stronger with every win but we have to keep the vigilance and  bring that into the next game.\"\n",
      "#Ludogorets; I've had a few. #Arsenal\n",
      "RT @ArsenalsRelated: El Jefe and der Chef.\n",
      "\n",
      "#Arsenal https://t.co/Cg1qo3Krxt\n"
     ]
    }
   ],
   "source": [
    "#Example of search with query on Twitter\n",
    "t = Twitter()\n",
    "for tweet in t.search('#arsenal', start=i, count=10):\n",
    "    print tweet.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google:\n",
    "Apart from the generic search functionality extended from the SearchEngine class, The Google API also allows the user access the functionalities offered by Google Translate API.\n",
    "Google.translate() returns the translated string in the given language.\n",
    "Google.identify() returns a (language code, confidence)-tuple for a given string\n",
    "\n",
    "Please note that the Translate API is a paid service, and it needs to enabled for the license being used with Pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.cmu.edu/ CMU is a global research university known for its world-class, interdisciplinary\n",
      "programs: arts, business, computing, engineering, humanities, policy and\n",
      "science.\n",
      "https://www.cmich.edu/ Students are offered educational experiences in the arts, humanities, and natural\n",
      "and social sciences, in addition to educational depth in at least one academic ...\n",
      "https://en.wikipedia.org/wiki/Carnegie_Mellon_University Coordinates: 40°26′36″N 79°56′37″W﻿ / ﻿40.443322°N 79.943583°W﻿ /\n",
      "40.443322; -79.943583. Carnegie Mellon University is a private research\n",
      "university ...\n",
      "https://www.scs.cmu.edu/ Education in computer music, data mining, machine learning, vision, and speech\n",
      "with a list of research topics.\n",
      "https://www.cmu.edu/silicon-valley/ About Carnegie Mellon University Silicon Valley. CMU Silicon Valley. At its\n",
      "Silicon Valley location, CMU's College of Engineering can integrate the rich\n",
      "heritage ...\n",
      "http://www.coloradomesa.edu/ A four-year state-supported institution in Grand Junction, Colorado. Site provides\n",
      "course schedules, degree programs, calendar, financial aid information and ...\n",
      "http://www.completemusicupdate.com/ 19 October 2016. Moby releases new Void Pacific Choir video. 19 October 2016.\n",
      "CMU's One Liners: Warner Music, Sony/ATV, SXSW, more. 19 October 2016.\n",
      "https://www.ri.cmu.edu/ The Robotics Institute Carnegie Mellon University The Robotics Institute ...\n",
      "Institute is part of the School of Computer Science, Carnegie Mellon University\n",
      "http://athletics.cmu.edu/ Official site of the Tartans with press releases, scores, rosters, game schedules\n",
      "and coaching staff.\n",
      "http://www.centralmethodist.edu/ Established in Fayette, Mo. in 1854 and fully accredited, CMU has several\n",
      "regional branch sites and a various online programs at the undergraduate and\n",
      "Master ...\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import Google\n",
    "engine = Google(license=None) #provide your license here, necessary to access Translate API\n",
    "for result in engine.search('CMU', cached=False):\n",
    "    print result.url, plaintext(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"Qui craint de souffrir, il souffre déjà de ce qu’il craint.\"\n",
    "g = Google()\n",
    "lang,conf=g.identify(s)\n",
    "print \"Language is \", lang\n",
    "print g.translate(s, input=lang, output='en', cached=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSS news feeds:\n",
    "It is also possible to run searches over any RSS news feed pages using the Newsfeed object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'Lecture 01: Introduction'\n",
      "u''\n",
      "u'Lecture 2: Data Collection and Scraping'\n",
      "u''\n",
      "u'Lecture 03: Jupiter Notebook Lab'\n",
      "u''\n",
      "u'Lecture 4: Relational Data'\n",
      "u''\n",
      "u'Lecture 5: Visualization and Data Exploration'\n",
      "u''\n",
      "u'Lecture 6: Matrices, Vectors, and Linear Algebra'\n",
      "u''\n",
      "u'Lecture 7: Graph and Network Processing'\n",
      "u''\n",
      "u'Lecture 8: Free text and natural language processing'\n",
      "u''\n",
      "u'Lecture 9: Free text, continued'\n",
      "u''\n",
      "u'Lecture 10: Linear Regression'\n",
      "u''\n"
     ]
    }
   ],
   "source": [
    "#Here, I am printing the list of topics available in the Panopto RSS feed for the 15-688 course\n",
    "from pattern.web import Newsfeed\n",
    "PDS = 'http://scs.hosted.panopto.com/Panopto/Podcast/Podcast.ashx?courseid=e33e67d2-5935-4103-b6b2-87c4097f8da4&type=mp4'\n",
    "for result in Newsfeed().search(PDS,):\n",
    "    print repr(result.title)\n",
    "    print repr(result.guid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Crawler:\n",
    "Pattern also provides a Crawler object, that takes a list of URLs, which it then visits. If they lead to a web page, the HTML content is parsed for new links. These are added to the list of links scheduled for a visit.\n",
    "As arguements during initialization , it expects domains and delay as parameters. The given domains is the list of URLs that the crawler is expected to visit. The given delay defines the number of seconds to wait before revisiting the same domain. <br/>\n",
    "The example below defines a crawler that prints every link that it visits. In this manner, the crawler can be set up to carry out any required task by overwriting the visit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visited: http://datasciencecourse.org/ from: \n",
      "visited: http://datasciencecourse.org/#assignments from: http://datasciencecourse.org/\n",
      "visited: http://datasciencecourse.org/#faq from: http://datasciencecourse.org/\n",
      "visited: http://datasciencecourse.org/#instructors from: http://datasciencecourse.org/\n",
      "visited: http://datasciencecourse.org/#overview from: http://datasciencecourse.org/\n",
      "visited: http://datasciencecourse.org/#page-top from: http://datasciencecourse.org/\n",
      "visited: http://datasciencecourse.org/#schedule from: http://datasciencecourse.org/\n",
      "failed: http://datasciencecourse.org/NetworkXBasics.ipynb\n",
      "failed: http://www.datasciencecourse.org/hw/1/handout.tar\n",
      "visited: https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=0071471a-c880-4390-aa0d-3709f3d37776 from: http://datasciencecourse.org/\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import Crawler\n",
    "class Printer(Crawler): \n",
    "    def visit(self, link, source=None):\n",
    "        print 'visited:', link.url, 'from:', link.referrer\n",
    "    def fail(self, link):\n",
    "        print 'failed:', link.url\n",
    "\n",
    "p = Printer(links=['http://datasciencecourse.org/'], delay=3)\n",
    "for i in range(0,10):\n",
    "    p.crawl(method=DEPTH, cached=False, throttle=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NLP module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pattern provides an NLP toolkit called Pattern.en for the English language, with a parser capable of tokenizing, stemming and parts-of-speech tagging; and a sentiment analyser being among its prominent features. There are similar modules for different languages as well, such as the Es, Dr, Fr and It modules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser:\n",
    "Pattern also provides a Parser which can identify words, sentences and parts of speech from text strings. This involves tokenization (breaking up the text into words, by removing spaces and punctuations), part-of-speech tagging (annotating words with their type, e.g., is can a noun or a verb?) and chunking (grouping word sequences that have some special meaning). \n",
    "\n",
    "The parse() function takes a string, and accepts several parameters that decide its functionality as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The/DT/B-NP/O quick/JJ/I-NP/O brown/JJ/I-NP/O fox/NN/I-NP/O ,/,/O/O \"/\"/O/O Qfox/UH/O/O \"/\"/O/O ,/,/O/O jumped/VBD/B-VP/O over/IN/B-PP/B-PNP the/DT/B-NP/I-PNP lazy/JJ/I-NP/I-PNP dog/NN/I-NP/I-PNP ,/,/O/O \"/\"/O/O Ldog/NN/B-NP/O \"/\"/O/O ././O/O\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import parse\n",
    "\n",
    "string='The quick brown fox, \"Qfox\", jumped over the lazy dog, \"Ldog\".'\n",
    "print parse(string,\n",
    "        tokenize = True,         # Splits punctuation marks from words\n",
    "        tags = True,             # Parses part-of-speech tags (NN, JJ, ...)\n",
    "        chunks = True,           # Parses chunks (NP, VP, PNP, ...)\n",
    "        relations = False,       # Parse chunk relations? (-SBJ, -OBJ, ...)\n",
    "        lemmata = False,         # Lemmatizes the words (ate => eat)\n",
    "        encoding = 'utf-8',      # Input string encoding.\n",
    "         tagset = None)          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common part-of-speech tags are NN (noun), VB (verb), JJ (adjective), RB (adverb) and IN (preposition).\n",
    "Common chunk tags are NP (noun phrase) and VP (verb phrase).\n",
    "Common chunk relations are NP-SBJ (subject) and NP-OBJ (object).\n",
    "\n",
    "But as seen in the above example, the code is not in a very readable format. As a result, a more convenient function to use is the parseTree() function, which stores a tagged string as a tree of nested objects that can be traversed to analyze the components of the string. It takes the same parameters as parse() and returns a Text object. A Text is a list of Sentence objects. Each Sentence is a list of Word objects. Word objects can be grouped in Chunk objects, which are related to other Chunk objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String representation is: \n",
      "[Sentence('The/DT/B-NP/O/O/the quick/JJ/I-NP/O/O/quick brown/JJ/I-NP/O/O/brown fox/NN/I-NP/O/O/fox ,/,/O/O/O/, \"/\"/O/O/O/\" Qfox/UH/O/O/O/qfox \"/\"/O/O/O/\" ,/,/O/O/O/, jumped/VBD/B-VP/O/O/jump over/IN/B-PP/B-PNP/O/over the/DT/B-NP/I-PNP/O/the lazy/JJ/I-NP/I-PNP/O/lazy dog/NN/I-NP/I-PNP/O/dog ,/,/O/O/O/, \"/\"/O/O/O/\" Ldog/NN/B-NP/O/O/ldog \"/\"/O/O/O/\" ././O/O/O/.'), Sentence('Slow/JJ/B-ADJP/O/O/slow and/CC/O/O/O/and steady/RB/B-VP/O/VP-1/steady wins/VBZ/I-VP/O/VP-1/win the/DT/B-NP/O/NP-OBJ-1/the race/NN/I-NP/O/NP-OBJ-1/race ,/,/O/O/O/, ask/VB/B-VP/O/VP-2/ask the/DT/B-NP/O/NP-OBJ-2/the Tortoise/NNP/I-NP/O/NP-OBJ-2/tortoise ././O/O/O/.')] \n",
      "\n",
      "Tree representation is:\n",
      "NP [(u'The', u'DT'), (u'quick', u'JJ'), (u'brown', u'JJ'), (u'fox', u'NN')]\n",
      "VP [(u'jumped', u'VBD')]\n",
      "PP [(u'over', u'IN')]\n",
      "NP [(u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n",
      "NP [(u'Ldog', u'NN')]\n",
      "ADJP [(u'Slow', u'JJ')]\n",
      "VP [(u'steady', u'RB'), (u'wins', u'VBZ')]\n",
      "NP [(u'the', u'DT'), (u'race', u'NN')]\n",
      "VP [(u'just', u'RB'), (u'ask', u'VB')]\n",
      "NP [(u'the', u'DT'), (u'Tortoise', u'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import parsetree\n",
    "\n",
    "string='The quick brown fox, \"Qfox\", jumped over the lazy dog, \"Ldog\". Slow and steady wins the race, just ask the Tortoise.'\n",
    "treestruct = parsetree(string, relations=True, lemmata=True)\n",
    "print \"String representation is: \\n\",repr(s),\"\\n\"\n",
    "print \"Tree representation is:\"\n",
    "for sentence in treestruct:\n",
    "    for chunk in sentence.chunks:\n",
    "        print chunk.type, [(w.string, w.type) for w in chunk.words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis:\n",
    "The pattern.en module bundles a corpus of adjectives (e.g., good, bad, amazing, irritating, annoying,etc.) that occur frequently in reviews and social media text, annotated with scores for sentiment polarity (positive vs. negative) and subjectivity (objective vs. subjective), in order to provide a sentiment analysis for the user provided strings.\n",
    "\n",
    "The sentiment() function returns a tuple for polarity and subjectivity for the given sentence, based on the adjectives it contains, where polarity is a value between -1.0(most negative sentiment) and +1.0(most positive sentiment) and subjectivity between 0.0 and 1.0. The sentence can be a string, or a datatype returned from parseTree(),such as Text, Sentence, Chunk, or Word. The returned value also has an assessments attribute, that provides the polarity and subjectivity score for each individual evaluated term/set of words.\n",
    "\n",
    "The positive() function returns True if the given sentence's polarity is above the threshold. The threshold can be lowered or raised, but overall +0.1 gives the best results for product reviews as mentioned in the Pattern documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.15, 0.8333333333333334)\n",
      "assessments:  [(['great'], 0.8, 0.75, None), (['difficult'], -0.5, 1.0, None), (['overwhelmingly', 'disappointed'], -0.75, 0.75, None)]\n",
      "judgement:  False\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import sentiment,positive\n",
    "sent= sentiment(\n",
    "        \"Batman v Superman has its moments though with great performances from almost every one,but it was difficult to not walk out of this film and feel overwhelmingly disappointed.\")\n",
    "print sent\n",
    "print \"assessments: \",sent.assessments\n",
    "print \"judgement: \",positive(sent, threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pattern.vector module contains machine learning tools, including bag-of-word document representations, latent semantic analysis and clustering and classification algorithms. \n",
    "\n",
    "Pattern provides Document as a bag-of-words representation of a text, i.e., unordered words along with their word count. The Document.vector maps the words (or features) to their weight (word count, tf-idf, etc.). The weight of a word represents its relevancy in the text. Given an unlabeled document, a classifier yields the label of the most similar document(s) in its training set. \n",
    "\n",
    "### Classification:\n",
    "Classification is a supervised machine learning method that uses labeled documents as training examples to statistically predict the label (class, type) of new documents, using the model built on the training examples. \n",
    "The Pattern.vector module implements four classification algorithms:\n",
    "NB: Naive Bayes, based on the probability that a feature occurs in a class.\n",
    "KNN: k-nearest neighbor, based on the k most similar documents in the training set.\n",
    "SLP: single-layer averaged perceptron, based on an artificial neural network.\n",
    "SVM: support vector machine, based on a representation of the documents in a high-dimensional space separated by hyperplanes\n",
    "\n",
    "These classifiers can be defined as follows, and all of them inherit from the Classifier base class:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pattern.vector import *\n",
    "classifier =  NB(train=[], baseline=MAJORITY, method=MULTINOMIAL, alpha=0.0001)\n",
    "classifier = KNN(train=[], baseline=MAJORITY, k=10, distance=COSINE)\n",
    "classifier = SLP(train=[], baseline=MAJORITY, iterations=1)\n",
    "classifier = SVM(train=[], type=CLASSIFICATION, kernel=LINEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Classifier base class provides 3 main functions, train() and classify().\n",
    "\n",
    "Classifier.train() trains the classifier with the given features and type (= class label). Not that train is called repeatedly with each new training input provided to the classfier.\n",
    "\n",
    "Classifier.classify() returns the label with the highest probability for the given input.\n",
    "\n",
    "Classifier.test() returns an (accuracy, precision, recall, F1-score)-tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from pattern.vector import Document, NB\n",
    "from pattern.db import csv\n",
    "\n",
    "nb = NB()\n",
    "\n",
    "reviews = [\"great action\",\"best CGI ever!\",\"waste of money\", \"could have been better\",\"terrible acting\"]\n",
    "ratings = [4,5,3,2,1]\n",
    "\n",
    "for review,rating in zip(reviews,ratings):\n",
    "    featureVec=Document(review, type=int(rating), stopwords=True)\n",
    "    nb.train(featureVec)\n",
    "\n",
    "print nb.classes\n",
    "print nb.classify(Document(\"terrible movie\"))\n",
    "print nb.classify(Document(\"one of the best movies of the year\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis:\n",
    "\n",
    "Latent Semantic Analysis (LSA) is a statistical technique based on singular value decomposition([SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)). It groups related features in the model into concepts (e.g., purr + fur + claw = feline concept). This is called dimensionality reduction. Each document in the model then gets a concept vector, a compressed approximation of the original vector that may be faster for cosine similarity, clustering and classification.\n",
    "\n",
    "LSA.transform() takes a Document and returns its Vector in concept space. This is useful for documents that are not part of the model, and can basically work as an alternative to the Classifier.classify() method.\n",
    "\n",
    "The following example demonstrates how related features are grouped after LSA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cat1\n",
      "(u'cat', 0.4618802153517004)\n",
      "(u'curiosity', 0.2309401076758502)\n",
      "(u'purrs', 0.6928203230275505)\n",
      "(u'killed', 0.2309401076758502)\n",
      "\n",
      "cat2\n",
      "(u'cat', 0.23094010767585008)\n",
      "(u'curiosity', 0.11547005383792504)\n",
      "(u'purrs', 0.34641016151377513)\n",
      "(u'killed', 0.11547005383792504)\n",
      "\n",
      "dog1\n",
      "(u'wags', 0.11547005383792498)\n",
      "(u'dog', 0.23094010767584996)\n",
      "(u'tail', 0.11547005383792498)\n",
      "(u'happy', 0.3464101615137754)\n",
      "\n",
      "dog2\n",
      "(u'wags', 0.23094010767585024)\n",
      "(u'dog', 0.4618802153517005)\n",
      "(u'tail', 0.23094010767585024)\n",
      "(u'happy', 0.6928203230275516)\n"
     ]
    }
   ],
   "source": [
    "from pattern.vector import Document, Model\n",
    "\n",
    "d1 = Document('The cat purrs.', name='cat1')\n",
    "d2 = Document('Curiosity killed the cat.', name='cat2')\n",
    "d3 = Document('The dog wags his tail.', name='dog1')\n",
    "d4 = Document('The dog is happy.', name='dog2')\n",
    "\n",
    "m = Model([d1, d2, d3, d4])\n",
    "m.reduce(2)\n",
    "\n",
    "for d in m.documents:\n",
    "    print\n",
    "    print d.name\n",
    "    for concept, w1 in m.lsa.vectors[d.id].items():\n",
    "        for feature, w2 in m.lsa.concepts[concept].items():\n",
    "            if w1 != 0 and w2 != 0:\n",
    "                print (feature, w1 * w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the model is reduced to two dimensions. So there are two concepts in the concept space. Each document has a concept vector with weights for each concept. As shown, cat features have been grouped together and dog features have been grouped together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation:\n",
    "This is a validation method where K tests are done on a given classifier, each time partitioning the given dataset into different subsets for training and testing, and returns the average of the performance results with each iteration. This is more generalized, and hence more reliable than always using the same training data.\n",
    "\n",
    "Pattern provides a method for this, and can be used as follows:\n",
    "\n",
    "kfoldcv(Classifier, documents=[], folds=10, target=None)\n",
    "\n",
    "it returns a tuple of the average accuracy, precision, recall, F1 score, and std deviation. Also, kfoldcv() takes any parameters of the given Classifier as optional parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application- movie review classification:\n",
    "For example, if we have a corpus of movie reviews (training data) for which the rating is known (labels, within range 0-1), we can use it to predict the rating of other reviews, based on features extracted from the training data. \n",
    "\n",
    "we can improve on the simple implementation shown earlier, by representing each review as a vector of adjectives (e.g., good, bad, awesome, awful, etc.) since positive reviews (good, awesome) will most likely contain different adjectives than negative reviews (bad, awful). Thus, we can use the parts of speech tagger from the En module to get the adjectives from each review and build the model using them. We can also include nouns and verbs as they form an essential part of the review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5113002922497332, 0.19196699556250568, 0.13615192244539878, 0.15834131073240393, 0.06052104170168932)\n"
     ]
    }
   ],
   "source": [
    "from pattern.vector import NB, kfoldcv, count\n",
    "from pattern.db import csv\n",
    "from pattern.en import parsetree\n",
    "\n",
    "def getFeatures(review):\n",
    "    tree = parsetree(review, lemmata=True)[0]\n",
    "    features= [w.lemma for w in v if w.tag.startswith(('JJ', 'NN', 'VB', '!'))]\n",
    "    return features \n",
    "\n",
    "with open('subj.txt') as f:\n",
    "    reviews = f.readlines()\n",
    "with open('rating.txt') as f:\n",
    "    ratings = f.readlines()\n",
    "\n",
    "data = [(v(review), float(rating)) for review, rating in zip(reviews,ratings)]\n",
    "print kfoldcv(NB, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another means of classification is to use the sentiment analysis results as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5088543687158985, 0.1788190509087837, 0.13835569937934494, 0.15452916315976306, 0.034193001685803484)\n"
     ]
    }
   ],
   "source": [
    "from pattern.vector import NB, kfoldcv, count\n",
    "from pattern.db import csv\n",
    "from pattern.en import sentiment,positive\n",
    "\n",
    "def getFeatures(review):\n",
    "    return sentiment(Document(review))[0] \n",
    "\n",
    "with open('subj.txt') as f:\n",
    "    reviews = f.readlines()\n",
    "with open('rating.txt') as f:\n",
    "    ratings = f.readlines()\n",
    "\n",
    "data = [(v(review), float(rating)) for review, rating in zip(reviews,ratings)]\n",
    "print kfoldcv(NB, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Pattern website: http://www.clips.ua.ac.be/pattern\n",
    "2. Movie review dataset: https://www.cs.cornell.edu/people/pabo/movie-review-data/\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
