{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib #urllib & json are used for accessing and formatting our API Calls\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gmaps #Google Maps\n",
    "from ipywidgets import * #You'll need this for dispalying Google Maps inside the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Real Estate Data:\n",
    "## Plotting Addresses on a Map & Building your own Zillow-Style Real Estate Estimator\n",
    "\n",
    "The intent of this tutorial is to expedite future work for individuals wishing to work with property data. We'll accomplish this using property data from Western Pennsylvania Regional Data Center (WPRDC). We will cover converting features to useful formats, visualization, and prediction. \n",
    "\n",
    "In this tutorial, we'll be leveraging data from two different sources -- Western Pennsylvania Regional Data Center and Google Maps' API. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "<ol>\n",
    "<li>System Setup</li>\n",
    "<li>Pulling Data from WDRPC's \"Allegheny County Property Assessment\" Database</li>\n",
    "<li>Plotting Addresses in Google Maps</li>\n",
    "<li>Your Own Zillow: Perceptron for Property Price Prediction</li>\n",
    "<li>Recommendations for Follow-On Projects</li>\n",
    "</ol>\n",
    "\n",
    "## System Setup\n",
    "### Google API Key\n",
    "\n",
    "Our first step is to setup our system so we can access and display the required data. \n",
    "\n",
    "The Google Maps API comes with some great documentation. Recent changes to Google's API policies requires users to have an API Key. You can get one by follwing their instructions here:  \n",
    "\n",
    "https://developers.google.com/maps/documentation/geocoding/get-api-key\n",
    "\n",
    "One you do that, copy and paste your API key here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "google_api_key = '' # Your key here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the Javascript Widget\n",
    "To visualize Google Maps in the notebook, we'll want the Javascript Widget from nbextenstion next. In a command line run the following:\n",
    "```\n",
    "pip install ipywidgets\n",
    "```\n",
    "then\n",
    "```\n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "```\n",
    "<strong>Note:</strong> If you have more than one kernel, make sure you swith to that environment before running the above commands.\n",
    "\n",
    "If you have trouble with those commands you can try this Conda alternative:\n",
    "```\n",
    "conda install -c conda-forge ipywidgets\n",
    "```\n",
    "### Installing the gmaps package in your environment\n",
    "With that in place, checkout Google's <a href=\"https://developers.google.com/maps/documentation/geocoding/intro\">documentation on geocoding</a>. \n",
    "\n",
    "We'll be accessing it through Python's gmaps 0.3.5. You can find more documentation at these two sites:\n",
    "<ul>\n",
    "<li>https://pypi.python.org/pypi/gmaps/0.3.5</li>\n",
    "<li>https://github.com/pbugnion/gmaps</li>\n",
    "</ul>\n",
    "\n",
    "You can easily install the required packages using the following command in a terminal:\n",
    "```\n",
    "pip install gmaps\n",
    "```\n",
    "\n",
    "\n",
    "## Pulling Data from WDRPC\n",
    "\n",
    "Using the <a href=\"https://data.wprdc.org/\">WPRDC's data</a> comes with several stipulations. Most are covered in the Terms of Use which you must agree to before you are allowed to access the numerous data sets available. However, be aware that there may be additional restrictions on individual data sets.\n",
    "\n",
    "WPRDC provides 146 datasets that cover everything from crash and police incidents to air quality, abestos permits, and even dog permits in Allegheny county. For this tutorial, we'll be focusing on the <a href=\"https://data.wprdc.org/dataset/property-assessments\">Allegheny County Property Assessments</a>.\n",
    "\n",
    "For this tutorial, we'll be using this dataset: https://data.wprdc.org/dataset/property-assessments/resource/518b583f-7cc8-4f60-94d0-174cc98310dc\n",
    "\n",
    "<strong>Note:</strong> Click on the green API icon in the top right corner of the page to get a feel for the different API calls you can make. Everything from SQL commads to built-in queries can be passed. \n",
    "\n",
    "For a detailed breakdown on specific features, this is the reference you want: https://data.wprdc.org/dataset/2b3df818-601e-4f06-b150-643557229491/resource/43a275de-4745-446b-8ba0-b9389568e568/download/property-assessment-data-dictionaryrev.pdf\n",
    "\n",
    "We'll use the function below to query the data in the format we want. Note that there are <strong>500,000+</strong> records available, but we'll only use 50,000 for our tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_propasses(limit=None):\n",
    "    # This function downloads the desired data set from WPRDC via a query. The query will return will be in JSON which\n",
    "    # we will convert to a Pandas DataFrame\n",
    "    # Inputs: URL of the desired dataset\n",
    "    # Output: Pandas DataFrame containing property assessement data\n",
    "    \n",
    "    url = 'https://data.wprdc.org/api/action/datastore_search?resource_id=518b583f-7cc8-4f60-94d0-174cc98310dc&q'\n",
    "    \n",
    "    # If the user specifies a limit, append it to the URL. There's 500k records, so we can get quite a lot!\n",
    "    if limit != None:\n",
    "        url += '&limit='+str(limit)\n",
    "    \n",
    "    # Query the WPRDC data base\n",
    "    fileobj = urllib.urlopen(url)\n",
    "    \n",
    "    # Convert the data to JSON format\n",
    "    data = json.loads(fileobj.read())\n",
    "    records = data['result']['records']\n",
    "    df = pd.read_json(json.dumps(records))\n",
    "    \n",
    "    # For the purpose of this tutorial, we will only consider residential property sold after 2014. Next,\n",
    "    # we will eliminate anything that doesn't meet these requirements. \n",
    "       \n",
    "    return df\n",
    "results = get_propasses(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, we have 50,000 records at our disposal. However, we need to do some filtering. WPRDC is in the process of cleaning up their dataset, but that also means not all of the records can be used at this time. In the next step, we're going to be doing some filtering. \n",
    "\n",
    "We're being overly strict for this implementation, but you're free to relax some of the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "\n",
    "    # We'll set PARID (Parcel Identifier) to the index\n",
    "    df = df.set_index(['PARID'])   \n",
    "    \n",
    "    # Remove so called \"skeleton\" listings. These have a USECODE of 001 & 002.\n",
    "    df = df[df['USECODE'] > 2]\n",
    "\n",
    "    # We're going to eliminate known outliers or special cases so we can focus on predicting fairmarket value \n",
    "    # for residential properties\n",
    "    df = df[df['SALEPRICE'] > 1000] # Play around with this number. You'll notice the perceptron struggles under a $100k\n",
    "    df = df[df['CARDNUMBER'] == 1] # Some parcels have more than one building, we'll restrict this to single-building properties\n",
    "    df = df[df['SALEDESC'] == 'VALID SALE'] # We only want records that county think were at fair market value.\n",
    "    df = df[df['TAXCODE'] == 'T']\n",
    "    df = df[df['OWNERCODE'] == 10] #Regular, residental owners only\n",
    "    \n",
    "    # We'll also remove any special use properties\n",
    "    df = df[pd.isnull(df['TAXSUBCODE'])]\n",
    "    df = df[pd.isnull(df['ABATEMENTFLAG'])]\n",
    "    df = df[pd.isnull(df['FARMSTEADFLAG'])]\n",
    "    df = df[pd.isnull(df['HOMESTEADFLAG'])]\n",
    "    df = df[pd.isnull(df['CLEANGREEN'])]\n",
    "\n",
    "    # Remove the now homogeneous columns\n",
    "    extracol = ['OWNERCODE', 'OWNERDESC','CARDNUMBER', 'SALEDESC','TAXCODE','TAXSUBCODE','USECODE', 'ABATEMENTFLAG', 'FARMSTEADFLAG', 'HOMESTEADFLAG', 'CLEANGREEN']\n",
    "    df = df.drop(extracol, axis=1)\n",
    "\n",
    "    # Correct the format of specific rows:\n",
    "    df['SALEDATE'] = pd.DatetimeIndex(df['SALEDATE'], errors='coerce') #convert to datetime format\n",
    "    df['LOTAREA'] = df['LOTAREA'].astype(float)\n",
    "    df['PROPERTYZIP'] = df['PROPERTYZIP'].astype(str)\n",
    "    df['PROPERTYHOUSENUM'] = df['PROPERTYHOUSENUM'].astype(int)\n",
    "    \n",
    "    #Fill in missing values\n",
    "    df['NEIGHDESC'] = df['NEIGHDESC'].fillna(\"\")\n",
    "    df['PROPERTYFRACTION'] = df['PROPERTYFRACTION'].fillna(\"\")\n",
    "    df['ROOFDESC'] = df['ROOFDESC'].fillna(\"\")\n",
    "    \n",
    "    # Remove \"skeleton\" listings that aren't properly labelled\n",
    "    df = df.dropna(subset=['BASEMENT','BEDROOMS','BSMTGARAGE','CONDITION','STORIES','TOTALROOMS','YEARBLT'],axis=0)\n",
    "\n",
    "    # Drop the columns that are redundant, we have no interest in, that would lead our algorithm astray, \n",
    "    # or are just filler\n",
    "    extracol = ['STYLE','NEIGHCODE','SALECODE','_id','CLASS','TAXDESC','MUNICODE','SCHOOLCODE','LEGAL1','LEGAL2','LEGAL3','ALT_ID','ASOFDATE','RECORDDATE','DEEDBOOK','DEEDPAGE','TAXSUBCODE_DESC','LOCALTOTAL','FAIRMARKETBUILDING','FAIRMARKETLAND','FAIRMARKETTOTAL','EXTERIORFINISH','ROOF','BASEMENTDESC','GRADEDESC','CONDITIONDESC','CDUDESC','PREVSALEDATE','PREVSALEPRICE','PREVSALEDATE2','PREVSALEPRICE2','CHANGENOTICEADDRESS1','CHANGENOTICEADDRESS2','CHANGENOTICEADDRESS3','CHANGENOTICEADDRESS4','COUNTYBUILDING','COUNTYLAND','COUNTYTOTAL','COUNTYEXEMPTBLDG','LOCALBUILDING','LOCALLAND','CLASSDESC','HEATINGCOOLING','TAXYEAR']\n",
    "    df = df.drop(extracol, axis=1)\n",
    "    \n",
    "    #Finally, we're going to cycle through and ensure that we only have no \"NaN\" in our numeric features\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtypes == float) | (df[col].dtypes == int):\n",
    "            df = df[pd.notnull(df[col])]\n",
    "    \n",
    "    return df\n",
    "\n",
    "prop_assess = preprocess(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our data should be clean and ready to be processed. Let's take a look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  BASEMENT  BEDROOMS  BSMTGARAGE CDU  CONDITION  \\\n",
      "PARID                                                             \n",
      "0104S00104000000       5.0       2.0         1.0  AV        3.0   \n",
      "0190N00200000000       5.0       2.0         0.0  AV        3.0   \n",
      "0190J00056000000       5.0       3.0         1.0  AV        3.0   \n",
      "0190G00174000000       5.0       3.0         1.0  AV        3.0   \n",
      "0190G00266000000       5.0       3.0         0.0  FR        3.0   \n",
      "\n",
      "                 EXTFINISH_DESC  FINISHEDLIVINGAREA  FIREPLACES  FULLBATHS  \\\n",
      "PARID                                                                        \n",
      "0104S00104000000          Frame              1170.0         1.0        1.0   \n",
      "0190N00200000000  Masonry FRAME              1012.0         1.0        1.0   \n",
      "0190J00056000000          Brick              1274.0         1.0        1.0   \n",
      "0190G00174000000  Masonry FRAME              1470.0         0.0        1.0   \n",
      "0190G00266000000          Brick              1519.0         0.0        1.0   \n",
      "\n",
      "                 GRADE   ...    PROPERTYZIP ROOFDESC   SALEDATE SALEPRICE  \\\n",
      "PARID                    ...                                                \n",
      "0104S00104000000    C-   ...          15106  SHINGLE 1999-06-11   63000.0   \n",
      "0190N00200000000     C   ...          15234  SHINGLE 1989-11-17   29100.0   \n",
      "0190J00056000000     C   ...          15234  SHINGLE 2010-05-04   93000.0   \n",
      "0190G00174000000     C   ...          15234  SHINGLE 2006-04-28   74900.0   \n",
      "0190G00266000000     C   ...          15234  SHINGLE 2010-01-25   82000.0   \n",
      "\n",
      "                     SCHOOLDESC STORIES  STYLEDESC TOTALROOMS        USEDESC  \\\n",
      "PARID                                                                          \n",
      "0104S00104000000      Carlynton     1.0   CAPE COD        5.0  SINGLE FAMILY   \n",
      "0190N00200000000  Keystone Oaks     2.0  OLD STYLE        5.0  SINGLE FAMILY   \n",
      "0190J00056000000  Keystone Oaks     1.0   CAPE COD        5.0  SINGLE FAMILY   \n",
      "0190G00174000000  Keystone Oaks     2.0   COLONIAL        6.0  SINGLE FAMILY   \n",
      "0190G00266000000  Keystone Oaks     2.0   COLONIAL        6.0  SINGLE FAMILY   \n",
      "\n",
      "                 YEARBLT  \n",
      "PARID                     \n",
      "0104S00104000000  1951.0  \n",
      "0190N00200000000  1941.0  \n",
      "0190J00056000000  1948.0  \n",
      "0190G00174000000  1971.0  \n",
      "0190G00266000000  1965.0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "BASEMENT                     float64\n",
      "BEDROOMS                     float64\n",
      "BSMTGARAGE                   float64\n",
      "CDU                           object\n",
      "CONDITION                    float64\n",
      "EXTFINISH_DESC                object\n",
      "FINISHEDLIVINGAREA           float64\n",
      "FIREPLACES                   float64\n",
      "FULLBATHS                    float64\n",
      "GRADE                         object\n",
      "HALFBATHS                    float64\n",
      "HEATINGCOOLINGDESC            object\n",
      "LOTAREA                      float64\n",
      "MUNIDESC                      object\n",
      "NEIGHDESC                     object\n",
      "PROPERTYADDRESS               object\n",
      "PROPERTYCITY                  object\n",
      "PROPERTYFRACTION              object\n",
      "PROPERTYHOUSENUM               int64\n",
      "PROPERTYSTATE                 object\n",
      "PROPERTYUNIT                  object\n",
      "PROPERTYZIP                   object\n",
      "ROOFDESC                      object\n",
      "SALEDATE              datetime64[ns]\n",
      "SALEPRICE                    float64\n",
      "SCHOOLDESC                    object\n",
      "STORIES                      float64\n",
      "STYLEDESC                     object\n",
      "TOTALROOMS                   float64\n",
      "USEDESC                       object\n",
      "YEARBLT                      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print prop_assess.head()\n",
    "print prop_assess.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that are listed as \"object\" will be treated as a nominal feature. Everything can safely be considered as a continuous feature.\n",
    "\n",
    "### Separating the data for further processing\n",
    "\n",
    "Now that our dataset has been cleaned, we'll divide it into convenient segments for traing. One caveet is that we'll create a \"Key.\" Property addresses are not readily useful for training our model. \n",
    "\n",
    "However, they are useful in bringing context to what the data. For that reason, we'll save it in a separate dataset for future reference.\n",
    "\n",
    "We'll do that here and build the other data sets later <strong>(after we have some fun with Google Maps).</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 PROPERTYCITY PROPERTYZIP PROPERTYSTATE  PROPERTYHOUSENUM  \\\n",
      "PARID                                                                       \n",
      "0104S00104000000     CARNEGIE       15106            PA               306   \n",
      "0190N00200000000   PITTSBURGH       15234            PA              3551   \n",
      "0190J00056000000   PITTSBURGH       15234            PA              3417   \n",
      "0190G00174000000   PITTSBURGH       15234            PA              3125   \n",
      "0190G00266000000   PITTSBURGH       15234            PA              3111   \n",
      "\n",
      "                 PROPERTYFRACTION PROPERTYADDRESS PROPERTYUNIT  \n",
      "PARID                                                           \n",
      "0104S00104000000                         BELL AVE               \n",
      "0190N00200000000                       LIBRARY RD               \n",
      "0190J00056000000                       POPLAR AVE               \n",
      "0190G00174000000                    BELLEVILLE ST               \n",
      "0190G00266000000                           MAY ST               \n",
      "(1475, 7)\n"
     ]
    }
   ],
   "source": [
    "key = prop_assess.loc[:,['PROPERTYCITY', 'PROPERTYZIP', 'PROPERTYSTATE','PROPERTYHOUSENUM', 'PROPERTYFRACTION','PROPERTYADDRESS','PROPERTYUNIT']].dropna(axis=1, how='all').fillna(\"\")\n",
    "print key.head()\n",
    "print key.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling Lattitude and Longitude from Google's Geocoding API\n",
    "\n",
    "In the desire to make sure you get your money's worth from your Google API Key, we'll convert our street addresses to  Now we'll leverage another Google tookit -- Geocoding.\n",
    "\n",
    "Geocoding works by converting the street addresses to grid coordinates (and yes, you can go the other way too). \n",
    "\n",
    "We'll write a function to handle the API call for use, <i>getlatlon</i>, shown below. The function returns the results of our API request and parses the lattitude and longitude for us.\n",
    "\n",
    "<strong>Note:</strong> You only have a 25,000 API calls in a 24 hour period. Use them wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getlatlon(address, api_key):\n",
    "    \n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json?address=\"+'\"'+address+'\"'+\"&key=\"+api_key\n",
    "\n",
    "    # Query the WPRDC data base\n",
    "    fileobj = urllib.urlopen(url)\n",
    "\n",
    "    # Convert the data to JSON format\n",
    "    data = json.loads(fileobj.read())\n",
    "\n",
    "    if data['status'] == 'OK':\n",
    "        lat = data['results'][0]['geometry']['location']['lat']\n",
    "        lon = data['results'][0]['geometry']['location']['lng']\n",
    "        return lat, lon\n",
    "    else:\n",
    "        print \"Address not found: \", data['status']\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll add two empty columns to our \"key\" dataframe for lattitude and longitude. Next, we'll cicle through our addresses (combining them into something readable for the API) and calling our function. Results are added to their respective columns in \"key\" as we recieve them. If any adresses aren't found, we'll drop that from our key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key['lat'] = np.zeros(key.shape[0])\n",
    "key['lon'] = np.zeros(key.shape[0])\n",
    "\n",
    "for idx, row in key.iterrows():\n",
    "    address = str(row['PROPERTYHOUSENUM'])+' '+str(row['PROPERTYADDRESS'])+', '+str(row['PROPERTYCITY'])+', '+str(row['PROPERTYSTATE'])+' '+str(row['PROPERTYZIP'])    \n",
    "    key.loc[idx,'lat'], key.loc[idx,'lon'] = getlatlon(address, google_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Coordinates in Google Maps\n",
    "\n",
    "Now that we have our grid coordinates, let's put them to use. We'll create a list of tuples (lat, lon) that Google will use to generate and display a heatmap from. \n",
    "\n",
    "You can see that we create our map object, add the heatmap layer, and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key = key.dropna(subset=['lat','lon']) #Drop rows that weren't found in the database\n",
    "\n",
    "coords = [[k['lat'], k['lon']] for i, k in key.iterrows()] # Creating that tuple I mentioned\n",
    "gmaps.configure(api_key = google_api_key)\n",
    "m = gmaps.Map()\n",
    "m.add_layer(gmaps.Heatmap(data=coords))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1474, 9)\n"
     ]
    }
   ],
   "source": [
    "print key.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Note:</strong> Key changed in size here. We'll have to remember that for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating datasets for training our perceptron\n",
    "\n",
    "We will do this in two steps. First, we will create our output vector from the \"SALEPRICE\" column in our property assessement data. \n",
    "\n",
    "You can see that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARID\n",
      "0104S00104000000    63000.0\n",
      "0190N00200000000    29100.0\n",
      "0190J00056000000    93000.0\n",
      "0190G00174000000    74900.0\n",
      "0190G00266000000    82000.0\n",
      "Name: SALEPRICE, dtype: float64\n",
      "(1475,)\n"
     ]
    }
   ],
   "source": [
    "prices = prop_assess.loc[:, 'SALEPRICE']\n",
    "print prices.head()\n",
    "print prices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our \"Prices\" dataset was the easy part. Creating our feature set will take a little more work.\n",
    "\n",
    "A quirk of perceptrons is that they are very suseptible to extreme feature values. For that reason we'll be scaling everything between [0,1] (but you could just as easily use [-1,1]).\n",
    "\n",
    "For nominal features, we'll need to create new features to compensate. So called \"One Hot\" encoding will allow us to us leverage these feature labels. You can read a <a href=\"http://stackoverflow.com/questions/17469835/one-hot-encoding-for-machine-learning#17470183\">great writeup here if you're curious.</a> \n",
    "\n",
    "We'll nest all of this into a fuction \"formatfeatures\" which is three in one. Since we prepocessed our features, we can easily process them based on their datatype. You can see the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def formatfeatures(df):\n",
    "    \n",
    "    def numericfeature(dfcol):\n",
    "    \n",
    "        mincol = min(dfcol)\n",
    "        maxcol = max(dfcol)\n",
    "\n",
    "        return (dfcol-mincol)/(maxcol-mincol)\n",
    "\n",
    "    def nominalfeature(col, df):\n",
    "    \n",
    "        labels = pd.get_dummies(df[col].unique())\n",
    "\n",
    "        for label in labels:\n",
    "            df[str(col+\"=\"+label)] = 1.0 * (df[col] == label)\n",
    "\n",
    "        df = df.drop([col],1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    for col in df.columns:\n",
    "        \n",
    "        if df[col].dtype == object:\n",
    "            df = nominalfeature(col, df)\n",
    "            \n",
    "        else:\n",
    "            df[col] = numericfeature(df[col])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll generate our feature set from the desired columns and pass them to our \"formatfeatures\" function. \n",
    "\n",
    "Don't forget to check the output (and makesure everything has been scaled correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  BASEMENT  BEDROOMS  BSMTGARAGE  CONDITION  \\\n",
      "PARID                                                         \n",
      "0104S00104000000       1.0     0.125        0.25   0.166667   \n",
      "0190N00200000000       1.0     0.125        0.00   0.166667   \n",
      "0190J00056000000       1.0     0.250        0.25   0.166667   \n",
      "0190G00174000000       1.0     0.250        0.25   0.166667   \n",
      "0190G00266000000       1.0     0.250        0.00   0.166667   \n",
      "\n",
      "                  FINISHEDLIVINGAREA  FIREPLACES  FULLBATHS  HALFBATHS  \\\n",
      "PARID                                                                    \n",
      "0104S00104000000            0.106430    0.333333       0.25   0.000000   \n",
      "0190N00200000000            0.083394    0.333333       0.25   0.000000   \n",
      "0190J00056000000            0.121592    0.333333       0.25   0.000000   \n",
      "0190G00174000000            0.150168    0.000000       0.25   0.333333   \n",
      "0190G00266000000            0.157312    0.000000       0.25   0.333333   \n",
      "\n",
      "                   LOTAREA  SALEDATE         ...          STYLEDESC=TUDOR  \\\n",
      "PARID                                        ...                            \n",
      "0104S00104000000  0.034608  0.591942         ...                      0.0   \n",
      "0190N00200000000  0.031609  0.364577         ...                      0.0   \n",
      "0190J00056000000  0.043260  0.851006         ...                      0.0   \n",
      "0190G00174000000  0.032301  0.755517         ...                      0.0   \n",
      "0190G00266000000  0.019986  0.844562         ...                      0.0   \n",
      "\n",
      "                  STYLEDESC=VICTORIAN  USEDESC=CONDEMNED/BOARDED-UP  \\\n",
      "PARID                                                                 \n",
      "0104S00104000000                  0.0                           0.0   \n",
      "0190N00200000000                  0.0                           0.0   \n",
      "0190J00056000000                  0.0                           0.0   \n",
      "0190G00174000000                  0.0                           0.0   \n",
      "0190G00266000000                  0.0                           0.0   \n",
      "\n",
      "                  USEDESC=CONDOMINIUM  USEDESC=FOUR FAMILY  USEDESC=ROWHOUSE  \\\n",
      "PARID                                                                          \n",
      "0104S00104000000                  0.0                  0.0               0.0   \n",
      "0190N00200000000                  0.0                  0.0               0.0   \n",
      "0190J00056000000                  0.0                  0.0               0.0   \n",
      "0190G00174000000                  0.0                  0.0               0.0   \n",
      "0190G00266000000                  0.0                  0.0               0.0   \n",
      "\n",
      "                  USEDESC=SINGLE FAMILY  USEDESC=THREE FAMILY  \\\n",
      "PARID                                                           \n",
      "0104S00104000000                    1.0                   0.0   \n",
      "0190N00200000000                    1.0                   0.0   \n",
      "0190J00056000000                    1.0                   0.0   \n",
      "0190G00174000000                    1.0                   0.0   \n",
      "0190G00266000000                    1.0                   0.0   \n",
      "\n",
      "                  USEDESC=TOWNHOUSE  USEDESC=TWO FAMILY  \n",
      "PARID                                                    \n",
      "0104S00104000000                0.0                 0.0  \n",
      "0190N00200000000                0.0                 0.0  \n",
      "0190J00056000000                0.0                 0.0  \n",
      "0190G00174000000                0.0                 0.0  \n",
      "0190G00266000000                0.0                 0.0  \n",
      "\n",
      "[5 rows x 207 columns]\n",
      "(1475, 207)\n"
     ]
    }
   ],
   "source": [
    "features = prop_assess.loc[:, ['BASEMENT','BEDROOMS','BSMTGARAGE','CDU','CONDITION','EXTFINISH_DESC','FINISHEDLIVINGAREA','FIREPLACES','FULLBATHS','GRADE','HALFBATHS','HEATINGCOOLINGDESC','LOTAREA','MUNIDESC','NEIGHDESC','ROOFDESC','SALEDATE','SCHOOLDESC','STORIES','STYLEDESC','TOTALROOMS','USEDESC','YEARBLT']]\n",
    "features = formatfeatures(features)\n",
    "print features.head()\n",
    "print features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating test and training sets\n",
    "\n",
    "Finally, we're going to divide our data into test and training sets.\n",
    "\n",
    "We'll do this probablistically using a threashold value and random values. For each index value (from \"PARID\" in this case), we'll generate a random value between [0,1]. If the random value is below the threshold (default = 0.7) the index added to the training set, otherwise it's added to the test set. \n",
    "\n",
    "<strong>Note:</strong> We want to pass the \"Key\" index instead of \"Price\" or \"Features\" since it may have eliminated several properties that Google couldn't find in the geocoding step above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generatesplit(index, thresh=0.7):\n",
    "    \n",
    "    train = []\n",
    "    test = []\n",
    "    \n",
    "    random.seed() #Uses the current time to \n",
    "    \n",
    "    for idx in index:\n",
    "        prob = random.random()\n",
    "        \n",
    "        if prob < thresh:\n",
    "            train.append(idx)\n",
    "        else:\n",
    "            test.append(idx)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "# Get the random indexes of our training and test sets\n",
    "itrain, itest = generatesplit(key.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of this function are two lists of indexes that we'll use for the training and testing sets. We'll use these to split our \"Prices\" and \"Features\" sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split our featrues and prices\n",
    "ptrain = prices[itrain]\n",
    "ptest = prices[itest]\n",
    "\n",
    "ftrain = features.loc[itrain]\n",
    "ftest = features.loc[itest]\n",
    "\n",
    "# Split our key (not really necessary)\n",
    "ktrain = key.loc[itrain]\n",
    "ktest = key.loc[itest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Perceptron\n",
    "Finally, we're ready to start training our perceptron. \n",
    "Here's the obligatory Wikipedia link:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Perceptron\n",
    "\n",
    "If that's too vague, you can also check out these links (which were simplified a lot of the fluff):\n",
    "<ul>\n",
    "<li>https://blog.dbrgn.ch/2013/3/26/perceptrons-in-python/</li>\n",
    "<li>http://glowingpython.blogspot.com/2011/10/perceptron.html</li>\n",
    "</ul>\n",
    "\n",
    "For our purposes, we'll create a weight vector with the same number of columns as our \"Features\" Set. We'll then initialize these weights to random initial values. \n",
    "\n",
    "When we <i>train</i> a perceptron, we train the weights. We're computing two equations. \n",
    "\n",
    "The first is the prediction equation, shown here:\n",
    "\n",
    "$y = max(\\sum{w_i * x_i}, 0)$ for $0 <= i < n$, where n is the number of columns in our feature set. \n",
    "\n",
    "We use our prediction to update weights using this equation for each sample in our training set:\n",
    "\n",
    "$w := w_i + error * learningrate * (d - y)* x_i$ for $0 <= i < n$\n",
    "\n",
    "The \"d\" is simply the \"Prices\" value that corresponds to the row we're reviewing. Learning rate is typicaly less than 1, we'll use 0.2. Since our training set is so small, we'll iterate through the entire set a few times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainperceptron(w, X, D, numiter=5, lr = 0.2):\n",
    "    count = 0\n",
    "    while count < numiter:\n",
    "        for i, x in X.iterrows():\n",
    "            #print x\n",
    "            #print i\n",
    "            d = D[i]\n",
    "            y = max(sum(w*x),0)\n",
    "            error = (d - y)\n",
    "            #print i, d, y, error\n",
    "            w = w + lr*error*x\n",
    "            \n",
    "        count += 1\n",
    "    return w\n",
    "# Initialize weights \n",
    "w = [random.random() for _ in xrange(len(features.columns))]\n",
    "# Train perceptron\n",
    "w = trainperceptron(w, ftrain, ptrain, numiter=5, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Perceptron Regression\n",
    "\n",
    "Predicting values is easy with perceptrons. We'll use this equation from above:\n",
    "\n",
    "$y = max(\\sum{w_i * x_i}, 0)$ for $0 <= i < n$\n",
    "\n",
    "We'll iterate through our testing set and output the results. We'll then check our results for accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average error: $ 32535.3370307\n"
     ]
    }
   ],
   "source": [
    "def predict(w, X):\n",
    "    y = pd.Series(index=X.index)\n",
    "    \n",
    "    for i, x in X.iterrows():\n",
    "        y[i] = sum(w*x)\n",
    "    return y\n",
    "predictedprices = predict(w,ftest)\n",
    "print \"average error: $\",sum(abs(predictedprices - ptest)/float(len(ptest)))\n",
    "\n",
    "# Uncomment this code to see the error for each \n",
    "#for i in range(len(predictedprices)):\n",
    "    #print predictedprices[i], ptest[i], abs(ptest[i] - predictedprices[i])/ptest[i]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our training set is so small, we're going to expect limited accuracy. You can experiment with both preprocessing and feature engineering to further refine results. \n",
    "\n",
    "## Recommendations for Follow-On Projects\n",
    "<ol>\n",
    "\n",
    "<li>Include more data</li>\n",
    "\n",
    "Preprocessing for this tutorial was quite heavy handed. Try your hand at experimenting with more relaxed techniques. \n",
    "\n",
    "<li>Calculate the distance from Downtown Pittsburgh.</li>\n",
    "\n",
    "Since we already have the grid coordinates for every property, we could quickly calculate their distance from Pittsburgh Better yet, include their local city center too! Hint: you can use <a href=\"https://pypi.python.org/pypi/geopy\">geopy.</a>\n",
    "\n",
    "<li>Compare results to Zillow (or Redfin).</li>\n",
    "\n",
    "Since we're builing our own estimator, you could check how we did against Zillow's \"Zestimates.\" You can read up on it here: http://www.zillow.com/howto/api/APIOverview.htm\n",
    "\n",
    "<li>Implement a Multi-Layer Perceptron</li>\n",
    "\n",
    "A single layer perceptron can be quite limited (for instance, it can't learn XOR relationships). Takin the time to implement a multi-layer perceptron network may improve your results. \n",
    "\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dataScience]",
   "language": "python",
   "name": "Python [dataScience]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "ec4b02993355477aa61faaf9685365f5": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
