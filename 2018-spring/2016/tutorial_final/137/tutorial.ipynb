{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Boto3 to Share Data Between Python and AWS S3 Buckets\n",
    "\n",
    "## Introduction\n",
    "Boto3 is a really useful tool to connect Python scripts with Amazon Web Service’s (AWS’s) S3 service. AWS’s S3 can categorize and save data, such as an image file’s binary data. Boto3 helps communicate what to save, where to save it and how to label it. To use Boto3, first it needs to be installed and the amazon credentials associated with the account from which S3 will be accessed (usually your account or your company’s account) need to be configured. This can be done like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And then in a file with the path \"~/.aws/credentials\", it should look like this, but with the [default] uncommented. (Just doesn't work here otherwise)\n",
    "# [default]\n",
    "aws_access_key_id = 'YOUR_ACCESS_KEY'\n",
    "aws_secret_access_key = 'YOUR_SECRET_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buckets\n",
    "#### Bucket Names\n",
    "The necessary information can be saved in what is called a bucket and can be accessed through the Python code or from the Amazon Web Service’s web console by going to the S3 section of the console. Importing Boto3, creating an S3 client object, creating an S3 resource object, accessing all of the S3 buckets and then accessing each bucket’s name (which we’ll just print for now) looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_df_bucket\n",
      "pds-tutorial\n",
      "pds-tutorial-config-bucket\n",
      "pds-tutorial-create-bucket\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sys\n",
    "\n",
    "# First method of finding credentials that will be tried by boto3 is with this client function\n",
    "client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id='AKIAIJHRV5BXBVRW5UDQ',\n",
    "    aws_secret_access_key='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'\n",
    "    )\n",
    "\n",
    "# This does not need to be hard coded, though, if the credentials file is configured properly\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Buckets\n",
    "\n",
    "Creating and deleting buckets is very simple, but absolutely crucial. There are two options to creating a bucket: a simple create or creating a bucket with specific configurations. Configuring a bucket is often used to specify a particular location that can access the bucket. The tricky thing when coming up with a bucket name is that all users share a namespace. So whatever bucket name you come up with, it has to be unique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket already exists!\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    bucket1 = s3.create_bucket(Bucket=\"pds-tutorial-create-bucket\")\n",
    "    bucket2 = s3.create_bucket(Bucket='pds-tutorial-config-bucket', CreateBucketConfiguration={\n",
    "    'LocationConstraint': 'us-west-1'})\n",
    "except:\n",
    "    print \"Bucket already exists!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Bucket Keys and Objects\n",
    "Each piece of information that is saved to a bucket is saved with a key (formatted as a string). This key is an identifier and can double as a way to further subdivide buckets. So, if we know that ‘pds-tutorial’ is the name of one of the buckets in our S3, we can then save an object to that bucket. This object that we are going to save includes the key that we want to use to identify the data as well as the data itself in the body of the object. We can do that for an image file we have on our local computer like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client.upload_file(\"text.txt\", \"pds-tutorial\", \"pds-text\")\n",
    "client.upload_file(\"text.txt\", \"pds-tutorial-create-bucket\", \"pds-text\")\n",
    "client.upload_file(\"text.txt\", \"pds-tutorial-config-bucket\", \"pds-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if the bucket name that we try to use is not actually the name of a bucket, we will get an error. The easiest way to deal with this is to use a try and except block around the line or lines where the bucket is accessed. In general, this would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    client.upload_file(\"text.txt\", \"pds-tutorial\", \"pds-text\")\n",
    "except:\n",
    "    print \"Unexpected error:\", sys.exc_info()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are even more examples of how to upload objects, though, all with a different nuance. This first example shows how to import a file object rather than just a file. The second example shows how to upload a file that has a dictionary of extra arguments that consists of a meta data dictionary. This meta data dictionary is what actually keeps track of whatever variables you want to save with this data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example 1\n",
    "try:\n",
    "    # Upload a file-like object to pds-tutorial at key pds-text2\n",
    "    with open(\"text.txt\", \"rb\") as f:\n",
    "        client.upload_fileobj(f, \"pds-tutorial\", \"pds-text2\")\n",
    "except:\n",
    "    print \"Unexpected error:\", sys.exc_info()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example 2\n",
    "try:\n",
    "    # Upload tmp.txt to pds-tutorial at key pds-text-args\n",
    "    client.upload_file(\n",
    "        \"text.txt\", \"pds-tutorial\", \"pds-text-args\",\n",
    "        ExtraArgs={\"Metadata\": {\"mykey\": \"myvalue\"}}\n",
    "    )\n",
    "except:\n",
    "    print \"Unexpected error:\", sys.exc_info()[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting Buckets\n",
    "But what if we realize we don't want all these buckets? Now it's time to delete the keys of the buckets we don't want anymore and then delete the buckets. This can be done as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket list starts as: \n",
      "my_df_bucket\n",
      "pds-tutorial\n",
      "pds-tutorial-config-bucket\n",
      "pds-tutorial-create-bucket\n",
      "Deleting key  s3.ObjectSummary(bucket_name='pds-tutorial-create-bucket', key=u'pds-text')  from bucket1\n",
      "Deleting bucket1\n",
      "Deleting key  s3.ObjectSummary(bucket_name='pds-tutorial-config-bucket', key=u'pds-text')  from bucket2\n",
      "Deleting bucket2\n",
      "Bucket list is: \n",
      "my_df_bucket\n",
      "pds-tutorial\n"
     ]
    }
   ],
   "source": [
    "print \"Bucket list starts as: \"\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)\n",
    "\n",
    "bucket1 = s3.Bucket(\"pds-tutorial-create-bucket\")\n",
    "bucket2 = s3.Bucket(\"pds-tutorial-config-bucket\")\n",
    "for key in bucket1.objects.all():\n",
    "    print \"Deleting key \", key, \" from bucket1\"\n",
    "    key.delete()\n",
    "print \"Deleting bucket1\"\n",
    "bucket1.delete()\n",
    "\n",
    "for key in bucket2.objects.all():\n",
    "    print \"Deleting key \", key, \" from bucket2\"\n",
    "    key.delete()\n",
    "print \"Deleting bucket2\"\n",
    "bucket2.delete()\n",
    "\n",
    "print \"Bucket list is: \"\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Head Bucket\n",
    "This is especially helpful in larger projects where the code is going to interact with a single bucket multiple times. In this case, we can just use the head_bucket function on the s3 client instance that we created and set a boolean to false if we get an error. This does, however, require another package import -- botocore. Then, any time we access s3 we can just place an object after doing a simple boolean check. It will then place the object in that same bucket whenever we make that kind of call. It is useful to note that the way we place an object in this case has different syntax. This would be done as shown: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "\n",
    "exists = True\n",
    "try:\n",
    "    client.head_bucket(Bucket='random-bucket')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    # If a client error is thrown, then check that it was a 404 error.\n",
    "    # If it was a 404 error, then the bucket does not exist.\n",
    "    error_code = int(e.response['Error']['Code'])\n",
    "    if error_code == 404:\n",
    "        exists = False\n",
    "        \n",
    "print exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucket Subdivisions\n",
    "This is also likely when we are going to want to have our information subdivided. For example, we can divide into when the data was posted on a website that the Python script is crawling. We could have the key be “year/month/day/time/file_name”. This would be done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    client.upload_file(\"text.txt\", \"pds-tutorial\", \"2016/10/19/2300/pds-text\")\n",
    "except:\n",
    "    print \"Unexpected error:\", sys.exc_info()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucket Info Accessing\n",
    "Then the information would be divided into year folders and further subdivided into month, day and time folders with the file name used as the data’s identifying name. The data can then be downloaded and its contents put into a file like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download object at pds-tutorial with the key pds-text to file-like object\n",
    "with open(\"tmp.txt\", \"wb\") as f:\n",
    "    client.download_fileobj(\"pds-tutorial\", \"2016/10/19/2300/pds-text\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And those are some of the basics of Boto3! But what if we want to include more complex kinds of data rather than just importing file data? Like, what if we want to save formatted data? We can do that, too! Let's make a new bucket and add some more complex data to it. You'll notice that we have a different way of adding this data. We're going to use the s3 object a bit more and demonstrate how to save this data.\n",
    "\n",
    "### Saving Formatted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Given the path to a csv file, creates a pandas DataFrame \n",
    "def format_data(csv_file):\n",
    "    df = pd.DataFrame()\n",
    "    df = df.from_csv(csv_file)\n",
    "    return df\n",
    "\n",
    "# Given bucket_name which is a string and df which is a pandas DataFrame\n",
    "# Saves each column in df as an object (with the key as the column name) in the bucket if a bucket with that name exists. \n",
    "def save_data(bucket_name, df):\n",
    "    exists = True\n",
    "    try:\n",
    "        s3.meta.client.head_bucket(Bucket=bucket_name)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        exists = False\n",
    "    if exists:  \n",
    "        for column in df:\n",
    "            bod = df[column].to_string()\n",
    "            s3.Object(bucket_name, column).put(Body=bod)\n",
    "            \n",
    "s3.create_bucket(Bucket='my_df_bucket')\n",
    "df = format_data(\"tweets.csv\")\n",
    "save_data(\"my_df_bucket\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3 Objects\n",
    "You can notice that here the most notable difference is using s3's Object function which creates and saves an object in bucket_name with the column as the key and then places the body's information in that object. It's important to recognize that the body's information had to be converted to a string, though, because the information saved in an s3 Object has to be a string, byte array or file type object. This does limit how the data is saved, but what's most important is that this does allow us to save DataFrame information somewhere that can then be accessed later. To demonstrate this, let's iterate through the keys and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_at\n",
      "favorite_count\n",
      "retweet_count\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "bucket = s3.Bucket(\"my_df_bucket\")\n",
    "for key in bucket.objects.all():\n",
    "    print(key.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add key metadata after adding a key and object to the bucket using the s3 Object. This is most helpful when tagging information after its been added to s3, but is good to know regardless. It can be done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for key in bucket.objects.all():\n",
    "    key.put(Metadata={'counter_val': str(counter)})\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORS Configuration\n",
    "Using our bucket object, we can also configure the cross-origin resource sharing of the bucket. This is useful for specifying the methods and sources that can be used with the bucket. It is also a very straight-forward bit of code, which helps. It's done as so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'date': 'Fri, 04 Nov 2016 03:28:29 GMT',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': 'VykKBVP+KK74Z6aGTKlBUnAEZpBUk/WZkCMaZUY5p+P+3b3pX588z7MgFsqXYx+4vW3rZdj86Ss=',\n",
       "   'x-amz-request-id': '58546DBAE400FA35'},\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HostId': 'VykKBVP+KK74Z6aGTKlBUnAEZpBUk/WZkCMaZUY5p+P+3b3pX588z7MgFsqXYx+4vW3rZdj86Ss=',\n",
       "  'RequestId': '58546DBAE400FA35',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cors = bucket.Cors()\n",
    "config = {\n",
    "    'CORSRules': [\n",
    "        {\n",
    "            'AllowedMethods': ['GET'],\n",
    "            'AllowedOrigins': ['*']\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "cors.put(CORSConfiguration=config)\n",
    "cors.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "And that's everything that relates boto3 to Amazon's S3 services! For some examples (including some of the examples shown here) and information about other resources boto3 provides for AWS, you can always check out the documentation here: https://media.readthedocs.org/pdf/boto3/latest/boto3.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tutorial]",
   "language": "python",
   "name": "conda-env-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
