{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Tutorial\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "I would like to talk about a special implmentation of neural network in this tutorial where we introduce recursions to it. And I am building a natural language processing model using RNN with the shakespear data we used in our lecture. So here comes the first question..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Would I Bother To Have Recursions In My Neural Network?\n",
    "### When both normal neural net and recursions themselves are already making my head hurts\n",
    "\n",
    "Have you asked yourself this question, what do I do when the patterns in my data changes with time? Or what do I do when my next data points is dependent of the previous one? For a traditional neural network, we assume that all our input data (each row of the input X matrix to be specific) are independent from each other. But that could be a really bad idea sometimes right? Taking our language model building problem for example, the previous word appears in a text must have some influence on the word appears next. So for building that language model, your best bet is to use a recurrent neural network or RNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay, Okay, Tell Me What Is An RNN Then\n",
    "\n",
    "An RNN is a deep learning model which has a simple structure with a built-in feedback loop, allowing it to model sequential data and act as a forecasting engine. You can also think of it as a neural network with some memory abilities. \n",
    "\n",
    "Wait, what do you mean by memories? Does a traditional neural network has memories of what it saw in previous time steps? If you think about this question, normally a neural network's information flow would look like this:\n",
    "\n",
    "\\begin{aligned}  input -> hidden -> output  \\end{aligned}\n",
    "\n",
    "Memory changes this framework. All memory does is that the hidden layer is now decided by both the current input and the hidden layer from previous time step.\n",
    "\n",
    "\\begin{aligned}  (input + prevHidden) -> hidden -> output  \\end{aligned}\n",
    "\n",
    "In a Recurrent Net the output of layer (value of hidden units) is added to the next input, and feed back into the same layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all love pictures so here is what a typical RNN looks like:\n",
    "\n",
    "![title](rnn.jpg)\n",
    "\n",
    "> A recurrent neural network and the unfolding in time of the computation involved in its forward computation.\n",
    "\n",
    "> Source: Nature\n",
    "\n",
    "As shown in the picture above, an RNN stores and feeds the previous hidden layer output to the next data point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me introduce the notations here in more details:\n",
    "\n",
    "- x_t is the input at time step t. \n",
    "- s_t is the hidden unit value at time step t. It’s the “memory” of the network. It captures information about what happened in all the previous time steps. s_t is calculated based on the previous hidden state and the input at the current step: \\begin{aligned}  s_t=f(Ux_t + Ws_{t-1})  \\end{aligned}. The first hidden state is typically initialized to all zeroes.\n",
    "- o_t is the output at step t.  \\begin{aligned}  o_t = \\mathrm{sigmoid}(Vs_t)  \\end{aligned}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a color demonstration of what \"memory\" means for an RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](rnn_color.png)\n",
    "\n",
    "> Using color to show how recurrent neural network memories information from previous input data.\n",
    "\n",
    "> Source: https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first time step, the outputs are only influenced by the input from time step 1. For the second time step, the outputs are influenced by both the input from time step 1 and time step 2. So on and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Know Enough To Build A Natural Language Model Using RNN Now!\n",
    "\n",
    "Given a sequence of words we would like to predict the probability of each word given the previous words. Language Models allow us to measure how likely a sentence is, which is widely used in machine translation.\n",
    "\n",
    "\n",
    "### 1. What Is A Language Model\n",
    "\n",
    "\n",
    "Recall from our homework and course on NLP, a language model allows us to predict the probability of observing the sentence (in a given dataset) as:\n",
    "\n",
    "\\begin{aligned}  P(w_1,...,w_m) = \\prod_{i=1}^{m} P(w_i \\mid w_1,..., w_{i-1})  \\end{aligned}\n",
    "\n",
    "The possibility of a sentence is given by the word that comes before it. For example, the probability of \"I love data science homework\" would be the probability of \"homework\" given \"I love data science\", multiplied by the probability of “science” given “I love data”, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Introduce RNN To A Language Model\n",
    "\n",
    "In Language Modeling our input is typically a sequence of words (encoded as one-hot vectors for example), and our output is the sequence of predicted words. When training the network we set\n",
    "\\begin{aligned}  o_t = x_{t+1}   \\end{aligned} \n",
    "since we want the output at step t to be the actual next word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How Does RNN Learn?\n",
    "\n",
    "The basic learning procedure is similar as a normal neural network. We do forward propogation and then backpropogation. The slight difference is that we do forward propogation from 1 to 4 and then backpropagating all the derivatives from 4 back to 1. So we are using the same weights (U, V, W) for these 4 steps. This means that we are performing the same operation at each step, just with different inputs. Other than that, it's just normal backpropagation. We will get to that in more details later. \n",
    "\n",
    "So hopefully you get the idea that under the fancy name, Recurrent Neural Network is nothing but a neural network which takes multiple inputs at each time step, propagates them to the hidden layer and then generates output based on all the information the hidden layer chooses to remember."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Do Some Fun Preprocessing For Our Data First.\n",
    "\n",
    "### 1. Text Tokenizing\n",
    "\n",
    "That's exactly what we did for our homework 3. We tokenize our input text by making them all to lower case and remove punctuations. As we are not removing punctuations this time, we don't need to write a process function ourselves, simple use the nltk package to do the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get Rare Words\n",
    "\n",
    "Recall from our homework 3 again, really rare words are either typos or not important. So we would like to exclude them from our text. In this special case, as we would like to keep our vocabulary size (the words that we care about modeling) relatively small, we delete words that appeared equal or less than 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rare_words(processed_text):\n",
    "    all_words = [pp for p in processed_text for pp in p]\n",
    "    word_dict = dict(Counter(all_words))\n",
    "    rare = [k for k, v in word_dict.iteritems() if v <= 5]\n",
    "    return sorted(rare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepend Special START And END Tokens\n",
    "\n",
    "We also want to learn which words tend start and end a sentence. To do this we prepend a special SENTENCE_START token, and append a special SENTENCE_END token to each sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Time To Read Our Shakespeare Data In And Build The Training Data Matrices!\n",
    "\n",
    "#### (1) Read Data\n",
    "First let's read in our Shakespeare data that is already downloaded into a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in local txt file containing all Shakespeare work\n",
    "f = open(\"shakespear.txt\", \"r\" )\n",
    "sentence = []\n",
    "for line in f:\n",
    "    new_line = \"SENTENCE_START\" + \" \" + line.strip().lower() + \" \" + \"SENTENCE_END\"\n",
    "    sentence.append(new_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the sentences we have read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence\n",
      "SENTENCE_START his tender heir might bear his memory: SENTENCE_END\n",
      "SENTENCE_START but thou contracted to thine own bright eyes, SENTENCE_END\n",
      "SENTENCE_START feed'st thy light's flame with self-substantial fuel, SENTENCE_END\n",
      "SENTENCE_START making a famine where abundance lies, SENTENCE_END\n",
      "SENTENCE_START thy self thy foe, to thy sweet self too cruel: SENTENCE_END\n",
      "--------------------------------------------\n",
      "Cheers! We parsed 124192 sentences in total!\n"
     ]
    }
   ],
   "source": [
    "print \"Example sentence\"\n",
    "for i in range(10, 15):\n",
    "    print sentence[i]\n",
    "print \"--------------------------------------------\"\n",
    "print \"Cheers! We parsed %d sentences in total!\" % (len(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Parse Sentence Into Words, And Tokenize The Words\n",
    "\n",
    "Now we would like to parse each sentence into separate words and at the same time, tokenize them using the nltk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence after tokenizing: \n",
      "['SENTENCE_START', 'the', 'sonnets', 'SENTENCE_END']\n",
      "['SENTENCE_START', 'SENTENCE_END']\n",
      "['SENTENCE_START', 'by', 'william', 'shakespeare', 'SENTENCE_END']\n",
      "['SENTENCE_START', 'SENTENCE_END']\n",
      "['SENTENCE_START', 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sent = [nltk.word_tokenize(sent) for sent in sentence]\n",
    "print \"Example sentence after tokenizing: \"\n",
    "for i in range(5):\n",
    "    print tokenized_sent[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Remove Rare Words From Our Vocabulary\n",
    "\n",
    "Now it's time to find the rare words and replace them from the text data with a unknown token symbol. The word UNKNOWN_TOKEN will become part of our vocabulary and we will predict it just like any other word. When we generate new text, we will replace the UNKNOWN_TOKEN with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21384 rare words.\n"
     ]
    }
   ],
   "source": [
    "# Find the rare words, ...\n",
    "rare = get_rare_words(tokenized_sent)\n",
    "print \"Found %d rare words.\" % len(rare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the rare words from our vocabulary list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7678 unique words left. This is our vocabulary size.\n"
     ]
    }
   ],
   "source": [
    "# And Remove them forever from our vocabulary...\n",
    "new_word = [b for a in tokenized_sent for b in a]\n",
    "unique_word = set(new_word)\n",
    "vocab = [a for a in unique_word if a not in rare]\n",
    "# Move \"SENTENCE_START\" to the 0th position and \"SENTENCE_END\" to the 1st position\n",
    "vocab.insert(0, vocab.pop(vocab.index('SENTENCE_START')))\n",
    "vocab.insert(1, vocab.pop(vocab.index('SENTENCE_END')))\n",
    "vocab_size = len(vocab)\n",
    "print \"%d unique words left. This is our vocabulary size.\" % vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a dictionary that maps from our unique words in our vocabulary to their indices. And a dictionary that maps from indices to the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_to_index = dict([(w,i) for i, w in enumerate(vocab)])\n",
    "word_to_index[\"UNKNOWN_TOKEN\"] = len(word_to_index) - 1\n",
    "\n",
    "index_to_word = dict((i, w) for i, w in enumerate(vocab))\n",
    "index_to_word[\"UNKNOWN_TOKEN\"] = len(word_to_index) - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And replace the rare words by 'UNKNOWN_TOKEN' in our word list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence after replacing rare words with unknown token: \n",
      "['SENTENCE_START', 'ten', 'times', 'thy', 'self', 'were', 'happier', 'than', 'thou', 'art', ',', 'SENTENCE_END']\n",
      "['SENTENCE_START', 'if', 'ten', 'of', 'thine', 'ten', 'times', 'UNKNOWN_TOKEN', 'thee', ':', 'SENTENCE_END']\n",
      "['SENTENCE_START', 'then', 'what', 'could', 'death', 'do', 'if', 'thou', 'shouldst', 'depart', ',', 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "# And replace them with the unknown token in the training example\n",
    "for i, sent in enumerate(tokenized_sent):\n",
    "    tokenized_sent[i] = [w if w in word_to_index else \"UNKNOWN_TOKEN\" for w in sent]\n",
    "print \"Example sentence after replacing rare words with unknown token: \"\n",
    "for i in range(100, 103):\n",
    "    print tokenized_sent[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Map Input To Indices\n",
    "Now the last step for input processing before we feed them to our recurrent neural network is to map our input words to indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sent])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a sample training example that we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x example:\n",
      "SENTENCE_START ten times thy self were happier than thou art ,\n",
      "[0, 3618, 840, 2396, 308, 3798, 1727, 6383, 4114, 7503, 6708]\n",
      "\n",
      "y example:\n",
      "ten times thy self were happier than thou art , SENTENCE_END\n",
      "[3618, 840, 2396, 308, 3798, 1727, 6383, 4114, 7503, 6708, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print an training data example\n",
    "x_example, y_example = X_train[100], y_train[100]\n",
    "print \"x example:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example)\n",
    "print \"\\ny example:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here Comes the Recurrent Neural Network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Infrastructure Again\n",
    "\n",
    "Recall the infrastructure of a RNN we saw before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](rnn.jpg)\n",
    "\n",
    "> A recurrent neural network and the unfolding in time of the computation involved in its forward computation. \n",
    "\n",
    "> Source: Nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only three weight matrices, U, V, and W. U and V maps from input to hidden layer and from hidden layer to output respectively. The new matrix W propagates from the hidden layer to the hidden layer at the next timestep.\n",
    "\n",
    "Let's get concrete and see what the RNN for our language model looks like. \n",
    "\n",
    "#### Input\n",
    "\n",
    "The input $x$ will be a sequence of words (just like the example printed above) and each $x_t$ would be a single word. Note here we are using a one-hot vector of size vocabulary. \n",
    "\n",
    "So, each $x_t$ will become a vector, and $x$ will be a matrix, with each row representing a word. \n",
    "\n",
    "#### Output\n",
    "\n",
    "The output of our network $o$ has a similar format as the input. Each $o_t$ is a vector as the same size as our vocabulary, and each element represents the probability of that word being the next word in the sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How To Train AN RNN\n",
    "\n",
    "Training a RNN is similar to training a traditional Neural Network. We also use the backpropagation algorithm, but with some changes. We can't simply calculate the gradient because the gradient depends not only on the current time step but also the previous time steps. In order to calculate the gradient at t=4 we would need to backpropagate 3 steps and sum up the gradients. This is called Backpropagation Through Time (BPTT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to deal with that later. Now let's first initialize some parameters for our RNN class. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialization\n",
    "\n",
    "For the weights, we initialize them to be in the interval from $\\left[-\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}}\\right]$ where $n$ is the number of incoming connections from the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, word_dim, hidden_dim = 100, bptt_truncate = 4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim # the input and output size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        np.random.seed(0)\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim)) \n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim)) \n",
    "        # Initialize the gradient to be all 0\n",
    "        self.V_grad = np.zeros(self.V.shape) \n",
    "        self.U_grad = np.zeros(self.U.shape) \n",
    "        self.W_grad = np.zeros(self.W.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define the sigmoid function here. It introduces nonlinearity to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Forward Propagation\n",
    "\n",
    "Now it's time for forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(self, X):\n",
    "    # The total number of time steps\n",
    "    t = len(X)\n",
    "    # Create a ndarray to store all the hidden states from the previous input. We have to use it later.\n",
    "    hidden_prev = np.zeros((t + 1, self.hidden_dim))\n",
    "    hidden_prev[-1] = np.zeros(self.hidden_dim)\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((t, self.word_dim))\n",
    "    # For each time step...\n",
    "    for time_step in np.arange(t):\n",
    "        # Hidden_layer = sigmoid(input + prvious_hidden_layer)\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        hidden_prev[time_step] = np.tanh(self.U[:, X[time_step]] + self.W.dot(hidden_prev[time_step - 1]))\n",
    "        # Output_layer = sigmoid(hidden_layer)\n",
    "        o[time_step] = sigmoid(self.V.dot(hidden_prev[time_step]))\n",
    "    return o, hidden_prev\n",
    "\n",
    "RNN.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure you understand the line of code to calculate the hidden_prev[t]. That's all the magic about RNN. We first propagate from the input to the hidden layer (self.U[:, X[t]]). Then, we propagate from the previous hidden layer to the current hidden layer (self.W.dot(hidden_prev[t - 1])). Then we sum these 2 vectors and pass the result to the sigmoid function to generate our hidden layer value. That's where our model actually \"memorize\" things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Making Predictions And Calculating The Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After forward propagation, we get out output. Now it's time to calculate the difference between our output and the true y. This is the loss we want to minimize when we train our model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, _ = self.forward_propagation(X)\n",
    "    return np.argmax(o, axis = 1)\n",
    "RNN.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction for each input X is a vector of 14714 possibilities which represent the possibilities of next word in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(self, X, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, hidden_prev = self.forward_propagation(X[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "RNN.loss = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check Our Forward Propagation Before Moving On\n",
    "\n",
    "We will print our prediction using one single input (one sentence in X_train) to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 7678)\n",
      "(11,)\n",
      "[5246 3219 5132 4138 2514 3691  602 3661  306  517 7234]\n",
      "[3618, 840, 2396, 308, 3798, 1727, 6383, 4114, 7503, 6708, 1]\n",
      "woodville infected ornaments befall'n friend exhibition cinna inc. waft guilt broker\n",
      "ten times thy self were happier than thou art , SENTENCE_END\n"
     ]
    }
   ],
   "source": [
    "model = RNN(vocab_size)\n",
    "o, s = model.forward_propagation(X_train[100])\n",
    "print o.shape\n",
    "pred = model.predict(X_train[100])\n",
    "print pred.shape\n",
    "print pred\n",
    "print y_train[100]\n",
    "print \" \".join([index_to_word[x] for x in pred])\n",
    "print \" \".join([index_to_word[x] for x in y_train[100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that our prediction is totally random. That's what we expected because the parameters are randomly initialized. Seems that our forward propagation method is okay. Let's move on to BPTT!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. BackPropagation Through Time (BPTT)\n",
    "\n",
    "Calculating the derivatives and gradient for each weight, so we can use it to do gradient descent in our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bptt(self, X, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    delta_o, s = self.forward_propagation(X)\n",
    "    # We accumulate the gradients in these variables\n",
    "    V_grad = self.V_grad\n",
    "    U_grad = self.U_grad\n",
    "    W_grad = self.W_grad\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        V_grad += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t - self.bptt_truncate), t + 1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            W_grad += np.outer(delta_t, s[bptt_step-1])              \n",
    "            U_grad[:,X[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return U_grad, V_grad, W_grad\n",
    "RNN.bptt = bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6. Updating The Weights By Their Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def gd(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    U_grad, V_grad, W_grad = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * U_grad\n",
    "    self.V -= learning_rate * V_grad\n",
    "    self.W -= learning_rate * W_grad\n",
    "RNN.gd = gd\n",
    "\n",
    "def train(model, X_train, y_train, learning_rate = 1e-4, iteration = 100):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    N = np.sum((len(y) for y in y_train))\n",
    "    for i in range(iteration):\n",
    "        # Output the loss every 5 iterations\n",
    "        if (i % 5 == 0 and i > 0):\n",
    "            total_loss = model.loss(X_train, y_train)\n",
    "            loss = total_loss / float(N)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            print \"Average loss after num_examples_seen = %d i = %d: %f\" % (num_examples_seen, i, loss)\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) >= 2 and losses[-1][1] >= losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5 \n",
    "                print \"Setting learning rate to %f\" % learning_rate\n",
    "        # For each training example...\n",
    "        for j in range(len(y_train)):\n",
    "            # One Gradient Descent step\n",
    "            model.gd(X_train[j], y_train[j], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to run the gradient descent on one single input and see if our implementation is efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 40.3 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit model.gd(X_train[100], y_train[100], 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, it takes about 3 milliseconds to run a single step of gradien descent. Seems that we can't process our whole dataset which has hundreds of thousands of rows. Let's just run with the first 100 input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after num_examples_seen = 500 i = 5: 72.666206\n",
      "Loss after num_examples_seen = 1000 i = 10: 110.926938\n",
      "Setting learning rate to 0.000050\n",
      "Loss after num_examples_seen = 1500 i = 15: 118.704145\n",
      "Setting learning rate to 0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tina/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after num_examples_seen = 2000 i = 20: 119.315965\n",
      "Setting learning rate to 0.000013\n",
      "Loss after num_examples_seen = 2500 i = 25: 115.688422\n",
      "Loss after num_examples_seen = 3000 i = 30: 110.990778\n",
      "Loss after num_examples_seen = 3500 i = 35: 106.110767\n",
      "Loss after num_examples_seen = 4000 i = 40: 104.444395\n",
      "Loss after num_examples_seen = 4500 i = 45: 100.539793\n",
      "Loss after num_examples_seen = 5000 i = 50: 96.820646\n",
      "Loss after num_examples_seen = 5500 i = 55: 93.683399\n",
      "Loss after num_examples_seen = 6000 i = 60: 90.088033\n",
      "Loss after num_examples_seen = 6500 i = 65: 89.402487\n",
      "Loss after num_examples_seen = 7000 i = 70: 89.262406\n",
      "Loss after num_examples_seen = 7500 i = 75: 83.562442\n",
      "Loss after num_examples_seen = 8000 i = 80: 84.422838\n",
      "Setting learning rate to 0.000006\n",
      "Loss after num_examples_seen = 8500 i = 85: 78.061657\n",
      "Loss after num_examples_seen = 9000 i = 90: 74.321762\n",
      "Loss after num_examples_seen = 9500 i = 95: 71.959543\n"
     ]
    }
   ],
   "source": [
    "model = RNN(vocab_size)\n",
    "losses = train(model, X_train[:100], y_train[:100], iteration = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Solutions\n",
    "\n",
    "Sorry this is not a perfect ending. Our model starts to converge when learning rate is around 1e-5 though.\n",
    "\n",
    "In the research field, when it comes to training a recurrent net, GPUs are an obvious choice over an ordinary CPU as it is able to train the nets 250 times faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "\n",
    "https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
