{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='orange'>Word2Vec Introduction </font>\n",
    "\n",
    "The Natural Languahe Processing for data science that was introduced in class discusses the ideas of topics as Bag of words and N-grams with a key note by professor mentioning “this lecture may be subject to change in the upcoming years, as massive improvements in “off-the-shelf” language understanding are ongoing.” This tutorial will introduce you to one such deep learning method known as Word2Vec which aims to learn the meaning of the words rather than relying on heuristic approaches.\n",
    "\n",
    "Limiting the scope of the tutorial and not going into deeper mathematics, word2vec can be explained as a model where each word is represented as a vector in N dimensional space. During the initialization stage, the vocabulary of corpus can be randomly initialized as vectors with some values. Then, using vector addition, if we add tiny bit of one vector to another, the vector moves closer to each other and similarly subtraction results in moving further away from one another. So while training a model we can pull the vector closer to words that it co occurs with within a specified window and away from all the other words. So in the end of the training words that are similar in meaning end up clustering nearby each other and many more patterns arises automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Technique</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec uses a technique called <b>skip-gram with negative sampling</b> to map the semantic space of a text corpus. I have tried to explain this using a passage taken from dataset used in our tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“This system is sometimes known as a presidential system because the government is answerable<font color='green'> <i>solely and exclusively to a </i></font><font color='red'><i><b> 'presiding' </b></i></font><font color='green'><i>activist head of state, and</i></font> is selected by and on occasion dismissed by the head of state without reference to the legislature.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1</b>  \n",
    "Let’s take a word from above passage as target and number of words occurring close to the target as context (five words on either side of target).  \n",
    "<i>Target</i>  = presiding  \n",
    "<i>Context</i> = solely and exclusively to a; activist head of state, and\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2</b>  \n",
    "Each of the word in the above paragragh should be represented as a vector in n dimensional space. In the beginning, these vector values can be randomly initialized.  \n",
    "Note: The dimension can be decided at the time of model creation, given as one of the parameters of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3</b>  \n",
    "Our goal is to get the target and context closer to each other in vector space. This can be done by taking the target and context vectors and pulling them together by a small amount by maximizing the log transformation of the dot product of the target and the context vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 4</b>  \n",
    "Another motive is to move target vector away from words that are not in it’s context. To achieve this, words are randomly sampled from rest of the corpus and they are pushed away from target vector by minimizing the log transformation of the dot product of the target and the non context sample words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above four steps is repeated for each target word. In the end the vectors that are often used together will be pulled towards each other and vectors for words that are rarely used together will be pushed apart. \n",
    "\n",
    "In our example if notion of ‘presiding’ resembles ‘activist head’ often in corpus then vectors for these two concepts will be close to one another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Data Collection</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used [ClueWeb datasets](http://lemurproject.org/clueweb12/) which consists of around (50 million Wikipedia documents) and indexed them using [Apache Lucene](https://lucene.apache.org/core/). After Indexing, I used Lucene Search Engine to extract the top 1000 documents for query \"president united states\". These documents are then stored in a local file \"wikidocs\"  \n",
    "I am using these 1000 documents to build the word2vec model to find the interested relationships between words.\n",
    "Please note that the entire process of indexing and retrieving the documents is beyond the scope of the tutorial.\n",
    "\n",
    "The file wikidocs can be downloaded from https://www.dropbox.com/s/rnu33c4j6ywnu6z/wikidocs?dl=0. Make sure to save the file in same directory as the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lenght of corpus is 80120356.\n"
     ]
    }
   ],
   "source": [
    "#Load the documents extracted from wikipedia corpus\n",
    "with open(\"wikidocs\") as f:\n",
    "    html = f.read()\n",
    "    html = unicode(html, errors='replace')\n",
    "    \n",
    "#Printing the lenght of the corpus\n",
    "print(\"The lenght of corpus is \"+str(len(html))+\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Install Libraries</font>\n",
    "The data collected is in the form of xml which needs to be cleaned to plain text format. I am using BeautifulSoup libraries to parse the wikipedia documents and nltk tokenizer to convert them to tokens.  \n",
    "Please note that these libraries are already been introduced in class and used in homeworks. Hence, running the below command should work for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vallarimehta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <font color='blue'> Parsing </font>  \n",
    " \n",
    "<b>Convert the docs into list of sentences</b>  \n",
    "Word2Vec toolkit in python expects input in the form of a list of sentences, each of which is a list of words.   \n",
    "\n",
    "\n",
    "<b>Remove punctuation and lowercase all words</b>  \n",
    "The words are converted into lowecase and punctuations are removed using regular expressions.\n",
    "\n",
    "<b>Stopwords</b>  \n",
    "Removal of stopwords is optional. It is better to not remove stopwords for word2vec algorithm for the first train as  the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors. However, if results are not satisfactory, the model can be trained again by removing stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to convert a document to a sequence of words,\n",
    "# optionally removing stop words.  \n",
    "# Returns a list of words.\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review,\"lxml\").get_text()\n",
    "    review_text = review_text.encode('utf-8')\n",
    "\n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower()\n",
    "    words = words.split()\n",
    "\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = stopwords.words(\"english\")\n",
    "        stops = set(stops)\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to split a document into parsed sentences using NLTK tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to split a review into parsed sentences. Returns a\n",
    "# list of sentences, where each sentence is a list of words\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False):\n",
    "\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply these functions to our Wikipedia Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 97832 sentences in our corpus.\n"
     ]
    }
   ],
   "source": [
    "#Parse documents to create list of sentences\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "sentences = review_to_sentences(html, tokenizer)\n",
    "\n",
    "print(\"There are \" + str(len(sentences)) + \" sentences in our corpus.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe what a sentence looks like after parsing it through above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'presidential',\n",
       " 'systems',\n",
       " 'the',\n",
       " 'head',\n",
       " 'of',\n",
       " 'state',\n",
       " 'often',\n",
       " 'has',\n",
       " 'power',\n",
       " 'to',\n",
       " 'veto',\n",
       " 'a',\n",
       " 'bill']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'>Generating Word2Vec Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Installing the libraries</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install gensim using pip:\n",
    "\n",
    "       $ pip install --upgrade gensim\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this fail, make sure you’re installing into a writeable location (or use sudo), and have following dependencies.  \n",
    "\n",
    "       Python >= 2.6\n",
    "       NumPy >= 1.3  \n",
    "       SciPy >= 0.7  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use conda package to install gensim, which takes care of all the dependencies.   \n",
    "        \n",
    "       $ conda install -c anaconda gensim=0.12.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Load libraries for word2vec</font>\n",
    "After you run all the installs, make sure the following commands work for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Training the Model </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "Import the built-in logging module and configure it so that Word2Vec creates nice output messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using built in logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec model requires some parameters for initialization. \n",
    "\n",
    "<b>size</b>  \n",
    "Size is number of dimensions you want for word vectors. If you have an idea about how many topics the corpus cover, you can use that as size here. For wikipedia documents I use around 50-100. Usually, you will need to experiment with this value and pick the one which gives you best result.\n",
    "\n",
    "<b>min_count</b>  \n",
    "Terms that occur less than min_count are ignored in calculations. This reduce noise in the vector space. I have used 10 for my experiment. Usually for bigger corpus size, you can experiment with higher values.\n",
    "\n",
    "<b>window</b>    \n",
    "The maximum distance between the current and predicted word within a sentence. This is explained in the technique section of the tutorial.\n",
    "\n",
    "<b>downsampling</b>    \n",
    "Threshold for configuring which higher-frequency words are randomly downsampled. Useful range is (0, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and train the model\n",
    "Train the model using the above parameters. This might take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don’t plan to train the model any further, calling init_sims will be better for memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Storing & Loading Models</font>  \n",
    "It can be helpful to create a meaningful model name and save the model for later use. \n",
    "You can load it later using Word2Vec.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#You can save the model using meaningful name\n",
    "model_name = \"wiki_100features_15word_count\"\n",
    "model.save(model_name)\n",
    "#Loading the saved model\n",
    "word2vec_model = gensim.models.Word2Vec.load(\"wiki_100features_15word_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Investigate the vocabulary</font>\n",
    "You can either use model.index2word which gives list of all the terms in vocabulary. Or model.vocab.keys() which gives keys of all the terms used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of vocabulary = 11674\n",
      "['congress', 'from', 'vice', 'that', 'var', 'was', 'this', 'have', 'with', 'office', 'which', 'any', 'senate', 'u', 'house', 'all', 'not', 'at', 'presidential', 'constitution', 'no', 'may', 'other', 'such', 'are', 'one', 'new', 'government', 'law', 'if', 'it', 'he', 'american', 'two', 'an', 'article', 'but', 'section', 'http', 'their', 'federal', 'd', 'each', 'us', 'who', 'amendment', 'his', 'representatives', 'executive']\n"
     ]
    }
   ],
   "source": [
    "#List of all the vcabulary terms\n",
    "vocab = model.index2word\n",
    "print \"Lenght of vocabulary =\",len(vocab)\n",
    "print vocab[20:69]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the word ‘obama’ exists in the vocabulary:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'obama' in model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the word 'beyonce’ exists in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'beyonce' in model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector representation of word ‘obama’ looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.86289869e-03,   1.27600618e-02,  -3.78985070e-02,\n",
       "         5.57042398e-02,  -8.71062372e-03,  -1.24862291e-01,\n",
       "        -8.76149163e-02,   1.56393647e-02,  -1.55269457e-02,\n",
       "         2.65169926e-02,  -8.65512937e-02,  -4.16206196e-02,\n",
       "        -8.79949480e-02,   7.72259906e-02,  -3.61485332e-02,\n",
       "         2.33706519e-01,  -2.25558355e-01,  -1.25489861e-01,\n",
       "        -2.32529882e-02,  -1.51585296e-01,  -1.04060665e-01,\n",
       "         9.67445523e-02,   4.27967943e-02,  -8.07592198e-02,\n",
       "         6.51278393e-03,   1.35209421e-02,  -8.75448212e-02,\n",
       "         8.17318112e-02,  -2.99313646e-02,  -2.53677946e-02,\n",
       "         3.37144290e-03,   7.65991956e-02,   2.20242664e-02,\n",
       "         6.49797618e-02,  -3.52603421e-02,  -6.62654489e-02,\n",
       "        -5.77722378e-02,  -6.54017627e-02,   2.97236126e-02,\n",
       "         5.49717434e-02,   3.95357832e-02,   7.72382468e-02,\n",
       "        -6.72882199e-02,  -4.48467880e-02,  -1.31184086e-01,\n",
       "        -8.92265216e-02,  -4.73204330e-02,  -3.11650950e-02,\n",
       "         1.50380395e-02,  -5.61095448e-03,   3.78648154e-02,\n",
       "         2.51847561e-02,   1.04024738e-01,   7.93596059e-02,\n",
       "        -8.05746615e-02,  -6.71499074e-02,   3.47965322e-02,\n",
       "        -1.08725084e-02,  -3.09646614e-02,  -8.58426467e-02,\n",
       "        -2.29166672e-02,   9.07798260e-02,   1.69785377e-02,\n",
       "         2.06001997e-01,   3.80690247e-02,  -8.56751278e-02,\n",
       "         5.71228638e-02,  -9.14165657e-03,   7.38771409e-02,\n",
       "        -6.80292025e-02,  -6.68878807e-03,  -1.24391522e-02,\n",
       "         3.53629328e-02,   2.28197034e-03,  -9.36675593e-02,\n",
       "        -1.21364389e-02,  -1.49688581e-02,  -1.14046142e-01,\n",
       "         6.38680458e-02,   2.98399515e-02,   8.60089138e-02,\n",
       "         9.60168988e-03,  -5.94212711e-02,   7.83034191e-02,\n",
       "         6.02749502e-03,  -4.97367419e-03,  -1.44316256e-02,\n",
       "        -1.55162450e-03,   1.00480229e-01,   5.80941513e-02,\n",
       "         2.58152764e-02,   6.22408465e-02,  -2.29691155e-02,\n",
       "         4.49000560e-02,  -5.11429906e-02,  -7.11087510e-02,\n",
       "        -1.13655068e-01,  -2.49901302e-02,  -2.28746846e-01,\n",
       "        -9.17477440e-03,   1.82117801e-02,  -5.67380898e-02,\n",
       "        -2.10908314e-04,   7.13647008e-02,  -7.45819956e-02,\n",
       "         1.65400673e-02,  -2.68318933e-02,  -5.70384152e-02,\n",
       "         1.32144630e-01,  -6.55482262e-02,   9.10492018e-02,\n",
       "         5.06989248e-02,  -1.34692146e-02,  -1.24212867e-03,\n",
       "        -1.63489744e-01,  -1.16806827e-01,   8.32927134e-03,\n",
       "        -4.60031182e-02,  -7.34247267e-03,   7.20574260e-02,\n",
       "        -2.63339579e-02,  -8.21589306e-02,   1.77287422e-02,\n",
       "         5.11046462e-02,   8.48096833e-02,  -3.19263451e-02,\n",
       "         9.94604602e-02,  -6.45196885e-02,   6.52314455e-04,\n",
       "         7.93120079e-03,  -4.26100846e-03,  -5.70470244e-02,\n",
       "         1.06657319e-01,  -5.19002713e-02,  -4.13851514e-02,\n",
       "         1.19595425e-02,  -1.50211707e-01,  -2.70022228e-02,\n",
       "        -7.14909062e-02,   4.23905551e-02,   1.05953135e-01,\n",
       "         7.00194463e-02,   7.84012452e-02,  -6.00336306e-02,\n",
       "        -5.03269210e-02,  -5.09397406e-03,   7.92037547e-02,\n",
       "         3.64874192e-02,   6.69582784e-02,   1.03715554e-01,\n",
       "        -4.35220115e-02,   2.54439414e-02,  -4.49040420e-02,\n",
       "        -1.21933654e-01,  -1.02371804e-01,  -1.15102800e-02,\n",
       "         8.33393447e-03,  -6.06687218e-02,   7.18082115e-02,\n",
       "         2.09439155e-02,  -1.72106288e-02,  -5.84707186e-02,\n",
       "        -2.83851307e-02,  -7.27276504e-02,  -7.41089657e-02,\n",
       "         8.82669091e-02,  -1.78150445e-01,  -1.38865104e-02,\n",
       "        -3.80072221e-02,  -8.31063278e-03,  -3.59355658e-02,\n",
       "        -5.53887784e-02,   9.54202861e-02,  -1.87227279e-02,\n",
       "        -8.53670109e-03,  -3.95045690e-02,  -3.48856002e-02,\n",
       "         1.02246478e-01,  -1.17959268e-01,  -4.56004106e-02,\n",
       "         4.23388742e-03,  -8.54675937e-03,   4.78542484e-02,\n",
       "        -5.35403751e-02,  -8.31410848e-03,   7.51999067e-03,\n",
       "        -8.22488666e-02,   1.12133622e-01,   3.47932018e-02,\n",
       "         1.71155054e-02,   1.42993936e-02,  -1.64073333e-01,\n",
       "         1.63139533e-02,  -4.39352915e-03,  -1.72256306e-03,\n",
       "         1.72341410e-02,  -7.70747438e-02,   3.97111941e-03,\n",
       "        -3.63414139e-02,  -1.32490583e-02], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['obama']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the words similar to \"obama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barack', 0.8767746686935425),\n",
       " ('michelle', 0.6473720669746399),\n",
       " ('biden', 0.5824570059776306),\n",
       " ('joe', 0.4867965579032898),\n",
       " ('palin', 0.46410897374153137),\n",
       " ('speaks', 0.45847803354263306),\n",
       " ('barak', 0.45301488041877747),\n",
       " ('gaza', 0.4382231831550598),\n",
       " ('statesbarack', 0.43616873025894165),\n",
       " ('mccain', 0.4352244436740875)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('obama',  topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Phrases</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use gensim models.phrases in order to detect common phrases from sentences. For example two single words \"new\" and \"york\" can be combined as one word \"new york\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generte the model using above bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_model = Word2Vec(bigram[sentences], workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's access the new vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of new vocabulary = 16802\n",
      "[u'eligible', u'electricity', u'senator_lee', u'performance_it', u'gun_violence', u'bedford_jun', u'sprawling_retreat', u'buck', u'lord', u'thomasville', u'have_originated', u'banzhaf_power', u'congress_assembled', u'deliberation', u'i_haven', u'american_liberalism', u'discuss_economic', u'regional', u'fourteenth_amendment', u'dell', u'sloven_ina', u'appropriation', u'domestic_vs', u'scrolltop', u'additional_citations', u'colfax', u'www_masterliness', u'bringing', u'his_career', u'america_telephone', u'four', u'popular_music', u'prize', u'wooden', u'including_debts', u'wednesday', u'jihad', u'cultural_influence', u'succession', u'excluding_indians']\n"
     ]
    }
   ],
   "source": [
    "vocab = new_model.vocab.keys()\n",
    "vocab = list(vocab)\n",
    "print \"Lenght of new vocabulary =\",len(vocab)\n",
    "print vocab[15:55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the word 'dominican republic’ exists in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dominican_republic' in new_model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s assess the relationship of words in our semantic vector space. For example, which words are most similar to the word ‘republic’?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ireland', 0.7163577079772949),\n",
       " (u'spain', 0.6763077974319458),\n",
       " (u'france', 0.6528872847557068),\n",
       " (u'central', 0.6357733011245728),\n",
       " (u'china', 0.6244645118713379),\n",
       " (u'parliament', 0.6186147332191467),\n",
       " (u'officially', 0.594301164150238),\n",
       " (u'quebec', 0.5794820189476013),\n",
       " (u'china_taiwan', 0.5794057846069336),\n",
       " (u'korea', 0.5775099992752075)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.most_similar('republic',  topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the phrase \"dominican republic\"?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ghana', 0.9546933174133301),\n",
       " (u'guinea', 0.9521167278289795),\n",
       " (u'morocco', 0.9403805732727051),\n",
       " (u'papua_new', 0.9389972686767578),\n",
       " (u'namibia', 0.9385524392127991),\n",
       " (u'paraguay_peru', 0.9382070302963257),\n",
       " (u'suriname', 0.9381791353225708),\n",
       " (u'luxembourg', 0.9376348257064819),\n",
       " (u'guyana_haiti', 0.9367575645446777),\n",
       " (u'costa_rica', 0.9362567067146301)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.most_similar('dominican_republic',  topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the results differ if we exclude the relationship between republic and dominican_republic?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 0.607554018497467),\n",
       " (u'delegated', 0.556876540184021),\n",
       " (u'government', 0.5254863500595093),\n",
       " (u'parliament', 0.5150182247161865),\n",
       " (u'parliamentary', 0.5106503367424011),\n",
       " (u'factions', 0.5051050782203674),\n",
       " (u'of', 0.49954289197921753),\n",
       " (u'collective_head', 0.48483002185821533),\n",
       " (u'is', 0.47944697737693787),\n",
       " (u'concurring', 0.4781777858734131)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.most_similar(positive=['republic'], negative=['dominican_republic'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Query Expansion</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the applications of word embedding is that it can be used in search engine in order to expand the query terms in order to produce better results.\n",
    "\n",
    "If you recall previously, the wikipedia documents are extracted from Lucene Search using the query \"president united states\". Now, let's use these three query terms to obtain expanded terms closest to query.\n",
    "\n",
    "Note: This idea is taken from the paper: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/acl2016.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function take each term in vocabulary and compare it to each term of query in terms of similarity score. The similarity scores are added for all terms and top k terms are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to expand a query term\n",
    "\n",
    "def expand_term(query,k):\n",
    "    \n",
    "    #Get the vocab of model\n",
    "    vocab = new_model.index2word\n",
    "    vocab = set(vocab)\n",
    "    term_score = {}\n",
    "\n",
    "    #Split the query terms\n",
    "    query_list = query.split()\n",
    "    \n",
    "    #Convert the query to lower case\n",
    "    query_list = [element.lower() for element in query_list]\n",
    "    \n",
    "    \n",
    "    #Remove stop words from query\n",
    "    stops = stopwords.words(\"english\")\n",
    "    stops = set(stops)\n",
    "    query = [word for word in query_list if word not in stops] \n",
    "    \n",
    "\n",
    "    #Filter the vocab to remove stopwords\n",
    "    filter_vocab = [word for word in vocab if word not in stops] \n",
    "    \n",
    "    #Calculate each score for terms in vocab\n",
    "    for term in filter_vocab:\n",
    "        term_score[term] = 0.0\n",
    "        for q in query:\n",
    "            if term in term_score:\n",
    "                term_score[term] += new_model.similarity(q,term)\n",
    "            else:\n",
    "                term_score[term] = new_model.similarity(q,term)\n",
    "\n",
    "    #Sort the top k terms of dict term_score\n",
    "    sorted_k_terms = sorted(term_score.iteritems(), key=lambda x:-x[1])[:k]\n",
    "    sorted_k_terms = dict(sorted_k_terms)\n",
    "    \n",
    "    #join the query term\n",
    "    q_term = ' '.join(query)\n",
    "    \n",
    "    #Return the expanded terms of query\n",
    "    return sorted_k_terms.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test our function to check the result for query \"president united states\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'kingdom', u'united', u'army', u'hospital', u'methodist', u'airlines', u'nations', u'states', u'methodist_church', u'health_care', u'way', u'president', u'highest', u'association', u'bank']\n"
     ]
    }
   ],
   "source": [
    "query = \"president united states\"\n",
    "k = 15  #k defines number of expanded terms\n",
    "result = expand_term(query,k)\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try for \"republic constitution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'commonwealth', u'parliament', u'assembly', u'constitution', u'great_britain', u'known_as', u'france_spain', u'republic', u'federal_government', u'ireland', u'constitution_tempered', u'central', u'america', u'officially', u'china']\n"
     ]
    }
   ],
   "source": [
    "query = \"republic constitution\"\n",
    "k = 15  #k defines number of expanded terms\n",
    "result = expand_term(query,k)\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Applications</font>\n",
    "\n",
    "Word2Vec model as described in this tutorial capture semantic and syntactic relationships among words in corpus. Hence, it can be used in search engines for synonyms, query expansion as well as recommendations (for example, recommending similar movies). \n",
    "\n",
    "In our experiments, word embeddings do not seem to provide enough discriminative power between related but distinct concepts. This could be due to smaller corpus size as well as word embeddings are in it's initial stage of development. Hence, there is a huge scope for improvements in the above technique for it to be fully utilized in commercial applications. \n",
    "\n",
    "This being said, word2vec are exteremely interesting and it's lot of fun to explore the relationships amongst different words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Refrences</font>\n",
    "[Google's code, writeup, and the accompanying papers](https://code.google.com/archive/p/word2vec/)   \n",
    "[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)   \n",
    "[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)   \n",
    "[Presentation on word2vec by Tomas Mikolov from Google](https://docs.google.com/file/d/0B7XkCwpI5KDYRWRnd1RzWXQ2TWc/edit)   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
