{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Introduction\n",
    "\n",
    "This tutorial will go through the Apache Spark basic and high level API focusing on Spark streaming. All the codes were run on Windows 7 OS.\n",
    "\n",
    "Apache Spark is an open source fast and general-purpose cluster computing system, it was originally developed at the University of California, Berkeley's AMPLab, later it was open sourced on Apache Software Foundation and maintenaced by Apache till now. Spark provides high level interface for clusters programming with implicit and efficient data parallelism and fault-tolerance. _(To make this tutorial easy to review in Juypter notebook, I ran Spark on a multi core CPU machine in pseudo-distributed local mode, this mode is for development and testing purposes)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is claimed to be 100x faster than Hadoop MapReduce in memory, or 10x faster on disk. After some research, there are three advantages make Spark faster:\n",
    "1. Unlike MapReduce persists each step's results on disk (e.g. Hadoop File System) and read by next step's computation as input, Spark can directly pass the result of previous step to next step, which saves lots of disk and network I/O. The advantage comes from the cheaper memory nowadays and the TB level memory addressing abilities of 64-bit platform.\n",
    "2. Apache Spark has an advanced Directed Acyclic Graph (DAG) execution engine that supports cyclic data flow and in-memory computing. It can optimize many operations into one stage, where in MapReduce these operations are scheduled in multiple stages.\n",
    "3. Apache Spark saves lots of Java Virtual Machine (JVM) setup time by keeping a running executor JVM on each cluster node, where in MapReduce a new JVM is created for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Include Spark in Jupyter Notebook\n",
    "\n",
    "Download pre-built Spark package: [Package Link](http://spark.apache.org/downloads.html).\n",
    "\n",
    "Options selected while writing this tutorial:\n",
    "- Spark release: 2.0.1 (Oct 03, 2016)\n",
    "- Package type: Pre-built for Hadoop 2.7 and later\n",
    "- Download type: direct download\n",
    "\n",
    "Download `spark-2.0.1-bin-hadoop2.7.tgz` and unzip it to the same folder of this notebook file, then include Spark into the notebook as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_home = os.getcwd()+'/spark-2.0.1-bin-hadoop2.7'\n",
    "os.environ['SPARK_HOME'] = spark_home\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python\\lib\\py4j-0.10.3-src.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test if Spark is included into this notebook successfully, please try to build SparkContext. There should be no warning or exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# In local mode, specify the number of CPU cores Spark can use in bracket, or use * to let Spark to detect\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"Spark Tutorial\")\n",
    "\n",
    "# specify the memory Spark can use\n",
    "conf.set(\"spark.executor.memory\", \"1g\")    \n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note1</b> :_ If exception _`[Java gateway process exited before sending the driver its port number]`_ is thrown on Windows OS, modify `spark-2.0.1-bin-hadoop2.7/bin/spark-class2.cmd` line 33, remove the double quotes\n",
    "\n",
    "<b>Note2</b> :_ While writing the tutorial, I encountered _`[global name 'accumulators' is not defined]`_ exception from `context.py`, I added <code>`print(accumulators)`</code> in <code>`_do_init_`</code> function body before the problematic code, then the exception mysteriously disappeared...\n",
    "\n",
    "<b>Note3</b> : In the cluster environment, you can pass the cluster URL to <code>conf.setMaster()</code>, like Spark, [Mesos](http://mesos.apache.org/) or [YARN](http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html) cluster URL.\n",
    "\n",
    "<b>Note4</b> : Always call sc.stop() before generate another SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3453\n"
     ]
    }
   ],
   "source": [
    "# test if Spark is functioning, count the number words in LICENSE file\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark Tutorial\").getOrCreate()\n",
    "lines = spark.read.text(spark_home+'LICENSE').rdd.map(lambda r: r[0])\n",
    "counts = lines.flatMap(lambda x: x.split(' ')).map(lambda x: 1).reduce(add)\n",
    "print counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be 3453."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Spark Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1 Create Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "Spark's cluster programming API is centered on this RDD data structure, RDD is a read-only multiset of data items can be easily distributed amony cluster nodes, also provide an easy way for Spark to use the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# create RDD from a list, (parallelizing an existing collection), then find the max value\n",
    "test_list = [random.randint(1, sys.maxint) for i in range(10000)]\n",
    "distData = sc.parallelize(test_list)\n",
    "max_val = distData.reduce(lambda a, b: a if a>b else b)\n",
    "print max_val == max(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create RDD from existing data file\n",
    "\n",
    "# Count number of appearance of each word in LICENSE file\n",
    "test_file = sc.textFile(spark_home+'LICENSE')  # reads it as a collection of lines\n",
    "word_counts = test_file.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1)).reduceByKey(add)\n",
    "output = word_counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First ten word-count pairs:\n",
    "<pre>\n",
    ": 1430\n",
    "all: 3\n",
    "customary: 1\n",
    "(org.antlr:ST4:4.0.4: 1\n",
    "managed: 1\n",
    "Works,: 2\n",
    "APPENDIX:: 1\n",
    "details.: 2\n",
    "granting: 1\n",
    "Subcomponents:: 1\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Persist RDD Object in Memory\n",
    "\n",
    "RDD object cannot be reused after reduce since RDD object is lazily created, i.e. only execute the RDD create operation when it is needed by reduce, and only the reduced result is returned, if the same RDD object is used later, RDD create operation needs to be executed again. If the generation of RDD object is time consuming, Spark can persist the RDD into memory for future uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a larger file by repeating LICENSE 2000 times to make file reading time longer, result file size is around 34.5 MB\n",
    "filenames = [spark_home+'LICENSE' for i in range(2000)]\n",
    "with open(spark_home+'LARGE_LICENSE', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[23] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_len = sc.textFile(spark_home+'LARGE_LICENSE').map(lambda x: len(x))\n",
    "line_len.persist()    # persist to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139 0 35024000\n",
      "Wall time: 8.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# time with persisted line_len\n",
    "max_len = line_len.reduce(lambda a,b : a if a>b else b)\n",
    "min_len = line_len.reduce(lambda a,b : a if a<b else b)\n",
    "total_len = line_len.reduce(add)\n",
    "\n",
    "print max_len, min_len, total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[23] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_len.unpersist()   # remove persisted RDD from memory, compare the time for the same operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139 0 35024000\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# time without persisted line_len\n",
    "max_len = line_len.reduce(lambda a,b : a if a>b else b)\n",
    "min_len = line_len.reduce(lambda a,b : a if a<b else b)\n",
    "total_len = line_len.reduce(add)\n",
    "\n",
    "print max_len, min_len, total_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Use Complex Function in Map and Reduce\n",
    "\n",
    "We can pass a complex function to map and reduce function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complexMap(s):\n",
    "    '''\n",
    "    Get the individual words of current line\n",
    "    '''\n",
    "    words = s.strip().split(\" \")\n",
    "    \n",
    "    words_num = len(words)\n",
    "    \n",
    "    # only counts line with more than ten words, and the first word starts with an alphabetic character\n",
    "    if words_num>10 and words[0][0].isalpha():\n",
    "         return (words[0], words_len)\n",
    "    else:\n",
    "        return (None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complexReduce(a, b):\n",
    "    '''\n",
    "    conditional sum reduce\n",
    "    '''\n",
    "    if a[0] and b[0]:\n",
    "        return (\"Total\", a[1]+b[1])\n",
    "    \n",
    "    if a[0]:\n",
    "        return (\"Total\", a[1])\n",
    "    \n",
    "    if b[0]:\n",
    "        return (\"Total\", b[1])\n",
    "    \n",
    "    return (None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Total', 583)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(spark_home+'LICENSE').map(complexMap).reduce(complexReduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Streaming is an extension of the core Spark API to process streaming data. Various streaming data sources can be applied like Kafka, Flume, Kinesis, or TCP sockets, Spark Streaming application checks newly arrived/created data with pre-defined time interval. Spark provides many high-level data processing method like <code>map</code>, <code>reduce</code>, <code>join</code>, <code>window</code> and their variants. At the end, processed data can be pushed out to filesystems, databases, and live dashboards. \n",
    "\n",
    "More conveniently, Spark’s machine learning and graph processing algorithms can also be applied to data streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"streaming-arch.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Streaming provides a high-level abstraction on streaming data representation, called discretized stream (DStream). DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or from another DStreams. Internally, a DStream is represented as a sequence of RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"streaming-dstream.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Spark Streaming Workflow\n",
    "\n",
    "1. Spark Streaming context initialization\n",
    "2. Input DStream creation (streaming data source)\n",
    "3. Streaming data processing definition, i.e. applying transformation and output operations to DStreams\n",
    "4. Start the application with <code>streamingContext.start()</code>\n",
    "5. Wait for the finish of processing with <code>streamingContext.awaitTermination()</code>, the streaming application will be terminated when exception occurs or <code>streamingContext.stop()</code> is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initialize Spark Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 1)  # time interval is defined as 1 second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Note1: </b>Spark Streaming needs at least one node/thread as data receiver, and needs extra nodes/threads to do the processing job. In pseudo-distributed local mode, use `local` or `local[1]` as master URL will leave Spark no data processing thread since the only one thread acts as data receiver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note2</b> : The batch interval should be set based on the latency requirements of the application and available cluster resources to make Spark streaming application stable, i.e. the application should be able to process data as fast as the streaming data being generated and received. More details on how to figure out the batch interal is on [Spark documentation](http://spark.apache.org/docs/latest/streaming-programming-guide.html#setting-the-right-batch-interval)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create Input DStream\n",
    "\n",
    "In cluster environment, we can create input DStream from socket, text file and binary file, each has a initialization function respectively. The text file and binary file are required to be hosted on Hadoop-compatible file system, everytime a new file is <b>moved</b> or <b>renamed</b> in the monitored directory, since Spark reads file data at once when new file is found by name, copied file or editing file will be seen as empty file by Spark.\n",
    "\n",
    "Since the local file system is not Hadoop-compatible, and Sparking streaming is more stable on distributed file system, we use another way to generate the input DStream in this tutorial, <code>queueStream()</code>. <code>queueStream()</code> generates a data stream from a collection of RDDs, then feed the StreamingContext one RDD per time interval. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Twitter API to get a random sample of most recent tweets (around 1% of total new tweets), then combine a number tweets into a group, then each group is transformed into a distributed dataset that can be operated on in parallel, a seriers of such dataset will be formed into data stream using <code>queueStream()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# install twitter package into Jupyter notebook\n",
    "!pip install twitter\n",
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connect_twitter():\n",
    "    '''\n",
    "    Connect to Twitter with developer API keys, then call Twitter API on TwitterStream\n",
    "    '''\n",
    "    consumer_key = \"bm7qkiTCNPMzsBIkkSnwgnzVU\"\n",
    "    consumer_secret = \"fkID5ttsNogh4eQyWiKpgRg7P80yXbsglj9nAYA6peN4QGSNlX\"\n",
    "    access_token = \"794210841440686081-utUrhHReNXcUcD3KligzLb95MpyXv7c\"\n",
    "    access_secret = \"PDD7XWdAoJNYaMbwEzNHNfc1UueWNfepQIep4ABPoHHpq\"\n",
    "    auth = twitter.OAuth(token = access_token, token_secret = access_secret, consumer_key = consumer_key, consumer_secret = consumer_secret)\n",
    "    return twitter.TwitterStream(auth=auth)\n",
    "\n",
    "twitter_stream = connect_twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweet(content_generator):\n",
    "    '''\n",
    "    Get valid Twitter content from a generator returned by Twitter sample API\n",
    "    '''\n",
    "    while True:\n",
    "        tweet = content_generator.next()\n",
    "        if 'delete' in tweet:\n",
    "            continue\n",
    "\n",
    "        return tweet['text'].encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_rdd_queue(twitter_stream, tweets_num=10, queue_len=10):\n",
    "    '''\n",
    "    Generate a RDD list out of the groups of tweets, this list will be transformed into data stream\n",
    "    '''\n",
    "    rddQueue = []\n",
    "    \n",
    "    # Get most recent tweets samples\n",
    "    content_generator = twitter_stream.statuses.sample(block=True)\n",
    "    \n",
    "    for q in range(queue_len):\n",
    "        contents = []\n",
    "        for i in range(tweets_num):\n",
    "            contents.append(get_tweet(content_generator))\n",
    "        \n",
    "        # Generate the distributed dataset from a group of tweets content\n",
    "        rdd = ssc.sparkContext.parallelize(contents, 5)\n",
    "        \n",
    "        rddQueue += [rdd]\n",
    "        \n",
    "    return rddQueue\n",
    "\n",
    "rddQueue = gen_rdd_queue(twitter_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stream Data Processing Definition\n",
    "\n",
    "We will get the word with top occurences from the samples within every time interval, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_tweet(new_values, last_sum):\n",
    "    '''\n",
    "    Word count update function\n",
    "    '''\n",
    "    return sum(new_values) + (last_sum or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(time, rdd, top = 10):\n",
    "    '''\n",
    "    Print the words with top occurences\n",
    "    '''\n",
    "    result = []\n",
    "    read = rdd.take(top)\n",
    "\n",
    "    print(\"Time: %s:\" % time)\n",
    "\n",
    "    for record in read:\n",
    "        print(record)\n",
    "\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Start and Stop the Streaming application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.checkpoint(\"./checkpoint-tweet\")\n",
    "\n",
    "# Get input DStream\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "# Get words by split space of raw tweet text, the count word ocurrence and sort in descending order\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "              .map(lambda word: (word, 1))\\\n",
    "              .updateStateByKey(process_tweet)\\\n",
    "              .transform(lambda rdd: rdd.sortBy(lambda x: x[1],False))\n",
    "\n",
    "# Print result on the result of each time interval\n",
    "counts.foreachRDD(output)\n",
    "\n",
    "ssc.start()\n",
    "print \"Spark Streaming started\"\n",
    "\n",
    "# To save time, the streaming application will be terminated manually after 30 seconds\n",
    "time.sleep(30)\n",
    "\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "print \"Spark Streaming finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "\n",
    "Spark Streaming started<br/>\n",
    "Time: 2016-11-03 18:19:05:<br/>\n",
    "('RT', 8)<br/>\n",
    "(\"j'ai\", 2)<br/>\n",
    "('de', 2)<br/>\n",
    "('en', 2)<br/>\n",
    "('que', 2)<br/>\n",
    "('\\xd8\\x8c', 2)<br/>\n",
    "('#MourinhoOut', 1)<br/>\n",
    "(')', 1)<br/>\n",
    "('somos', 1)<br/>\n",
    "('m\\xc3\\xa1s', 1)<br/>\n",
    "('#Gala9GH17', 1)<br/>\n",
    "<br/>\n",
    "Time: 2016-11-03 18:19:06:<br/>\n",
    "('RT', 13)<br/>\n",
    "('e', 4)<br/>\n",
    "('\\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x87', 3)<br/>\n",
    "('\\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x87', 2)<br/>\n",
    "('\\xce\\xba\\xce\\xb1\\xce\\xb9', 2)<br/>\n",
    "('en', 2)<br/>\n",
    "('\\xd9\\x85\\xd9\\x86', 2)<br/>\n",
    "('de', 2)<br/>\n",
    "(\"j'ai\", 2)<br/>\n",
    "('que', 2)<br/>\n",
    "('\\xd8\\xb9\\xd9\\x86', 2)<br/>\n",
    "<br/>\n",
    "Time: 2016-11-03 18:19:07:<br/>\n",
    "('RT', 18)<br/>\n",
    "('de', 6)<br/>\n",
    "('e', 5)<br/>\n",
    "('to', 4)<br/>\n",
    "('for', 4)<br/>\n",
    "('no', 4)<br/>\n",
    "('\\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x87', 3)<br/>\n",
    "('it', 3)<br/>\n",
    "('', 2)<br/>\n",
    "('dos', 2)<br/>\n",
    "('win', 2)<br/>\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Further Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Spark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Spark DataFrame is a distributed collection of data organized into named columns, more powerful lambda functions can be applied on. It is conceptually equivalent to a table in a relational database or a data frame in Python, but with more optimizations under the hood. Spark SQL provides ability to execute SQL queries on Spark data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference Link: <http://spark.apache.org/docs/latest/sql-programming-guide.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Spark Machine Learning Library (MLlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib is a rich library on common machine learning algorithms and methods, it makes the machine learning application more scalable and easier. These functionalities provided by MLlib can be applied on RDD easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference Link: <http://spark.apache.org/docs/latest/ml-guide.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Apache Spark Wikipedia: (https://en.wikipedia.org/wiki/Apache_Spark)\n",
    "2. Spark Programming Guides: (http://spark.apache.org/docs/latest/programming-guide.html)\n",
    "3. Why Spark is faster: (https://www.quora.com/What-makes-Spark-faster-than-MapReduce)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
