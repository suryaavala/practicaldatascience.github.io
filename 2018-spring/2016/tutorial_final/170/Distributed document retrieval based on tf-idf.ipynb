{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Author: Wenjie Tan\n",
    "Andrew ID: wenjiet\n",
    "Date: 2016/10/31\n",
    "\n",
    "In this tutorial, we will build a distributed retrieval system, and have an exploration about building index on inverted list by map reduce, calculating scores based on tf-idf, improving precision and recall rate based on BM25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Map Reduce Introduction\n",
    "\n",
    "MapReduce is a programming model and an associated implementation for processing and generating large data sets with a parallel, distributed algorithm on a cluster. Conceptually similar approaches have been very well known since 1995 with the Message Passing Interface standard having reduce and scatter operations.\n",
    "\n",
    "--from Wikipedia\n",
    "\n",
    "Today, we will use Map Reduce to process the raw text, then we will get the tf and idf for future reference. This job will be done in Amazon Web Service using Elastic Map Reduce Cluster.\n",
    "\n",
    "First, we need to have a familiar with MapReduce on AWS. The basic architecture of MapReduce on AWS is a pipeline environment. We need to write two python scripts, each describing how map and reduce works, and upload them into the AWS S3. After that we will need to configure an EMR, choose the mapper and reducer as what we have just uploaded. Then we specific the input directory and output directory, and then launch the cluster. Once in a while we finish the map reduce job, we will get all the reduce output in the output directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Map Reduce Sample Word Count\n",
    "\n",
    "First of all, we need to be familiar with the usage of EMR on AWS. This sample we need to write a mapper and a reducer to read a lot of raw files, and then output the the count number of different word.\n",
    "\n",
    "In Map reduce phrase, first we need a mapper and a reducer. Mapper will accept a lot of input, parsing them into (key, value) pair, and the system will do a shuffle sort based on key. Then the result will be sent to reducer as the input of the reducer. The reducer will parse these input and output the final (key, value) result.\n",
    "\n",
    "For Mapper, it should read from the standard input, get each word of the text file, and output the word with \"word\\tcount\", here count should be 1.\n",
    "\n",
    "For Reducer, it should read from the standard input, concate the same word and add the count together, then output the word with \"word\\tcount\". Note, word should have been sorted in the shuffle state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "def word_count_mapper_main():\n",
    "    line = sys.stdin.readline()\n",
    "    try:\n",
    "        while line:\n",
    "            words = line.strip().split(\" \")\n",
    "            for word in words:\n",
    "                print \"%s\\t%s\" % (word, 1)\n",
    "            line = sys.stdin.readline()\n",
    "    except \"end of file\":\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    word_count_mapper_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "def word_count_reducer_main():\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    word = None\n",
    "    line = sys.stdin.readline()\n",
    "    try:\n",
    "        while line:\n",
    "            word, count = line.split(\"\\t\")\n",
    "            try:\n",
    "                count = int(count)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if current_word == word:\n",
    "                current_count += count\n",
    "            else:\n",
    "                if current_word:\n",
    "                    print \"%s\\t%s\" % (current_word, current_count)\n",
    "                current_count = count\n",
    "                current_word = word\n",
    "            line = sys.stdin.readline()\n",
    "    except \"end of file\":\n",
    "        if current_word == word:\n",
    "            print \"%s\\t%s\" % (current_word, current_count)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    word_count_reducer_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can simply test our mapper and reducer locally with the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat sample_doc_0.txt | python mapper.py | sort | python reducer.py > word_count_result.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The results should be as follows:\n",
    "\n",
    "a\t3\n",
    "document.\t3\n",
    "is\t3\n",
    "sample\t3\n",
    "test\t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the Inverted Index\n",
    "\n",
    "Inverted List is a really useful tool for document retrieval. \n",
    "\n",
    "An inverted index catalogs a collection of objects in their textual representations. Given a set of documents, keywords and other attributes (possibly including relevance ranking) are assigned to each document. The inverted index is the list of keywords and links to the corresponding document. Frequently there are several restrictions which limit the keywords in an index. \n",
    "\n",
    "A collection of stopwords--keywords that are not considered relevant. This collection normally contains words considered too common to function as keywords (articles, prepositions, conjunctions, etc.) or words outside the context of the index.\n",
    "\n",
    "A set of rules that define keywords in a document. This controls the manner in which keywords are found. For instance, keywords might be defined as character sequences surrounded by white- space.\n",
    "\n",
    "A set of rules that restrict indexable words. For instance, a rule that causes keywords containing numbers not to be indexed.\n",
    "\n",
    "A collection of \"synonyms\"--keywords that should be indexed using a different keyword.\n",
    "The InvertedIndex module provides simple tools for creating and maintaining inverted indices with support for incremental indexing and for stopword, synonym and stemming databases.\n",
    "\n",
    "--from http://legacy.python.org/workshops/1996-11/papers/InvertedIndex.html\n",
    "\n",
    "Here, we still try to use mapreduce to produce the inverted index by EMR.\n",
    "\n",
    "The input is the same as above, each file with a name as \"doc_1\", \"doc_2\", ..., \"doc_n\", we need to process the text, and output the inverted index like:\n",
    "\n",
    "school doc_1 doc_13 doc_203 ...\n",
    "time doc_13 doc_14 doc_150 ...\n",
    "....\n",
    "\n",
    "In AWS EMR, we could use the environment variable 'map_input_file' to get the file name of the input document. And in our local run, we only need to set the environment varuable 'map_input_file' in our test script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def word_count_mapper_main():\n",
    "    line = sys.stdin.readline()\n",
    "    try:\n",
    "        while line:\n",
    "            words = line.strip().split(\" \")\n",
    "            for word in words:\n",
    "                print \"%s\\t%s\" % (word, os.environ[\"map_input_file\"])\n",
    "            line = sys.stdin.readline()\n",
    "    except \"end of file\":\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    word_count_mapper_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "\n",
    "def output_word(word, doc):\n",
    "    print(word, end = \"\")\n",
    "    for d in doc:\n",
    "        print(\" \" + d, end = \"\")\n",
    "    print(\"\")\n",
    "\n",
    "def word_count_reducer_main():\n",
    "    current_word = None\n",
    "    current_doc = None\n",
    "    all_doc = []\n",
    "    line = sys.stdin.readline()\n",
    "\n",
    "    while line:\n",
    "        word, doc = line.strip().split(\"\\t\")\n",
    "        if current_word == word:\n",
    "            if current_doc != doc:\n",
    "                all_doc.append(doc)\n",
    "                current_doc = doc\n",
    "        else:\n",
    "            if current_word:\n",
    "                output_word(current_word, all_doc)\n",
    "            current_word = word\n",
    "            current_doc = doc\n",
    "            all_doc = []\n",
    "            all_doc.append(doc)\n",
    "        line = sys.stdin.readline()\n",
    "\n",
    "    if current_word:\n",
    "        output_word(current_word, all_doc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    word_count_reducer_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can simply test our mapper and reducer locally with the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export map_input_file=\"sample_doc_0.txt\"\n",
    "cat sample_doc_0.txt | python mapper.py > mapper_output0.txt\n",
    "export map_input_file=\"sample_doc_1.txt\"\n",
    "cat sample_doc_1.txt | python mapper.py > mapper_output1.txt\n",
    "cat mapper_output* | sort | python reducer.py > inverted_list_result.txt\n",
    "rm mapper_output*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The results should be as follows:\n",
    "\n",
    "another sample_doc_1.txt\n",
    "a sample_doc_0.txt\n",
    "document. sample_doc_0.txt sample_doc_1.txt\n",
    "is sample_doc_0.txt sample_doc_1.txt\n",
    "sample sample_doc_0.txt sample_doc_1.txt\n",
    "test sample_doc_0.txt sample_doc_1.txt\n",
    "this sample_doc_0.txt sample_doc_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Boolean Retrieval Model\n",
    "\n",
    "The Boolean model of information retrieval (BIR)[1] is a classical information retrieval (IR) model and, at the same time, the first and most adopted one. It is used by many IR systems to this day.\n",
    "\n",
    "--from https://en.wikipedia.org/wiki/Standard_Boolean_model\n",
    "\n",
    "In Boolean Retrieval, we will have a query of a boolean logical formation, indicating the requirement of this query. For example, we have a query like:\n",
    "\n",
    "\"Carnegie\" AND \"Mellon\" AND \"University\" AND \"Data\" AND \"Science\" AND \"Course\"\n",
    "\n",
    "This query requires us to return all the documents which should contain all the words \"Carnegie\", \"Mellon\", ..., \"Course\".\n",
    "\n",
    "Boolean Retrieval also could contain \"Or\", \"Near\", \"Window\" operator, howeverm here, we will try to achieve boolean retrieval model only containing \"AND\".\n",
    "\n",
    "Remember, we already have the inverted list, and in the inverted list, we already have documents name sorted for each query word. As a result, it is simply that we use several pointers to all query words, move forward and judge. This way will save a lot of time. Also, pointer movements could be achieved by Priority Queue.\n",
    "\n",
    "Now we actually have a retrieval system, which will take the inverted list as its argument, and take each line of the standard input as one query, parsing it, and output the result specific to this query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval System: (Please see this model in file boolean_retrieve.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can simply test our mapper and reducer locally with the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat qtest.txt | python boolean_retrieval.py inverted_list_result.txt > boolean_retrieval_result.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The results should be as follows:\n",
    "\n",
    "['sample_doc_0.txt']\n",
    "['sample_doc_0.txt', 'sample_doc_1.txt']\n",
    "['sample_doc_1.txt']\n",
    "['sample_doc_0.txt', 'sample_doc_1.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use AWS EMR for large data processing\n",
    "\n",
    "Now we have everything work well locally, and then we will put the script into AWS, and use EMR to generate the inverted index distributedly.\n",
    "\n",
    "Actually, till now, everything is almost the same as we already have the Mapper and Reducer. The only thing we need to do is to set the input s3 directory, output s3 directory, mapper, reducer, machine number and type, and then lunch cluster. It is easier than you think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-processing\n",
    "\n",
    "Before we continue on AWS EMR for large data, we need to do some filter job, i.e. stopwords remove, before the next step.\n",
    "\n",
    "In typical English written document, there will be some words which occurred multiple times. Those words will consume a lot of space in retrieval model, and actually they have no meaning for us when searching. We call those words as stop_words, which means they need to be removed when building the index.\n",
    "\n",
    "Besides, English words have different shapes and cases, also some punctuations. We need to have a clean through all those words, and change them into the same format we what.\n",
    "\n",
    "In this part, we need to rewrite our Mapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "\n",
    "def word_count_mapper_main():\n",
    "    stopwords = {\"\"}\n",
    "    file = open(\"stopwords.txt\", \"r\")\n",
    "    line = file.readline()\n",
    "    while line:\n",
    "        stopwords.add(line.strip())\n",
    "        line = file.readline()\n",
    "    \n",
    "    line = sys.stdin.readline()\n",
    "    while line:\n",
    "        text = line.strip().lower().replace(\"'s\", \"\").replace(\"'\", \"\")\n",
    "        for p in string.punctuation:\n",
    "            text = text.replace(p, \" \")\n",
    "        for word in text.split(\" \"):\n",
    "            if len(word) > 0 and word not in stopwords:\n",
    "                print \"%s\\t%s\" % (word, os.environ[\"map_input_file\"])\n",
    "        line = sys.stdin.readline()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    word_count_mapper_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Boolean retrieval Model on EMR\n",
    "\n",
    "At first, we need to have an account at AWS. After registration, we could need to generate the key pair to access the cluster.\n",
    "\n",
    "Then, we need to create a s3 bucket to store our documents as the input, our Mapper.py and Reducer.py and the mapper and reducer, and specific locations for logging and outputing.\n",
    "\n",
    "In the end, we could use Boto to lunch the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto.emr\n",
    "from boto.emr.connection import EmrConnection\n",
    "from boto.emr.step import StreamingStep\n",
    "\n",
    "conn = boto.emr.connect_to_region('us-west-2')\n",
    "step = StreamingStep(name='boolean retrieval index build',\n",
    "                     mapper='s3n://<s3 bucket>/Mapper.py',\n",
    "                     reducer='s3n://<s3 bucket>/Reducer.py',\n",
    "                     input='s3n://<s3 bucket>/input',\n",
    "                     output='s3n://<s3 bucket>/output')\n",
    "jobid = conn.run_jobflow(name='index building job flow',\n",
    "                         log_uri='s3://<s3 bucket>/logs',\n",
    "                         step=[step])\n",
    "conn.add_jobflow_steps(jobid, [second_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Retrieval Model\n",
    "\n",
    "This part will introduce some advanced ways in document retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Boolean Retrieval with TF-IDF\n",
    "\n",
    "In information retrieval, tf-idf, short for term frequency-inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "--from https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "\n",
    "The difference between boolean retrieval and TF-IDF way is that we need to add a score for each query and document pair. The calculation of this score is regard as:\n",
    "\n",
    "Score(q, d) = tf(q, d) * idf(q)\n",
    "\n",
    "Here, tf(q, d) means how many times query word q occurred in document d, and idf means how the inverse of the number of how many documents have the query word q.\n",
    "\n",
    "IDF is useful unpon the intuitive idea that rare words should weight more. Here, we have a small change to IDF function:\n",
    "\n",
    "idf(q) = log(N / n(q))\n",
    "\n",
    "N is the total number of the documents, while n(q) is the number of documents containing word q. And then, we add the score for every word in q to calculate the final score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "\n",
    "def output_word(word, doc):\n",
    "    print(word, end = \"\")\n",
    "    for d in doc:\n",
    "        print(\" \" + d, end = \"\")\n",
    "    print(\"\")\n",
    "\n",
    "def word_count_reducer_main():\n",
    "    current_word = None\n",
    "    current_doc = None\n",
    "    current_num = 0\n",
    "    all_doc = []\n",
    "    line = sys.stdin.readline()\n",
    "\n",
    "    while line:\n",
    "        word, doc = line.strip().split(\"\\t\")\n",
    "        if current_word == word:\n",
    "            if current_doc != doc:\n",
    "                all_doc.append(doc + \",\" + str(current_num))\n",
    "                current_doc = doc\n",
    "                current_num = 1\n",
    "            else:\n",
    "                current_num += 1\n",
    "        else:\n",
    "            if current_word:\n",
    "                output_word(current_word, all_doc)\n",
    "            current_word = word\n",
    "            current_doc = doc\n",
    "            current_num = 1\n",
    "            all_doc = []\n",
    "            all_doc.append(doc + \",\" + str(current_num))\n",
    "        line = sys.stdin.readline()\n",
    "\n",
    "    if current_word:\n",
    "        output_word(current_word, all_doc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    word_count_reducer_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval System: (please see this model in tf_idf_retrieve.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BM25\n",
    "\n",
    "In information retrieval, Okapi BM25 (BM stands for Best Matching) is a ranking function used by search engines to rank matching documents according to their relevance to a given search query. It is based on the probabilistic retrieval framework developed in the 1970s and 1980s by Stephen E. Robertson, Karen Spärck Jones, and others.\n",
    "\n",
    "--from https://en.wikipedia.org/wiki/Okapi_BM25\n",
    "\n",
    "Actually, BM25 is a revised version of TFIDF. The only difference is the way to calculate the score of query and document.\n",
    "\n",
    "Here is the formulation:\n",
    "\n",
    "score(q, d) = idf(q) * tf(q, d) * (k1 + 1) / (tf(q, d) + k1 * (1 - b + b * |D| / avg_doc_len))\n",
    "\n",
    "here, k1 and b are two parameters, and we set to k1=1.2, b=0.75;\n",
    "|D| is the total number of the documents, while avg_doc_len is the average document length.\n",
    "\n",
    "So we could get the final BM25 retrieval system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval System:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "from Queue import PriorityQueue\n",
    "import math\n",
    "\n",
    "def bm25_retrieval(inverted_list, D, q_words):\n",
    "    # get the total size of the documents\n",
    "    N = len(reduce(lambda x, y: x | y,\n",
    "               map(lambda i: set(map(lambda j: j.split(',')[0], i)),\n",
    "                   inverted_list.values())))\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    avg_doc_len = tot_doc_len / N\n",
    "    words, ret = [], []\n",
    "    for word in q_words:\n",
    "        if word not in inverted_list:\n",
    "            return ret\n",
    "        words.append(word)\n",
    "    n = len(words)\n",
    "\n",
    "    # current index for each id\n",
    "    idx = [0] * n\n",
    "\n",
    "    # initialize priority queue\n",
    "    # in PQ, (current_word, id) is stored\n",
    "    # cur_max_word stores the maximum word in PQ\n",
    "    PQ = PriorityQueue()\n",
    "    cur_max_doc = \"\"\n",
    "    for i in range(n):\n",
    "        cur_doc = inverted_list[words[i]][0].split(',')[0]\n",
    "        PQ.put((cur_doc, i))\n",
    "        cur_max_doc = max(cur_max_doc, cur_doc)\n",
    "    while True:\n",
    "        cur_min_doc, id = PQ.get()\n",
    "        if cur_min_doc == cur_max_doc:\n",
    "            # calculate the sum of tf-idf score of all q_word\n",
    "            sum_tfidf = 0.0\n",
    "            for i in range(n):\n",
    "                doc, num = inverted_list[words[i]][idx[i]].split(',')\n",
    "                tf = float(num)\n",
    "                df = len(inverted_list[words[i]])\n",
    "                idf = math.log(1.0 * N / df)\n",
    "                # change to BM25 score function\n",
    "                sum_tfidf += idf * tf * (k1 + 1) / (tf + k1 * (1 - b + b * D[doc] / avg_doc_len))\n",
    "            ret.append((sum_tfidf, cur_min_doc))\n",
    "        idx[id] += 1\n",
    "        if idx[id] >= len(inverted_list[words[id]]):\n",
    "            # sort the result, and then output\n",
    "            ret.sort(reverse=True)\n",
    "            return ret\n",
    "        next_doc = inverted_list[words[id]][idx[id]].split(',')[0]\n",
    "        cur_max_doc = max(cur_max_doc, next_doc)\n",
    "        PQ.put((next_doc, id))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 2:\n",
    "        print \"inverted file name needed.\"\n",
    "        exit()\n",
    "    inverted_list = {}\n",
    "    file = open(sys.argv[1], \"r\")\n",
    "    line = file.readline()\n",
    "    while line:\n",
    "        parts = line.strip().split(\" \")\n",
    "        inverted_list[parts[0]] = parts[1:]\n",
    "        line = file.readline()\n",
    "\n",
    "    D = {}  # store (doc, len)\n",
    "    tot_doc_len = 0.0\n",
    "    for word_value in inverted_list.values():\n",
    "        for value in word_value:\n",
    "            doc, num = value.split(',')\n",
    "            if doc not in D:\n",
    "                D[doc] = 0.0\n",
    "            D[doc] += float(num)\n",
    "            tot_doc_len += float(num)\n",
    "\n",
    "    q_line = sys.stdin.readline()\n",
    "    while q_line:\n",
    "        q_words = q_line.strip().split(\" \")\n",
    "        doc = bm25_retrieval(inverted_list, D, q_words)\n",
    "        print doc\n",
    "        q_line = sys.stdin.readline()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
