{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we're going to discuss some popular clustering techniques in data mining, particularly focusing on the most popular iterative centroid-based divisive algorithm--k-means clustering algorithmfor(for grouping unlabeled items). Furthermore, we'll introduce a more efficient version of k-means clustering called bisecting k-Means clustering. Finally, we'll compare the k-menas clustering with the bisecting k-means clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Installing the libraries](#Installing-the-libraries)\n",
    "- [Introduction of k-means Clustering](#Introduction-of-k-means-Clustering) \n",
    "- [k-means algorithm support functions](#k-means-algorithm-support-functions)\n",
    "- [k-mean Clustering Algorithm](#k-mean-Clustering-Algorithm)\n",
    "- [Bisecting k-Means Clustering](#Bisecting-k-Means-Clustering)\n",
    "- [Bisecting k-Means Clustering Algorithm](#Bisecting-k-Means-Clustering-Algorithm)\n",
    "- [Plot Test Results & Comparison](#Plot-Test-Results-&-Comparison)\n",
    "\n",
    "Note: In this tutorial, we will introduce clustering analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, you'll need to install the various libraries that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "import math\n",
    "from random import sample \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of k-means Clustering \n",
    "Firstly, we are going to study one type of the most wildely used clustering technique, which called k-means, but what's clustring? \n",
    "\n",
    "### Cluster identification\n",
    "\n",
    "Before we get into k-means clustering, we are going to brief introduce cluster analysis at first. Basically, cluster analysis divides data set into groups(clusters) for the further meaningful data summarization.\n",
    "\n",
    "Clustering (also called unsupervised classification) is a type of unsupervised learning that forms cluster of similar things automatically. It's like an automatic classification, but in classification we know what we're looking for, however in clustering we don't. To say it more precisly, the difference between clustering and classification is that without having the predefined classes, clustering can also produce the same result as classfication. \n",
    "\n",
    "\n",
    "### General idea of k-means clustering algorithm\n",
    "\n",
    "k-means is an algorithm that will find k clusters for a given dataset. We need to define the number of clusters k, and each cluster is described by a single point known as the Center (or called Centroid) of all the points belongs to that clusters.\n",
    "\n",
    "Next, we find the closest center for each point, and assign the points to their corresponding cluster. After that, the centers are all updated by taking the mean value of all the points in that cluster. We iterate until none of the data points changes its cluster.\n",
    "\n",
    "\n",
    "#### The goal of k-means clustering \n",
    "Goal:  Find a set of centers $\\{c_1...c_n\\}\\in(\\mathbb{R})^d$ that minimize the k-mean objective:\n",
    "\n",
    "$\\sum_{j=1}^{n} \\displaystyle\\min_{\\forall i \\in \\{1,...,k\\}} \\|x_j - c_i\\|^2$ \n",
    "\n",
    "#### Input \n",
    "The dataset, and the number of clusters to generate are the only two parameters required.\n",
    "\n",
    "1. Given a data set $S$ = $\\{x_1...x_n\\}\\subset(\\mathbb{R})^d$ of n points in d-dimensional space. \n",
    "\n",
    "2. k, the numbers of clusters we want to find.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means algorithm support functions\n",
    "\n",
    "Now, let's set some helper functions that we'll need to use for k-means algorithm.\n",
    "For ease of use in later function, we  the first functionm load_DataSet( ), which helps us to load text file of tab-delimited floats lines into a list of floats.\n",
    "The next function dist_Euclidian( ), calculates the Euclidean distance of points and cluster centers.\n",
    "Finally, the last function is random_Center( ), it helps us to create a set of k centers randomly for a given dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_DataSet(filename):\n",
    "    \"\"\"\n",
    "    loads a text file containing tab-delimited floats lines into floats\n",
    "    Each lists is appended to a list called data\n",
    "\n",
    "    Args:\n",
    "        text: string containing space-separated words, on which to compute\n",
    "\n",
    "    Returns: \n",
    "        data: a list of lists of floats\n",
    "\n",
    "    \"\"\"\n",
    "    data = []              \n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            sp_Line = line.strip().split('\\t')\n",
    "            fltLine = map(float, sp_Line)   #map all elements to float()\n",
    "            data.append(fltLine)\n",
    "        return data\n",
    "    \n",
    "#class k_Means():\n",
    "def dist_Euclidian (A, B):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between A, B\n",
    "    \n",
    "    Args: A (vector of points), B (vector of centers)\n",
    "\n",
    "    Return: the Euclidean distance between A, B\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    for i in range(len(B)):\n",
    "        total += (A[i] - B[i]) ** 2\n",
    "    return sqrt(total)\n",
    "\n",
    "\n",
    "def random_Center(dataSet, k):\n",
    "    \"\"\"\n",
    "    Create a set of random k centers with given dataset. \n",
    "\n",
    "    Args: data (list of lists of floats) n by p \n",
    "\n",
    "    Return: centers (the positions of centers)\n",
    "    \"\"\"\n",
    "    centers = []\n",
    "    n = len(data)\n",
    "    rand_indexes = sample(range(0, n), k)\n",
    "    for rand_index in rand_indexes:\n",
    "            centers.append(data[rand_index])\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.15241297194665, 3.1385639614095], [0.851774597928675, 3.21423861525527]]\n"
     ]
    }
   ],
   "source": [
    "# First, we create data from a test_data.txt file.\n",
    "data = load_DataSet('test_data.txt')\n",
    "#print data\n",
    "#data_matrix = np.mat(data_matrix)\n",
    "# Let's see what the min and max values in our matrix\n",
    "# print  min(data_matrix[:,0])\n",
    "# print  min(data_matrix[:,1])\n",
    "# print  max(data_matrix[:,0])\n",
    "# print  max(data_matrix[:,1])\n",
    "# Now, let's check if random_Center() works well, which can give us a value between min and max\n",
    "print random_Center(data, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-mean Clustering Algorithm\n",
    "\n",
    "Now, we're ready to implement the full k-means algorithm. The algorithm will create k centers of clusters to us, then we assign each point to their closest center, and recalculate the centers. We'll iterate this process until none of the data points changing its cluster, in other words, the centers of clusters stop changing. \n",
    "\n",
    "By checking this stop condition, we set up a map called Membership, whcih is a dictionary maps index of cluster (key) to list of index of points that belongs to this cluster (value). Then, we create a function called $equalMembership()$ for comparing membership and the next membership for each cluster center k. If $equalMembership()$ returns False, which meas the cluster center changed, we update the membership by the next membership and continue iterating. After that, we use function getCenterFromMembership() to get new cluster centers, then use function $getMembershipFromCenter()$ to get the next membership by the latest new cluster centers. We iterate until the function $equalMembership()$ returns True, then the iteration stops, and we find the optimal center for points in each cluster. \n",
    "\n",
    "#### Steps of k-mean Clustering Algorithm\n",
    "    Step 1: Generated a set of random k centers as the initial centers for given dataset.\n",
    "    \n",
    "    Step 2: Created k clusters by assign each point a center index of their nearest center\n",
    "    \n",
    "    Step 3: Updated the new center of each cluster with recomputing the mean of all points in each cluster.\n",
    "    \n",
    "    Step 4: Repeated Step 2 and Step 3 until convergence has been reached, which the centers of each clusters stop changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getCenterIndex(point, center):\n",
    "    \"\"\"\n",
    "    Assign each point a center index of their nearest center, which has the minimum distance to each point. \n",
    "    Args: point, center(list of ints)\n",
    "    return: center_index(list of ints)\n",
    "       \n",
    "    \"\"\"\n",
    "    min_center_index = 0;\n",
    "    min_dist = dist_Euclidian(point, center[0])\n",
    "    for i in range(len(center)):\n",
    "        dist = dist_Euclidian(point, center[i])\n",
    "        if dist < min_dist:\n",
    "            min_center_index = i\n",
    "    return min_center_index\n",
    "\n",
    "\n",
    "def getMembershipFromCenter(center, data):\n",
    "    \"\"\"\n",
    "    membership: map index of cluster to list of index of points that belongs to this cluster\n",
    "    Args: \n",
    "        data(list of lists of floats): input data \n",
    "        center(list of ints)\n",
    "    return: membership: {index of cluster : [indexes of points]}\n",
    "            e.g. {0: [4], 1: [0, 1, 2, 6, 7, 8], 2: [3, 5]}\n",
    "    \"\"\" \n",
    "    membership = {} \n",
    "    for i in range(len(data)):\n",
    "        point = data[i]\n",
    "        cluster_index = getCenterIndex(point, center)\n",
    "        if not cluster_index in membership:\n",
    "            membership[cluster_index] = []\n",
    "        membership[cluster_index].append(i)\n",
    "    return membership\n",
    "\n",
    "def getCenterFromMembership(data, k, membership):\n",
    "    \"\"\"\n",
    "    From memembership to get centers\n",
    "    Args:\n",
    "        data(list of lists of floats): input data\n",
    "        k: number of centers(int)\n",
    "        membership: {center_index : [member_index]}\n",
    "    return: center(list of ints)\n",
    "    \"\"\" \n",
    "    p = len(data[0])\n",
    "    center = [0] * k\n",
    "    for center_index in membership:\n",
    "        members = membership[center_index]\n",
    "        point = [0.0] * p\n",
    "        for member_index in members:\n",
    "            for i in range(p):\n",
    "                point[i] += data[member_index][i]\n",
    "        for i in range(p):\n",
    "            point[i] /= len(members)\n",
    "        center[center_index] = point\n",
    "    for i in range(len(center)):\n",
    "        if center[i] == 0:\n",
    "            center[i] = random_Center(data, 1)[0]\n",
    "    return center\n",
    "\n",
    "def equalMembership(k, membership1, membership2):\n",
    "    \"\"\"\n",
    "    Comparing the equivalent of membership1 and membership2 \n",
    "    Args:\n",
    "        k: number of centers(int)\n",
    "        membership1/membership1 : {center_index : [member_index]}\n",
    "    return: True or False\n",
    "    \"\"\"       \n",
    "    \n",
    "    for i in range(k):\n",
    "        if i in membership1 and i in membership2:\n",
    "            set1 = set(membership1[i])\n",
    "            set2 = set(membership2[i])\n",
    "            if not set1 == set2:\n",
    "                return False\n",
    "        elif (i not in membership1) and (i not in membership2):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def kMeans(data, k):\n",
    "    \"\"\"\n",
    "    Get k cluster centers\n",
    "    Args: \n",
    "        data(list of lists of float): input data\n",
    "        k(int): number of centers\n",
    "        error(float): stop condition: stop if adjacent loop \n",
    "    return: \n",
    "        center (list): a list of the center points for each cluster\n",
    "        membership (dictionary): the key of dictionary is the index of centers\n",
    "                                 the value of the dictionary is the index of \n",
    "                                 sample points in that corresponding cluster\n",
    "                                 \n",
    "    \"\"\"\n",
    "    center = random_Center(data, k)\n",
    "    membership = getMembershipFromCenter(center, data)\n",
    "    center = getCenterFromMembership(data, k, membership)\n",
    "    next_membership = getMembershipFromCenter(center, data)\n",
    "    while not equalMembership(k, membership, next_membership):\n",
    "        membership = next_membership\n",
    "        center = getCenterFromMembership(data, k, membership)\n",
    "        next_membership = getMembershipFromCenter(center, data)\n",
    "    return center, membership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes: Counter Cases:\n",
    "There are some counter cases we may want to be careful.\n",
    "1. If the initial cluster center (since we assigned them randomly at the beginning) is empty, or the cluster center has no points associated with it, we replace those centers with random data point.\n",
    "\n",
    "2. In equalMembership$()$ function, if the cluster center is not in the keys of membership1 and membership2, we just continue our loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[1.5, 1.5], [1.0, 1.0], [72.5, 29.166666666666668]], {0: [0, 1], 1: [2], 2: [3, 4, 5, 6, 7, 8]})\n"
     ]
    }
   ],
   "source": [
    "# Let's first test our code with a small sample data below\n",
    "data_sample = [[1,2],[2,1],[1,1],[40,60], [45,55], [50, 55],[100, 1], [99, 2], [101,2]]\n",
    "print kMeans(data_sample, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our implementation, it returns a random output like this:\n",
    "\n",
    "[45.0, 56.666666666666664], [1.3333333333333333, 1.3333333333333333], [100.0, 1.6666666666666667]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.3204363663366423, 0.9146584246494122], [3.249033097660742, 1.1100367428845483], [2.98946384182756, 2.96177942022594], [2.01068650551852, 2.98071350711951], [3.09795819535077, 0.547562132211542], [2.996460448222953, 0.989851938205289], [2.3919482029177717, 0.9658627114439073], [3.057193952447126, 1.9048015458061252], [1.7573315142176673, 2.204610363255312]]\n"
     ]
    }
   ],
   "source": [
    "# Now, un our codes with a bigger dataset (900*2)\n",
    "centers, membership = kMeans(data, 9)\n",
    "print centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The output of center  ([[3.052589063796065, 0.5874453529235966], [2.09104375631811, 2.85262197068138], [2.92587771564402, 1.20352759495764], [0.795375667956239, 3.12770461334811], [3.31454020304876, 0.768878195112492], [2.915282063214967, 0.7569860378230464], [3.0394499547178193, 1.0200372273592586], [2.333689669568666, 1.1182835132461506], [1.8283460189205516, 2.256770827356669]],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For our implementation, this returns the following $center$ points (k = 9):  \n",
    "\n",
    "$[[3.052589063796065, 0.5874453529235966], [2.09104375631811, 2.85262197068138], [2.92587771564402, 1.20352759495764], [0.795375667956239, 3.12770461334811], [3.31454020304876, 0.768878195112492], [2.915282063214967, 0.7569860378230464], [3.0394499547178193, 1.0200372273592586], [2.333689669568666, 1.1182835132461506], [1.8283460189205516, 2.256770827356669]]$\n",
    "\n",
    "The result of $membership$ should be like this format:\n",
    "\n",
    "$\\{0: [601, 630, 669, 699], 1:[504, 533, 543, 553, 557, 571, 598], 2: [556, 51, ],..., ]\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisecting k-Means Clustering \n",
    "\n",
    "Next, we'll discuss a more efficient version of k-means called bisecting k-means. \n",
    "\n",
    "For K-means clustering algorithm, we know that it may converge to a local minimum sometimes, to avoid this problem well another algorithm has been developed, which by minimizing the Sum of Square Error (SSE) to evaluate the performance of clustering. The method of clustering called as Bisecting-K-means.\n",
    "\n",
    "### Basic Idea:\n",
    "\n",
    "In the beginning, we have only one initial cluster, which is our whole data set. To obtain K clusters, we firstly split the initial cluster (set of all data points) into two clusters, then select one of the cluster to split, and keep repeating this procedure so on until we have created K clusters as we desired. There are many different ways to select which cluster to split, here we compare the total Sum of Square Error (SSE) for each splitting sub-cluster, and select the largest reduced squared error, which is the least total SSE for all the clustering.\n",
    "\n",
    "\n",
    "### Steps of Algorithm\n",
    "\n",
    "Step 1: Split the dataset(the whole input data set) into two clusters by using k-means (k=2 for kMean(dataset, k)), then we have two centers of the whole data set as our initial two clusters, and initialize a center list to contain all the centers index(now we only have 2 centers). \n",
    "\n",
    "Step 2: To select the best cluster to split: we go over all the clusters in the cluster list, we create a new_dataSet, which contains the sample points of their corresponding cluster (i). Before we try to split the two clusters ($C_{1}$, $C_{2}$), we calculate the sum squared error (SSE) named as \"error_before\" for them by using the following formula:\n",
    "       \n",
    "SSE = $\\sum_{i=1}^{K} \\sum_{\\displaystyle x\\in C_{i}} dist(c_{i}, x)^2$\n",
    "       \n",
    "Then, we splits both clusters(eg. $C_{1}$, $C_{2}$) into 2 sub-clusters(eg. for $C_{1}$, we have $S_{1L}$ and $S_{1R}$; for $C_{2}$, we have its sub-cluster $S_{2L}$ and $S_{2R}$ by using k-means algorithm(gives us the two new centers) simutaneously. After that we compute both the sum of squared error(SSE) for each points with their corresponding sub-cluster center, then sum up those two SSE together, and we named them as \"error_after\" (eg. \"error_after\" for $C_1$ is SSE($S_{1L}$) + SSE ($S_{1R}$), \"error_after\" for $C_{2}$ is SSE($S_{2L}$) + SSE ($S_{2R}$)).   \n",
    "\n",
    "Step 3: Calculate the Reduced Sum of Square Error $(SSE)$ by subtracting error_before and their corresponding error_after (eg. reduced_error = $SSE(C_{1})-(SSE(S_{1L} + SSE (S_{1R}))$). Then, we compare the reduce SSE (reduced_error) for both clusters and select the cluster from them, which has the maximum reduced SSE (The least total SSE for all the clusters). \n",
    "        \n",
    "Step 4: Modify the data index according, and append the optimal centers index of new two sub-cluster to center list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bisecting K-means & Initialization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a sense of that the bisecting k-means clustering has less trouble with initialization problems, because it has tried several bisections and select the lowest total Sum of Square Error (SSE), and there are only two centers in each iteration step. We can see the simple iteration procedure of how the sequence of clusterings produced by bisecting K-means algorithm finds four clusters visually in the following Figure below,\n",
    "<img src=\"bisect_process.pgn\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisecting k-Means Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getCenterError(dataSet, center, data_ids):\n",
    "    error = 0;\n",
    "    for data_id in data_ids:\n",
    "        data = dataSet[data_id]\n",
    "        error += dist_Euclidian(data, center) ** 2\n",
    "    return error\n",
    "\n",
    "def biKmeans(dataSet, k):\n",
    "    \"\"\"\n",
    "    Assign each point a center index of their nearest center, which has the minimum distance to each point. \n",
    "    Args: Given data set\n",
    "          k: the number of clusters you want\n",
    "    return: centers, cluster_assessment ()\n",
    "       \n",
    "    \"\"\"\n",
    "    centers, membership = kMeans(dataSet, 2)\n",
    "    \n",
    "    while len(centers) < k:\n",
    "        opt_center_id = -1;\n",
    "        max_reduced_error = 0;\n",
    "        \n",
    "        #find the optimum center id to split\n",
    "        for center_id in range(len(centers)):\n",
    "            center = centers[center_id]\n",
    "            error_before = getCenterError(dataSet, center, membership[center_id])\n",
    "            new_dataSet = []\n",
    "            for data_id in membership[center_id]:\n",
    "                data = dataSet[data_id]\n",
    "                new_dataSet.append(data)\n",
    "            if len(new_dataSet) <= 0:\n",
    "                continue\n",
    "            new_centers, new_membership = kMeans(new_dataSet, 2)\n",
    "            \n",
    "            error_after = 0\n",
    "            if 0 in new_membership:\n",
    "                error_after += getCenterError(new_dataSet, new_centers[0], new_membership[0])\n",
    "                \n",
    "            if 1 in new_membership:\n",
    "                error_after += getCenterError(new_dataSet, new_centers[1], new_membership[1])\n",
    "                \n",
    "        # get the maximum reduced error between error_before and error_after  \n",
    "            reduced_error = error_before - error_after\n",
    "            if max_reduced_error < reduced_error:\n",
    "                max_reduced_error = reduced_error\n",
    "                opt_center_id = center_id\n",
    "         \n",
    "        \n",
    "        #split the center\n",
    "        new2old = {}\n",
    "        new_dataSet = []\n",
    "        for data_id in membership[opt_center_id]:\n",
    "            data = dataSet[data_id]\n",
    "            new2old[len(new_dataSet)] = data_id\n",
    "            new_dataSet.append(data)\n",
    "        new_centers, new_membership = kMeans(new_dataSet, 2)\n",
    "        centers[opt_center_id] = new_centers[0]\n",
    "        centers.append(new_centers[1])\n",
    "        data_id_list0 = []\n",
    "        if 0 in new_membership:\n",
    "            for new_id in new_membership[0]:\n",
    "                data_id_list0.append(new2old[new_id])\n",
    "        membership[opt_center_id] = data_id_list0;\n",
    "        data_id_list1 = []\n",
    "        if 1 in new_membership:\n",
    "            for new_id in new_membership[1]:\n",
    "                data_id_list1.append(new2old[new_id])\n",
    "        membership[len(membership)] = data_id_list1;\n",
    "    return centers, membership\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0120343002076153, 2.9301196586981817], [1.0461508383513176, 1.0182152406613805], [2.855959182095021, 2.033993697431677], [2.98646029508743, 1.0358255156991776], [1.9732198287393015, 2.757198893487244], [3.013304863796108, 2.983728792635465], [1.165232206441763, 1.964712989461876], [2.0496492952705645, 1.1232806299321896], [2.83742486145063, 1.547424953487]]\n"
     ]
    }
   ],
   "source": [
    "# Now, test our codes with our test dataset (900*2)\n",
    "centers, membership = biKmeans(data, 9)\n",
    "print centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our implementation, this returns the following center points (k = 9) as:\n",
    "[[1.9701987757228576, 1.2043994932104172], [2.046155217339732, 2.8181666319642873], [1.047979225240116, 2.9686805104564997], [2.851167728307258, 1.9682201863958153], [1.05395676586136, 0.848181586076502], [3.0317238289558497, 2.945712218285016], [1.010059046402461, 1.0343947158934823], [2.953318285421977, 1.0182129504112625], [1.1590569071191617, 2.0459069304875412]]\n",
    "\n",
    "The result of $membership$ should be like this format:\n",
    "\n",
    "$\\{0: [300, 301, 303, 304, 305, 306,...], 1: [100, 101, 102, 103, 104, 105, 106, 108,...], 2: [400, 401, 402, 403, 404,...],...,8: [200, 201, 202, 203,...] \\}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Test Results & Comparison\n",
    "\n",
    "The loading test data has 900 sample points (2D a), and we set the number of clusters as 9 (k = 9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the plot the cluster assignment after running the K-means algorithm\n",
    "\n",
    "centers, membership = kMeans(data, 9)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "color = ['pink', 'purple','r', 'b', 'g', 'k', 'y', 'c', 'm']\n",
    "for i in xrange(9):\n",
    "    plt.plot([centers[i][0]], [centers[i][1]], color = color[i], marker = 'o', ls = 'None')\n",
    "    datax = []\n",
    "    datay = []\n",
    "    if not i in membership:\n",
    "        continue\n",
    "    for data_id in membership[i]:\n",
    "        datax.append(data[data_id][0])\n",
    "        datay.append(data[data_id][1])\n",
    "    plt.plot(datax, datay, color = color[i], marker = '+', ls = 'None')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of cluster resulting from k-means algorithm (different colors representing different clusters):\n",
    "\n",
    "Note: the cluster centers are marked with a circle, and the data points are marked with a cross\n",
    "\n",
    "<img src=\"kmeans_plot.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the plot of the cluster assignment after running the Bisecting K-means algorithm \n",
    "centers, membership = biKmeans(data, 9)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "color = ['pink','purple', 'r', 'b', 'g', 'm', 'k', 'y', 'c']\n",
    "for i in xrange(9):\n",
    "    plt.plot([centers[i][0]], [centers[i][1]], color = color[i], marker = 'o', ls = 'None')\n",
    "    datax = []\n",
    "    datay = []\n",
    "    for data_id in membership[i]:\n",
    "        datax.append(data[data_id][0])\n",
    "        datay.append(data[data_id][1])\n",
    "    plt.plot(datax, datay, color = color[i], marker = '+', ls = 'None')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of cluster assignment after running the bisecting k-means algorithm showed as following. \n",
    "<img src=\"biKmeans_plot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing \n",
    "\n",
    "Implementing the k-means and bisecting k-means clustering algorithms in the same test data set, and set the same number of clusters (k = 9), we can see from the results and the plot sections obviously that the the bisecting k-means clustering algorithm can produce much better clusters of our test data than k-means clustering algorithm. The plot result also shows that the k-means get troubled with local minimum to produce poor cluster, but bisecting k-means can avoid that case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References & Resources\n",
    "\n",
    "This tutorial highlighted just a few analysis of k-means clustering and bisecting k-means clustering with small scale  dataset in Python. Much more detail about k-means algorithms are available from the following links.\n",
    "\n",
    "1. Wikipedia : https://en.wikipedia.org/wiki/K-means_clustering\n",
    "2. An Efficient k-Means Clustering Algorithm: Analysis and Implementation: https://www.cs.umd.edu/~mount/Projects/KMeans/pami02.pdf\n",
    "3. \"A comparison of document clustering techniques\", M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n",
    "4. Introduction to Data Mining, Chapter 8. Cluster Analysis: Basic Concepts and Algorithm http://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf\n",
    "5. Machine Learning in Action https://www.manning.com/books/machine-learning-in-action"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
