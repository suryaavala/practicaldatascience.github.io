{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15688 Tutorial for AWS Python SDK (boto2)\n",
    "\n",
    "author: Yuqi Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "  Having experienced in pracitcal data science for several weeks, it is not hard to find out that better data analysis result usually comes from better and larger datasets. But when we acquire huge volume of data, how and where do we store and preprocess it? \n",
    "  **Appearly** using our local computer is not the answer. Thus here comes a new concept comes: **cloud computing**. AWS(Amazon Web Service), provided by Amazon Inc., is one of the popular choices. It offers various services:\n",
    "  * Amazon EC2 (Elastic Compute Cloud) \n",
    "  * S3 (Simple Storage System) \n",
    "  * Elastic Block Store (EBS)\n",
    "  * Elastic Map Reduce (EMR)\n",
    "  * DynamoDB        \n",
    "  \n",
    "\n",
    "  For more information about AWS, please refer to the [AWS website](https://aws.amazon.com/). In this tutorial, we will mainly focus on using ec2 and EMR to make data analysis much more effectively.We can easily and quickly register a new account and start to use differenct kinds of service right away.\n",
    "  You may create your own AWS account [here](https://aws.amazon.com/). One thing here to point out is that AWS service is not a free service, different service charges in different standards. We need to be cautious before we start that service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before start\n",
    "  ##### Generate Access Key ID & Secret Key pair on AWS\n",
    "  After creating your own account, we need to generate an Access Key ID and Secret Key pair after signing into our account. \n",
    "  [Here is the user manual:][1]\n",
    "  * choose Security Credentials from the drop-down menu on the top right corner. \n",
    "  * Under Access Keys click on Create a new Access Key and note down the Access Key ID and the Secret Access Key. \n",
    "  * If you ever need to refer to these keys in the future, simply visit the Security Credentials page to access them.\n",
    "  [1]:https://theproject.zone/f16-15619/aws-intro\n",
    "  ** Store your keys in safe place or export them as an environment variables in case of any information leaking happens. **\n",
    "  ##### Configure Boto Credentials  \n",
    "  * Simply use pip install boto to install boto library.\n",
    "  * Create a ~/.boto file with these contents:  \n",
    "  [Credentials]  \n",
    "  aws_access_key_id = YOURACCESSKEY  \n",
    "  aws_secret_access_key = YOURSECRETKEY\n",
    "  \n",
    "  If you already have a AWS account and installed aws cli tool, you can also use shell command 'aws configure' to configure your credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library importing\n",
    "  **boto** is the aws python sdk and it is the key element we use in this tutorial. Its API document is [here](http://boto.cloudhackers.com/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto\n",
    "import sys\n",
    "from boto.ec2.connection import EC2Connection\n",
    "import boto.emr\n",
    "from boto.emr.connection import EmrConnection\n",
    "from boto.emr.instance_group import InstanceGroup\n",
    "from boto.emr.step import StreamingStep\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making connections to EC2 instances\n",
    "  First thing we want to do here is to lauch an EC2 instance which acts as  a virtual machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RegionInfo:us-east-1, RegionInfo:cn-north-1, RegionInfo:ap-northeast-1, RegionInfo:ap-southeast-2, RegionInfo:sa-east-1, RegionInfo:ap-southeast-1, RegionInfo:ap-northeast-2, RegionInfo:us-west-2, RegionInfo:us-gov-west-1, RegionInfo:us-west-1, RegionInfo:eu-central-1, RegionInfo:eu-west-1]\n"
     ]
    }
   ],
   "source": [
    "# Given a valid region name, return a boto.ec2.connection.EC2Connection.\n",
    "def ec2_connection(region='us-east-1'):\n",
    "    conn = boto.ec2.connect_to_region(region)\n",
    "    return conn\n",
    "ec2_conn = ec2_connection()\n",
    "\n",
    "# In case you forget what available regions are, we can call method like:\n",
    "\n",
    "print boto.ec2.regions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch an EC2 instance\n",
    "  The functions listed here are:\n",
    "  * create a secuiry group for later use. Using the connection we acquired above, to create a security group(sg).\n",
    "  * delete unneccessery or wrongly created security group.\n",
    "  * validate a url.This function can be used to test whether an instance is successfully launched.**Note: This function can only be applied to instance with server installed on port 80. If you start with a bare instance, you can simply launch it and try ssh into it a few seconds later.**\n",
    "  * **launch an ec2 instance**\n",
    "  * Important thing to noteice here: If you generate a keypair, remember to change its permission or else it will be too open to connect to your instance. e.x. chmod 700 ``<KEYPAIRNAME>``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SecurityGroup:sg1\n"
     ]
    }
   ],
   "source": [
    "# sg_name is the name of the sg, sg_desc is the description of the sg.\n",
    "def sg_creator(conn, sg_name, sg_desc):\n",
    "    sg = conn.create_security_group(sg_name, sg_desc)\n",
    "    # Here we create a sg allows every traffic in and out\n",
    "    sg.authorize(ip_protocol=\"-1\", from_port=None, to_port=None,\n",
    "                 cidr_ip=\"0.0.0.0/0\", src_group=None, dry_run=False)\n",
    "    return sg\n",
    "sg1 = sg_creator(ec2_conn, 'sg1', 'first sg')\n",
    "print sg1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# This function is used to delete a security group with the name passed in\n",
    "def delete_sg(conn, sg_name):\n",
    "    return conn.delete_security_group(name=sg_name)\n",
    "print delete_sg(ec2_conn, 'sg1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2-54-147-4-185.compute-1.amazonaws.com\n",
      "HTTPConnectionPool(host='ec2-54-147-4-185.compute-1.amazonaws.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x106e26910>: Failed to establish a new connection: [Errno 60] Operation timed out',))\n",
      "HTTPConnectionPool(host='ec2-54-147-4-185.compute-1.amazonaws.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x106e26950>: Failed to establish a new connection: [Errno 61] Connection refused',))\n",
      "HTTPConnectionPool(host='ec2-54-147-4-185.compute-1.amazonaws.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x106e26c50>: Failed to establish a new connection: [Errno 61] Connection refused',))\n",
      "[0] instance created successfully!\n",
      "ec2-54-147-4-185.compute-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "def url_validation(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return True\n",
    "        print response.content\n",
    "        return False\n",
    "    except Exception, e:\n",
    "        print e\n",
    "        \n",
    "# This function is used to launch an instance with parameters passed in. \n",
    "# instance_type is the type of instance to run.\n",
    "def ec2_launch(conn, ami_type, key_name, instance_type, security_groups=None):\n",
    "    # returns a boto.ec2.instance.Reservation associated with the request for machines\n",
    "    reservation = conn.run_instances(\n",
    "        ami_type, key_name=key_name, instance_type=instance_type, security_groups=security_groups)\n",
    "    # get our instance\n",
    "    instance = reservation.instances[0]\n",
    "    # add tags to our instance\n",
    "    #instance.add_tags({'15688': 'test'})\n",
    "    # manually update its running status\n",
    "    state = instance.update()\n",
    "    while state == 'pending':\n",
    "        time.sleep(5)\n",
    "        state = instance.update()\n",
    "    public_dns_name = instance.public_dns_name\n",
    "    print public_dns_name\n",
    "    while not url_validation('http://' + str(public_dns_name)):\n",
    "        time.sleep(5)\n",
    "    print '[0] instance created successfully!'\n",
    "    return public_dns_name\n",
    "\n",
    "print ec2_launch(ec2_conn, 'ami-dc6f05cb','test','t2.micro',[sg1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can do with this instance\n",
    "* ssh into the instance using your keypair and the public dns name\n",
    "* use this instance to as a storage system or data processing helper\n",
    "* send and receive files between this instance & local PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Service: EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We learned how to create an ec2 instance using boto API above, now we can start to explore further on how aws can cope with data science. One of its services named Elastic Map Reduce(EMR) is the most practical and easy tool from my view, for students who just step in data science field. To shortly introduce what EMR is, I conclude it as **an automation tool which mainly contains two parts: mapper and reducer, and can process data in a much faster way with the help of hadoop which can run jobs parallelly.**   \n",
    "Hadoop is an open-source implementation of Google's MapReduce. In a MapReduce program, all the data is processed and stored as Key/Value pairs. Input data is processed by a map function. Mappers that execute the map function, output a transformed set of Key/Value pairs, which are subsequently processed by a reduce function and produce Key/Value pairs as output.  \n",
    "For more information about EMR, please refer to the [EMR introduction](https://aws.amazon.com/emr/).  \n",
    "Then let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Making connections to EMR service\n",
    "  Pretty similar to what we did to connect to ec2 instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmrConnection:elasticmapreduce.us-east-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Given a valid region name, return a boto.ec2.connection.EC2Connection.\n",
    "def emr_connection(region='us-east-1'):\n",
    "    conn = boto.emr.connect_to_region(region)\n",
    "    return conn\n",
    "emr_conn = emr_connection()\n",
    "print emr_conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we design a MapReduce Job Flow to do data analytics on Wikimedia dataset.\n",
    "\n",
    "### About the dataset\n",
    "Wikimedia maintains hourly page view statistics for all objects stored in Wikimedia servers as publicly accessible datasets. We will use these statistics to analyse page-view trends and derive the trending topics on Wikipedia for a particular time range.Every record in dataset is like this:   \n",
    "**domain_code page_title count_views total_response_size.**  \n",
    "Domain_code is the domain name of the request to the server, page_title contains the requested page title, count_views is the number of times this page has been viewed in the respective hour. Total_response_size istThe total response size caused by the requests for this page in the respective hour.  \n",
    "domain_code has two parts, a language identifier and a sub-project suffix. Sub-project suffix (domain trailing part) is abbreviated, for example, (no suffix), .b , .w are abbreviations for sub-project suffix .wikipedia.org, .wikibooks.org and .mediawiki.org. Here is one record:  \n",
    "**en Carnegie_Mellon_University 32 3035632**  \n",
    "which means 32 requests to \"en.wikipedia.org/wiki/Carnegie_Mellon_University\", the Wikipedia desktop site for Carnegie Mellon University in English, which accounted in total for 3035632 response bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Scenario\n",
    "Meet following requirements:\n",
    "* Filter out elements based on the following rules:  \n",
    "  1. filter out lines which do not have 4 colums\n",
    "  2. title should start with 'en' or 'en.m'\n",
    "  3. '%3a' and '%3A' in the title should be excluded\n",
    "  4. first letter in title should not be lowercase\n",
    "  5. exclude all invalid patterns\n",
    "  6. add the page counts of 'en' and 'en.m' together when their titles are the same\n",
    "* Get the input filename from within a Mapper: As the date/time information is encoded in the filename, Hadoop streaming makes the filename available to every map task through the environment variables mapreduce_map_input_file. For example, the filename can be accessed in python using the statement os.environ[\"mapreduce_map_input_file\"], or in Java using the statement System.getenv(\"mapreduce_map_input_file\")\n",
    "* Aggregate the pageviews from hourly views to daily views.\n",
    "* Calculate the total pageviews for each article.\n",
    "* For every article that has over 100,000 page-views (100,000 excluded), print the following line as output (\\t is the tab character): [total month views]\\t[article name]\\t[date1:page views for date1]\\t[date2:page views for date2]..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample output record:  \n",
    "139175\\t%C3%81lvaro_Morata\\t20160501:1157\\t20160502:1240\\t20160503:1034\\t20160504:1238\\t20160505:1759\\t20160506:1261\\t20160507:885\\t20160508:1013\\t20160509:1155\\t20160510:1181\\t20160511:1493\\t20160512:3472\\t20160513:3993\\t20160514:3762\\t20160515:2253\\t20160516:2209\\t20160517:5694\\t20160518:9451\\t20160519:6747\\t20160520:4369\\t20160521:10302\\t20160522:10663\\t20160523:11454\\t20160524:9558\\t20160525:8111\\t20160526:9318\\t20160527:5517\\t20160528:4192\\t20160529:3246\\t20160530:3215\\t20160531:8233"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a mapper and a reducer function to meet your own needs\n",
    "  Things we need to remember:\n",
    "  * mapper and reducer are seperate python files which should include shellbang explicitly.\n",
    "  * mapper and reducer read in input from stdin and give output through stdout.\n",
    "  * mapper should organize output in a format as key,value pairs and every reducer will treat the first element of mapper's output, split by your customized delimiter as the key.\n",
    "  * there is only one key in each reducer.  \n",
    "  \n",
    "Example mapper and reducer python code are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os, sys\n",
    "\n",
    "def dataFilter(line):\n",
    "    # prefix_blacklist is a set which stores all prefixes that should be excluded\n",
    "    prefix_blacklist = set([x.lower() for x in [\n",
    "        'Media:', 'Special:', 'Talk:', 'User:', 'User_talk:', 'Wikipedia:', 'Wikipedia_talk:', 'File:',\n",
    "        'File_talk:', 'MediaWiki:', 'MediaWiki_talk:', 'Template:', 'Template_talk:', 'Help:',\n",
    "        'Help_talk:', 'Category:', 'Category_talk:', 'Portal:', 'Portal_talk:', 'Book:', 'Book_talk:',\n",
    "        'Draft:', 'Draft_talk:', 'Education_Program:', 'Education_Program_talk:', 'TimedText:',\n",
    "        'TimedText_talk:', 'Module:', 'Module_talk:', 'Gadget:', 'Gadget_talk:', 'Gadget_definition:',\n",
    "        'Gadget_definition_talk:', 'Topic:']])\n",
    "    # suffix_blacklist is a set which stores all suffixes that should be excluded\n",
    "    suffix_blacklist = {'.png', '.gif', '.jpg', '.jpeg', '.tiff', '.tif', '.xcf', '.mid', '.ogg', '.ogv', '.svg',\n",
    "                        '.djvu', '.oga', '.flac', '.opus', '.wav', '.webm', '.ico', '.txt'}\n",
    "    # bad_pages is a set which stoes all bad pags that should be excluded\n",
    "    bad_pages = {'404_error/', 'Main_Page', 'Hypertext_Transfer_Protocol', 'Search'}\n",
    "    field = line.strip().split(' ')\n",
    "    # rule 1: four attributes\n",
    "    if len(field) == 4:\n",
    "        # rule 2: title should start with 'en' or 'en.m'\n",
    "        if field[0] in ['en', 'en.m'] and field[1]:\n",
    "            # rule 3: '%3a' and '%3A' in the title should be excluded\n",
    "            prefix, suffix = field[1].replace('%3a', ':').replace('%3A', ':').lower().split(':')[0], \\\n",
    "                             field[1].lower().split('.')[-1]\n",
    "            # rule 4: first letter in title should not be lowercase\n",
    "            # rule 5: exclude all invalid patterns\n",
    "            if not field[1][0].islower() and field[1] not in bad_pages \\\n",
    "                    and prefix + ':' not in prefix_blacklist and '.' + suffix not in suffix_blacklist:\n",
    "                value = int(field[2])\n",
    "                # rule 6: add the page counts of 'en' and 'en.m' together when their titles are the same\n",
    "                return field[1], field[2]\n",
    "    return (None, None)\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    article_name, hour_page_count = dataFilter(line)\n",
    "    if article_name and hour_page_count:\n",
    "        input_file = os.environ[\"mapreduce_map_input_file\"]\n",
    "        #input_file = 'pagecounts-20160502-000000'\n",
    "        # add date as an attribute into mapper\n",
    "        date = input_file.split('-')[-2]\n",
    "        print '%s\\t%s\\t%s' % (article_name, date, hour_page_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "# print_month_article(article_name, day_count, month_now) is used to print a formatted string.\n",
    "def print_month_article(article_name, day_counts, month_now):\n",
    "    output = ''\n",
    "    # add everyday's page count into month\n",
    "    month_page_count = sum(day_counts)\n",
    "    if month_page_count > 100000:\n",
    "        output += '%d\\t%s' % (month_page_count, article_name)\n",
    "        for day, day_count in enumerate(day_counts):\n",
    "            output += '\\t%s%02d:%d' % (month_now, day + 1, day_count)\n",
    "        print output\n",
    "\n",
    "\n",
    "current_article = ''\n",
    "month_now = '201605'\n",
    "day_counts = [0] * 31\n",
    "for line in sys.stdin:\n",
    "    article_name, date, hour_page_count_s = line.strip().split('\\t')\n",
    "    hour_page_count_i = int(hour_page_count_s)\n",
    "    day_now = int(date[-2:])\n",
    "    # month_now = int(date[:-2])\n",
    "    # add every hour's page count into day\n",
    "    if current_article == article_name:\n",
    "        day_counts[day_now - 1] += hour_page_count_i\n",
    "    else:\n",
    "        if current_article:\n",
    "            print_month_article(current_article, day_counts, month_now)\n",
    "        day_counts = [0] * 31\n",
    "        day_counts[day_now - 1] += hour_page_count_i\n",
    "        current_article = article_name\n",
    "else:\n",
    "    print_month_article(current_article, day_counts, month_now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch an EMR service\n",
    "  After creating a connection to EMR, the next thing is to create one or more jobflows steps. There are two kinds of steps: streaming and custom jar. We will use the first step here since it is already configured, easy to use and we just have to set up several file path.\n",
    "  There are five arguments which are extremely important to make step running correctly.\n",
    "  * step_name: as a name identifier.\n",
    "  * mapper: mapper file name with extension.\n",
    "  * reducer: reducer file name with extension.\n",
    "  * input: input dataset location.\n",
    "  * output: output result location.\n",
    "  * file_s3_location: specify the mapper and reducer location on s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<boto.emr.emrobject.Cluster object at 0x106cce510>\n",
      "PENDING\n",
      "PENDING\n",
      "PENDING\n",
      "PENDING\n",
      "PENDING\n",
      "PENDING\n",
      "PENDING\n",
      "PENDING\n",
      "[0] Your emr launched seccussfully!\n"
     ]
    }
   ],
   "source": [
    "def EMR_launch(emr_conn, step_name, mapper, reducer, input, output, job_name, log_uri, file_s3_location):\n",
    "    instance_groups = []\n",
    "    # feel free to change the num_instance, type and name here\n",
    "    instance_groups.append(InstanceGroup(num_instances=1, role=\"MASTER\", type=\"m1.medium\", market=\"ON_DEMAND\", name=\"Main node\"))\n",
    "    instance_groups.append(InstanceGroup(num_instances=2, role=\"CORE\", type=\"m1.medium\", market=\"ON_DEMAND\", name=\"Worker nodes\"))\n",
    "    #\n",
    "    step = StreamingStep(name=step_name, mapper=mapper, reducer=reducer, input=input, output=output,jar='command-runner.jar',step_args=list(file_s3_location))\n",
    "    api_params = {\n",
    "    'ReleaseLabel': 'emr-5.0.3',\n",
    "    #'Instances.Ec2SubnetId': 'subnet-6a58b740',\n",
    "    }\n",
    "    jobid = emr_conn.run_jobflow(name=job_name, log_uri=log_uri,steps=[step], instance_groups=instance_groups, job_flow_role=\"EMR_EC2_DefaultRole\",\n",
    "    service_role=\"EMR_DefaultRole\", api_params=api_params)\n",
    "    print emr_conn.describe_cluster(jobid)\n",
    "    while emr_conn.list_steps(jobid).steps[0].status.state != 'RUNNING':\n",
    "        print emr_conn.list_steps(jobid).steps[0].status.state\n",
    "        time.sleep(60)\n",
    "    print '[0] Your emr launched seccussfully!'\n",
    "\n",
    "EMR_launch(emr_conn, 'Wikipedia pagecount example', 'mapper.py', 'reducer.py',\n",
    "           's3://cmucc-datasets/wikipediatraf/201605', 's3://yuki777/output1','My jobflow', 's3://yuki777/log', \"hadoop-streaming\", \"-files\", \"s3://yuki777/mapper.py,s3://yuki777/reducer.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing the message showing your emr service is running, we can go to aws console or use aws cli to check its status. If your task completed successfully, you could see the output files lying in the output path you specified eariler. Here are two screenshots here to give you a quick view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.call([\"open\", \"result.jpg\"])\n",
    "subprocess.call([\"open\",\"result1.jpg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you enjoy this tutorial!  \n",
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
