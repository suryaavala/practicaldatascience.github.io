{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To Keras\n",
    "\n",
    "\n",
    "This tutorial will introduce you a hig-level neural networks library(Keras), written in Python and capable of running on top of either TensorFlow or Theano. It's userful for researchers to go from idea to result with the least possible delay.\n",
    "\n",
    "Here are some fancy features of Keras:\n",
    "> - Allows for easy and fast prototyping (through total **modularity, minimalism, and extensibility**)\n",
    "- Supports both convolutional networks and recurrent networks, as well as combinations of two\n",
    "- Supports arbitrary connectivity schemes \n",
    "- Runs seamlessly on CPU and GPU\n",
    "\n",
    "\n",
    "Let's go through these Keras features with more details:\n",
    "- **Modularity**. A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as little restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models.\n",
    "- **Minimalism**. Each module should be kept short and simple. Every piece of code should be transparent upon first reading. No black magic: it hurts iteration speed and ability to innovate.\n",
    "- **Easy extensibility**. New modules are dead simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.\n",
    "- **Work with Python**. No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.\n",
    "\n",
    "Keras is compatible with **Python 2.7-3.5**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "\n",
    "We will lead you to get started with Keras through a good example to show you how Keras works. Once you get the general idea, you can follow the instuctions to begin your first program with Keras to solve a hand-writting recognition problem. You should get familiar with some common techniques to optimizer your deep learning model.\n",
    "\n",
    "1. [Installing the Libraries](#Installing-the-Libraries)\n",
    "0.background 1.example 2.code 3.graph (no need to run)\n",
    "2. [Getting Started in 5 Minutes](#Getting-Started-in-5-Minutes)\n",
    "3. [Create Model for Digit Recognition](#Create-Model-for-Digit-Recognition)\n",
    "4. [Simple Convolutional Neural Network for MNIST](#Simple-Convolutional-Neural-Network-for-MNIST) \n",
    "5. [Further Topics](#Further-Topics)\n",
    "6. [Summary](#Summary)\n",
    "7. [References](#References)\n",
    "\n",
    "If you want to know more, read the complete documentation at [Keras.io](https://keras.io/). But just spend only **30 seconds** to get started with Keras!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras uses the following packages:\n",
    "- keras\n",
    "- numpy, scipy\n",
    "- pyyaml\n",
    "\n",
    "Use your most familiar way to install these Python packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started in 5 Minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data structure of Keras is a model, a way to organize layers. The main type of model is the Sequential model, a linear stack of layers.\n",
    "\n",
    "Here is a simple example of `Sequential` model, but no data is ready to run here(we will run real data in next part):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, this is only an example of Keras model and no data to run here.\n",
      "You may see the results in the next parts.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(output_dim=60, input_dim=784))\n",
    "model.add(Activation(\"relu\")) \n",
    "model.add(Dense(output_dim=1))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# The training data and testing data are assumed to be assigned values before\n",
    "try:\n",
    "    model.fit(X_train, Y_train, nb_epoch=5, batch_size=1)\n",
    "    #model.train_on_batch(X_batch, Y_batch)\n",
    "\n",
    "    loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "    classes = model.predict_classes(X_test, batch_size=32)\n",
    "    #proba = model.predict_proba(X_test, batch_size=32)\n",
    "except:\n",
    "    print 'Hi, this is only an example of Keras model and no data to run here.'\n",
    "    print 'You may see the results in the next parts.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Sequential Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of models available in Keras: \n",
    "- **Sequential model**\n",
    "- **Model class used with functional API** ï¼ˆmore complex models, such as multi-output models, directed acyclic graphs, or models with shared layers).\n",
    "\n",
    "The Sequential model is a linear stack of layers.\n",
    "You can create a Sequential model by passing a list of layer instances to the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking layers: use `.add()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.add(Dense(output_dim=64, input_dim=100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=10))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the learning process with `compile()` once your model looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments for `complie`\n",
    "- optimizer: optimizer object.\n",
    "- loss: objective function. \n",
    "- metrics: list of metrics to be evaluated by the model during training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further configure your optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit your model with training data in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)\n",
    "except:\n",
    "    # No data ready for here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments for `fit`\n",
    "- batch_size: integer. Number of samples per gradient update.\n",
    "- nb_epoch: integer, the number of epochs to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively, you may feed batches to your model manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model.train_on_batch(X_batch, Y_batch)\n",
    "except:\n",
    "    # No data ready for here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate your performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "except:\n",
    "    # No data ready for here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument for `evaluate`:\n",
    "\n",
    "- batch_size: integer. Number of samples per gradient update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate predictions on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    classes = model.predict_classes(X_test, batch_size=32)\n",
    "    proba = model.predict_proba(X_test, batch_size=32)\n",
    "except:\n",
    "    # No data ready for here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates output predictions for the input samples, processing the samples in a batched way.\n",
    "\n",
    "Arguments for `predict_classes(or predict_proba)`:\n",
    "- batch_size: integer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model for Digit Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've installed and loaded the libraries, let's solve the first popular problem of **MNIST Handwritten Digit Recognition**.\n",
    "\n",
    "The MNIST problem is a dataset developed by Yann LeCun, Corinna Cortes and Christopher Burges for evaluating machine learning models on the handwritten digit classification problem.\n",
    "\n",
    "Images of digits were taken from a variety of scanned documents, normalized in size and centered. This makes it an excellent dataset for evaluating models, allowing the developer to focus on the machine learning with very little data cleaning or preparation required.\n",
    "\n",
    "Each image is a 28 by 28 pixel square (784 pixels total). A standard spit of the dataset is used to evaluate and compare models, where 60,000 images are used to train a model and a separate set of 10,000 images are used to test it.\n",
    "\n",
    "Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\n",
    "\n",
    "<img src=\"http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2016/05/Examples-from-the-MNIST-dataset.png\" width=700>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model with Multi-Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get very good results using a very simple neural network model with a single hidden layer. \n",
    "\n",
    "In this section we will create a simple multi-layer perceptron model that achieves an error rate of 1.74%. We will use this as a baseline for comparing more complex convolutional neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all package we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the results of our script are reproducible, we use constant random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the MNIST dataset in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the function `mnist.load_data()`\n",
    "- Return:\n",
    "\n",
    "    - 2 tuples:\n",
    "        - X_train, X_test: uint8 array of grayscale image data with shape (nb_samples, 28, 28).\n",
    "        - y_train, y_test: uint8 array of digit labels (integers in range 0-9) with shape (nb_samples,)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten the images pixels into a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is structured as a **3-dimensional** array of instance, image width and image height. For a multi-layer perceptron model we must **reduce the images down into a vector of pixels**. In this case the 28Ã—28 sized images will be 784 pixel input values.\n",
    "\n",
    "We can do this transform easily using the reshape() function on the NumPy array. We can also reduce our memory requirements by forcing the precision of the pixel values to be 32 bit, the default precision used by Keras anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize grey value for each pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel values are gray scale between 0 and 255. It is almost always a good idea to perform some scaling of input values when using neural network models. We can very quickly normalize the pixel values to the range 0 and 1 by dividing each value by the maximum of 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding of multi-class values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output variable is an integer from 0 to 9. This is a multi-class classification problem. As such, it is good practice to use a **one hot encoding** of the class values, transforming the vector of class integers into a binary matrix.\n",
    "\n",
    "We can easily do this using the built-in np_utils.to_categorical() helper function in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, init='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a simple neural network with one hidden layer with the same number of neurons as there are inputs (784). A rectifier activation function is used for the neurons in the hidden layer.\n",
    "\n",
    "A softmax activation function is used on the output layer to turn the outputs into probability-like values and allow one class of the 10 to be selected as the modelâ€™s output prediction. \n",
    "\n",
    "Logarithmic loss is used as the loss function (called categorical_crossentropy in Keras) and the efficient ADAM gradient descent algorithm is used to learn the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit and evaluate the model. The model is fit over 10 epochs with updates every 200 images. The test data is used as the validation dataset, allowing you to see the skill of the model as it trains. A verbose value of 2 is used to reduce the output to one line for each training epoch.\n",
    "\n",
    "Finally, the test dataset is used to evaluate the model and a classification error rate is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "4s - loss: 0.2791 - acc: 0.9203 - val_loss: 0.1421 - val_acc: 0.9577\n",
      "Epoch 2/10\n",
      "5s - loss: 0.1122 - acc: 0.9678 - val_loss: 0.0996 - val_acc: 0.9696\n",
      "Epoch 3/10\n",
      "6s - loss: 0.0724 - acc: 0.9790 - val_loss: 0.0784 - val_acc: 0.9749\n",
      "Epoch 4/10\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model \n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example might take a few minutes when run on a CPU (you may adjust the nb_epoch for shorter time). You should see the output below. This very simple network defined in very few lines of code achieves a respectable error rate of 1.74%."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "Train on 60000 samples, validate on 10000 samples\n",
    "Epoch 1/10\n",
    "11s - loss: 0.2791 - acc: 0.9203 - val_loss: 0.1422 - val_acc: 0.9583\n",
    "Epoch 2/10\n",
    "11s - loss: 0.1121 - acc: 0.9680 - val_loss: 0.0994 - val_acc: 0.9697\n",
    "Epoch 3/10\n",
    "12s - loss: 0.0724 - acc: 0.9790 - val_loss: 0.0786 - val_acc: 0.9748\n",
    "Epoch 4/10\n",
    "12s - loss: 0.0508 - acc: 0.9856 - val_loss: 0.0790 - val_acc: 0.9762\n",
    "Epoch 5/10\n",
    "12s - loss: 0.0365 - acc: 0.9897 - val_loss: 0.0631 - val_acc: 0.9795\n",
    "Epoch 6/10\n",
    "12s - loss: 0.0263 - acc: 0.9931 - val_loss: 0.0644 - val_acc: 0.9798\n",
    "Epoch 7/10\n",
    "12s - loss: 0.0188 - acc: 0.9956 - val_loss: 0.0613 - val_acc: 0.9803\n",
    "Epoch 8/10\n",
    "12s - loss: 0.0149 - acc: 0.9967 - val_loss: 0.0628 - val_acc: 0.9814\n",
    "Epoch 9/10\n",
    "12s - loss: 0.0108 - acc: 0.9980 - val_loss: 0.0595 - val_acc: 0.9816\n",
    "Epoch 10/10\n",
    "12s - loss: 0.0072 - acc: 0.9989 - val_loss: 0.0577 - val_acc: 0.9826\n",
    "Baseline Error: 1.74%\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Convolutional Neural Network for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try a relatiely advanced model-Convolutional Neural Network for MNIST.\n",
    "\n",
    "If you are not familiar with Convolutional Neural Network(CNN), here's a general introduction for it: [CNN Introduction](http://cs231n.github.io/convolutional-networks/#overview)\n",
    "\n",
    "Here are an example of CNN (not relevant to our codes and problem to solve):\n",
    "\n",
    "<img src=\"https://www.ais.uni-bonn.de/deep_learning/images/Convolutional_NN.jpg\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNN_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks are more complex than standard multi-layer perceptrons, so we will start by using a simple structure to begin with that uses all of the elements for state of the art results. Below summarizes the network architecture.\n",
    "\n",
    "1. The first hidden layer is a convolutional layer called a Convolution2D. The layer has 32 feature maps, which with the size of 5Ã—5 and a rectifier activation function. This is the input layer, expecting images with the structure outline above [pixels][width][height].\n",
    "\n",
    "2. Next we define a pooling layer that takes the max called MaxPooling2D. It is configured with a pool size of 2Ã—2.\n",
    "\n",
    "3. The next layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 20% of neurons in the layer in order to reduce overfitting.\n",
    "\n",
    "4. Next is a layer that converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by standard fully connected layers.\n",
    "\n",
    "5. Next a fully connected layer with 128 neurons and rectifier activation function.\n",
    "Finally, the output layer has 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class.\n",
    "\n",
    "6. As before, the model is trained using logarithmic loss and the ADAM gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model the same way as before with the multi-layer perceptron. The CNN is fit over 10 epochs with a batch size of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = CNN_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, the accuracy on the training and validation test is printed each epoch and at the end of the classification error rate is printed.\n",
    "\n",
    "Epochs may take 60 to 90 seconds to run on the CPU, or about 15 minutes in total depending on your hardware. You can see that the network achieves an error rate of 1.10, which is better than our simple multi-layer perceptron model above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "Train on 60000 samples, validate on 10000 samples\n",
    "Epoch 1/10\n",
    "84s - loss: 0.2065 - acc: 0.9370 - val_loss: 0.0759 - val_acc: 0.9756\n",
    "Epoch 2/10\n",
    "84s - loss: 0.0644 - acc: 0.9802 - val_loss: 0.0475 - val_acc: 0.9837\n",
    "Epoch 3/10\n",
    "89s - loss: 0.0447 - acc: 0.9864 - val_loss: 0.0402 - val_acc: 0.9877\n",
    "Epoch 4/10\n",
    "88s - loss: 0.0346 - acc: 0.9891 - val_loss: 0.0358 - val_acc: 0.9881\n",
    "Epoch 5/10\n",
    "89s - loss: 0.0271 - acc: 0.9913 - val_loss: 0.0342 - val_acc: 0.9891\n",
    "Epoch 6/10\n",
    "89s - loss: 0.0210 - acc: 0.9933 - val_loss: 0.0391 - val_acc: 0.9880\n",
    "Epoch 7/10\n",
    "89s - loss: 0.0182 - acc: 0.9943 - val_loss: 0.0345 - val_acc: 0.9887\n",
    "Epoch 8/10\n",
    "89s - loss: 0.0142 - acc: 0.9956 - val_loss: 0.0323 - val_acc: 0.9904\n",
    "Epoch 9/10\n",
    "88s - loss: 0.0120 - acc: 0.9961 - val_loss: 0.0343 - val_acc: 0.9901\n",
    "Epoch 10/10\n",
    "89s - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0353 - val_acc: 0.9890\n",
    "Classification Error: 1.10%\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [How to Build Complex Models](https://keras.io/getting-started/functional-api-guide/)\n",
    "- [How to Save Keras Model You Trained](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)\n",
    "- [How to Run It on GPU](https://keras.io/getting-started/faq/#how-can-i-run-keras-on-gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Keras** is a very popular and useful Python package for deep learning, especially for research. You will fall into the love for it because of the high efficiency for development. In addition, if you have the access to GPU, you can easily run your code with simple configuration and commands.\n",
    "\n",
    "Keras documentation is very clear and easy to read. This tutorial is just a simple start for you, so you may have a closer look at the original documentation if you have interest at it.\n",
    "\n",
    "Good luck for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Keras Documentation: https://keras.io/\n",
    "2. Handwritten Digit Recognition using Convolutional Neural Networks in Python with Keras: http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
    "3. Convolutional Neural Networks: http://cs231n.github.io/convolutional-networks/#overview"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
