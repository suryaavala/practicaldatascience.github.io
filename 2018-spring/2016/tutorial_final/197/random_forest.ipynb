{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Random Forest\n",
    "\n",
    "## Introduction\n",
    "Random forests (also called random decision forests) construct multiple decision trees at training time. The output of a random forest is often the mode class of individual trees when it is a classification problem, or an average of prediction of individual trees when it is a regression problem. One major advantage of random forests is that they can correct the overfitting problem random trees suffer from. (Reference: https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "During this tutorial we will first introduce the notion of entropy and mutual information, as a prerequisite for random trees. After that we are going to implement a random tree class that can grow on some training data recursively, based on a split method called ID3, which we will explain later. Finally we are going to use our random tree class to construct our random forest.\n",
    "\n",
    "## Entropy and mutual information\n",
    "### Entropy\n",
    "The entropy of a distribution is the expected amount of information we get when we observe a possible outcome of the distribution. It is used to evaluate the uncertainty of the distribution. But how can we measure how much information we get when we observe a possible outcome? A intuitive answer is that the more unlikely the outcome happens, the more information we get. To be specific, we have the following definition (After [Abramson 63]):\n",
    "\n",
    "Let $E$ be some event which occurs with probability $P(E)$, if we are told that $E$ has occurred, then we sat that we have received \n",
    "\n",
    "$Info(E) = log_2\\frac{1}{P(E)}$\n",
    "\n",
    "bits of information.\n",
    "\n",
    "Complete the following function to calculate the bits of information we received when we are told that some event with probability $p$ occurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def Info(p):\n",
    "    \"\"\"given the probability of some event, return the bits of information we receives if it occurs\n",
    "        \n",
    "    Args:\n",
    "        p(float): the probability of some event occurs\n",
    "        \n",
    "    Return:\n",
    "        (float): the bits of information we receive\n",
    "    \"\"\"\n",
    "    \n",
    "    return math.log(1/p, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of a distribution $D$, denoted by $H(D)$, is simply the expected amount of information we get when we get a possible outcome from the distribution. It is given by the following equation:\n",
    "\n",
    "$H(D) = \\sum_{E \\in D} P(E)I(E)$\n",
    "\n",
    "Complete the following function to calculate the entropy of a discrete distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def H(p):\n",
    "    \"\"\"given a discrete probability distribution, return its entropy\n",
    "    Args:\n",
    "        p(list of float): the list of probabilities that each event in the distribution occurs with\n",
    "        \n",
    "    Return:\n",
    "        (float): the entropy of this distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    entropy_sum = 0\n",
    "    for event in p:\n",
    "        if event > 0:\n",
    "            entropy_sum += event * Info(event)\n",
    "    return entropy_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.58496250072\n"
     ]
    }
   ],
   "source": [
    "# Simple examples to test your code:\n",
    "# The entropy of a fair coin should be 1.0\n",
    "print H([0.5, 0.5])\n",
    "# The entropy of a fair dice should be 2.58\n",
    "print H([1.0/6] * 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "### Mutual Information\n",
    "To illustreate what is mutual information, let's look at an example first:\n",
    "Suppose the two variables, $gosports$ and $weather$ have the following joint distribution:\n",
    "\n",
    "\n",
    "|             | weather(sunny) | weather(cloudy) | weather(rainy) |\n",
    "|-----------|-------------|\n",
    "|gosports(yes) |     0.3        |      0.2        |       0.1      |\n",
    "|gosports(no)  |     0.1        |      0.1        |       0.2      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.970950594455\n",
      "0.811278124459\n",
      "0.918295834054\n",
      "0.918295834054\n",
      "0.875488750216\n",
      "0.0954618442383\n"
     ]
    }
   ],
   "source": [
    "# Calculate the entropy of gosports(should be 0.97)\n",
    "entropy_sports = H([0.3 + 0.2 + 0.1, 0.1 + 0.1 + 0.2])\n",
    "print entropy_sports\n",
    "\n",
    "# Calculate the entropy of gosports conditioned on weather = sunny, cloudy, and rainy (should be 0.81, 0.92, 0.92 respectively)\n",
    "entropy_sports_sunny = H([0.3 / 0.4, 0.1 / 0.4])\n",
    "print entropy_sports_sunny\n",
    "entropy_sports_cloudy = H([0.2 / 0.3, 0.1 / 0.3])\n",
    "print entropy_sports_cloudy\n",
    "entropy_sports_rainy = H([0.1 / 0.3, 0.2 / 0.3])\n",
    "print entropy_sports_rainy\n",
    "\n",
    "# Calculated the expected entropy of gosports if we are told weather(should be 0.88)\n",
    "entropy_sports_weather =  entropy_sports_sunny * 0.4 + entropy_sports_cloudy * 0.3 + entropy_sports_rainy * 0.3\n",
    "print entropy_sports_weather\n",
    "\n",
    "# Calculate the expected reduced entropy of gosports if we know weather(should be 0.095)\n",
    "entropy_reduced = entropy_sports - entropy_sports_weather\n",
    "print entropy_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the example, we know that if we are told the information about weather, the entropy (uncertainty) of gosports will reduce by 0.095. We call the reduced entropy of distribution X given distribution Y the $mutual$ $information$ between X and Y, denoted by I(X, Y).\n",
    "\n",
    "Formally, mutual information can be calculated as:\n",
    "\n",
    "$I(X, Y) = H(X) - H(X|Y)$\n",
    "\n",
    "where $H(X|Y)$ is just a short hand for $E_Y[H(X|Y = y)]$\n",
    "\n",
    "complete the following function to calculate mutual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def I(joint_dist):\n",
    "    \"\"\"given the joint distribution of two variables, calculate the mutual information between them\n",
    "    Agrs:\n",
    "        joint_dist(list of list of float): the joint distribution between two variables, for example, \n",
    "        for the example above joint_dist = [[0.3, 0.2, 0.1], [0.1, 0.1, 0.2]]\n",
    "        \n",
    "    Return:\n",
    "        (float) the mutual information between these two variables:\n",
    "    \"\"\"\n",
    "    m = len(joint_dist)\n",
    "    if m <= 0:\n",
    "        return -1;\n",
    "    n = len(joint_dist[0])\n",
    "    probs = []\n",
    "    for i in range(m):\n",
    "        probs.append(sum(joint_dist[i]))\n",
    "    H_total = H(probs)\n",
    "    H_reduced = 0.0\n",
    "    for j in range(n):\n",
    "        total_prob = 0.0\n",
    "        probs = []\n",
    "        for i in range(m):\n",
    "            total_prob += joint_dist[i][j]\n",
    "            probs.append(joint_dist[i][j])\n",
    "        for i in range(m):\n",
    "            probs[i] /= total_prob\n",
    "        H_reduced += total_prob * H(probs)\n",
    "    return H_total - H_reduced\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0954618442383\n",
      "0.0954618442383\n"
     ]
    }
   ],
   "source": [
    "# result should be 0.09546\n",
    "print I([[0.3, 0.2, 0.1], [0.1, 0.1, 0.2]])\n",
    "\n",
    "#result should also be 0.09546\n",
    "print I([[0.3, 0.1], [0.2, 0.1], [0.1, 0.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "Consider the following dataset. Each row represents a single training data.\n",
    "\n",
    "| outlook | humidity | wind | play sports? |\n",
    "|--------|--------|--------|--------|\n",
    "| overcast | high | strong | yes|\n",
    "| overcast | normal | weak | yes |\n",
    "| overcast | high | weak | yes |\n",
    "| overcast | normal | strong | yes |\n",
    "| sunny | high | strong | no |\n",
    "| sunny | normal | weak | yes |\n",
    "| rain | high | strong | no |\n",
    "| rain | normal | weak | yes|\n",
    "\n",
    "Based on the training data above, we what to predict weather play sports is yes or no based on the information of outlook, humidity, and wind. After a close look at the dataset, we may find that, if the outlook is overcast, then play sports is yes. Otherwise, it also depends on other attributes. To be specific, if outlook is sunny, play sports = yes iff humidity is normal. if outlook is rain, play sports = yes iff wind = strong.\n",
    "\n",
    "We can express the if-else process above as a tree as follow:\n",
    "\n",
    "                                               outlook\n",
    "                                              /   |   \\\n",
    "                                             /    |    \\\n",
    "                                        sunny overcast rain\n",
    "                                          |       |     |\n",
    "                                     humidity    yes   wind\n",
    "                                     /     \\          /    \\\n",
    "                                   high   normal   strong  weak\n",
    "                                    |       |        |      |\n",
    "                                    no     yes      no      yes\n",
    "This is a simple example of decision tree.\n",
    "Please note the the tree above is the not only tree to be consistent with our training dataset.\n",
    "In general, in a decision tree, at each note we look at one of the attributes, and partition our dataset according to different values taken on this attribute, thus splitting our dataset. When a split dataset contains only one kind of label, the process is stopped. Otherwise we select new attributes and partitions out dataset recursively until the data in the same dataset are all of the same label, or some other stop condition is reached.\n",
    "\n",
    "However, in the process above there is a center problem remains unsolved: when we need to split our dataset, which attribute should we use? One common approach is to select the attribute that can reduce the uncertainty(entropy) of training data, which is also called the ID3 method. In other words:\n",
    "\n",
    "1. View each attribute as a distribution. For example, in the dataset above, there are 4 overcast, 2 sunny, and 2 rain. Then the distribution of outlook is [0.5, 0.25, 0.25].\n",
    "\n",
    "2. Select the attribute(distribution) that has highest mutual information with the label(which can also be view as a distribution). \n",
    "\n",
    "3. Use the selected attribute to participate current dataset. Do recursion if necessary.\n",
    "\n",
    "Complete the following class of DecisionTreeNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionTreeNode():\n",
    "    \n",
    "    def __init__(self, X, Y, used_attr_num = 0):\n",
    "        \"\"\"\n",
    "        return the tree node on dataset (X, Y). Build it children recursively.\n",
    "        pick up the attribute which has maximum mutual information with our label Y, and use that attribute to split \n",
    "        data into several classes, each class corresponding to one possible value of that attribute.\n",
    "        The stopping condition is that a node contains all of the same label.\n",
    "        If there are multiple attributes with the same mutual information, pick up the one with smallest index\n",
    "        \n",
    "        args:\n",
    "            X(list of list of integer): X[i][j] is the value of the jth attribute of the ith training data\n",
    "            For any given attribute, we encode the set of possible values it can take as 0, 1, 2, ...\n",
    "            \n",
    "            Y(list of integer): Y[i] is the label of the ith training data. We encode the set of possible values\n",
    "            as 0, 1, 2, ...\n",
    "            \n",
    "            For example, for the outlook/humidity/rain/play sports dataset above:\n",
    "            X = [[0, 0, 0],\n",
    "                [0, 1, 1],\n",
    "                [0, 0, 1],\n",
    "                [0, 1, 0],\n",
    "                [1, 0, 0],\n",
    "                [1, 1, 1],\n",
    "                [2, 0, 0],\n",
    "                [2, 1, 1]]\n",
    "            Y = [0, 0, 0, 0, 1, 0, 1, 0]\n",
    "            constrain: (X, Y) should represent at least one sample\n",
    "            \n",
    "            used_attr_num(integer): number of used attributes: if all of the attributes has been used but the dataset\n",
    "            is not composed of same label(which means there are conflicting data), predict the label as majority and\n",
    "            stop recursion\n",
    "            \n",
    "        recommended members:\n",
    "            self.label (None or integer): if the node is leaf, it has a non None label indicating its label\n",
    "            self.attr_index (integer, defined when self.labe is not None): the index of the splitting attribute\n",
    "            for this ndoe\n",
    "            self.child (dict, mapping from label for child nodes): used to find the next child if this node is not \n",
    "            leaf\n",
    "        \"\"\"\n",
    "        if len(set(Y)) <= 1:\n",
    "            self.label = Y[0];\n",
    "            return\n",
    "        if used_attr_num >= len(X[0]):\n",
    "            #attrs has been used up\n",
    "            self.label = max(set(Y), key=Y.count)\n",
    "            return\n",
    "        self.label = None\n",
    "        #select a proper attribute as the split attribute for this Node\n",
    "        sample_num = len(X)\n",
    "        attr_num = len(X[0])\n",
    "        max_mutual_info = 0\n",
    "        opt_attr_index = -1\n",
    "        for attr_index in range(attr_num):\n",
    "            d = {}\n",
    "            for sample_index in range(sample_num):\n",
    "                attr = X[sample_index][attr_index]\n",
    "                label = Y[sample_index]\n",
    "                if not attr in d:\n",
    "                    d[attr] = {}\n",
    "                if not label in d[attr]:\n",
    "                    d[attr][label] = 0\n",
    "                d[attr][label] += 1\n",
    "            joint_dist = []\n",
    "            for attr in d:\n",
    "                row_dist = []\n",
    "                for label in set(Y):\n",
    "                    if not label in d[attr]:\n",
    "                        row_dist.append(0.0)\n",
    "                    else:\n",
    "                        row_dist.append(float(d[attr][label]) / sample_num)\n",
    "                joint_dist.append(row_dist)\n",
    "            mutual_info = I(joint_dist)\n",
    "            if mutual_info > max_mutual_info:\n",
    "                max_mutual_info = mutual_info\n",
    "                opt_attr_index = attr_index\n",
    "        #opt_attr_index is selected, split child based on the index\n",
    "        self.attr_index = opt_attr_index\n",
    "        self.child = {}\n",
    "        attrToData = {}\n",
    "        for sample_index in range(sample_num):\n",
    "            attr = X[sample_index][self.attr_index]\n",
    "            if not attr in attrToData:\n",
    "                attrToData[attr] = [[],[]]; #map attr to [X, Y] pair for child\n",
    "            attrToData[attr][0].append(X[sample_index])\n",
    "            attrToData[attr][1].append(Y[sample_index])\n",
    "            \n",
    "        for attr in attrToData:\n",
    "            self.child[attr] = DecisionTreeNode(attrToData[attr][0], attrToData[attr][1], used_attr_num + 1)\n",
    "            \n",
    "    def printNode(self, depth = 0, attr_names = None, attr_values = None, output_names = None):\n",
    "        \"\"\"\n",
    "        print the tree recursively, mainly used for debug\n",
    "        args:\n",
    "            depth: (int) the depth of current node, depth of root is zero\n",
    "            attr_names: a list of string, denoting the name for each attribute\n",
    "            attr_values: a list of list of string, attr_values[i][j] is the name of the ith attribute, jth value\n",
    "            output_names: a list of string, denoting the name for each kind of output\n",
    "        \"\"\"\n",
    "        if self.label is not None:\n",
    "            print '\\t' * depth,\n",
    "            if output_names is None:\n",
    "                print 'label = ' + str(self.label)\n",
    "            else:\n",
    "                print 'label = ' + output_names[self.label]\n",
    "        else:\n",
    "            for attr in self.child:\n",
    "                print '\\t' * depth,\n",
    "                if attr_names is None or attr_values is None:\n",
    "                    print 'attr[' + str(self.attr_index) + '] = ' + str(attr) + ':'\n",
    "                else:\n",
    "                    print attr_names[self.attr_index] + ' = ' + attr_values[self.attr_index][attr] + ':'\n",
    "                self.child[attr].printNode(depth + 1, attr_names, attr_values, output_names)\n",
    "                \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        given a input data x, predict its label according to the tree.\n",
    "        implement it recursively\n",
    "        if some value of the splitting attribute has never been seen by the node, return None\n",
    "        args:\n",
    "            x (list of integers) an input sample:\n",
    "        return:\n",
    "            label(int) the predicted label of this input\n",
    "        \"\"\"\n",
    "        if self.label is not None:\n",
    "            return self.label\n",
    "        try:\n",
    "            attr = x[self.attr_index]\n",
    "            return self.child[attr].predict(x)\n",
    "        except KeyError:\n",
    "            return None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Construct Decision Tree Node\n",
    "In this subsection of decision tree you will implement the construction function of node. Please follow the following specifications:\n",
    "1. If the dataset for the current node is composed of the same label, stop recursion and use that label as the label for this tree node\n",
    "2. If the dataset for the current node has at least two kinds of labels, you should split this label using some attribute(feature). The selection of feature should follow the following principle: choose the attribute that has the maximum mutual information with current labels. If multiple attributes have the same mutual information, choose the attribute with the smallest index. In terms of minimizing the depth of the tree, this may not be the optimum solution, but generally speaking it works well. Actually calculating the tree with minimum depth is NP-hard. Thus it is almost impossible to produce the optimum result unless P = NP\n",
    "\n",
    "After implementing the construction function, please test your code using the following simple test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TestCode for node construction       \n",
    "Y = [0, 0, 0, 0, 1, 0, 1, 0]\n",
    "X = [[0, 0, 0],[0, 1, 1],[0, 0, 1],[0, 1, 0],[1, 0, 0],[1, 1, 1],[2, 0, 0],[2, 1, 1]]\n",
    "node = DecisionTreeNode(X, Y)\n",
    "print node.attr_index\n",
    "print node.child[0].label\n",
    "print node.child[1].attr_index\n",
    "print node.child[2].attr_index\n",
    "#The code above should produce the following results:\n",
    "0\n",
    "0\n",
    "1\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Print Tree Node\n",
    "Implement the printTree(depth = 0) function to visualize tree node structure recursively. This function can help you can a intuitive feeling of the decision tree. \n",
    "We do not have strict requirement for the implementation function. However, your function should indicate the label, splitting attribute, and value of splitting attribute clearly. We recommend to use indention to represent the structure of tree. Please run the following test code after implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " attr[0] = 0:\n",
      "\tlabel = 0\n",
      " attr[0] = 1:\n",
      "\tattr[1] = 0:\n",
      "\t\tlabel = 1\n",
      "\tattr[1] = 1:\n",
      "\t\tlabel = 0\n",
      " attr[0] = 2:\n",
      "\tattr[1] = 0:\n",
      "\t\tlabel = 1\n",
      "\tattr[1] = 1:\n",
      "\t\tlabel = 0\n"
     ]
    }
   ],
   "source": [
    "# TestCode for printNode()      \n",
    "node.printNode()\n",
    "\n",
    "#The code above should give the following tree structure:\n",
    "# \n",
    "#  attr[0] = 0:\n",
    "#         label = 0\n",
    "#  attr[0] = 1:\n",
    "#         attr[1] = 0:\n",
    "#                 label = 1\n",
    "#         attr[1] = 1:\n",
    "#                 label = 0\n",
    "#  attr[0] = 2:\n",
    "#         attr[1] = 0:\n",
    "#                 label = 1\n",
    "#         attr[1] = 1:\n",
    "#                 label = 0\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Label for Test Data\n",
    "Now you have implemented the construction of tree node. Next step is to predict labels for training labels. You should do it recursively. Recommended Stopping condition is that self.label is not None for some node:\n",
    "Please run the following test code after implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Test code for predict()\n",
    "print node.predict([2,0,1]) # output should be 1\n",
    "print node.predict([0,0,0]) # output should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Random Forest\n",
    "We train our random forest based on a general technique called bootstrap aggregating, or bagging. It means that given a training set X = x1, ..., xn with labels Y = y1, ..., yn, we repeatedly (B times) selects a set of random samples with replacement and train a decision based on our selection (reference: https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "For $b = 1, ..., B$:\n",
    "1. Sample, with replacement, n training examples from $X$, $Y$; call these $X_b$, $Y_b$.\n",
    "2. Train a decision or regression tree $f_b$ on $X_b$, $Y_b$.\n",
    "  \n",
    "After training, predictions on an unseen samples x can be made by averaging the predictions from all the individual regression trees on x:\n",
    "\n",
    "$\n",
    "f(x) = {\\frac {1}{B}}\\sum _{b=1}^{B} f_b(x)\n",
    "$\n",
    "\n",
    "or by taking the majority vote in the case of decision trees.\n",
    "\n",
    "You should implement the construction function of RandomForest and predict() of RandomForest in the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class RandomForest:\n",
    "    def __init__(self, X, Y, B, n):\n",
    "        \"\"\"\n",
    "        Construct a Random Forest using DecisionTreeNode.\n",
    "        args:\n",
    "            X(list of list of integer): X[i][j] is the value of the jth attribute of the ith training data\n",
    "            For any given attribute, we encode the set of possible values it can take as 0, 1, 2, ...\n",
    "\n",
    "            Y(list of integer): Y[i] is the label of the ith training data. We encode the set of possible values\n",
    "            as 0, 1, 2, ...\n",
    "            \n",
    "            B(integer): number of decision trees this forest has\n",
    "            \n",
    "            n(integer): number of samples used to train each decision tree. Samples are draw from X, Y uniformaly\n",
    "            with replacement\n",
    "            \n",
    "        recommended member:\n",
    "            roots (list of DecisionTreeNode): a list of decision tree roots. len(roots) = n\n",
    "        \"\"\"\n",
    "        self.roots = []\n",
    "        sample_num = len(X)\n",
    "        for i in range(B):\n",
    "            index = range(sample_num)\n",
    "            random.shuffle(index)\n",
    "            X_b = []\n",
    "            Y_b = []\n",
    "            for j in range(n):\n",
    "                X_b.append(X[index[j]])\n",
    "                Y_b.append(Y[index[j]])\n",
    "            self.roots.append(DecisionTreeNode(X_b, Y_b))\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the label of input x using majority voting, if multiple labels receive the same amount of vote,\n",
    "        return anyone of them\n",
    "        Note: since each tree is trained based on a random set of samples, it is possible that a decision tree has\n",
    "        never seen some of the values of some features. In this case, you should except the KeyError in your\n",
    "        implementation and cancel the vote of this tree\n",
    "        If the votes of all tree has been canceled, you should return None\n",
    "        args:\n",
    "            x (list of integers) an input sample:\n",
    "        return:\n",
    "            (int) the predicted label of this input, by majority voting of all of its random forests\n",
    "        \"\"\"\n",
    "        cnt = {}\n",
    "        for root in self.roots:\n",
    "            pred = root.predict(x)\n",
    "            if pred != None:\n",
    "                if pred in cnt:\n",
    "                    cnt[pred] += 1\n",
    "                else:\n",
    "                    cnt[pred] = 1\n",
    "        opt_pred = None\n",
    "        max_cnt = 0\n",
    "        for pred in cnt:\n",
    "            if cnt[pred] > max_cnt:\n",
    "                max_cnt = cnt[pred]\n",
    "                opt_pred = pred\n",
    "        return opt_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Test code for random forest:\n",
    "randomForest = RandomForest(X, Y, 4, 4)\n",
    "print randomForest.predict([2,0,0]) # 1 with prob around 0.6, 0 with prob aroud 0.4\n",
    "print randomForest.predict([1,1,1]) # 1 with prob around 0.1, 0 with prob around 0.9\n",
    "\n",
    "# Write your code here to test their probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Play with Real Data!\n",
    "Hope now you have implemented the RandomForest class. You can play with it on some real data instead of the artificial examples regarding play sports.\n",
    "\n",
    "We use a dataset from http://archive.ics.uci.edu/ml/datasets/Car+Evaluation. In this dataset, we want to predict the acceptability (unacc, acc, good, vgood) of a car based on the following properties:\n",
    "1. buying price: vhigh, high, med, low. \n",
    "2. maintaining price: vhigh, high, med, low. \n",
    "3. number of doors: 2, 3, 4, 5more. \n",
    "4. positions for person: 2, 4, more. \n",
    "5. size of luggage boot : small, med, big. \n",
    "6. safety: low, med, high. \n",
    "\n",
    "First we need to load data and connnsdcstruct our training and test dataset:\n",
    "(There are some dirty work here, I recommend that we should not ask students to work on this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dirty work start, you can ignore it\n",
    "# load data, convert data to our representation, and split data into training and testing data\n",
    "f = open('car_acceptability.csv')\n",
    "train_sample_num = 1200\n",
    "lines = f.readlines()\n",
    "X = []\n",
    "Y = []\n",
    "convert_dict = [{}, {}, {}, {}, {}, {}, {}]\n",
    "for line in lines:\n",
    "    attrs = line.split(',')\n",
    "    row = []\n",
    "    for i in range(7):\n",
    "        attr = attrs[i]\n",
    "        if not attr in convert_dict[i]:\n",
    "            convert_dict[i][attr] = len(convert_dict[i])\n",
    "        row.append(convert_dict[i][attr])\n",
    "    X.append(row[:6])\n",
    "    Y.append(row[6])\n",
    "\n",
    "sample_num = len(X)\n",
    "\n",
    "index = range(sample_num)\n",
    "random.shuffle(index)\n",
    "Xtrain = []\n",
    "Xtest = []\n",
    "Ytrain = []\n",
    "Ytest = []\n",
    "for i in range(sample_num):\n",
    "    if i < train_sample_num:\n",
    "        Xtrain.append(X[index[i]])\n",
    "        Ytrain.append(Y[index[i]])\n",
    "    else:\n",
    "        Xtest.append(X[index[i]])\n",
    "        Ytest.append(Y[index[i]])\n",
    "# construct attr_names, attr_values, and output_names to make the printNode more readable\n",
    "# not something important\n",
    "attr_names = ['buy price', 'maintain price', 'num of doors', 'num of person', 'size of lug', 'safety']\n",
    "output_names = [0] * (max(convert_dict[6].values()) + 1)\n",
    "for key in convert_dict[6]:\n",
    "    output_names[convert_dict[6][key]] = key\n",
    "attr_values = [0] * 6\n",
    "for i in range(6):\n",
    "    attr_values[i] = [0] * (max(convert_dict[i].values()) + 1)\n",
    "    for key in convert_dict[i]:\n",
    "        attr_values[i][convert_dict[i][key]] = key\n",
    "# Dirty work end, attention please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Play with Decision Tree on Real Data\n",
    "In this part you should use $Xtrain$ and $Ytrain$ to train your decision tree. After training, you print the tree and test the accuracy on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.907196969697\n"
     ]
    }
   ],
   "source": [
    "# Train your tree and print it here\n",
    "root = DecisionTreeNode(Xtrain, Ytrain)\n",
    "#root.printNode(0, attr_names, attr_values, output_names)\n",
    "\n",
    "# The begining of your tree should looks like:\n",
    "# safety = low:\n",
    "#     label = unacc\n",
    "\n",
    "#      safety = med:\n",
    "#         num of person = 2:\n",
    "#             label = unacc\n",
    "\n",
    "#         num of person = 4:\n",
    "#                 buy price = vhigh:\n",
    "#                         maintain price = vhigh:\n",
    "#                                 label = unacc\n",
    "\n",
    "#                         maintain price = high:\n",
    "#                                 label = unacc\n",
    "\n",
    "#                         maintain price = med:\n",
    "#                                 size of lug = small:\n",
    "#                                         label = unacc\n",
    "\n",
    "# test your tree on Xtest and Ytest here:\n",
    "pred = []\n",
    "cnt = 0\n",
    "for i in range(len(Xtest)):\n",
    "    if Ytest[i] == root.predict(Xtest[i]):\n",
    "        cnt += 1;\n",
    "print float(cnt) / len(Ytest)\n",
    "# our implementation has accuracy between 0.85-0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with Random Forest on Real Data\n",
    "In this part you should use $Xtrain$ and $Ytrain$ to train your random forest. After training, you test the accuracy on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.886363636364\n"
     ]
    }
   ],
   "source": [
    "# Train your tree and print it here\n",
    "forest = RandomForest(Xtrain, Ytrain, 100, 200)\n",
    "\n",
    "## Test your tree on Xtest and Ytest here:\n",
    "pred = []\n",
    "cnt = 0\n",
    "for i in range(len(Xtest)):\n",
    "    if Ytest[i] == forest.predict(Xtest[i]):\n",
    "        cnt += 1;\n",
    "print float(cnt) / len(Ytest)\n",
    "# When B = 100 and n = 1000, our implementation has accuracy around 0.92.\n",
    "# Compared with the accuracy of a single decision tree, you can see the improvement of random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How accuracy changes with number of samples\n",
    "We have noticed that as $n$ changes, the accuracy changes accordingly. In the last section you should plot how accuracy changes with $n$, when the other parameters are fixed. To get a relatively stable result, we recommend that you should take repeated experiments for at least 10 times and take the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your code here:\n",
    "import matplotlib.pyplot as plt\n",
    "repeat = 10\n",
    "ns = range(50, 1250, 50)\n",
    "res = []\n",
    "B = 100\n",
    "for n in ns:\n",
    "    total = 0.0\n",
    "    for r in range(repeat):\n",
    "        forest = RandomForest(Xtrain, Ytrain, B, n)\n",
    "        pred = []\n",
    "        cnt = 0\n",
    "        for i in range(len(Xtest)):\n",
    "            if Ytest[i] == forest.predict(Xtest[i]):\n",
    "                cnt += 1;\n",
    "        total += float(cnt) / len(Ytest)\n",
    "    res.append(total / repeat)\n",
    "plt.plot(ns, res)\n",
    "plt.xlabel('number of samples')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation shows that as n increase, the accuracy increases first, then decrease.\n",
    "\n",
    "This result is quite reasonable. As your final task, think about why it happens.\n",
    "\n",
    "<img src=\"files/example.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
