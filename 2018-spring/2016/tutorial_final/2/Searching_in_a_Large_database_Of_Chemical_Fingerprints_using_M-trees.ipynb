{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "Chem-informatics is a field dealing with chemistry and information science, with the primary motivation being the use of data mining, information retrieval and machine learning techniques to make predictions and inferences which can later be verified experimentally. Chemical Compounds are frequently represented as feature vectors, where every feature represents the absence, presence or the frequency count of certain important substructures or other chemical properties. These feature vectors are known as ”Chemical Fingerprints” . You can read more about cheminformatics here, https://www.emolecules.com/info/molecular-informatics\n",
    "\n",
    "The conversion of the compounds to the vector format is known as fingerprinting. In the field of computer science, fingerprinting is a technique that hashes or maps every large data item to a much shorter string, generally a bit vector, also known as its fingerprint. This fingerprint is used to uniquely identify the data item for all purposes, very similar to how fingerprints of humans can be mapped uniquely for every single individual. Fingerprinting as mentioned earlier is used for avoiding transfer and comparison of large bulky data.\n",
    "\n",
    "The technique we will use to store the vectors is an indexing method based on the structure of the Metric-tree or M-tree which we will discuss here. The standard similarity measure used for chemical fingerprints is the Tanimoto Similarity/ Min-Max similarity. The corresponding distance measure satisfies triangle inequality. The M-tree structure helps us exploit this fact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measure\n",
    "The similarity measure used to compare two fingerprints X, Y is generally the Tanimoto similarity method which compares two bit vectors . Assume that each vector is of length N, corresponding to the total number of features.\n",
    "\n",
    " $$Tanimoto~Similarity~(X, Y) = \\frac{\\sum_{i=i}^N X_i \\cap Y_i}{\\sum_{i=i}^N X_i \\cup Y_i} $$\n",
    " \n",
    "Notice that the similarity measure always lies between 0 and 1. Hence they define the corresponding distance measure of the two fingerprints as :\n",
    "\n",
    " $$Distance~(X,Y) = 1 - sim(X,Y) $$\n",
    " \n",
    " where 'sim' is the Tanimoto similarity\n",
    " \n",
    " You can read more about chemical similarity here : https://en.wikipedia.org/wiki/Chemical_similarity\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation of fingerprints\n",
    "\n",
    "Chemical fingerprints are vectors which are highly sparse and have a high number of features. Hence storing them as a list would be space consuming. Rather ideally a chemical fingerprint should be stored as a set of feature indices and feature values for the non-zero features. Hence we will be storing each fingerprint as a list of feature indices which correspond to the non-zero features. Note we are only going to be dealing with binary fingerprints, hence the feature value is always 1.0.\n",
    "\n",
    "Lets say for example, there is a chemical fingerprint which has only its 2nd, 3rd and 500th feature bit set to 1.0. We will store it as [2,3,500].\n",
    "\n",
    "Now lets answer what the question of what these features represent. The features generally represent particular attributes of the compound. Some examples for the features are, absence or presence of a double Oxygen bond or hydrogen atoms, is it photosensitive, is the molecular weight of compound is greater than a threshold, how is its reaction with specific compounds, how many carbon atoms are present, is the bond angle very high, etc. \n",
    "\n",
    "I have generated a synthetic dataset in the file \"fingerprint2.txt\" which we will use for our calculations. The dataset has been generated with statistics gathered from actual data to produce similar patterns in the data.\n",
    "\n",
    "Let us first compute the similarity of two compounds given their fingerprint representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Computing similarity and Distance measures\n",
    "\n",
    "def Tanimoto_sim(X,Y):\n",
    "    #Input: X, Y are lists of lists of feature indices for the two fingerprints\n",
    "    #Output: similairty measure , which is a double value\n",
    "    return 1.0*len(set(X)& set(Y))/len(set(X) | set(Y))  \n",
    "    pass\n",
    "    \n",
    "def distance(X,Y):\n",
    "    #Input: X, Y are lists of lists of feature indices for the two fingerprints\n",
    "    #Output: distance measure , which is a double value\n",
    "    return 1 - Tanimoto_sim(X,Y)\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:[2, 3, 4]\n",
      "Y:[4, 5, 6]\n",
      "The Tanimoto similarity of the two fingerprints, X and Y is: 0.2\n",
      "The distance of the two fingerprints is: 0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Example for finding Tanimoto similarity\n",
    "# In the example below there the two fingerprints share only one feature, which is feature number 4\n",
    "fingerprint_1 = [2,3,4]\n",
    "fingerprint_2 =[4,5,6]\n",
    "print \"X:\"+ str(fingerprint_1)\n",
    "print \"Y:\"+ str(fingerprint_2)\n",
    "print \"The Tanimoto similarity of the two fingerprints, X and Y is: \"+ str(Tanimoto_sim(fingerprint_1, fingerprint_2))\n",
    "print \"The distance of the two fingerprints is: \"+ str(distance(fingerprint_1, fingerprint_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of points in our dataset is :10000\n",
      "\n",
      "The following statistics of the dataset show how sparse it is.\n",
      "The maximum number of features set in a fingerprint is: 100\n",
      "The maximum number of features set in a fingerprint is: 50\n",
      "The total number of unique features in the dataset is: 1500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Dataset load from file\n",
    "fingerprints=[]\n",
    "with open('fingerprint2.txt') as f:\n",
    "    for line in f:\n",
    "        line= line.split(\" \")\n",
    "        fingerprints.append([int(i) for i in line[:-1]])\n",
    "        \n",
    "print \"The total number of points in our dataset is :\" + str(len(fingerprints)) + \"\\n\"\n",
    "\n",
    "print \"The following statistics of the dataset show how sparse it is.\"\n",
    "print \"The maximum number of features set in a fingerprint is: \"+ str(max([len(i) for i in fingerprints]))\n",
    "print \"The maximum number of features set in a fingerprint is: \"+ str(min([len(i) for i in fingerprints]))\n",
    "print \"The total number of unique features in the dataset is: \"+ str(max([max(i) for i in fingerprints]) + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M-tree\n",
    "\n",
    "M-tree, also known as the Metric tree is a tree data structure constructed using a metric distance measure, and which relies on the triangle inequality for efficient range search queries. Similar to other tree data structures, an M-tree data structure has Leaf Nodes and non-Leaf nodes. Every non-leaf node has a pointer to its parent node, a pointer to its sub-tree, its object information and a covering radius denoting maximum distance of a node to any node in its sub-tree. Every leaf node keeps a pointer to its parent and object information.\n",
    "\n",
    "The M-tree data structure compartments the objects into nodes, which define regions of the metric space.  For each database object to be indexed, there is some node of the tree to which it corresponds. A node of the tree stores the object information i.e mainly the data point feature values, object id, a pointer to its subtree and the maximum distance of the nodes to any of its children in the subtree. M-tree organizes the metric space into a set of, possibly overlapping, regions, to which the same principle is recursively applied. You can think of M-tree as clustering the dataset into groups and doing this recursively in each group till the group size reaches a minimum.\n",
    "<img src=\"mtree.jpg\">\n",
    "\n",
    "To summarize, the following information is stored in each entry of a node in our tree:\n",
    "1. Object identifier id\n",
    "2. Object features i.e  a feature vector corresponding to the object stored in the node\n",
    "2. Pointer to sub-tree $S_i$ ( a way to do this would be to store the immediate children in an array which in turn would store the grandchildren)\n",
    "3. Farthest child distance i.e. Covering radius $r_i$ \n",
    "\n",
    "Other information which you can store in the node are pointers to the parent, number of children in the subtree, depth of node in the M-tree, etc. You can find more information about the M-tree here, https://en.wikipedia.org/wiki/M-tree. There are many variants of an M-tree. We will be using a simple non-overlapping tree where every node can a child of only one other node and there are no overlapping regions as shown in the figure above.\n",
    "\n",
    "Let us first construct the class Node and write some helper functions which we will require while constructing the M-tree. Note that the build-Tree function has been explained in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "class Node:\n",
    "    #\n",
    "    def __init__(self, idx, features):\n",
    "        #Initilaize Node class with node id and feature vector\n",
    "        # covered nodes correspond to its children\n",
    "        # covering radius is the max distance of the node to any node in its subtree\n",
    "        self.features =  features\n",
    "        self.idx = idx\n",
    "        self.covered_nodes = []\n",
    "        self.covering_radius = 0.0\n",
    "        \n",
    "        self.subtree_ids =[]\n",
    "        pass\n",
    "    \n",
    "       \n",
    "    #Add list of nodes to its children    \n",
    "    def add_all_nodes(self, nodes, idxs):\n",
    "        if idxs == None:\n",
    "            return\n",
    "        self.covered_nodes.extend(nodes)\n",
    "    \n",
    "    # Get distance to given node\n",
    "    def get_distance(self,node):\n",
    "        return distance(self.features, node.features)\n",
    "        pass\n",
    "    \n",
    "    # Recurisvely build a subtree by choosing pivots\n",
    "    # The algorithm is explained in the section \"Building the M-tree Index\"\n",
    "    def buildTree(self, nodes, idxs, P, M):\n",
    "        if self.features == None:\n",
    "            return\n",
    "        self.subtree_ids = [i for i in idxs]\n",
    "        for node in nodes:    \n",
    "            dist=self.get_distance(node)\n",
    "            if dist > self.covering_radius:\n",
    "                self.covering_radius = dist\n",
    "        num=len(nodes)\n",
    "        if(num < M or num < P):\n",
    "            self.add_all_nodes(nodes, idxs)\n",
    "            return\n",
    "        \n",
    "        rnd_idc =[i for i in range(num)]\n",
    "        shuffle(rnd_idc)\n",
    "        pivots = [nodes[rnd_idc[i] ] for i in range(P)]\n",
    "        nodes_to_be_alloted = [nodes[rnd_idc[i]] for i in xrange(P,num)]\n",
    "        idxs_lst =[[] for i in range(P)]\n",
    "        idxs_lst_actual =[[] for i in range(P)]\n",
    "        for i in range(len(nodes_to_be_alloted)):\n",
    "            idx = self.find_closest_pivot(nodes_to_be_alloted[i], pivots)\n",
    "            idxs_lst[idx].append(idxs[P+i])\n",
    "            idxs_lst_actual[idx].append(i)\n",
    "        self.add_all_nodes(pivots,idxs[:P])\n",
    "        j=0\n",
    "        for node in self.covered_nodes:\n",
    "            node.buildTree([nodes_to_be_alloted[i]  for i in idxs_lst_actual[j]], idxs_lst[j], P, M)\n",
    "            j+=1\n",
    "        return\n",
    "    \n",
    "    # Find the closest node among the list of pivots \n",
    "    def find_closest_pivot(self,node, pivots):\n",
    "        min_distance = 1.0\n",
    "        min_idx =0\n",
    "        for i in range(len(pivots)):\n",
    "            curr_distance = node.get_distance(pivots[i])\n",
    "            if curr_distance < min_distance:\n",
    "                min_distance = curr_distance\n",
    "                min_idx=i\n",
    "        return min_idx\n",
    "    \n",
    "    # Recursive test function to print all nodes in the subtree\n",
    "    def print_children(self):\n",
    "        if self.covered_nodes == None:\n",
    "            return\n",
    "        print self.idx, [node.idx for node in self.covered_nodes]\n",
    "        for node in self.covered_nodes:\n",
    "            node.print_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature indices of the node are [2, 3, 4]\n",
      "The distance of the node from itself is 0.0\n"
     ]
    }
   ],
   "source": [
    "# lets initialize a node with id =0, features =[2,3,4]\n",
    "node_test = Node(0,[2,3,4])\n",
    "print \"The feature indices of the node are \"+ str(node_test.features)\n",
    "print \"The distance of the node from itself is \" + str(node_test.get_distance(node_test))\n",
    "\n",
    "# We will see more examples below while building the M-tree index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the M-tree index\n",
    "\n",
    "While building the M-tree, we are partitioning the data-set into groups using pivots which enables us to exploit the triangle inequality. The choice of pivots in the baseline approach is done randomly. The algorithm has been formally explained below. \n",
    "1. Select the given number of random pivots from the database of chemical compounds. \n",
    "2. After choosing pivots we assign every other chemical compound in the database to one of the pivots based on the similarity to the pivots. A chemical compound is assigned to the pivot which is nearest to it i.e. the pivot with which it shares the highest Min-Max similarity. This is shown in the figure below.\n",
    "3. We apply this process recursively in each partition till we reach a partition of size less than M, which is another input to the algorithm.\n",
    "\n",
    "The figure below shows the first iteration of building a M-tree with P=3, M=6. Since each partition has less than 6 nodes, the indexing process terminates at this step.\n",
    "<img src=\"mtree-2.jpg\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for similar compounds in the database\n",
    "\n",
    "In this section we will provide the motivation for why M-trees are used to store chemical fingerprints. Our primary goal is to be able to perform range queries on our compound database as fast as possible. This is because in the real world, scientists are always looking for alternatives for drugs. For example if particular drugs fail to come to production due to many reasons, like high molecular weight, unstability, they start looking for similar drugs with similar features. Hence we are dealing with range queries, so that given a compound, we want to find all compounds at a distance within a threshold from the query compound, and the answert could be a suitable alternative for the compound. We are looking at accurate searching of similar compounds when given a query compound. The similarity of chemical fingerprints is established using the Min-Max distance which is the generalization of Tanimoto distance for non-binary data-sets (explained in the previous section). \n",
    "\n",
    "So given a query compound, the goal is to find all compounds within a distance threshold of $\\delta$. The trivial way to do this is to compare it with all compounds in the database one by one. But this method makes us look at the entire database and is time consuming. Rather we will use the M-tree efficiently to search for similar compounds. The main motivation behind using M-trees comes mainly because chemical fingerprints exhibit the following properties. Firstly, they have very high number of features and secondly, most of these features are zero, which means that the feature vector is very sparse. The structure of the M-tree helps us exploit triangle inequality\n",
    "\n",
    "Given a query chemical fingerprint q and a distance threshold $\\theta$ we want to find the set of chemical fingerprints from the database whose distance to the query is less than the threshold. We exploit the triangle inequality for the same. The basic idea is to be able to prune sub-trees based on the covering radius of the pivot of the sub-tree and the distance of the query to the pivot. The procedure for the range search querying can be described by the following steps:\n",
    "1. Let the query fingerprint be q and the fingerprint pivot be $p_i$ with sub-tree $S_i$. We can calculate the maximum distance of any node in $S_i$ from q.(We start with the root of the tree as $p_i$).\n",
    "2. Let the covering radius of pivot $p_i$ be $r_i$. Hence the maximum distance of any node in $S_i$ to the query will be dist(q, $p_i$) + $r_i$ due to triangle inequality. Similarly the minimum distance of any node in $S_i$ is max(dist(q, $p_i$) - $r_i$, 0).\n",
    "3. Hence we can compute the range of the distance of any node in $S_i$ to q.\n",
    "4. If the upper bound of the range or the maximum distance is lesser than the threshold distance $θ$, we can add all the nodes of the sub-tree Si to our resultant set.\n",
    "5. If the lower bound of the range is greater than the threshold distance θ, we can prune the sub-tree Si, since we can say with certainty that the distance of every node in the sub-tree Si to the query point is greater than the threshold θ.\n",
    "6. If there is an intersection in the intervals we recursively apply this technique on the second level of children in the sub-tree Si until we reach a leaf node. We make each of the children of the root of the sub-tree Si as the new pi and repeat the steps in each of the corresponding sub-trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class M_tree:\n",
    "    # Initilaize M-tree with parameter variables\n",
    "    # P is number of pivots\n",
    "    # M is the minimum size of a group required for us to choose pivots in it\n",
    "    def __init__(self, P,M):\n",
    "        self.root = None\n",
    "        self.P = P\n",
    "        self.M =M\n",
    "        pass\n",
    "    \n",
    "    # Builds the M-tree by calling the build-tree function in class Node\n",
    "    def make_pivot(self, data):\n",
    "        self.root = Node(0,data[0])\n",
    "        nodes =[Node(i, data[i]) for i in xrange(1,len(data))]\n",
    "        self.root.buildTree(nodes,[i for i in xrange(1,len(data))],self.P,self.M)\n",
    "        pass\n",
    "    \n",
    "    # The range search function\n",
    "    # Given a threshold theta, and a query q, finds all compunds in the database which are within\n",
    "    # the threshold distance from q\n",
    "    def range_search(self, pivot,query, threshold):\n",
    "        lst_idxs=[]\n",
    "        dst_to_qry=pivot.get_distance(query)\n",
    "        if dst_to_qry < threshold:\n",
    "            lst_idxs.append(pivot.idx)\n",
    "        max_distance = min(1.0,dst_to_qry)+pivot.covering_radius\n",
    "        if max_distance < threshold:\n",
    "            lst_idxs.extend(pivot.subtree_ids)\n",
    "            return lst_idxs\n",
    "        min_distance = max(0.0, dst_to_qry - pivot.covering_radius)\n",
    "        if min_distance > threshold:\n",
    "            return lst_idxs\n",
    "        for node in pivot.covered_nodes:\n",
    "            new_idcs = self.range_search(node, query, threshold)\n",
    "            lst_idxs.extend(new_idcs)\n",
    "        return lst_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation by Comparison with Linear Scan\n",
    "\n",
    "We compare the running times of searching in the database using M-tree indexing versus using a linear scan of the database. We will evaluate the model for different parameters for threshold, for different queries. Note that the step of choosing pivots is random, hence the model must be empirically validated using few queries. hence every time you run the indexing method, it produces a different index and the structure of the tree will differ with every run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The indices for the first level pivots are:[8003, 4938, 5291, 2974, 7630, 8928, 4182, 1643, 2987, 658]\n",
      "The nodes within a threshold distance of 0.75 from the root is :[0]\n",
      "\n",
      " Evaluation of M-tree vs linear scan\n",
      "The average time taken by M-tree :0.228827238083\n",
      "The average time taken by Linear Scan :0.26600420475\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Ideally queries are compounds from outside the database\n",
    "# I have used compounds from within the database to show correctness.\n",
    "queries=[Node(1000,fingerprints[2]),Node(1001,fingerprints[134]),Node(1002,fingerprints[567]),Node(1003,fingerprints[889])]\n",
    "thresholds=[0.01,0.2, 0.25, 0.3]\n",
    "\n",
    "mtree = M_tree(10,20)\n",
    "mtree.make_pivot(fingerprints)\n",
    "nodes =[Node(i, fingerprints[i]) for i in xrange(len(fingerprints))]\n",
    "\n",
    "\n",
    "# Lets look at the first level pivots\n",
    "print \"The indices for the first level pivots are:\" + str([ i.idx for i in mtree.root.covered_nodes])\n",
    "\n",
    "# Lets find all nodes within a distance of 0.75 for the root\n",
    "# ideally this would contain 0 (which is the id of the root) because it is within a range of 0.75 from itself. \n",
    "# if there are other nodes, those would be returned as well\n",
    "print \"The nodes within a threshold distance of 0.75 from the root is :\" + str(mtree.range_search(mtree.root,mtree.root,0.75))\n",
    "\n",
    "\n",
    "#Note that every time you run the indexing method, it produces a different index\n",
    "#Hence the structure of the tree will differ with every run\n",
    "\n",
    "print \"\\n Evaluation of M-tree vs linear scan\"\n",
    "m_tree_time=0\n",
    "linear_time=0\n",
    "for i in range(4):\n",
    "    query = queries[i]\n",
    "    threshold = thresholds[i]\n",
    "    \n",
    "    start = time.time()\n",
    "    range_query = mtree.range_search(mtree.root,query,threshold)\n",
    "    end = time.time()\n",
    "    m_tree_time +=end-start\n",
    "    \n",
    "    lst=[]\n",
    "    start = time.time()\n",
    "    for node in nodes:\n",
    "        if node.get_distance(query) < threshold:\n",
    "            lst.append(node.idx)\n",
    "    end = time.time()\n",
    "    linear_time +=end-start\n",
    "    \n",
    "    \n",
    "print \"The average time taken by M-tree :\" + str(m_tree_time*1.0/4)\n",
    "print \"The average time taken by Linear Scan :\" + str(linear_time*1.0/4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This notebook covers how an M-tree is beneficial to index chemical compounds owing to various reasons which have been explained above. We have shown that linear scan of the database takes much more time for range queries than while using a M-tree. \n",
    "\n",
    "The basic idea which I want you take from this notebook is how an M-tree can be used for storing points in a database, why they are useful to store chemical compounds, whose similarity is defined by Tanimoto similarity and how triangle inequality can be leveraged to perform range search queries in a M-tree indexed database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
