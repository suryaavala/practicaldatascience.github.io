{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Introduction\n",
    "This tutorial will introduce you to some basic methods for using spark to process data analysis.\n",
    "I assume everyone already know about MapReduce before we start this tutorial. If not, please refer to this [link](https://en.wikipedia.org/wiki/MapReduce). You only need to go over the logical view part for the basic computation model and the map and reduce concept. \n",
    "\n",
    "Spark is an open source cluster computing framework developed at the UC Berkeley AMPLab. It uses in-memory primitives that allow it to perform over 100x faster than traditional MapReduce for certain applications. \n",
    "\n",
    "To use Spark, you need to write a driver program to connect to a Spark cluster of workers.\n",
    "\n",
    "\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up for Spark and Notebook\n",
    "#### Install Spark on Mac OS\n",
    "```\n",
    "    > brew install scala\n",
    "    > brew install hadoop\n",
    "    > brew install apache-spark\n",
    "    > pyspark \n",
    "```\n",
    "Type '`sc`' in the pyspark shell, should have output like this: `<pyspark.context.SparkContext at 0x11222ab90>`\n",
    "Great! Spark is working! Now stop the pyspark by Ctr+D\n",
    "    \n",
    "#### Setup environment variable\n",
    "```\n",
    "    > export PYSPARK_DRIVER_PYTHON=\"jupyter\" \n",
    "    > export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" \n",
    "    > pyspark \n",
    "``` \n",
    "The PYSPARK_DRIVER_PYTHON parameter and the PYSPARK_DRIVER_PYTHON_OPTS parameter are used to launch the PySpark shell in Jupyter Notebook. \n",
    "\n",
    "**Now run pyspark again, a new jupyter notebook should be opened in your browser and you are able to run the following testing code. Close the old notebook and all the code should be able to run here.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10d9c9b90>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should have output like this:\n",
    "# <pyspark.context.SparkContext at 0x11222ab90>\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts \n",
    "#### SparkContext, RDD\n",
    "\n",
    "- In the PySpark shell, a special interpreter-aware **SparkContext** is already created for you, in the variable called sc. Making your own SparkContext will not work.\n",
    "\n",
    "- At the heart of the Spark framework lies a new data abstraction called **Resilient Distributed Datasets** or (**RDD**s), which allow for a distributed dataset to remain in-memory in the nodes of a cluster during various stages of computation. RDDs also store the lineage information about the data, keeping a record of all the operations that were performed to bring an RDD to its present state. This way, if a node fails on a Spark cluster, the data that was in-memory and lost can be re-loaded from the source (often the distributed file system) and the operations that were recorded in the lineage information can be re-applied to bring the data to its present state. Thus, data can remain in memory through multiple stages of transformation without spilling to disk, and applications can run many times faster than traditional frameworks that rely on disk accesses in between stages.\n",
    "\n",
    "#### Creating RDD\n",
    "\n",
    "- There are two ways to create RDDs: **parallelizing** an existing collection in your driver program, or **loading from file**, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat. Once created, the rdd can be operated on in parallel. \n",
    "\n",
    "#### Look at your RDD & Output\n",
    "- To print all elements on the driver, one can use the `collect()` method to first bring the RDD to the driver. This can cause the driver to run out of memory when your data is big, though, because `collect()` fetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the `take()`. Or use `count()` to get the total number of elements.\n",
    "\n",
    "- There are different methods to save your output as files into hadoop file system or local file systems. Following is  an example of save to local file system use `saveAsTextFile()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[762] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create rdd by parallelizing an existing collection\n",
    "rdd1 = sc.parallelize([1,2,3,4,5])\n",
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data.txt MapPartitionsRDD[828] at textFile at null:-1"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create rdd from external file\n",
    "rdd2 = sc.textFile(\"data.txt\")\n",
    "rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[1, 2, 3, 4]\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count elements in rdd\n",
    "print rdd1.count()\n",
    "# take sample of 4 \n",
    "print rdd1.take(4)\n",
    "# take first sample\n",
    "print rdd1.first()\n",
    "# collect rdd to a collection list data structure\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'one', u'two']\n",
      "one\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'one', u'two', u'three']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print rdd2\n",
    "print rdd2.take(2)\n",
    "print rdd2.first()\n",
    "# a list of str\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output to textfile\n",
    "rdd2.saveAsTextFile('sample_output1')\n",
    "# output saved as a directory under sample_output in seperate parts as distributed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations\n",
    "RDDs support two types of operations: \n",
    "* **Transformation**, which create a new rdd from an existing one\n",
    "\n",
    "* **Action**, which return a value to the driver program after running a computation on the dataset. \n",
    "\n",
    "For example, `map` is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, `reduce` is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel `reduceByKey` that returns a distributed dataset).\n",
    "\n",
    "#### Common methods of Transformation and Action\n",
    "- Transformation\n",
    "\n",
    "``` map, flatMap, filter, sample, union, intersection, distinct, groupByKey, reduceByKey, aggregateByKey, sortByKey, join```\n",
    "\n",
    "- Action\n",
    "\n",
    "```reduce, collect, count, first, take, countByKey, saveAsTextFile```\n",
    "\n",
    "Most of them are pretty straight forward as suggest by their names, and using the right method would improve your calculation performance a lot. We would show how to use some of them next.\n",
    "\n",
    "\n",
    "#### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd =  [1, 2, 3]\n",
      "filter:  [2, 3]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3])\n",
    "print 'rdd = ', rdd.collect()\n",
    "filtered = rdd.filter(lambda x: x > 1)\n",
    "print 'filter: ', filtered.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map vs. flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " flatMap:  [0, 1, 1, 2, 1, 3]\n",
      "map:  [[0, 1], [1, 2], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "# compare map, flatMap\n",
    "rdd1 = rdd.flatMap(lambda x:[x / 2, x])\n",
    "print 'flatMap: ', rdd1.collect()\n",
    "rdd2= rdd.map(lambda x:[x / 2, x])\n",
    "print 'map: ',rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce vs. reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd =  [1, 2, 3]\n",
      "reduce:  6\n",
      "rdd2 =  [[0, 1], [1, 2], [1, 3]]\n",
      "reduceByKey: [(0, 1), (1, 5)]\n"
     ]
    }
   ],
   "source": [
    "# compare reduceByKey, reduce\n",
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd2= rdd.map(lambda x:[x / 2, x])\n",
    "print 'rdd = ', rdd.collect()\n",
    "# total is a number instead of rdd\n",
    "total = rdd.reduce(lambda a,b: a + b)\n",
    "print 'reduce: ', total\n",
    "print 'rdd2 = ', rdd2.collect()\n",
    "print 'reduceByKey:', rdd2.reduceByKey(lambda a,b: a+b).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd2 =  [[0, 1], [1, 2], [1, 3]]\n",
      "groupByKey: [(0, <pyspark.resultiterable.ResultIterable object at 0x110e74710>), (1, <pyspark.resultiterable.ResultIterable object at 0x110e74e90>)]\n",
      "convert to list:  [{0: [1]}, {1: [2, 3]}]\n"
     ]
    }
   ],
   "source": [
    "rdd2= rdd.map(lambda x:[x / 2, x])\n",
    "print 'rdd2 = ', rdd2.collect()\n",
    "print 'groupByKey:', rdd2.groupByKey().collect()\n",
    "rdd3 = rdd2.groupByKey()\n",
    "print 'convert to list: ', rdd3.map(lambda x: {x[0]: list(x[1])}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All **transformations** in Spark are **`lazy`**, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
    "\n",
    "By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the `persist` (or `cache`) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.\n",
    "\n",
    "#### lazy computing of transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd2 =  [u'one', u'two', u'three']\n",
      "PythonRDD[939] at RDD at PythonRDD.scala:48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3, 3, 5]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.textFile(\"data.txt\")\n",
    "print 'rdd2 = ', rdd2.collect()\n",
    "# get length of each line in rdd2\n",
    "lineLengths_rdd = rdd2.map(lambda s: len(s))\n",
    "# Important!! It is not immediately computed, due to laziness\n",
    "print lineLengths_rdd\n",
    "# see what is in the rdd, collect to a list\n",
    "lineLengths_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.textFile(\"data.txt\")\n",
    "lineLengths_rdd = rdd2.map(lambda s: len(s))\n",
    "# sum up all length in the rdd\n",
    "total1 = lineLengths_rdd.reduce(lambda a, b: a + b)\n",
    "# If we also wanted to use lineLengths again later, we could add:\n",
    "lineLengths_rdd.persist()\n",
    "total2 = lineLengths_rdd.reduce(lambda a, b: a + b)\n",
    "# if lineLengths_rdd not persist in memory, \n",
    "# the total2 will be calculating starting from rdd2 again\n",
    "total1, total2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function call\n",
    "As you would notice we are using Lambda expressions for simple functions for map and reduce. We can also define a longer function and call the function in spark.\n",
    "\n",
    "Following example defines a function count each characters for given string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('h', 1), ('r', 1), ('e', 2), ('t', 1)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "def my_func(line):\n",
    "    ch_count = defaultdict(int)\n",
    "    for ch in line:\n",
    "        if ch != ' ':\n",
    "            ch_count[ch] += 1\n",
    "    return list(ch_count.items())\n",
    "my_func('three')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the defined function in `flatMap()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd2 =  data.txt MapPartitionsRDD[798] at textFile at null:-1\n",
      "rdd3 =  [(u'e', 1), (u'o', 1), (u'n', 1), (u'o', 1), (u't', 1), (u'w', 1), (u'h', 1), (u'r', 1), (u'e', 2), (u't', 1)]\n",
      "rdd4 =  [(u'e', 3), (u'o', 2), (u'w', 1), (u'h', 1), (u'r', 1), (u't', 2), (u'n', 1)]\n"
     ]
    }
   ],
   "source": [
    "# flatMap\n",
    "rdd2 = sc.textFile(\"data.txt\")\n",
    "print 'rdd2 = ', rdd2\n",
    "rdd3 = rdd2.flatMap(my_func)\n",
    "print 'rdd3 = ', rdd3.collect()\n",
    "# reduceByKey\n",
    "rdd4 = rdd3.reduceByKey(lambda a,b:a + b)\n",
    "# rdd4 is the count for each character in all three words of rdd2\n",
    "print 'rdd4 = ', rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the defined function in `filter()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[954] at RDD at PythonRDD.scala:48\n",
      "['error: something wrong', 'exception: unknown']\n"
     ]
    }
   ],
   "source": [
    "lines = ['this is a good log','error: something wrong','exception: unknown']\n",
    "error_words = ['exception', 'error']\n",
    "def has_error(line):\n",
    "    for w in error_words:\n",
    "        if w in line:\n",
    "            return True\n",
    "    return False\n",
    "err = sc.parallelize(lines)\\\n",
    "    .filter(has_error)\n",
    "print err\n",
    "print err.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Examples of data analysis using Spark \n",
    "We will be analyzing tweets data from homework2, for the sake of not display too many data I am running code on a small subset of the original data. Data fields are as following:\n",
    "```creen_name,created_at,retweet_count,favorite_count,text```\n",
    "\n",
    "#### Example 1\n",
    "Let's first take a look at the familiar wordcount, we would be count all the words of all tweets.\n",
    "Only one line of code!\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 3), (u'Matt', 1), (u'#MAGA', 1), (u'questions', 1)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile(\"smalltweet.csv\")\n",
    "\n",
    "counts = text.flatMap(lambda line: line.split(\",\")[-1].split()) \\\n",
    "        .map(lambda word: (word, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "counts.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2  \n",
    "\n",
    "Find most frequent words for each tweet user, return top 5 words and word counts for each user(except stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_tweet: [(u'realDonaldTrump', [u'Final', u'poll', u'results', u'from', u'NBC', u'on', u'last', u'nights', u'Commander-in-Chief', u'Forum.', u'Thank', u'you!', u'#ImWithYou', u'#MAGA', u'https://t.co/C5ipaxUN7B'])]\n",
      "user_wordcount: [(u'realDonaldTrump', (u'poll', 3))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'realDonaldTrump',\n",
       "  [(u'last', 5),\n",
       "   (u'nights', 3),\n",
       "   (u'Mexico', 3),\n",
       "   (u'results', 3),\n",
       "   (u'Hillary', 3)])]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we did in hw3 get stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "STOPWORDS=nltk.corpus.stopwords.words('english')\n",
    "THRESHOLD = 1\n",
    "\n",
    "# return the top 5 count of words\n",
    "def top5((name, list_wordcount)):\n",
    "    dic = {word:count for word, count in list_wordcount}\n",
    "    top5 = Counter(dic).most_common(5)\n",
    "    return name, top5\n",
    "\n",
    "text = sc.textFile(\"smalltweet.csv\")\n",
    "\n",
    "user_tweet = text.map(lambda line: line.split(\",\")) \\\n",
    "    .filter(lambda line: len(line)>4 and line[0] != 'screen_name') \\\n",
    "    .map(lambda line: (line[0],line[-1].split())) \n",
    "print 'user_tweet:', user_tweet.take(1)\n",
    "\n",
    "user_wordcount = user_tweet.flatMap(lambda (user, tweet) : [(user, word) for word in tweet]) \\\n",
    "        .filter(lambda(user, word): word not in STOPWORDS) \\\n",
    "        .map(lambda key : (key, 1)) \\\n",
    "        .reduceByKey(lambda a,b : a + b) \\\n",
    "        .filter(lambda (key,count): count > THRESHOLD) \\\n",
    "        .map(lambda ((name, word), count): (name,(word,count)))\n",
    "print 'user_wordcount:', user_wordcount.take(1)\n",
    "\n",
    "user_top5 = user_wordcount.groupByKey() \\\n",
    "            .map(top5)\n",
    "user_top5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3\n",
    "\n",
    "Let's do some data analysis on a graphic dataset.\n",
    "Dataset is the same one we used in hw2: edges.csv\n",
    "Data fields are as following:\n",
    "```follower, followee```\n",
    "\n",
    "* How many distinct edges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct edge count =  16180\n"
     ]
    }
   ],
   "source": [
    "edges = sc.textFile(\"edges.csv\") \n",
    "edge_count = edges.distinct() \\\n",
    "            .count()\n",
    "print 'distinct edge count = ', edge_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many distinct users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct user count =  12405\n"
     ]
    }
   ],
   "source": [
    "edges = sc.textFile(\"edges.csv\") \n",
    "user_count = edges.flatMap(lambda line: line.split(',')) \\\n",
    "            .distinct() \\\n",
    "            .count()\n",
    "print 'distinct user count = ', user_count"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
