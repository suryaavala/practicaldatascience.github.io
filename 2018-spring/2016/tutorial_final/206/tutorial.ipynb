{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction\n",
    "\n",
    "In this tutorial, we will learn how to extract keywords from a text in Python. We assume that a reader understands Python code and basic probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "* ### Motivation\n",
    "* ### Setup\n",
    "* ### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "A keyword is defined as \"*a word which occurs in a text more often than we would expect to occur by chance alone*\" [[source]].\n",
    "\n",
    "Today, the Internet has so much information. It would be impossible for a human to read all of this online data, even for just one topic. However, computers are much faster at computation than humans. Therefore, we can use computers to help filter through the mass of online information.\n",
    "\n",
    "This idea then begs the question, \"how do we do this?\" Well, that's a loaded question because there are many different approaches for doing text search. But, we can boil it down to a general process. \n",
    "\n",
    "#### Preprocess data\n",
    "Let's think of the Internet as a sea of text documents. In order to work with these documents, we need to categorize them in some way. This will then allow us to organize the data in a neat, searchable data structure. Therefore, we can take some text document and process it into a format that will be useful for search.\n",
    "\n",
    "#### Organize data\n",
    "Once we have our data processed, we need to organize in some way. This most likely involves creating one or more data structures with quick lookup or query time.\n",
    "\n",
    "\n",
    "#### Query data\n",
    "After our data is processed and organized, we should have some kind of interface to make queries on our data. This might involve working directly with the data structure that we used to organize our data, or we might build a tool around our data structure(s).\n",
    "\n",
    "\n",
    "Now that we have a broad idea of how looking up information works, we can finally figure out where keyword extraction comes in. When making a search query, people will usually include keywords related to a topic. Since we want to match documents that contain information about these keywords, we can use keyword extraction as part of our preprocessing state. This means that we can use keywords in a text as a way to represent a document.\n",
    "\n",
    "[source]: https://en.wikipedia.org/wiki/Keyword_(linguistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "For this tutorial, make sure that you have Python 2.7 installed. In addition, you will need to install a library called NLTK (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rapid Automatic Keyword Extraction\n",
    "The first method that we will cover is caled rapid automatic keyword extraction (RAKE). \n",
    "\n",
    "Stop words are common words that provide little to no information about the content in a document when it comes to information retrieval. For example, the words \"of\" or \"the\" might be considered stop words because they don't provide any information by themselves.\n",
    "\n",
    "The idea behind RAKE is that keywords and key phrases in a text usually do not contain stop words. In addition, keywords tend to occur frequently in a text and co-occur in a text with other keywords.\n",
    "\n",
    "Using these ideas, this is how RAKE works:\n",
    "\n",
    "1. Split the input text on stopwords. The list of phrases that result from this will be called the candidates for keywords.\n",
    "\n",
    "2. In order to pick which words are keywords, the candidates are ranked based upon the number of times a word occurs and based upon the number of times certain words co-occur. The candidates with the highest scores are selected as keywords.\n",
    "\n",
    "\n",
    "You can read more about RAKE [here][RAKE].\n",
    "\n",
    "[RAKE]: http://www.cbs.dtu.dk/courses/introduction_to_systems_biology/chapter1_textmining.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing RAKE\n",
    "Implementation adjusted from [here][SOURCE]\n",
    "\n",
    "[SOURCE]: http://sujitpal.blogspot.jp/2013/03/implementing-rake-algorithm-with-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First get a list of stopwords. \n",
    "# NLTK provides a list of stopwords for English that we will use.\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def get_candidates(text):\n",
    "    sentences = text.split()\n",
    "    candidates = list()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        candidates.extend(sentence.split(stop_words))\n",
    "        \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement scoring each candidate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_counts(candidates):\n",
    "    counts = dict()\n",
    "    for phrase in candidates:\n",
    "        for word in phrase:\n",
    "            if word in counts:\n",
    "                counts[word] += 1\n",
    "            else:\n",
    "                counts[word] = 1\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def get_degrees(candidates):\n",
    "    counts = dict()\n",
    "    \n",
    "    for phrase in candidates:\n",
    "        length = len(phrase)\n",
    "        for word in phrase:\n",
    "            if word in counts:\n",
    "                counts[word] += length\n",
    "            else:\n",
    "                counts[word] = length\n",
    "    return counts\n",
    "    \n",
    "def get_word_scores(candidates):\n",
    "    word_counts = get_counts(candidates)\n",
    "    word_degrees = get_degrees(candidates)\n",
    "    scores = dict()\n",
    "    \n",
    "    for word in word_counts:\n",
    "        scores[word] = float(word_degrees[word]) / float(word_counts[word])\n",
    "    return scores\n",
    "    \n",
    "def get_phrase_scores(candidates):\n",
    "    word_scores = get_word_scores(candidates)\n",
    "    scores = list()\n",
    "    \n",
    "    for phrase in candidates:\n",
    "        score = 0.0\n",
    "        for word in phrase:\n",
    "            score += word_scores[word]\n",
    "            \n",
    "        sentence = ' '.join(phrase)\n",
    "        scores.append((sentence, score))\n",
    "    return scores\n",
    "        \n",
    "    \n",
    "def extract_keywords(text):\n",
    "    candidates = get_candidates(text)\n",
    "    phrase_scores = sorted(get_phrase_scores(candidates), key=lambda x : x[1], reverse=True)\n",
    "    return phrase_scores\n",
    "\n",
    "def top_k_keywords(text, k):\n",
    "    keywords = extract_keywords(text)\n",
    "    if len(keywords < k):\n",
    "        return keywords\n",
    "    else:\n",
    "        return keywords[:k+1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
