{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Frequent Itemset Mining using A-priori and PCY Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "The goal of this tutorial is to find the most frequently N(We set N=3 here, but it can be any number you want) ingredients are used together in recipes all over the world. We will utilize a real dataset which contains 37852 recipes and 298 foods all over the world. It may be too small for observing the performance of optimization, but it perfectly shows how the real algorithm works and help us to find the relationship between datasets.\n",
    "\n",
    "\n",
    "Frequent itemset problem is an interesting and useful branch in data mining and analytics which focuses on finding items that often appear together in a large number of  datasets. It is usually described in market-basket model. That is, we regard each data element as an item and each dataset as a basket, and the Normally the number of the item in one basket is much less than the total number of the items. Then we define a set of items which appears more than a specific threshold to be a frequent itemset. \n",
    "\n",
    "However, applications of frequent itemset analysis are not limited to market baskets. The same model can be used to mine many other kinds of data. Some examples are: \n",
    "\n",
    "\n",
    "1. **Related Documents**. Let items be words, and let baskets be documents (e.g., Web pages, blogs, tweets). A basket/document contains those items/words that are present in the document. If we ignore all the most common words, then we would hope to find among the frequent pairs some pairs of words that represent a joint concept. For example, we would expect a pair like {Brad, Angelina} to appear with surprising frequency. \n",
    "\n",
    "2. **Plagiarism** Let the items be documents and the baskets be sentences. An item/document is \"in\" a basket/sentence if the sentence is in the document. This arrangement appears backward, but it is exactly what we need, and we should remember that the relationship between items and baskets is an arbitrary many-many relationship. In practice, even one or two sentences in common is a good indicator of plagiarism. \n",
    "\n",
    "1. **Biomarkers** Let the items be of two types – biomarkers such as genes or blood proteins, and diseases. Each basket is the set of data about a patient: their genome and blood-chemistry analysis, as well as their medical history of the disease. A frequent itemset that consists of one disease and one or more biomarkers suggests a test for the disease.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "#### Naive Algorithm\n",
    "\n",
    "Let's start with the most intuitive method, Which is both easy to understand and implement. The detailed step is shown in the following:\n",
    "\n",
    "1. We iterate every line in the original file and obtain all singletons (Duplicates has been removed).\n",
    "2. We generate all triple combinations based on these singletons.\n",
    "3. Iterate to the file again and count the occurence of each combination/\n",
    "4. Generate the top K frequent itemsets.\n",
    "\n",
    "We will only use the first 20 lines of the whole dataset to do a demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import itertools\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combine(nums, N=3):\n",
    "    \"\"\"\n",
    "        Generate the N-wise combination on the given array nums\n",
    "        Args:\n",
    "            nums: target array\n",
    "            N: number of elements in the permutation\n",
    "        Returns:\n",
    "            Python array, containing all combinations of size N, using nums\n",
    "        Helper functions, from Leetcode Discuss, https://discuss.leetcode.com/topic/14371/fast-simple-python-code-recursive\n",
    "    \"\"\"\n",
    "    return set(i for i in itertools.combinations(nums, N))\n",
    "\n",
    "def get_all_items(recipes):\n",
    "    items = set()\n",
    "    for recipe in recipes:\n",
    "        items = items | set(recipe)\n",
    "    return items\n",
    "\n",
    "def counting(coms, recipes, N):\n",
    "    \"\"\"\n",
    "        For each itemset in coms, we iterate through recipes and ensure whether these condidates appear in the recipe. \n",
    "    \"\"\"\n",
    "    counter = {}\n",
    "    for recipe in recipes:\n",
    "        recipe_coms = combine(recipe, N)\n",
    "        for com in recipe_coms:\n",
    "            if com in coms:\n",
    "                if com not in counter:\n",
    "                    counter[com] = 0\n",
    "                counter[com] += 1\n",
    "    return counter\n",
    "\n",
    "def get_top_K(counter, K=5):\n",
    "    return sorted(zip(counter.values(), counter.keys()))[-K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_recipes(filename):\n",
    "    lines = []\n",
    "    with open(filename, 'r') as fp:\n",
    "        for line in fp:\n",
    "            lines.append(sorted([int(i) for i in line.strip().split()]))\n",
    "\n",
    "    return lines\n",
    "\n",
    "recipes = get_all_recipes(\"data_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, (51, 187, 529)), (3, (73, 470, 658)), (3, (205, 529, 966)), (3, (205, 529, 984)), (3, (622, 887, 888))]\n",
      "[(3, (51, 187, 529)), (3, (73, 470, 658)), (3, (205, 529, 966)), (3, (205, 529, 984)), (3, (622, 887, 888))]\n",
      "[(3, (51, 187, 529)), (3, (73, 470, 658)), (3, (205, 529, 966)), (3, (205, 529, 984)), (3, (622, 887, 888))]\n",
      "[(3, (51, 187, 529)), (3, (73, 470, 658)), (3, (205, 529, 966)), (3, (205, 529, 984)), (3, (622, 887, 888))]\n",
      "1 loop, best of 3: 10.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "def naive_algorithm(recipes):\n",
    "    items = get_all_items(recipes)\n",
    "    coms = combine(items)\n",
    "    print get_top_K(counting(coms, recipes, 3))\n",
    "    \n",
    "%timeit naive_algorithm(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the result \n",
    "\n",
    "*[(3, '529 51 187'), (3, '529 966 205'), (3, '529 984 205'), (3, '622 887 888'), (3, '658 73 470')]*\n",
    "\n",
    "The naive method is feasible. However, it is far from satisfactory both in time and space complexity.\n",
    "\n",
    "When we use **timeit**, we found it cost about 10s to find out top 10 frequent items.\n",
    "\n",
    "When the number of single items grows, the number of triple combinations also grows at a very fast speed. Our dataset contains about 50000 lines of recipes, but the naive algorithm could only work for ~20 recipes. Also, for each recipe in the original file, we need to iterate through the whole combination list, given that most of them did not even appear once in the original data set. There is still a lot of optimization that we can do.\n",
    "\n",
    "Before any introduction to Apriori Algorithm, let's first define the Associate Rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Associated Rule (Some of the material comes from Wikipedia)\n",
    "\n",
    "The Apriori Algorithm is based on a collection of if–then rules, called association rules. The form of an association rule is I → j, where I is a set of items and j is an item. The implication of this association rule is that if all of the items in I appear in some basket, then j is likely to appear in that basket as well. Inversely, if any subset of the candidate basket is not a frequent itemset, then there is no possibility that this candidate basket will also be frequent itemset. \n",
    "\n",
    "The reason is simple. Let J ⊆ I. Then every basket that contains all the items in I surely contains all the items in J. Thus, **the count for J must be at least as great as the count for I**, and if the count for J is at most s, then the count for I is also at most s. Since J may be contained in some baskets that are missing one or more elements of I − J, it is entirely possible that the count for I is strictly less than the count for J.\n",
    "\n",
    "Association rule typically does not consider the order of items either within a transaction or across transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apriori Algorithm\n",
    "\n",
    "Apriori Algorithm makes full use of Associated Rule. The core of Apriori Algorithm is that, **Assume an itemset I, if I is a frequent itemset, so are all its subsets. Conversely, if one of its subsets is not a frequent itemset, I is also not a frequent itemset.**\n",
    "\n",
    "![title](imgs/image01.png)\n",
    "![title](imgs/image09.png)\n",
    "\n",
    "If the length of our target itemset is K, and we define N as the threshold of a frequent itemset, then Apriori Algorithm will work as following:\n",
    "\n",
    "1. Begin for all item singletons, load the data and eliminates all singletons that have the less frequency than N.\n",
    "2. Form tuples using the remaining singletons, load the data again and eliminate all tuples that have the less frequency than N.\n",
    "3. Repeat step 2, until we form itemsets of size K. The subsets of these itemsets are all frequent itemsets, so they are legal candidates and eligible for the final screening.\n",
    "4. We run the final screening on all frequent itemsets and finally come up with all frequent items of size K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set our target itemset size to 3 and the threshold to 3, and re-write the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 100 ms per loop\n"
     ]
    }
   ],
   "source": [
    "def find_frequent_itemsets(counter, threshold=3):\n",
    "    candidates = []\n",
    "    for k, v in counter.items():\n",
    "        if v >= threshold:\n",
    "            candidates.append(k)\n",
    "    return candidates\n",
    "\n",
    "def generate_triple_from_tuple(candidates_2):\n",
    "    candidate_set = set([str(t[0]) + \" \" + str(t[1]) for t in candidates_2])\n",
    "    candidates_3 = []\n",
    "    items = set()\n",
    "    for i in candidates_2:\n",
    "        items.add(i[0])\n",
    "        items.add(i[1])\n",
    "    \n",
    "    items, length = sorted(list(items)), len(items)\n",
    "    for i in range(length):\n",
    "        for j in range(i + 1, length):\n",
    "            if str(items[i]) + \" \" + str(items[j]) in candidate_set:\n",
    "                for k in range(j + 1, length):\n",
    "                    if str(items[i]) + \" \" + str(items[k]) in candidate_set and str(items[j]) + \" \" + str(items[k]) in candidate_set:\n",
    "                        candidates_3.append((items[i], items[j], items[k]))\n",
    "                        \n",
    "    return candidates_3\n",
    "\n",
    "def apriori_algorithm(recipes):\n",
    "    \"\"\"\n",
    "        Given recipes, this function runs Apriori Algorithm and obtains the frequent itemset\n",
    "        Args:\n",
    "            recipes: An array of recipe\n",
    "        Returns:\n",
    "            An array containing all frequent itemsets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all singletons\n",
    "    items = get_all_items(recipes)\n",
    "    coms = combine(items, 1)\n",
    "    \n",
    "    # Retains frequent singletons of size 1\n",
    "    counter_1 = counting(coms, recipes, 1)\n",
    "    freq_1 = find_frequent_itemsets(counter_1)\n",
    "    candidates_1 = sorted([t[0] for t in freq_1])\n",
    "    \n",
    "    # Get all tuples\n",
    "    coms = combine(candidates_1, 2)\n",
    "    counter_2 = counting(coms, recipes, 2)\n",
    "    candidates_2 = find_frequent_itemsets(counter_2)\n",
    "\n",
    "    # Get all triples\n",
    "    candidates_3 = generate_triple_from_tuple(candidates_2)\n",
    "\n",
    "    # Final screening and return the result\n",
    "    return find_frequent_itemsets(counting(candidates_3, recipes, 3))\n",
    "    \n",
    "%timeit apriori_algorithm(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time it costs us only 103ms to get the result. This is about **100X speedup!** In fact, if we analyze 4 times larger size of data, it still costs only 19.7 seconds to complete (5X speedup). But this is still not that satisfying. Can we do even better? We will introduce a further optimized algorithm, the **PCY Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCY Algorithm\n",
    "\n",
    "PCY Algorithm is an improvement of Apriori Algorithm, so most pars of PCY Algorithm is the same as Apriori Algorithm. We noticed that there are still some unused memory in round 1. \n",
    "\n",
    "![title](imgs/image08.png)\n",
    "\n",
    "\n",
    "We can still utilize these memory to further reduce the number of candidates. This is done by hashing. In the real world, normally the itemsets of size 2 will occupy the most amount of memory. So when we calculate the frequency of singletons, we can also make counters for pairs. Of course, we cannot assign counter for each pairs, but we could a counter for all pairs that have the same hash values. When we generate all candidates frequent pairs, we need also to look at the hash counter, to see whether the corresponding bucket has more frequency than our threshold (or called \"overflowed\")\n",
    "\n",
    "![title](imgs/image00.png)\n",
    "\n",
    "1. If the bucket is not overflowed, then we could directly conclude that the pair will not possible to become a frequency itemsets and thus, we should eliminate it before final screening.\n",
    "2. If the bucket is overflowed, then we could conclude that the pair will be a valid frequent pair. Please note that **we cannot ensure that this pair is frequent, since the bucket may contains other pairs, and the frequency of the bucket is actually the sum of all inside pairs.**\n",
    "\n",
    "A good question is that how to design the size of the hash counter and the hash functions. If the size of the hash table is too large, then we may consume too much memory. Conversely, if the size of the hash table is too small, then most of the bucket will overflow, and we cannot eliminate many pairs as we expected. Here we also provide a function to calcuate the overflow rate of the hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtable = {}\n",
    "\n",
    "def overflow_rate(hashtable, threshold):\n",
    "    counter = 0.0\n",
    "    for v in hashtable.values():\n",
    "        if v >= threshold:\n",
    "            counter += 1.0\n",
    "    return counter / len(hashtable)\n",
    "\n",
    "def get_hash(pair):\n",
    "    \"\"\"\n",
    "        This hash function calculates the hash value by multiply and then add these numbers\n",
    "    \"\"\"\n",
    "    return pair[0] * 2 + pair[1] * 3\n",
    "\n",
    "def pcy_counting_pair(hashtable, coms, recipes):\n",
    "    \"\"\"\n",
    "        For each itemset in coms, we iterate through recipes and ensure whether these condidates appear in the recipe.\n",
    "        This is the special version of PCY counting. It will hash the pairs to a bucket for each recipe\n",
    "    \"\"\"\n",
    "\n",
    "    counter = {}\n",
    "    for recipe in recipes:\n",
    "        recipe_coms = combine(recipe, 1)\n",
    "        for com in recipe_coms:\n",
    "            if com in coms:\n",
    "                if com not in counter:\n",
    "                    counter[com] = 0\n",
    "                counter[com] += 1   \n",
    "                \n",
    "        # Core component of PCY\n",
    "        recipe_coms = combine(recipe, 2)\n",
    "        for com in recipe_coms:\n",
    "            hash_value = get_hash(com)\n",
    "            if hash_value not in hashtable:\n",
    "                hashtable[hash_value] = 0\n",
    "            hashtable[hash_value] += 1\n",
    "   \n",
    "    return counter\n",
    "\n",
    "def pcy_combine(hashtable, nums, threshold=3):\n",
    "    \"\"\"\n",
    "        Generate the N-wise combination on the given array nums\n",
    "        This is the special version of PCY combining. It will also check if the generated pairs are in the hashtable\n",
    "        Args:\n",
    "            nums: target array\n",
    "            N: number of elements in the permutation\n",
    "        Returns:\n",
    "            Python array, containing all combinations of size N, using nums\n",
    "    \"\"\"\n",
    "    return set(i for i in itertools.combinations(nums, 2) if get_hash(i) in hashtable and hashtable[get_hash(i)] >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 111 ms per loop\n",
      "The overflow rate is\t1.0\n"
     ]
    }
   ],
   "source": [
    "def pcy_algorithm(hashtable, recipes):\n",
    "    \"\"\"\n",
    "        Given recipes, this function runs PCY Algorithm and obtains the frequent itemset\n",
    "        Args:\n",
    "            recipes: An array of recipe\n",
    "        Returns:\n",
    "            An array containing all frequent itemsets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all singletons\n",
    "    items = get_all_items(recipes)\n",
    "    coms = combine(items, 1)\n",
    "    \n",
    "    # Retains frequent singletons of size 1\n",
    "    counter_1 = pcy_counting_pair(hashtable, coms, recipes)\n",
    "    freq_1 = find_frequent_itemsets(counter_1)\n",
    "    candidates_1 = sorted([t[0] for t in freq_1])\n",
    "    \n",
    "    # Get all tuples\n",
    "    coms = pcy_combine(hashtable, candidates_1)\n",
    "    counter_2 = counting(coms, recipes, 2)\n",
    "    candidates_2 = find_frequent_itemsets(counter_2)\n",
    "    \n",
    "    # Get all triples\n",
    "    candidates_3 = generate_triple_from_tuple(candidates_2)\n",
    "\n",
    "    # Final screening and return the result\n",
    "    return find_frequent_itemsets(counting(candidates_3, recipes, 3))\n",
    "\n",
    "hashtable = {}\n",
    "%timeit pcy_algorithm(hashtable, recipes)\n",
    "print \"The overflow rate is\\t\", overflow_rate(hashtable, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it costs us 110ms to get the result for small dataset, we analyze 4 times larger size of data, it costs only 4.9 seconds to complete (4X speedup). That's great! But can we do even better?\n",
    "\n",
    "#### Multi-hash PCY Algorithm\n",
    "\n",
    "We can further optimize our algorithm by using Multi-hash version of PCY Algorithm. Instead of keeping only one hashtable, we can use multiple hashtables and hash functions, and we regard a candidate pair to be frequent only when the all corresponding buckets containing this pair have overflown. This will further reduce the number of candidates, thus speed up our algorithm.\n",
    "\n",
    "We will not provide implementation here since it's straightforward to implement this function. You are welcomed to implement Multi-hash Algorithm by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You need MapReduce and Spark\n",
    "\n",
    "You may find that we are using very small amount of data to do a demo. This is because these problems are in fact more suitable for distributed parallel computing. A single machine does not have enough memory and computing power to calculate frequent pairs over a large dataset. To transform our current implementation into a MapReduce or Spark version is not that hard. For example, Apriori Algorithm can be rewritten as a K-phase MapRduce program, corresponding to the size of the itemset, ranging from 1 to N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we explore several implementations in order to find frequent itemsets from buckets. We begin with the naive method, which blindly generates all possible candidate itemset and counting them one-by-one. Then we introduce Associate Rule and Apriori Algorithm, which will efficient eliminate most of the candidates before final screening. After that, we further optimizing the memory usage by introducing PCY Algorithm, which works pretty well when the amount of dataset grows."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
