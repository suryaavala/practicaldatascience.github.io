{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring the data \n",
    "\n",
    "In this tutorial, we are going to use stock data from Quandl API free edition, to estimate whether the specific target is worth to invest or not. \n",
    "\n",
    "The benefit for using this API is that the raw data you get is saved in pandas data type, so you can implement the data right away without parsing. \n",
    "\n",
    "Document for Quandl API.\n",
    "https://www.quandl.com/docs/api#introduction\n",
    "\n",
    "In order to use the Quandl API, first you need to download the Quandl liberary. Use pip install quandl to install the liberary. Make sure you already install Numby, Scipy and Pandas liberary in your python.\n",
    "\n",
    "First thing you need to do is import the liberaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import quandl\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Quandl API is free. If you would like to make more than 50 calls a day, however, you will need to create a free Quandl account, set your API key and authenticate you key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quandl.ApiConfig.api_key = 'JYRNJTevRtk46AwBNXDN'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's lots of database you can call in quandl, we're going to use the simplest dataset.\n",
    "\n",
    "Take Apple.inc for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = quandl.get(\"WIKI/AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Open   High    Low  Close     Volume  Ex-Dividend  Split Ratio  \\\n",
      "Date                                                                          \n",
      "1980-12-12  28.75  28.87  28.75  28.75  2093900.0          0.0          1.0   \n",
      "1980-12-15  27.38  27.38  27.25  27.25   785200.0          0.0          1.0   \n",
      "1980-12-16  25.37  25.37  25.25  25.25   472000.0          0.0          1.0   \n",
      "1980-12-17  25.87  26.00  25.87  25.87   385900.0          0.0          1.0   \n",
      "1980-12-18  26.63  26.75  26.63  26.63   327900.0          0.0          1.0   \n",
      "\n",
      "            Adj. Open  Adj. High  Adj. Low  Adj. Close  Adj. Volume  \n",
      "Date                                                                 \n",
      "1980-12-12   0.427992   0.429779  0.427992    0.427992  117258400.0  \n",
      "1980-12-15   0.407597   0.407597  0.405662    0.405662   43971200.0  \n",
      "1980-12-16   0.377675   0.377675  0.375889    0.375889   26432000.0  \n",
      "1980-12-17   0.385119   0.387054  0.385119    0.385119   21610400.0  \n",
      "1980-12-18   0.396432   0.398219  0.396432    0.396432   18362400.0  \n"
     ]
    }
   ],
   "source": [
    "print data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset include 12 feature we can utilize from 1980 to now. For this kind of predict problem, we can use the basic machine learning Artificial Neural Networks to solve this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Artificial Neural Networks is a comman machine learning method people are useing nowadays. Unlike the SVM model, ANN can creat a non linear model used for regression or classifications. The structure of ANN looks like below.\n",
    "\n",
    "<img src=\"files/ann.png\">\n",
    "\n",
    "The number of node and layers is the variable we can control to improve the result. In every node there is a weight link to another node. Our goal is to find the best weight set that fit our data the most. \n",
    "\n",
    "A widely used type of composition is the nonlinear weighted sum, which is sum of the input times their weight and pass through a either sigmoid for step function.\n",
    "\n",
    "<img src=\"files/ff.png\">\n",
    "\n",
    "Most of the algorithms used in training artificial neural networks is gradient descent and using backpropagation to compute the actual gradients. After we found the local error minimum, We can use this trained model to predict our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to predict how today's feature is going to influence tomorrow's prize. Therefore, our input will be today's feature, and output will be tomorrow's close price minus today's close price (To figure out how much it increase or decrease.) \n",
    "\n",
    "There are lots of ANN liberary we can utilize. We are going to use PyBrain in this tutorial.\n",
    "\n",
    "Document for PyBrain.\n",
    "http://pybrain.org/docs/index.html\n",
    "\n",
    "First, we build a empty Feed Forward Networks structure with 12 input, 20 hidden layer and 1 output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pybrain.structure import FeedForwardNetwork\n",
    "from pybrain.structure import LinearLayer, SigmoidLayer\n",
    "from pybrain.structure import FullConnection\n",
    "\n",
    "n = FeedForwardNetwork()\n",
    "\n",
    "inLayer = LinearLayer(12)\n",
    "hiddenLayer = SigmoidLayer(20)\n",
    "outLayer = LinearLayer(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use them, we have to add them to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n.addInputModule(inLayer)\n",
    "n.addModule(hiddenLayer)\n",
    "n.addOutputModule(outLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And connect all the nural to each others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNetwork-6\n",
      "   Modules:\n",
      "    [<LinearLayer 'LinearLayer-3'>, <SigmoidLayer 'SigmoidLayer-7'>, <LinearLayer 'LinearLayer-8'>]\n",
      "   Connections:\n",
      "    [<FullConnection 'FullConnection-18': 'SigmoidLayer-7' -> 'LinearLayer-8'>, <FullConnection 'FullConnection-19': 'LinearLayer-3' -> 'SigmoidLayer-7'>, <FullConnection 'FullConnection-4': 'SigmoidLayer-7' -> 'LinearLayer-8'>, <FullConnection 'FullConnection-5': 'LinearLayer-3' -> 'SigmoidLayer-7'>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_to_hidden = FullConnection(inLayer, hiddenLayer)\n",
    "hidden_to_out = FullConnection(hiddenLayer, outLayer)\n",
    "\n",
    "n.addConnection(in_to_hidden)\n",
    "n.addConnection(hidden_to_out)\n",
    "n.sortModules()\n",
    "\n",
    "print n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input a simple dataset to see whether it works or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51195787])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " n.activate([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to create the Dataset so we can train our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputData = []\n",
    "\n",
    "i = 0\n",
    "while i < len(data):\n",
    "    d = data.iloc[i,:].tolist()\n",
    "    inputData.append(d)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputData = []\n",
    "\n",
    "i = 0\n",
    "while i < len(data) - 1:\n",
    "    d = data.iloc[i+1,3] - data.iloc[i,3]\n",
    "    outputData.append(d)\n",
    "    i = i + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that length of inputData and outputData are not the same because of outputData is subtraction of tomorrow's price and today's price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9053\n",
      "9053\n",
      "9052\n"
     ]
    }
   ],
   "source": [
    "print len(data)\n",
    "print len(inputData)\n",
    "print len(outputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the last data (Today's feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9052\n"
     ]
    }
   ],
   "source": [
    "del inputData[-1]\n",
    "print len(inputData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are going to use supervised regression training. We create the Dataset to store our previous input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9052"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pybrain.datasets import SupervisedDataSet\n",
    "DS = SupervisedDataSet(12, 1)\n",
    "\n",
    "i = 0\n",
    "while i < len(inputData):\n",
    "    DS.appendLinked(inputData[i], outputData[i])\n",
    "    i = i + 1\n",
    "    \n",
    "len(DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After create the ANN model and dataset, we can now train our data by creating a trainer by PyBrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "trainer = BackpropTrainer(n, DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .train() operator to train our data. The trainer now knows about the network and the dataset and we can train the net on the data. This call trains the net for one full epoch and returns a double proportional to the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.02740855737029"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we only run one epoch, therefor the error is still pretty big. We can you another function .trainUntilConvergence() to train the model until it converge. We can input some variable such as max epochs, proportion of the validation data to improve our training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-errors: [  28.553605  28.450876  28.534078  28.395738  28.573129  28.522702  28.466523  28.442355  28.427197  28.607837  28.460477  28.503947  28.295400  28.448472  28.496585  28.389732  28.400720  28.491007  28.462959  28.314853  28.484003  28.466464  28.491665  28.447130  28.365838  28.526775  28.411707  28.494318  28.460812  28.399632  28.474114  28.463323  28.434803  28.432820  28.473464  28.397497  28.489896  28.529773  28.384167  28.535530  28.524136  28.429077  28.465292  28.537875  28.523297  28.344236  28.514974  28.487402  28.454055  28.521658  28.544148  28.447536  28.555750  28.461761  28.525229  28.435509  28.543606  28.623802  28.357472  28.434045  28.358381  28.607963  28.492648  28.505051  28.497292  28.440902  28.427037  28.419809  28.426526  28.417991  28.488478  28.281443  28.495775  28.434601  28.457321  28.331872  28.354853  28.463425  28.539600  28.409772  28.426272  28.438169  28.425740  28.415118  28.475638  28.513107  28.444468  28.451692  28.452744  28.415130  28.441428  28.444641  28.471704  28.425883  28.168354  28.414938  28.461143  28.561075  28.557351  28.495164  28.515163  25.323755]\n",
      "valid-errors: [  5.649790  8.066145  9.481849  5.850492  4.776510  4.823711  6.269922  5.726780  4.827780  5.169444  6.518627  5.095646  5.174669  5.063637  5.692066  5.143430  5.734713  4.830222  4.745141  4.966462  4.921914  4.948479  6.289207  4.913740  4.799381  4.786539  4.751685  4.888634  5.336790  4.825648  4.757664  4.760411  4.861608  4.796077  5.029297  4.798510  5.068100  6.752693  4.757949  4.819779  5.129553  4.928398  4.765522  5.518068  5.116982  4.785974  4.872966  5.362227  6.460446  5.015631  4.748561  5.557417  4.888008  4.771335  4.773517  5.256596  5.063195  5.112185  5.109381  4.763903  4.865928  5.240166  4.968141  4.793854  4.741405  5.677762  4.791977  6.337152  4.826527  4.795867  4.747992  4.847895  4.840367  6.239154  8.613062  4.739572  4.831362  4.817600  4.745289  4.813375  4.824755  4.783967  20.972289  4.795642  6.340738  4.908551  4.775208  4.740799  4.748436  4.734810  4.747662  4.830119  9.505511  4.747934  4.998343  4.862248  5.267148  4.909247  4.753485  6.347009  4.781924  4.905629]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([28.553605395034666,\n",
       "  28.450876302543222,\n",
       "  28.534078419492648,\n",
       "  28.395738262865347,\n",
       "  28.573129046916396,\n",
       "  28.522702311341831,\n",
       "  28.466522859307432,\n",
       "  28.442355463529573,\n",
       "  28.427197250143962,\n",
       "  28.607836531603567,\n",
       "  28.460476891745515,\n",
       "  28.503947362154147,\n",
       "  28.29540025270488,\n",
       "  28.448472294385557,\n",
       "  28.496585045133912,\n",
       "  28.389732455887213,\n",
       "  28.400720477568491,\n",
       "  28.491006539850503,\n",
       "  28.462958532501929,\n",
       "  28.314853061052272,\n",
       "  28.484002961051239,\n",
       "  28.466464429819592,\n",
       "  28.491664735955901,\n",
       "  28.447130410145093,\n",
       "  28.365838018546114,\n",
       "  28.526775181724812,\n",
       "  28.411706918037151,\n",
       "  28.494318450611313,\n",
       "  28.460811684885435,\n",
       "  28.399632221765064,\n",
       "  28.474114291187519,\n",
       "  28.463323014616048,\n",
       "  28.434803028866678,\n",
       "  28.432820361523127,\n",
       "  28.473464203009311,\n",
       "  28.397497261251871,\n",
       "  28.489896265626882,\n",
       "  28.52977268991043,\n",
       "  28.384167407969183,\n",
       "  28.535529955046197,\n",
       "  28.524135516295217,\n",
       "  28.429077493953198,\n",
       "  28.465291917155032,\n",
       "  28.537874849818625,\n",
       "  28.523297414701389,\n",
       "  28.344236251588406,\n",
       "  28.514973901190501,\n",
       "  28.48740245355263,\n",
       "  28.45405461039832,\n",
       "  28.521657532965293,\n",
       "  28.544147631887657,\n",
       "  28.447536059853739,\n",
       "  28.55575010002385,\n",
       "  28.461760763742237,\n",
       "  28.52522912608897,\n",
       "  28.435508550645636,\n",
       "  28.543605785134414,\n",
       "  28.623801815391502,\n",
       "  28.357472179816543,\n",
       "  28.434045045266949,\n",
       "  28.358380779949023,\n",
       "  28.607963017638074,\n",
       "  28.492648340346747,\n",
       "  28.505051016798696,\n",
       "  28.497291773165138,\n",
       "  28.440902083922499,\n",
       "  28.427036514661996,\n",
       "  28.419808644738623,\n",
       "  28.426525717489561,\n",
       "  28.417990813469579,\n",
       "  28.488477717745617,\n",
       "  28.281443296829348,\n",
       "  28.49577510181302,\n",
       "  28.434601051378987,\n",
       "  28.457320768972895,\n",
       "  28.331872087896972,\n",
       "  28.35485307474374,\n",
       "  28.463425446756062,\n",
       "  28.539600257982929,\n",
       "  28.4097716224226,\n",
       "  28.426272046281888,\n",
       "  28.438168595152685,\n",
       "  28.425740441930579,\n",
       "  28.41511848997775,\n",
       "  28.475637734839886,\n",
       "  28.513107392192055,\n",
       "  28.444467892671792,\n",
       "  28.451691889870357,\n",
       "  28.452743680327117,\n",
       "  28.415130235643048,\n",
       "  28.441427853636139,\n",
       "  28.444640622724197,\n",
       "  28.471704168411655,\n",
       "  28.425882814379534,\n",
       "  28.168353549299358,\n",
       "  28.414937964424094,\n",
       "  28.461142608030208,\n",
       "  28.561075492206562,\n",
       "  28.5573505782071,\n",
       "  28.495163949510189,\n",
       "  28.515163032821338,\n",
       "  25.323754693078182],\n",
       " [5.6497899438576367,\n",
       "  8.066145210159835,\n",
       "  9.4818492500086933,\n",
       "  5.8504920541239613,\n",
       "  4.7765096417487021,\n",
       "  4.8237111765687271,\n",
       "  6.2699223870010332,\n",
       "  5.7267795757120421,\n",
       "  4.8277802082664083,\n",
       "  5.1694443327898565,\n",
       "  6.5186272440172504,\n",
       "  5.0956462881028282,\n",
       "  5.1746692468535773,\n",
       "  5.0636369560865377,\n",
       "  5.692065833700239,\n",
       "  5.1434296195668265,\n",
       "  5.7347127744410491,\n",
       "  4.8302224309898341,\n",
       "  4.7451407667428205,\n",
       "  4.9664623148695357,\n",
       "  4.9219135911744401,\n",
       "  4.9484792446585502,\n",
       "  6.2892074129511268,\n",
       "  4.9137401086660573,\n",
       "  4.799380925162529,\n",
       "  4.7865392108668061,\n",
       "  4.7516845451267979,\n",
       "  4.8886343103024172,\n",
       "  5.3367897198430656,\n",
       "  4.8256481000455471,\n",
       "  4.7576643483568875,\n",
       "  4.760411041140773,\n",
       "  4.8616081078931339,\n",
       "  4.7960769012224169,\n",
       "  5.029296628313527,\n",
       "  4.7985098982950225,\n",
       "  5.0681003724226494,\n",
       "  6.7526932697654107,\n",
       "  4.7579494886296194,\n",
       "  4.8197787122673574,\n",
       "  5.1295531681595206,\n",
       "  4.9283975523542303,\n",
       "  4.7655224142023869,\n",
       "  5.5180681961301126,\n",
       "  5.1169817690868094,\n",
       "  4.785973761845737,\n",
       "  4.8729662880786728,\n",
       "  5.3622271765050957,\n",
       "  6.4604458074555788,\n",
       "  5.0156312632089257,\n",
       "  4.7485613112247256,\n",
       "  5.5574170404586418,\n",
       "  4.8880083075792644,\n",
       "  4.7713351362689762,\n",
       "  4.7735168005247335,\n",
       "  5.2565958090887088,\n",
       "  5.0631947472655439,\n",
       "  5.1121852007261763,\n",
       "  5.1093814821435677,\n",
       "  4.7639033655623235,\n",
       "  4.8659281619033736,\n",
       "  5.2401659135593963,\n",
       "  4.9681412893726815,\n",
       "  4.7938539175425179,\n",
       "  4.7414053917681915,\n",
       "  5.6777615558721042,\n",
       "  4.7919769188661752,\n",
       "  6.3371515868512676,\n",
       "  4.8265265124544268,\n",
       "  4.795867408356016,\n",
       "  4.7479918155349567,\n",
       "  4.8478954091517243,\n",
       "  4.840367104999217,\n",
       "  6.2391539969445677,\n",
       "  8.6130624882857667,\n",
       "  4.7395715434214614,\n",
       "  4.8313623126196861,\n",
       "  4.8175996182025544,\n",
       "  4.7452885130020235,\n",
       "  4.8133747983520685,\n",
       "  4.8247549918993702,\n",
       "  4.7839669252316677,\n",
       "  20.972288758219129,\n",
       "  4.7956422562421208,\n",
       "  6.3407384890457275,\n",
       "  4.9085509510402101,\n",
       "  4.7752083199489013,\n",
       "  4.7407987138355718,\n",
       "  4.7484359607549633,\n",
       "  4.7348098327497148,\n",
       "  4.7476618148648706,\n",
       "  4.830119027542441,\n",
       "  9.5055106318503846,\n",
       "  4.7479337313993382,\n",
       "  4.9983430037120389,\n",
       "  4.8622484921386491,\n",
       "  5.2671480618225219,\n",
       "  4.9092471341590356,\n",
       "  4.7534848311462943,\n",
       "  6.3470086831253685,\n",
       "  4.7819240704552328,\n",
       "  4.9056288014058058])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.trainUntilConvergence(verbose = True, validationProportion = 0.15, maxEpochs = 100, continueEpochs = 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Revisit the model\n",
    "\n",
    "Because the model is relatively complex. A classification would be easier to train and better in result. We create 6 classifier in this model, (+ 0% ~ 5%), (+ 5% ~ 10%), (+ 10% ~ ) and negitive one. \n",
    "\n",
    "Now we need to revisit our output data. The out put data now become a classifier with 6 variable. For example, (1,0,0,0,0,0) for (+ 10% ~ ), (0,1,0,0,0,0) for (+ 5% ~ 10%) and so on. \n",
    "\n",
    "After the data run through our model, the out put will be a list with 6 number. We chose the higher one to be our result. For example, (0.7, 0.2, 0.5, 0.4, 0.3, 0.1) we will classify it to (1,0,0,0,0,0) which is (+ 10% ~ )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.datasets import ClassificationDataSet\n",
    "from pybrain.tools.shortcuts import buildNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the previous output data to fit the classfier we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputDataClass = []\n",
    "\n",
    "i = 0\n",
    "while i < len(data) - 1:\n",
    "    d = (data.iloc[i+1,3] - data.iloc[i,3])/data.iloc[i,3]\n",
    "    if d > 0.1:\n",
    "        d = 0\n",
    "    elif d <= 0.1 and d > 0.05:\n",
    "        d = 1\n",
    "    elif d <= 0.05 and d > 0:\n",
    "        d = 2\n",
    "    elif d <= 0 and d > - 0.05:\n",
    "        d = 3\n",
    "    elif d <= -0.05 and d > - 0.1:\n",
    "        d = 4\n",
    "    elif d <= -0.1:\n",
    "        d = 5\n",
    "    outputDataClass.append(d)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the lengh of our output is correct to make sure we didn't miss any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9052\n"
     ]
    }
   ],
   "source": [
    "print len(outputDataClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a classification dataset, it got 12 input features, 1 output class and total classes number of 6.\n",
    "\n",
    "Split the data into 75% for training 25% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DSC = ClassificationDataSet(12, 1, nb_classes=6)\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < len(inputData):\n",
    "    DSC.addSample(inputData[i], outputDataClass[i])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tstdata, trndata = DSC.splitWithProportion( 0.25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trndata._convertToOneOfMany( )\n",
    "tstdata._convertToOneOfMany( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training patterns:  6789\n",
      "Input and output dimensions:  12 6\n",
      "First sample (input, target, class):\n",
      "[  2.87500000e+01   2.88700000e+01   2.87500000e+01   2.87500000e+01\n",
      "   2.09390000e+06   0.00000000e+00   1.00000000e+00   4.27992245e-01\n",
      "   4.29778648e-01   4.27992245e-01   4.27992245e-01   1.17258400e+08] [0 0 0 0 1 0] [ 4.]\n"
     ]
    }
   ],
   "source": [
    "print \"Number of training patterns: \", len(trndata)\n",
    "print \"Input and output dimensions: \", trndata.indim, trndata.outdim\n",
    "print \"First sample (input, target, class):\"\n",
    "print trndata['input'][0], trndata['target'][0], trndata['class'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of our training data is 6789. And the input and output dimensions is correct.\n",
    "\n",
    "Print out the first sample in out traing dataset. \n",
    "\n",
    "Looks wonderful, we are good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple network called fnn. Specify the input dimensions, numbers of hidden node, out put dimensions and linked the nodes together.\n",
    "\n",
    "Again create a trainer and to train our fnn network, this time we add some momentum to increase its training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fnn = buildNetwork( trndata.indim, 20, trndata.outdim)\n",
    "\n",
    "fnn = FeedForwardNetwork()\n",
    "\n",
    "inLayer = LinearLayer(12)\n",
    "hiddenLayer = SigmoidLayer(20)\n",
    "outLayer = LinearLayer(1)\n",
    "\n",
    "fnn.addInputModule(inLayer)\n",
    "fnn.addModule(hiddenLayer)\n",
    "fnn.addOutputModule(outLayer)\n",
    "\n",
    "in_to_hidden = FullConnection(inLayer, hiddenLayer)\n",
    "hidden_to_out = FullConnection(hiddenLayer, outLayer)\n",
    "\n",
    "fnn.addConnection(in_to_hidden)\n",
    "fnn.addConnection(hidden_to_out)\n",
    "fnn.sortModules()\n",
    "\n",
    "\n",
    "classTrainer = BackpropTrainer( fnn, dataset=trndata, momentum=0.1, verbose=True, weightdecay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error: 0.0638048225759\n",
      "Total error: 0.0509010353693\n",
      "Total error: 0.0507027125006\n",
      "Total error: 0.0507605724702\n",
      "Total error: 0.0508695266371\n",
      "Total error: 0.0507172102764\n",
      "Total error: 0.0509452069536\n",
      "Total error: 0.0511356610802\n",
      "Total error: 0.0507979775721\n",
      "Total error: 0.0508791874927\n",
      "Total error: 0.0505876462502\n",
      "Total error: 0.0506849220118\n",
      "Total error: 0.0505828725658\n",
      "Total error: 0.0507852382341\n",
      "Total error: 0.0508089991174\n",
      "Total error: 0.0504770965099\n",
      "Total error: 0.0511221468637\n",
      "Total error: 0.0513101172487\n",
      "Total error: 0.0515267950013\n",
      "Total error: 0.0515438999045\n",
      "Total error: 0.0508353630618\n",
      "Total error: 0.0506452406477\n",
      "Total error: 0.0510346286347\n",
      "Total error: 0.0509282289389\n",
      "Total error: 0.0510165845963\n",
      "Total error: 0.0516498835615\n",
      "Total error: 0.0519517015875\n",
      "Total error: 0.0520851928759\n",
      "Total error: 0.051717467268\n",
      "Total error: 0.0522422933662\n",
      "Total error: 0.0514518442668\n",
      "Total error: 0.0514376221099\n",
      "Total error: 0.0511513491329\n",
      "Total error: 0.0511788942963\n",
      "Total error: 0.0511956717866\n",
      "Total error: 0.0517409677485\n",
      "Total error: 0.0517925989802\n",
      "Total error: 0.052377606083\n",
      "Total error: 0.0522028916044\n",
      "Total error: 0.0519599116904\n",
      "Total error: 0.051615191789\n",
      "Total error: 0.0512965914864\n",
      "Total error: 0.0514678169037\n",
      "Total error: 0.051832705928\n",
      "Total error: 0.0521796200511\n",
      "Total error: 0.0524949306447\n",
      "Total error: 0.0524746083843\n",
      "Total error: 0.052769331127\n",
      "Total error: 0.0524085885537\n",
      "Total error: 0.0508806966008\n",
      "Total error: 0.0510859640791\n",
      "Total error: 0.050324744287\n",
      "Total error: 0.0505309423421\n",
      "Total error: 0.0505883835916\n",
      "Total error: 0.0505345279565\n",
      "Total error: 0.049974610207\n",
      "Total error: 0.0498269427881\n",
      "Total error: 0.05005540113\n",
      "Total error: 0.0499122263535\n",
      "Total error: 0.0497864787357\n",
      "Total error: 0.050547033596\n",
      "Total error: 0.0512958422173\n",
      "Total error: 0.0519513150364\n",
      "Total error: 0.0518264992012\n",
      "Total error: 0.0518877937804\n",
      "Total error: 0.0523771932755\n",
      "Total error: 0.0520244886084\n",
      "Total error: 0.0513612987345\n",
      "Total error: 0.0517683577872\n",
      "Total error: 0.0516661939935\n",
      "Total error: 0.0518554064526\n",
      "Total error: 0.0513463459598\n",
      "Total error: 0.0505800351567\n",
      "Total error: 0.0504790171889\n",
      "Total error: 0.0501600340397\n",
      "Total error: 0.0502879225009\n",
      "Total error: 0.0496596510437\n",
      "Total error: 0.0500565640192\n",
      "Total error: 0.0500771530784\n",
      "Total error: 0.0500736494911\n",
      "Total error: 0.0496480029772\n",
      "Total error: 0.0498489763232\n",
      "Total error: 0.0498379366785\n",
      "Total error: 0.0503810663774\n",
      "Total error: 0.0505566899026\n",
      "Total error: 0.0504290732729\n",
      "Total error: 0.0500868718016\n",
      "Total error: 0.0506574825487\n",
      "Total error: 0.0507449838028\n",
      "Total error: 0.0509806076062\n",
      "Total error: 0.0508669368593\n",
      "Total error: 0.0514524040372\n",
      "Total error: 0.0515892462137\n",
      "Total error: 0.0515012396401\n",
      "Total error: 0.0510286515764\n",
      "Total error: 0.0504291115842\n",
      "Total error: 0.049168069182\n",
      "Total error: 0.0494414197268\n",
      "Total error: 0.0492634034087\n",
      "Total error: 0.0495792039918\n",
      "Total error: 0.0499011373772\n",
      "train-errors: [  0.063805  0.050901  0.050703  0.050761  0.050870  0.050717  0.050945  0.051136  0.050798  0.050879  0.050588  0.050685  0.050583  0.050785  0.050809  0.050477  0.051122  0.051310  0.051527  0.051544  0.050835  0.050645  0.051035  0.050928  0.051017  0.051650  0.051952  0.052085  0.051717  0.052242  0.051452  0.051438  0.051151  0.051179  0.051196  0.051741  0.051793  0.052378  0.052203  0.051960  0.051615  0.051297  0.051468  0.051833  0.052180  0.052495  0.052475  0.052769  0.052409  0.050881  0.051086  0.050325  0.050531  0.050588  0.050535  0.049975  0.049827  0.050055  0.049912  0.049786  0.050547  0.051296  0.051951  0.051826  0.051888  0.052377  0.052024  0.051361  0.051768  0.051666  0.051855  0.051346  0.050580  0.050479  0.050160  0.050288  0.049660  0.050057  0.050077  0.050074  0.049648  0.049849  0.049838  0.050381  0.050557  0.050429  0.050087  0.050657  0.050745  0.050981  0.050867  0.051452  0.051589  0.051501  0.051029  0.050429  0.049168  0.049441  0.049263  0.049579  0.049901  0.047753]\n",
      "valid-errors: [  4.036106  0.049784  0.053977  0.049349  0.049051  0.051199  0.050283  0.050536  0.047889  0.048096  0.048059  0.050213  0.048622  0.056716  0.049699  0.057406  0.053902  0.055412  0.047335  0.048030  0.047750  0.051030  0.054238  0.047277  0.047761  0.047909  0.048490  0.047892  0.052065  0.053560  0.052018  0.050286  0.048164  0.049609  0.067630  0.054760  0.047879  0.048528  0.047552  0.049433  0.049316  0.048702  0.048110  0.048000  0.048126  0.048099  0.049922  0.054227  0.050031  0.048269  0.052515  0.048740  0.050188  0.056241  0.053153  0.047477  0.049253  0.047892  0.047878  0.048292  0.047696  0.050568  0.047404  0.048492  0.047932  0.047750  0.051821  0.047572  0.047498  0.051397  0.048440  0.048389  0.057344  0.048877  0.047404  0.052848  0.049376  0.050777  0.057110  0.049480  0.050417  0.047305  0.047694  0.051014  0.047943  0.048018  0.048472  0.047436  0.047838  0.052378  0.051347  0.050743  0.047848  0.050613  0.052198  0.047626  0.047725  0.047288  0.047925  0.047993  0.047783  0.047209]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.063804822575856998,\n",
       "  0.0509010353692535,\n",
       "  0.050702712500644835,\n",
       "  0.050760572470228769,\n",
       "  0.050869526637057377,\n",
       "  0.050717210276405779,\n",
       "  0.050945206953577994,\n",
       "  0.051135661080247655,\n",
       "  0.050797977572075756,\n",
       "  0.050879187492698957,\n",
       "  0.050587646250246081,\n",
       "  0.050684922011788404,\n",
       "  0.05058287256582935,\n",
       "  0.050785238234051955,\n",
       "  0.050808999117377629,\n",
       "  0.050477096509889387,\n",
       "  0.051122146863684229,\n",
       "  0.051310117248672085,\n",
       "  0.05152679500129307,\n",
       "  0.051543899904514308,\n",
       "  0.050835363061766566,\n",
       "  0.05064524064770521,\n",
       "  0.051034628634669336,\n",
       "  0.050928228938866969,\n",
       "  0.051016584596283873,\n",
       "  0.051649883561489598,\n",
       "  0.051951701587489893,\n",
       "  0.052085192875887858,\n",
       "  0.051717467268018028,\n",
       "  0.052242293366197304,\n",
       "  0.051451844266802382,\n",
       "  0.051437622109888892,\n",
       "  0.051151349132916843,\n",
       "  0.051178894296255772,\n",
       "  0.051195671786575775,\n",
       "  0.051740967748494432,\n",
       "  0.051792598980151539,\n",
       "  0.052377606082964087,\n",
       "  0.052202891604446591,\n",
       "  0.051959911690378766,\n",
       "  0.051615191788989052,\n",
       "  0.051296591486350691,\n",
       "  0.051467816903667557,\n",
       "  0.051832705927953533,\n",
       "  0.052179620051082017,\n",
       "  0.05249493064468095,\n",
       "  0.052474608384340649,\n",
       "  0.052769331127002703,\n",
       "  0.052408588553733079,\n",
       "  0.05088069660080477,\n",
       "  0.051085964079128035,\n",
       "  0.050324744287035379,\n",
       "  0.050530942342064414,\n",
       "  0.050588383591555285,\n",
       "  0.050534527956533598,\n",
       "  0.049974610207009648,\n",
       "  0.049826942788135854,\n",
       "  0.050055401129962269,\n",
       "  0.049912226353494155,\n",
       "  0.049786478735747527,\n",
       "  0.050547033596013764,\n",
       "  0.051295842217332313,\n",
       "  0.051951315036400783,\n",
       "  0.051826499201161128,\n",
       "  0.051887793780402758,\n",
       "  0.052377193275532766,\n",
       "  0.052024488608372312,\n",
       "  0.051361298734481496,\n",
       "  0.051768357787249607,\n",
       "  0.051666193993515123,\n",
       "  0.051855406452596238,\n",
       "  0.051346345959823063,\n",
       "  0.050580035156689893,\n",
       "  0.050479017188939976,\n",
       "  0.050160034039737747,\n",
       "  0.050287922500936541,\n",
       "  0.049659651043681696,\n",
       "  0.050056564019190689,\n",
       "  0.050077153078424572,\n",
       "  0.050073649491053099,\n",
       "  0.049648002977176475,\n",
       "  0.049848976323245774,\n",
       "  0.049837936678539377,\n",
       "  0.050381066377373156,\n",
       "  0.050556689902582046,\n",
       "  0.050429073272907105,\n",
       "  0.05008687180156933,\n",
       "  0.05065748254870539,\n",
       "  0.050744983802766368,\n",
       "  0.050980607606187396,\n",
       "  0.050866936859349579,\n",
       "  0.051452404037189342,\n",
       "  0.051589246213663696,\n",
       "  0.05150123964007948,\n",
       "  0.051028651576435741,\n",
       "  0.050429111584194344,\n",
       "  0.049168069181970676,\n",
       "  0.049441419726753831,\n",
       "  0.049263403408685139,\n",
       "  0.049579203991796335,\n",
       "  0.049901137377215556,\n",
       "  0.047752631522477683],\n",
       " [4.0361055251988951,\n",
       "  0.04978371864396481,\n",
       "  0.053977014715006946,\n",
       "  0.049349193042447496,\n",
       "  0.049050711508722949,\n",
       "  0.051199366746718857,\n",
       "  0.050282862249047315,\n",
       "  0.050536012777974343,\n",
       "  0.047889130198438025,\n",
       "  0.048095869255468569,\n",
       "  0.048058716315086861,\n",
       "  0.050213024285508008,\n",
       "  0.048622173243836962,\n",
       "  0.056715689398902466,\n",
       "  0.04969852113740042,\n",
       "  0.057406469602096694,\n",
       "  0.053901840699938017,\n",
       "  0.055412311287201255,\n",
       "  0.047335308262863959,\n",
       "  0.048030388812105265,\n",
       "  0.047750026620612118,\n",
       "  0.051030470135042673,\n",
       "  0.054238079774835864,\n",
       "  0.047277344351498621,\n",
       "  0.047760759799616734,\n",
       "  0.047908947633528695,\n",
       "  0.048489915531859697,\n",
       "  0.047891894577268612,\n",
       "  0.052064824950527555,\n",
       "  0.053560301814463661,\n",
       "  0.052017553205376023,\n",
       "  0.050286384633902545,\n",
       "  0.048164067151771205,\n",
       "  0.049608568533006298,\n",
       "  0.067629594038980514,\n",
       "  0.054759879936291764,\n",
       "  0.047878703811849076,\n",
       "  0.04852776024582367,\n",
       "  0.047552471704916513,\n",
       "  0.049433100235596217,\n",
       "  0.049316214996163218,\n",
       "  0.048702202293546777,\n",
       "  0.048110235548680928,\n",
       "  0.048000082427557197,\n",
       "  0.048126344742336465,\n",
       "  0.048098668370150185,\n",
       "  0.049922150812168858,\n",
       "  0.054227494228924582,\n",
       "  0.050030753271380672,\n",
       "  0.048269284556765969,\n",
       "  0.052514831817641515,\n",
       "  0.048739982395527341,\n",
       "  0.050188279827736965,\n",
       "  0.056241468331100108,\n",
       "  0.053153065521316163,\n",
       "  0.047477101108815076,\n",
       "  0.049253010266717287,\n",
       "  0.047892416339660085,\n",
       "  0.047877660973029472,\n",
       "  0.048292196927490844,\n",
       "  0.047695671998113733,\n",
       "  0.050567531452298732,\n",
       "  0.047404406676109311,\n",
       "  0.048491500505056177,\n",
       "  0.047931528953442756,\n",
       "  0.047749907600712516,\n",
       "  0.051820917306715307,\n",
       "  0.047572015962663791,\n",
       "  0.047498068001115772,\n",
       "  0.051397012274872464,\n",
       "  0.048440175709104367,\n",
       "  0.048388676832810215,\n",
       "  0.05734418110420722,\n",
       "  0.04887719578596441,\n",
       "  0.047403833894028101,\n",
       "  0.052847648893979998,\n",
       "  0.049375570513880064,\n",
       "  0.050777208888979269,\n",
       "  0.057110347486009738,\n",
       "  0.049480181800752095,\n",
       "  0.050416903594433411,\n",
       "  0.04730520595313796,\n",
       "  0.047693652940519152,\n",
       "  0.051014438749676719,\n",
       "  0.04794251833264461,\n",
       "  0.048017533303177151,\n",
       "  0.048471807691075043,\n",
       "  0.047436136262660702,\n",
       "  0.047837927475592172,\n",
       "  0.052378247053787894,\n",
       "  0.051347174123488326,\n",
       "  0.050742853841698049,\n",
       "  0.047848144976173436,\n",
       "  0.050613070554451051,\n",
       "  0.052197566164104287,\n",
       "  0.04762574538985305,\n",
       "  0.047724579448702988,\n",
       "  0.047287899170035189,\n",
       "  0.04792489998971456,\n",
       "  0.047993430422498706,\n",
       "  0.047783106003794457,\n",
       "  0.047209420830041302])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classTrainer.trainUntilConvergence( verbose = True, validationProportion = 0.15, maxEpochs = 100,  continueEpochs = 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error of the classifier is much smaller then regression. Now we can use this ANN to predic our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre = fnn.activateOnDataset( trndata )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output of this prediction is still looks like [0.7, 0.2, 0.5, 0.4, 0.3, 0.1]. The highest output activation gives the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "preClass = np.argmax(pre, axis=1)\n",
    "\n",
    "print preClass[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By our definition, class 3 means that the stock price will decrease about 0 ~ 0.05 percent. We can now successfully predict the value of stock price.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "ANN is a very good model to make your prediction. Befor you train your data, like other machine learning model, you should check you input features to see whether it is reasonable or not. There are lots of variable you can adjust to make your model better. In this tutorial, we only consider one hidden layer and 20 hiden node. Those are the variables that you can change to improve your model. \n",
    "\n",
    "In reality, useing only ANN to predict the financial problem is not wisely. ANN can not point out the potention risk behind the data, which may cause a big loss in stock market. However, It can be a useful tool for you do decide which company that is worth to invest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
