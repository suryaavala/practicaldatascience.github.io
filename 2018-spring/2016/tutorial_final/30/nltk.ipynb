{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Have you ever wondered what are some writing habits of Herman Melville, the author of the Americal national novel, *Moby Dick*? What are his favorite verbs and adjectives in this great novel? In this toturial, we will show some basic natural language processing techniques with python. \n",
    "\n",
    "[<img src=\"https://images-na.ssl-images-amazon.com/images/I/71Q4R237BZL.jpg\" style=\"width: 400px;\">](https://images-na.ssl-images-amazon.com/images/I/71Q4R237BZL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "\n",
    "This tutorial will mainly show how to do basic natural language processing with python with [`nltk`](http://www.nltk.org/) package.\n",
    "\n",
    "`nltk` is short for Natural Language Toolkit. It is a package that provides easy-to-use interfaces for python programs to process natural language data. `nltk` is a massive library that can't all be covered within the realm of this tutorial. This tutorial would discuss nltk about its ability to tokenize, stem, tag and semantic reasoning.\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Tokenizing](#Tokenizing)\n",
    "- [Stemming](#Stemming)\n",
    "- [Natural Language](#Natural Language)\n",
    "- [Putting things together: basic NLP on *Moby Dick*](#Putting things together)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `nltk` using `pip`:\n",
    "\n",
    "    $ pip install --upgrade nltk\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download necessary `nltk` packages manually. For this tutorial, please install the following packages.\n",
    "\n",
    "  ```python\n",
    "  >>>nltk.download('treebank')\n",
    "  >>>nltk.download('wordnet')\n",
    "  >>>nltk.download('punkt')\n",
    "  >>>nltk.download('gutenberg')\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression is powerful and handy to use. `nltk` tokenizer supports regular expression to control how to tokenize text. However, regular expression can be costly and over complicated, so it's better to use it do simple tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "regex_tokenizer = RegexpTokenizer('[\\w]+')\n",
    "print regex_tokenizer.tokenize('This is a sentence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treebank Tokenizer\n",
    "`TreebankWordTokenizer` is the underlying instance for `word_tokenizer()`, which creates a list of words from a string using spaces and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "print treebank_tokenizer.tokenize('This is a sentence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `TreebankWordTokenizer` keeps the punctuation. This gives to more freedom to decide on how to deal with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer for Other Languages\n",
    "nltk is built with human language in mind, and there are many languages in the world. It can tokenize languages other than English. The following code tokenizes spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Buenos dias!', 'Como estas?']\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "print spanish_tokenizer.tokenize('Buenos dias! Como estas?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming removes suffixes from a word and only keeps the 'stem' of the word. Some frequent suffix to be removed are \"es\", \"ing\". \"y\" is replaced with \"i\". Here are 2 examples of stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preffix\n",
      "look\n",
      "lucki\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "print porter_stemmer.stem('preffixes')\n",
    "print porter_stemmer.stem('looking')\n",
    "print porter_stemmer.stem('lucky')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stemming algorithm used above is Porter Stemming. You can find more information about the algorithm [here](http://snowball.tartarus.org/algorithms/porter/stemmer.html). There are many stemming algorithms, but they have similar processes and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming vs. Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing find the root word of a word. It converts a variation of a word back to its original form. The biggest difference between lemmatization and stemming is that lemmatization always produces a valid word, while stemming simply get rid of unwanted parts of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "themselves\n",
      "themselv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "word_net_lemmatizer = WordNetLemmatizer()\n",
    "print word_net_lemmatizer.lemmatize('themselves')\n",
    "print porter_stemmer.stem('themselves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some applications choose a stemmer rather than a lemmatizer because stemmed words are more uniform and are usually more suitable for further processes. Also, a precise lemmatizer requires comprehensive knowledge of all variations of all words from a text. It's a much costly option than a stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging\n",
    "`nltk` provides powerful tagging tools to tag words with part-of-speech. We uses `POS-tagger` to tag sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sentence', 'NN')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(regex_tokenizer.tokenize('This is a sentence.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will be using **singular noun: 'NN', verb: 'VB', adjective: 'JJ', and adverb: 'RB'**. \n",
    " \n",
    " Here's the full [list](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) of part-of-speech tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous sections are mainly about how to preprocess texts. Natural language processing is not just about strings, but about natural languages. `nltk` provides libraries that help working with semantic part of natural language processing.\n",
    "\n",
    "`WordNet` is a lexical database with which you can play with means of English words, synonyms, hypernyms, antonyms, meronyms and more.\n",
    "\n",
    "First, create a `wordnet.synsets` object. The example simply uses 'computer'. `synsets` is a set of synonyms of the word 'computer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "synsets = wn.synsets('computer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each object in 'synsets' has a name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'computer.n.01', u'calculator.n.01']\n"
     ]
    }
   ],
   "source": [
    "print[syns.name() for syns in synsets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each name, there are 3 parts. For example, 'computer.n.01', 'telephone' is the synonyms itself; 'n' means for this word, it uses the noun meaning; '01' means it uses the 1st definition of the noun form of this word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each object in `synsets`, we can get the definition of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a machine for performing calculations automatically', u'an expert at calculation (or at operating calculating machines)']\n"
     ]
    }
   ],
   "source": [
    "print [syns.definition() for syns in synsets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the word itself instead of the name of an object in synsets, we need to use the lemma of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'computer', u'calculator']\n"
     ]
    }
   ],
   "source": [
    "lemma_names = [syns.lemmas()[0].name() for syns in synsets]\n",
    "print lemma_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With words themselves, we can get antonyms of them by getting antonyms of each lemmas object. We use another example word 'good' because there's no antonym for computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'bad', u'ill', u'evil', None])\n"
     ]
    }
   ],
   "source": [
    "lemmas = [syns.lemmas()[0] for syns in wn.synsets('good')]\n",
    "print set(None if len(l.antonyms()) == 0 else l.antonyms()[0].name() for l in lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity\n",
    "The `wordnet` database comes with functions to get semantic related-ness of words. `wordnet.synset.wu_similarity` uses the [Wu and Palmer method](http://search.cpan.org/dist/WordNet-Similarity/lib/WordNet/Similarity/wup.pm).\n",
    "\n",
    "Here are some fun words to play with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A programmer has lower similarity with human than an ape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ape: any of various primates with short tails or no tail at all\n",
      "0.8888888888888888\n",
      "\n",
      "programmer: a person who designs and writes and tests computer programs\n",
      "0.521739130435\n"
     ]
    }
   ],
   "source": [
    "w1 = wn.synset('human.n.01')\n",
    "w2 = wn.synset('ape.n.01')\n",
    "print 'ape: ' + w2.definition()\n",
    "print `w1.wup_similarity(w2)` + '\\n'\n",
    "w3 = wn.synset('programmer.n.01')\n",
    "print 'programmer: ' + w3.definition()\n",
    "print (w1.wup_similarity(w3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that programmers are more related to liquor than coffee:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee: a beverage consisting of an infusion of ground coffee beans\n",
      "0.3076923076923077\n",
      "\n",
      "liquor: an alcoholic beverage that is distilled rather than fermented\n",
      "0.428571428571\n"
     ]
    }
   ],
   "source": [
    "w1 = wn.synset('programmer.n.01')\n",
    "w2 = wn.synset('coffee.n.01')\n",
    "print 'coffee: ' + w2.definition()\n",
    "print `w1.wup_similarity(w2)` + '\\n'\n",
    "w3 = wn.synset('liquor.n.01')\n",
    "print 'liquor: ' + w3.definition()\n",
    "print w1.wup_similarity(w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmers are not that anti-social. After all, they love partying as much as they love coding XD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code: (computer science) the symbolic arrangement of data or instructions in a computer program or the set of such instructions\n",
      "0.15384615384615385\n",
      "\n",
      "party: (computer science) the symbolic arrangement of data or instructions in a computer program or the set of such instructions\n",
      "0.153846153846\n"
     ]
    }
   ],
   "source": [
    "w1 = wn.synset('programmer.n.01')\n",
    "w2 = wn.synset('code.n.03')\n",
    "print 'code: ' + w2.definition()\n",
    "print `w1.wup_similarity(w2)` + '\\n'\n",
    "w3 = wn.synset('party.n.02')\n",
    "print 'party: ' + w2.definition()\n",
    "print w1.wup_similarity(w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting things together\n",
    "`nltk.corpus` not only has functions that manipulates natural languages, but also contains actual text from books, web chat rooms, and many other sources. In this section, we are going to apply some of the techniques presented above to *Moby Dick* from `nltk.corpus`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg Corpus - *Moby Dick*\n",
    "Project Gutenberg has more than 20,000 free electronic books. `nltk.corpus` has a small number of the collection. We are going to use a classic American novel, *Moby Dick*, as our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'austen-emma.txt',\n",
       " u'austen-persuasion.txt',\n",
       " u'austen-sense.txt',\n",
       " u'bible-kjv.txt',\n",
       " u'blake-poems.txt',\n",
       " u'bryant-stories.txt',\n",
       " u'burgess-busterbrown.txt',\n",
       " u'carroll-alice.txt',\n",
       " u'chesterton-ball.txt',\n",
       " u'chesterton-brown.txt',\n",
       " u'chesterton-thursday.txt',\n",
       " u'edgeworth-parents.txt',\n",
       " u'melville-moby_dick.txt',\n",
       " u'milton-paradise.txt',\n",
       " u'shakespeare-caesar.txt',\n",
       " u'shakespeare-hamlet.txt',\n",
       " u'shakespeare-macbeth.txt',\n",
       " u'whitman-leaves.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the books in `nltk.corpus.gutenberg`. We will be using 'melville-moby_dick.txt'. Get the book and some basic metrics of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "moby_dick_sents = gutenberg.sents('melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the text is from Project Gutenberg, there are some sentences that's about the project but not *Moby Dick*. Let's start with getting then content of the book.\n",
    "\n",
    "*Moby Dick* starts with \"Call me Ishmael.\" and ends with \"It was the devious-cruising Rachel, that in her retracing search after her missing children, only found another orphan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_sent = treebank_tokenizer.tokenize('Call me Ishmael.');\n",
    "last_sent = regex_tokenizer.tokenize('It was the devious-cruising Rachel, that in her retracing search after her missing children, only found another orphan.')\n",
    "for i in range(len(moby_dick_sents)):\n",
    "    if first_sent == moby_dick_sents[i]:\n",
    "        first_idx = i\n",
    "    if set(last_sent).issubset(moby_dick_sents[i]):\n",
    "        last_idx = i\n",
    "\n",
    "moby_dick_sents = moby_dick_sents[first_idx:last_idx+2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do more cleaning on words in *Moby Dick*. Remove stopwords and puncuations so that we only focus on meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moby Dick has 111868 words, and 9767 sentences.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuations = set(string.punctuation)\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend([''])\n",
    "moby_dick_words = []\n",
    "for sent in moby_dick_sents:\n",
    "    for word in sent:\n",
    "        word = word.lower()\n",
    "        if word not in punctuations\\\n",
    "            and word not in stopwords\\\n",
    "            and len(word) != 1:\n",
    "            moby_dick_words.append(word)\n",
    "print 'Moby Dick has ' + `len(moby_dick_words)` + ' words, and '\\\n",
    "                       + `len(moby_dick_sents)` + ' sentences.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming vs. Lemmatizing on *Moby Dick*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the number of unique words in *Moby Dick*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16768"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(moby_dick_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as we know from previous sections, many words are variations of same words. For example, apples is the plural form of apple and they are essentially the same word. So the result above doesn't reflect the real number of unique words in *Moby Dick*. \n",
    "\n",
    "Let's compare the two ways to normalize words: `lemmatize` and `stem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14751 unique words after lemmatizing.\n",
      "There are 10568 unique words after stemming.\n"
     ]
    }
   ],
   "source": [
    "lemmatized_words = [word_net_lemmatizer.lemmatize(w) for w in moby_dick_words]\n",
    "stemmed_words = [porter_stemmer.stem(w) for w in moby_dick_words]\n",
    "\n",
    "print 'There are ' + `len(set(lemmatized_words))` + ' unique words after lemmatizing.'\n",
    "print 'There are ' + `len(set(stemmed_words))` + ' unique words after stemming.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a big difference between the number of unique words processed with lemmatizing and stemming. There are 50% more lemmatized words than stemmed words. The reason might be that the lemmatizing requires the knowledge of words: it needs to know all variations of the same word in order to lemmatize them. If a variation of a word is not in the library of lemmatizer, it wouldn't be able to lemmatize it. On the contrary, stemming only checks suffix of words. This difference caused large difference between the 2 results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag *Moby Dick* Words\n",
    "Now let's tag all words with `nltk`'s POS-tagger. Although there are fewer stemmed words, they might not be valid English words. We should choose lemmatized words instead of stemmed ones for tagging.\n",
    "Again, we are only interested in nouns, verbs, adjectives and adverbs. (The following cell takes about 20 seconds to run as it's tagging all words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_words = nltk.pos_tag(lemmatized_words)\n",
    "pos_cat = defaultdict(list)\n",
    "for tag in tagged_words:\n",
    "    pos_cat[tag[1]].append(tag[0])\n",
    "nouns = pos_cat['NN']\n",
    "verbs = pos_cat['VB']\n",
    "adjs = pos_cat['JJ']\n",
    "advs = pos_cat['RB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Moby Dick, there are 43594 nouns, 2545 verbs, 24337 adjectives and 8867 adverbs \n"
     ]
    }
   ],
   "source": [
    "print 'In Moby Dick, \\\n",
    "there are {0} nouns, {1} verbs, {2} adjectives and {3} adverbs '.format(len(nouns), \n",
    "                                                                       len(verbs), \n",
    "                                                                       len(adjs), \n",
    "                                                                       len(advs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top words used by Herman Melville\n",
    "With tagging information and nicely stemmed words, we can get top words with counting and sorting.\n",
    "\n",
    "First, top nouns used by Herman Melville in *Moby Dick*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noun_counts = defaultdict(int)\n",
    "for noun in nouns:\n",
    "    noun_counts[noun] += 1\n",
    "sorted_noun_counts = sorted(noun_counts.items(), key=operator.itemgetter(1))\n",
    "sorted_noun_counts.reverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From top ten nouns, we can easily tell that this novel is about whale, man and sea. Adventures is rolling out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun: whale\tcount:689\n",
      "noun: man\tcount:525\n",
      "noun: sea\tcount:504\n",
      "noun: boat\tcount:457\n",
      "noun: time\tcount:444\n",
      "noun: ship\tcount:441\n",
      "noun: head\tcount:391\n",
      "noun: hand\tcount:342\n",
      "noun: thing\tcount:316\n",
      "noun: way\tcount:289\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print 'noun: {0}\\tcount:{1}'.format(sorted_noun_counts[i][0], sorted_noun_counts[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same process for verbs, adjectives and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verb: go\tcount:106\n",
      "verb: take\tcount:84\n",
      "verb: see\tcount:63\n",
      "verb: keep\tcount:63\n",
      "verb: get\tcount:61\n",
      "verb: come\tcount:42\n",
      "verb: say\tcount:35\n",
      "verb: give\tcount:34\n",
      "verb: let\tcount:34\n",
      "verb: make\tcount:31\n"
     ]
    }
   ],
   "source": [
    "verb_counts = defaultdict(int)\n",
    "for verb in verbs:\n",
    "    verb_counts[verb] += 1\n",
    "sorted_verb_counts = sorted(verb_counts.items(), key=operator.itemgetter(1))\n",
    "sorted_verb_counts.reverse()\n",
    "for i in range(10):\n",
    "    print 'verb: {0}\\tcount:{1}'.format(sorted_verb_counts[i][0], sorted_verb_counts[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj: whale\tcount:681\n",
      "adj: old\tcount:446\n",
      "adj: great\tcount:292\n",
      "adj: white\tcount:281\n",
      "adj: last\tcount:278\n",
      "adj: little\tcount:235\n",
      "adj: good\tcount:216\n",
      "adj: u\tcount:186\n",
      "adj: ahab\tcount:185\n",
      "adj: sperm\tcount:173\n"
     ]
    }
   ],
   "source": [
    "adj_counts = defaultdict(int)\n",
    "for adj in adjs:\n",
    "    adj_counts[adj] += 1\n",
    "sorted_adj_counts = sorted(adj_counts.items(), key=operator.itemgetter(1))\n",
    "sorted_adj_counts.reverse()\n",
    "for i in range(10):\n",
    "    print 'adj: {0}\\tcount:{1}'.format(sorted_adj_counts[i][0], sorted_adj_counts[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adv: yet\tcount:344\n",
      "adv: still\tcount:312\n",
      "adv: well\tcount:224\n",
      "adv: never\tcount:203\n",
      "adv: ever\tcount:202\n",
      "adv: almost\tcount:194\n",
      "adv: long\tcount:192\n",
      "adv: even\tcount:188\n",
      "adv: far\tcount:163\n",
      "adv: away\tcount:159\n"
     ]
    }
   ],
   "source": [
    "adv_counts = defaultdict(int)\n",
    "for adv in advs:\n",
    "    adv_counts[adv] += 1\n",
    "sorted_adv_counts = sorted(adv_counts.items(), key=operator.itemgetter(1))\n",
    "sorted_adv_counts.reverse()\n",
    "for i in range(10):\n",
    "    print 'adv: {0}\\tcount:{1}'.format(sorted_adv_counts[i][0], sorted_adv_counts[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that Herman Melville often used are not much different from what we use daily. The greatness of *Moby Dick* not lies in the words it uses, but in the story and spirit that drives Americans to persue their dreams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and References\n",
    "This tutorial only covers a few functions that one can use with `nltk`. Deeper natural language analysis is possible with more knowledge on both language theories and tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Porter Stemming Algorithm](http://snowball.tartarus.org/algorithms/porter/stemmer.html)\n",
    "\n",
    "- [Full List of Part-of-Speech Tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "\n",
    "- [Wu and Palmer Similarity](http://search.cpan.org/dist/WordNet-Similarity/lib/WordNet/Similarity/wup.pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bird, Steven, Edward Loper and Ewan Klein (2009), *Natural Language Processing with Python.* O’Reilly Media Inc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
