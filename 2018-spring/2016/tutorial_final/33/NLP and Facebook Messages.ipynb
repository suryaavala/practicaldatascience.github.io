{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who are Your Friends? Natural Language Processing and You \n",
    "\n",
    "This tutorial will teach you some interesting analyses you can do using basic natural language processing tools. Extracting information from natural language is a fundamental aspect of data science: specifically, it doesn't charge us with solving true natural language processing, but rather attempts to generalize large amounts of text using computer science and statistics. There are many types of questions that natural language processing attempts to answer: is the writer of a piece of text enthusiastic or pessimistic? What topic does this document talk about the most? Do you generally use positive or negative language when talking with your friends? These are all questions natural language processing attempts to answer, and has surprising success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "If you use Facebook often, there it's highly likely you've seen those posts that say \"Who should you live with?!\" or \"Who is your best friend on Facebook?!\" They're ubiquitous, though very limited as they can only access public information on your profile. So they use very generic standards to measure your relationship with other people on Facebook: how many pictures you share with them, how often you post on each others walls, etc. However, this can be fairly unrepresentative of your true friendship with people, and while can make a good guess, doesn't give any qualitative analysis or why it chose that person as your friend (at best, it'll show a loading bar saying it's \"calculating your best friend\" whatever that means).\n",
    "\n",
    "I'd argue that a much better metric for measuring your relationship with your friends on Facebook is how often you chat with people and what you chat about with them. This tutorial will cover how to retrieve your archived facebook messages, visualizing how frequent you chat with your frineds, and making a good guess at the true relationship between you and your friends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Content\n",
    "\n",
    "In this tutorial,  I am assuming that you have a Facebook you use frequently, specifically to talk with friends. If you do not, then this tutorial probably isn't for you. We will be using the [fb-archive-parser](https://pypi.python.org/pypi/fbchat_archive_parser) and nltk.\n",
    "\n",
    "\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "\n",
    "* Installing the libraries\n",
    "* Extracting the data from your Facebook\n",
    "* Converting your Archived Messages to Useful Formats\n",
    "* Visualizing Chat Frequency With Friends\n",
    "* Determining Relationships With Friends \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Before we get started, we first need to install fb-archive-parser. You can install it via pip by going to your Terminal and typing in the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pip install fbchat-archive-parser```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to have nltk. If you are using a jupyter notebook, then this should alraedy be pre-installed for you. Ohterwise, you can easily install nltk the same way you installed fbchat-archive-parser via pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pip install nltk```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For nltk, we will need the Stopword and Opinion Lexicon corpora. You can download these by going into a python shell and executing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will open up a GUI selection screen where you can select all types of corpora. A corpus is simply a collection of curated words that are useful for natural language processing. In general, you should avoid downloading all corpora, as you will rarely ever need all of them. You can find the opinion lexicon and stopword corpus under the corpora tab. After running these installations, make sure the following runs properly for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, string, sys, operator, math, pandas\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the Data From Facebook\n",
    "\n",
    "This will probably be the worst part of this tutorial, because you'll have to wait. Facebook has a relatively unknown feature where you are allowed to ask for all the data Facebook has on you: pictures, relationships, posts, messages... basically anything you can think of that you do on Facebook, you are allowed to request. This is called Facebook Archive Download. Go to the Drop down menu next to your privacy shortcuts on the main page of Facebook > Settings. From here, there should be some grey text beneath the bars letting you know you can download a copy of your Facebook data. Click there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"fb1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"fb2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the \"Start My Archive\" button, and wait. This'll take awhile, especially if you do a lot on Facebook. Standard time it takes for archive requests to be fulfilled is about 6 to 8 hours. Go outside. Enjoy life. You'll get an email to the account that is linked with your Facebook when your archive is ready. Download the archive, and unpack the zip file. Just a warning, it might be really big, so don't worry if it takes awhile to open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Your Messages to Useful Formats\n",
    "\n",
    "Inside the unzipped folder, you'll find three folders and an html file that you can use to explore your data in a GUI format. We're only interested in one file though. Navigate to html > messages.htm\n",
    "\n",
    "This file is a collection of all of your Facebook chat history in xml format. If you open up, you might notice that people you talk with a lot are split into seperate instances. For some reason, Facebook caps individual conversation segments to 200 messages â€“ don't worry, the rest of the conversation is still there, it's just scattered throughout the document. Since we cannot work in this format, we'll need to convert it to something more useful. fb-archive-chat supports yaml, json, and csv formats. For this tutorial, let's use the json format. As Professor Kolter said, if you dont't have to write your own xml parser, then don't do it. In your Terminal, cd into the html folder and run the following command: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```fbcap ./messages.htm -f json > file.json```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename file.json to anything you deem appropriate. This will convert all of your messages into a json format. Fair warning this might take awhile; I usually wait around 20 minutes. Also note, Facebook might give you XML that is not well formed, so you might see a warning messages that looks something like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```The streaming parser crashed due to malformed XML. Falling back to the less strict/efficient python html.parser. It may take a while before you see output... ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is fine, just know it might take an extra few minutes to process everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Now that we have our data what can we do with it? One built in function of fb-chat-archive is that you can run summary statistics for yourself. Run the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```fbcap ./message.htm -f stats```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give you summary statistics for your top 10 chats. Already, this is probably a better measure of your top 10 friends than any random guess will give you. Here is part of my output for reference. Some of the names are blurred out becasue I did not obtain permission from them to use their name in public. The second chat is actually an off branch of my first chat which is why it's mostly the same members. Something interesting to note is that people who change their name will have both names show up in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = stats.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be hard to read your json file (espeically since the parser puts everything in one line) so it'd be useful to print and find participants of group chats in your top 10. This is easily done using the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file.json is the name of your file that contains your json data\n",
    "def printParticipants():\n",
    "    with open('file.json') as chatHistory:\n",
    "        raw = json.load(chatHistory)\n",
    "        for thread in raw['threads']:\n",
    "            print(thread['participants'])\n",
    "\n",
    "def findParticipants(targets):\n",
    "    result = list()\n",
    "    with open('chatHistory.json','r') as f:\n",
    "        raw = json.load(f)\n",
    "        for thread in raw['threads']:\n",
    "            if targets == set(thread['participants']):\n",
    "                with open('file.json','w') as out:\n",
    "                        json.dump(thread['messages'],out, indent = 4)\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start quantiatively analyzing our data. It is very useful to know what we talk about with our friends. We can use something called the co-occurence matrix in order to find the number of times words co-occur in the same message.\n",
    "\n",
    "This structure simply counts the number of times a word appears in your documents. The code for that is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findCom(file):\n",
    "    STOP = stopwords.words('english')\n",
    "    com = defaultdict(lambda : defaultdict(int))\n",
    "    with open(file,'r') as f:\n",
    "        # f is the file pointer to the JSON data set\n",
    "        data = json.loads(f.read())\n",
    "        for line in data:\n",
    "            message = line\n",
    "            terms_only = [term for term in message['message']\n",
    "                          if term not in STOP]\n",
    "            # Build co-occurrence matrix\n",
    "            for i in range(len(terms_only)-1):            \n",
    "                for j in range(i+1, len(terms_only)):\n",
    "                    word1, word2 = sorted([terms_only[i], terms_only[j]])                \n",
    "                    if word1 != word2:\n",
    "                        com[word1][word2] += 1\n",
    "    return com\n",
    "\n",
    "def findComMax(file, n=10):\n",
    "    com = findCom(file)\n",
    "    com_max = []\n",
    "    for t1 in com:\n",
    "        #Find the top 10 terms in the COM\n",
    "        t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:10]\n",
    "        for t2, t2_count in t1_max_terms:\n",
    "            com_max.append(((t1, t2), t2_count))\n",
    "    # Get the most frequent co-occurrences\n",
    "    terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "    return terms_max[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My output looks like this\n",
    "\n",
    "```[(('guys', 'u'), 804), (('And', 'like'), 732), (('LOL', 'like'), 732), (('feel', 'like'), 604), (('like', 'one'), 577), (('like', 'u'), 529), (('dont', 'think'), 456), (('dont', 'like'), 455), (('know', 'u'), 449), ((\"I'm\", 'like'), 412)]```\n",
    "\n",
    "If you couldn't tell, we use the word 'like' a lot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Chat Frequency\n",
    "\n",
    "The first thing we are interested in is seeing how often we chat with out friends. These functions can be used to create json files recording how often we chat with friends. These files are easily interpretable using any visualizization package. I used d3, however, you can use matplotlib to try it yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countMessages(file):\n",
    "    result = {}\n",
    "    with open(file,'r') as f:\n",
    "        raw = json.load(f)\n",
    "        for message in raw['threads'][0]['messages']:\n",
    "            date = message['date'][:10]\n",
    "            if date in result:\n",
    "                result[date] += 1\n",
    "            else:\n",
    "                result[date] = 0\n",
    "    with open('output.json','w') as out:\n",
    "\n",
    "\n",
    "def createTranscript(file,target):\n",
    "    result = list()\n",
    "    with open(file,'r') as f:\n",
    "        raw = json.load(f)\n",
    "        for message in raw['threads'][0]['messages']:\n",
    "            if message['sender'] == target:\n",
    "                result.append(message)\n",
    "    with open('transcript.json', 'w') as out:\n",
    "        json.dump(result,out,indent=4)\n",
    "\n",
    "        \n",
    "def countTranscript(file):\n",
    "    result = {}\n",
    "    with open(file,'r') as f:\n",
    "        raw = json.load(f)\n",
    "        for message in raw:\n",
    "            date = message['date'][:10]\n",
    "            if date in result:\n",
    "                result[date] += 1\n",
    "            else:\n",
    "                result[date] = 0\n",
    "    with open('file.json','w') as out:\n",
    "        json.dump(result,out,indent = 4)\n",
    "\n",
    "def makeTimeSeriesJson(file):\n",
    "    result = []\n",
    "    with open(file,'r') as f:\n",
    "        raw = json.load(f)\n",
    "        for date in raw:\n",
    "            dataPoint = dict()\n",
    "            dataPoint['date'] = date\n",
    "            dataPoint['value'] = raw[date]\n",
    "            result.append(dataPoint)\n",
    "    with open('timeSeriesOutput.json','w') as out:\n",
    "        json.dump(result,out,indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = chat_history.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x-axis is partitioned by days, and on the day we talked the most, we sent up to 2000 messages. One thing to note is that if your friends changed names, you'll need to include that in the target of the ```create_transcript``` function. If you don't you'll get a straight line like the one below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"nickname_history.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Relationship With Friends\n",
    "\n",
    "To simplify this tutorial, let's assume that the top 10 chats from your summary statistics are the people who are your best friends. For me, I'll only do the first chat in my list of top 10.\n",
    "\n",
    "We can figure out what topics you talk about the most and how you feel about them using very basic NLP unsupervised technique known as sentiment analysis.\n",
    "\n",
    "\n",
    "## Finding the Semantic Orientation\n",
    "\n",
    "To figure out good of a relationship you have with your friends, we'll be calculating something called the semantic orientation. This is a very simple concept with strong implication. Essentially given two dictionaries of \"good\" and \"bad\" words, if we count the number of times \"good\" words appear versus the number of times \"bad\" words appear, then we now have a quantifiable metric for determining how good of a relationship you have with your friend. To define this mathematically first we must define Pointwise Mutual Information (PMI) which is\n",
    "\n",
    "$PMI(t_1, t_2) = \\log(\\frac{P(t_1 \\cap t_2)}{P(t_1) \\cdot P(t_2)})$\n",
    "\n",
    "where term $t_1,t_2,$ are two different terms in a document (in our case a single message). Then the semantic orientation for this term is\n",
    "\n",
    "$SO(t) = \\displaystyle \\sum_{t' \\in V^+} PMI(t,t') -\\sum_{t' \\in V^-} PMI(t,t') $\n",
    "\n",
    "$V^+$ and $V^-$ represent positive and negative vocabularies. We use the Bing Liu opinion dictioary that we imported at the beginning as our positive and negative vocabularies. These are especially good as they include frequent misspellings of common words.\n",
    "\n",
    "The code for calulcating the semantic orientation is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findSemanticOrientation(file):\n",
    "    messageAdded = set()\n",
    "    n_docs = findNDocs(file)\n",
    "    count_stop_single = mostCommon(file)\n",
    "    com = findCom(file)\n",
    "    # n_docs is the total n. of messages\n",
    "    p_t = {}\n",
    "    p_t_com = defaultdict(lambda : defaultdict(int))\n",
    "    for term, n in count_stop_single.items():\n",
    "        p_t[term] = n / n_docs\n",
    "        for t2 in com[term]:\n",
    "            p_t_com[term][t2] = com[term][t2] / n_docs\n",
    "    positive_vocab = opinionWords(\"positive.txt\")\n",
    "    negative_vocab = opinionWords(\"negative.txt\")\n",
    "    pmi = defaultdict(lambda : defaultdict(int))\n",
    "    for t1 in p_t:\n",
    "        for t2 in com[t1]:\n",
    "            denom = p_t[t1] * p_t[t2]\n",
    "            pmi[t1][t2] = math.log2(p_t_com[t1][t2] / denom)\n",
    "    semantic_orientation = {}\n",
    "    for term, n in p_t.items():\n",
    "        positive_assoc = sum(pmi[term][tx] for tx in positive_vocab)\n",
    "        negative_assoc = sum(pmi[term][tx] for tx in negative_vocab)\n",
    "        semantic_orientation[term] = positive_assoc - negative_assoc\n",
    "    return semantic_orientation\n",
    "\n",
    "#N_docs counts the number of messages\n",
    "\n",
    "def findNDocs(file):\n",
    "    with open(file,'r') as f:\n",
    "        data = json.loads(f.readline())\n",
    "        return len(data)\n",
    "\n",
    "def mostCommon(file):\n",
    "    fname = file\n",
    "    with open(fname, 'r') as f:\n",
    "        count_all = Counter()\n",
    "        data = json.loads(f.readline())\n",
    "        for line in data:\n",
    "            message = line\n",
    "            # Create a list with all the terms\n",
    "            try:\n",
    "                terms_only = [term for term in message['text'] \n",
    "                              if term not in STOP] \n",
    "            except:\n",
    "                continue\n",
    "            count_all.update(terms_only)\n",
    "    return count_all\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Your Results\n",
    "\n",
    "If you followed along correctly, you should get a list of tuples with a term and its semantic orientation like so:\n",
    "\n",
    "```\n",
    "[(LOL, 93.45424444219)]\n",
    "```\n",
    "\n",
    "While some of the terms may not make any sense, you will occasionally get words that are topical. You can see in general what you feel about certain topics. For example, with my friends one of my results is\n",
    "\n",
    "```\n",
    "[(math, -87.3131451313)]\n",
    "```\n",
    "\n",
    "\n",
    "Not surprising.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Messing around with your Facebook friends activity is a very fun past time. With this tutorial, we learned how to retrieve and convert raw xml messags from Facebook into usable formats. We also learned how to visualize our data, and how to use the semantic orientation of our messages in order to learn more about how we communicate with our friends. Some other fun things you could do is make a chatbot from your friend's messages. Hopefully, you learned a little bit more about natural language processing and had fun reading this tutorial.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the libraries and documentation for some of the classes we used.\n",
    "\n",
    "* NLTK: http://www.nltk.org/\n",
    "* facebook-archive-parser: https://github.com/CopOnTheRun/FB-Message-Parser\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
