{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tutorial\n",
    "\n",
    "<!-- include motivation -->\n",
    "\n",
    "<!-- include introduction to what neural networks are in general -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will introduce you to some basic techniques and ideas you will need to start building Neural Networks in Keras. A Neural network is simply a large collection of \"neural units\" that are connected to each other loosely modeling the way the human brain solves problems. They are modeled after clusers of neurons connected by axons. Neural networks typically consist of several layers, each layer having several units that generally take inputs from the previous layer, perform some function, then give outputs to the next layer.\n",
    "\n",
    "Neural networks can be utilized to solve a varirety of different tasks. They have been used in the past for things like classifying data and making predictions. More recently we have heard about neural networks beating the top players in Go with AlphaGo as well as research projects out of Google where neural networks were able to \"learn\" their own primitive encryption scheme.\n",
    "\n",
    "Throughout this tutorial we will be exploring how to create these very powerful networks with the tool \"Keras\" which makes all these advanced ideas simple to implement. Hope you enjoy this tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, you'll need to install and important the libraries we will use throughout this tutorial. You will need to install both Theanos and Keras using `pip`:\n",
    "\n",
    "    > pip install theanos\n",
    "    > pip install keras\n",
    "    \n",
    "Keras defaults to using TensorFlow as a back-end for its computation but I will be using Theanos for this tutorial because it is compatable with Windows.\n",
    "\n",
    "After installing these libraries, you will need to change some configuration settings in the json file `C:\\Users\\$USER\\.keras\\keras.json`. You should be able to just copy paste the following:\n",
    "\n",
    "`{\n",
    "    \"image_dim_ordering\": \"tf\", \n",
    "    \"epsilon\": 1e-07, \n",
    "    \"floatx\": \"float32\", \n",
    "    \"backend\": \"theano\"\n",
    "}`\n",
    "\n",
    "You may also want to install GCC speed optimization with theano. On Windows you can install \"TDM GCC,\" making sure to enable OpenMP support during the installation (http://tdm-gcc.tdragon.net/).\n",
    "\n",
    "We will be importing Keras modules as needed throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading example data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to go over the basics of Keras we will have to start by loading a dataset. Keras uses numpy arrays thoughout its implementation as inputs and outputs. We will start by loading an exceedingly simple dataset described as \"perhaps the best known database to be found in pattern recognition literature.\" The dataset can be found here `https://archive.ics.uci.edu/ml/datasets/Iris`. The following loads the dataset in the necessary format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 'train sequences')\n",
      "(30, 'test sequences')\n"
     ]
    }
   ],
   "source": [
    "def classConverter(o):\n",
    "    if o == \"Iris-versicolor\":\n",
    "        return 0\n",
    "    elif o == \"Iris-virginica\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "np.random.seed(5)\n",
    "    \n",
    "totData = np.loadtxt(\"iris.data\", delimiter=\",\", converters= {4: classConverter})\n",
    "#shuffle data so we get good distribution in train/test\n",
    "np.random.shuffle(totData)\n",
    "#take last 30 (20 %) as test\n",
    "trainData = totData[:120,:]\n",
    "testData = totData[120:,:]\n",
    "#then have to split attributes and labels\n",
    "trainX = trainData[:,:4]\n",
    "trainY = np.array(map(lambda cat: np.array([0.0 if x != cat else 1.0\\\n",
    "                                            for x in range(3)]), trainData[:,4]))\n",
    "testX = testData[:,:4]\n",
    "testY = np.array(map(lambda cat: np.array([0.0 if x != cat else 1.0\\\n",
    "                                           for x in range(3)]), testData[:,4]))\n",
    "\n",
    "print(len(trainX), 'train sequences')\n",
    "print(len(testX), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data and the labels are stored together in the data file we had to manually split them up. Additionally, we had to shuffle the data so they weren't clustered.\n",
    "\n",
    "Another issue with the data is that the labels were categorical strings, neural networks (and Keras) are unable to handle strings as labels and so we had to split the labels into arrays with indicator variables.\n",
    "\n",
    "We can get a sense of what our data looks like here, with the left side being the attributes of the data and the right side being the categorical indicator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.7  3.1  5.6  2.4] [ 0.  1.  0.]\n",
      "[ 6.4  3.2  4.5  1.5] [ 1.  0.  0.]\n",
      "[ 7.6  3.   6.6  2.1] [ 0.  1.  0.]\n",
      "[ 5.5  3.5  1.3  0.2] [ 0.  0.  1.]\n",
      "[ 6.5  3.2  5.1  2. ] [ 0.  1.  0.]\n",
      "[ 5.   3.6  1.4  0.2] [ 0.  0.  1.]\n",
      "[ 6.9  3.1  5.1  2.3] [ 0.  1.  0.]\n",
      "[ 5.1  3.5  1.4  0.2] [ 0.  0.  1.]\n",
      "[ 6.6  2.9  4.6  1.3] [ 1.  0.  0.]\n",
      "[ 5.4  3.9  1.7  0.4] [ 0.  0.  1.]\n",
      "[ 6.3  2.9  5.6  1.8] [ 0.  1.  0.]\n",
      "[ 7.2  3.   5.8  1.6] [ 0.  1.  0.]\n",
      "[ 4.5  2.3  1.3  0.3] [ 0.  0.  1.]\n",
      "[ 4.9  2.5  4.5  1.7] [ 0.  1.  0.]\n",
      "[ 5.6  2.8  4.9  2. ] [ 0.  1.  0.]\n",
      "[ 7.2  3.2  6.   1.8] [ 0.  1.  0.]\n",
      "[ 6.7  3.1  4.7  1.5] [ 1.  0.  0.]\n",
      "[ 4.8  3.1  1.6  0.2] [ 0.  0.  1.]\n",
      "[ 6.7  3.1  4.4  1.4] [ 1.  0.  0.]\n",
      "[ 5.1  3.8  1.9  0.4] [ 0.  0.  1.]\n",
      "[ 5.2  3.5  1.5  0.2] [ 0.  0.  1.]\n",
      "[ 5.5  2.4  3.8  1.1] [ 1.  0.  0.]\n",
      "[ 5.7  2.5  5.   2. ] [ 0.  1.  0.]\n",
      "[ 5.   3.4  1.5  0.2] [ 0.  0.  1.]\n",
      "[ 6.8  3.   5.5  2.1] [ 0.  1.  0.]\n",
      "[ 4.4  2.9  1.4  0.2] [ 0.  0.  1.]\n",
      "[ 6.1  2.8  4.7  1.2] [ 1.  0.  0.]\n",
      "[ 6.7  3.3  5.7  2.5] [ 0.  1.  0.]\n",
      "[ 7.7  2.6  6.9  2.3] [ 0.  1.  0.]\n",
      "[ 5.7  2.8  4.1  1.3] [ 1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(len(testX)):\n",
    "    print testX[i], testY[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data structure of Keras is a model, and the main type of model is a \"Sequential\" model. This just means that the layers of our neural network is going to be layed out in a linear stack format (other options may involve multiple inputs at different layers or layers that are shared). Using different kinds of layers and different parameters, we are able to build out a neural network like we described in the introduction. We will start by creating a simple sequntial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a base model defined we can begin adding layers to it. Layers in Keras are just a representation of the layers in a neural network. As you may have guessed, there are many different types of layers to choose from. These include layers Keras in grouping such as core layers (dense, activation, flatten, masking,...), convolutional layers (1d convolutions, cropping, upsampling,...), normalization layers, ect. Stacking different types of layers onto our model is incredibly easy - you can just use the method `.add()` on your model.\n",
    "\n",
    "In our Iris example we will just be using the simplist and most classic kind of layer, the `Dense` layer. This is just a fully connected layer, meaning that each node is connected to every single node in the next output. We also will be giving our layers additional attributes: input_dim (for the first layer), and activation type. The activation type is just the function each node in the layer will use to give an output based on its inputs.\n",
    "\n",
    "Now we add our layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model.add(Dense(4, input_dim=4, init=\"normal\", activation='relu'))\n",
    "model.add(Dense(3, init=\"normal\", activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to use a sigmoid activation function for the final layer  so that our output values will be between 0 and 1. We need this so that we can interpret them as probabilities and pick the largest one as our predicted category. We can now configure its learning process using the method `.compile()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a loss function of `categorical_crossentropy` A.K.A multiclass logloss, this is a good loss function to use for our binary label arrays. There are of course a plethora of other loss functions to choose from depending on your needs and fancy. We also just stick to the a very standard stochastic gradient descent optimizer, there are also an overabundance of optimizers to pick from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our model we can use it. First we want to train our model on our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120/120 [==============================] - 0s - loss: 1.0991 - acc: 0.3083     \n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s - loss: 1.0988 - acc: 0.3500     \n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s - loss: 1.0987 - acc: 0.3417     \n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s - loss: 1.0985 - acc: 0.1583     \n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s - loss: 1.0984 - acc: 0.2833         \n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s - loss: 1.0982 - acc: 0.3583     \n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s - loss: 1.0980 - acc: 0.3500     \n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s - loss: 1.0977 - acc: 0.3583     \n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s - loss: 1.0975 - acc: 0.3583     \n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s - loss: 1.0972 - acc: 0.3583     \n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s - loss: 1.0970 - acc: 0.3583     \n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s - loss: 1.0967 - acc: 0.3583         \n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s - loss: 1.0964 - acc: 0.3583     \n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s - loss: 1.0960 - acc: 0.3583         \n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s - loss: 1.0956 - acc: 0.3583     \n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s - loss: 1.0951 - acc: 0.3583     \n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s - loss: 1.0945 - acc: 0.3583     \n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s - loss: 1.0937 - acc: 0.3583     \n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s - loss: 1.0929 - acc: 0.3583     \n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s - loss: 1.0918 - acc: 0.3583         \n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s - loss: 1.0905 - acc: 0.3667     \n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s - loss: 1.0890 - acc: 0.4250     \n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s - loss: 1.0871 - acc: 0.3833     \n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s - loss: 1.0846 - acc: 0.3917     \n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s - loss: 1.0817 - acc: 0.4500     \n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s - loss: 1.0781 - acc: 0.5333     \n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s - loss: 1.0736 - acc: 0.7917     \n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s - loss: 1.0685 - acc: 0.7833     \n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s - loss: 1.0622 - acc: 0.7083     \n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s - loss: 1.0543 - acc: 0.6250     \n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s - loss: 1.0453 - acc: 0.6000     \n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s - loss: 1.0345 - acc: 0.6583     \n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s - loss: 1.0215 - acc: 0.6417     \n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s - loss: 1.0071 - acc: 0.5833         \n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s - loss: 0.9902 - acc: 0.6167     \n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s - loss: 0.9720 - acc: 0.6417     \n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s - loss: 0.9511 - acc: 0.6417     \n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s - loss: 0.9288 - acc: 0.6500     \n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s - loss: 0.9047 - acc: 0.6500     \n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s - loss: 0.8802 - acc: 0.6833     \n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s - loss: 0.8552 - acc: 0.6750     \n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s - loss: 0.8294 - acc: 0.7000     \n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s - loss: 0.8046 - acc: 0.7833     \n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s - loss: 0.7795 - acc: 0.7750     \n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s - loss: 0.7554 - acc: 0.8667     \n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s - loss: 0.7308 - acc: 0.8917     \n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s - loss: 0.7069 - acc: 0.8917     \n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s - loss: 0.6841 - acc: 0.9583     \n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s - loss: 0.6604 - acc: 0.9833     \n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s - loss: 0.6387 - acc: 0.8833     \n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s - loss: 0.6175 - acc: 0.9167     \n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s - loss: 0.5968 - acc: 0.8500     \n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s - loss: 0.5765 - acc: 0.9167     \n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s - loss: 0.5566 - acc: 0.9250     \n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s - loss: 0.5409 - acc: 0.8750     \n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s - loss: 0.5229 - acc: 0.9000     \n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s - loss: 0.5048 - acc: 0.9333     \n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s - loss: 0.4921 - acc: 0.8917     \n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s - loss: 0.4787 - acc: 0.8667     \n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s - loss: 0.4696 - acc: 0.9083     \n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s - loss: 0.4543 - acc: 0.9083     \n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s - loss: 0.4459 - acc: 0.9083     \n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s - loss: 0.4219 - acc: 0.8583     \n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s - loss: 0.4197 - acc: 0.8833     \n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s - loss: 0.4120 - acc: 0.9333     \n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s - loss: 0.3968 - acc: 0.8917     \n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s - loss: 0.3897 - acc: 0.9333     \n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s - loss: 0.3796 - acc: 0.8917     \n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s - loss: 0.3708 - acc: 0.9500     \n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s - loss: 0.3645 - acc: 0.8750     \n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s - loss: 0.3531 - acc: 0.9167     \n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s - loss: 0.3484 - acc: 0.9167     \n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s - loss: 0.3356 - acc: 0.9250     \n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s - loss: 0.3277 - acc: 0.9333     \n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s - loss: 0.3287 - acc: 0.9333     \n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s - loss: 0.3160 - acc: 0.9250     \n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s - loss: 0.3088 - acc: 0.9333     \n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s - loss: 0.3005 - acc: 0.9417     \n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s - loss: 0.2953 - acc: 0.9500     \n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s - loss: 0.2886 - acc: 0.9250     \n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s - loss: 0.2783 - acc: 0.9167     \n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s - loss: 0.2823 - acc: 0.9417     \n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s - loss: 0.2714 - acc: 0.9583     \n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s - loss: 0.2603 - acc: 0.9500     \n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s - loss: 0.2680 - acc: 0.9250     \n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s - loss: 0.2596 - acc: 0.9417     \n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s - loss: 0.2492 - acc: 0.9500     \n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s - loss: 0.2371 - acc: 0.9417     \n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s - loss: 0.2536 - acc: 0.9250     \n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s - loss: 0.2312 - acc: 0.9583     \n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s - loss: 0.2376 - acc: 0.9250     \n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s - loss: 0.2275 - acc: 0.9667     \n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s - loss: 0.2122 - acc: 0.9417     \n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s - loss: 0.2234 - acc: 0.9250     \n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s - loss: 0.2159 - acc: 0.9583     \n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s - loss: 0.2120 - acc: 0.9583     \n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s - loss: 0.2093 - acc: 0.9583     \n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s - loss: 0.2212 - acc: 0.9250     \n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s - loss: 0.2016 - acc: 0.9667     \n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s - loss: 0.1950 - acc: 0.9417     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd47db70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, nb_epoch=100, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Keras gives us progress bars and accuracy/loss values after each epoch. This is useful to see how the training is going.\n",
    "\n",
    "Now that we have finished training, we should try evaluating how this model does on the 30 examples we withheld from the model to test our accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s\n",
      "Test loss:  0.144060507417\n",
      "Test accuracy %:  1.0\n"
     ]
    }
   ],
   "source": [
    "perf = model.evaluate(testX, testY)\n",
    "print \"Test loss: \", perf[0]\n",
    "print \"Test accuracy %: \", perf[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here the output of `.evaluate()` has two parts: the loss on the test examples, and the `score` which is the percent it got right. In this case thats 100%! Woohoo! If you're not convinced that we've predicted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/30 [>.............................] - ETA: 0s\n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: versicolor actual: versicolor\n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: setosa     actual: setosa    \n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: setosa     actual: setosa    \n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: setosa     actual: setosa    \n",
      "T predicted: versicolor actual: versicolor\n",
      "T predicted: setosa     actual: setosa    \n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: setosa     actual: setosa    \n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: versicolor actual: versicolor\n",
      "T predicted: setosa     actual: setosa    \n",
      "T predicted: versicolor actual: versicolor\n",
      "T predicted: setosa     actual: setosa    \n",
      "T predicted: setosa     actual: setosa    \n",
      "T predicted: versicolor actual: versicolor\n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: setosa     actual: setosa    \n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: setosa     actual: setosa    \n",
      "T predicted: versicolor actual: versicolor\n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: veriginica actual: veriginica\n",
      "T predicted: versicolor actual: versicolor\n"
     ]
    }
   ],
   "source": [
    "idxToClass = {0: \"versicolor\", 1:\"veriginica\", 2:\"setosa\"}\n",
    "pClasses = model.predict_classes(testX, batch_size=1)\n",
    "origClasses = map(lambda x: list(x).index(1), testY)\n",
    "print\n",
    "for i in xrange(len(pClasses)):\n",
    "    print \"{} predicted: {:<10} actual: {:<10}\".format(\"T\" if pClasses[i] == origClasses[i] else \"F\",\n",
    "                                                       idxToClass[pClasses[i]],\n",
    "                                                       idxToClass[origClasses[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example application: IMDB Movie reviews sentiment classification\n",
    "\n",
    "<!-- Small image classification https://keras.io/datasets/ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example to delve more into different types of Keras layers and settings we are going to go over and use the IMDB Movie reviews sentiment classification example dataset included with Keras. The original network we're adapting from can be found in the references bellow.\n",
    "\n",
    "Before we begin, we will import all of the necessary utilities we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define some constants we will be using later and load our data into variables. We set an nb_words flag on our imdb data set to specify we only want to consider the `max_features` number of top most frequent words. We also set a seed so that we can get the same data every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 'train sequences')\n",
      "(25000, 'test sequences')\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"imdb_full.pkl\",\n",
    "                                                      nb_words=max_features,\n",
    "                                                      seed=388)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then choose to preprocess our data using the Keras sequence preprocessing library to cut each examples text to `maxlen` number of words out of the most frequent words. This shortens the data and will allow us to train faster on the most \"relevant\" (frequent) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('X_train shape:', (25000L, 80L))\n",
      "('X_test shape:', (25000L, 80L))\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've processed our data enough we can begin building our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses many new layer types we have not been exposed to before - more detailed information can be found in Keras documentation pages. The Embedding layer creates a sort of map from the words in the dataset to some continuous vector space. This is a natural language processing trick that is meant to help with text learning. We then use an LSTM layer which stands for Long-Short term memory unit, which is a layer type proposed by Sepp Hochreiter in 1997 - it is considered well-suited to learn to classify things when there are very long time lags of unknown size between important events. How this helps learning on this dataset is left as an exercise to the reader. As you can see, there are very complicated layers built upon amazing research that you can utilize by writing one simple line in Keras.\n",
    "\n",
    "Next we have a dense layer like we've seen before which then outputs to an Activation layer which just applies the sigmoid function which maps the ou tput to a float between 0 and 1.\n",
    "\n",
    "We also can define different loss functions and optimizers. In this case we use `binary_crossentropy` as a loss function, also known as logloss. We also use a different type of optimizer now, `adam`, which is just another method of stochastic optimization. We see again how Keras has provided such power and complexity at our fingertips - we can prototype with awesome speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 138s - loss: 0.5288 - acc: 0.7334 - val_loss: 0.3872 - val_acc: 0.8272\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 141s - loss: 0.3732 - acc: 0.8390 - val_loss: 0.3732 - val_acc: 0.8326\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 141s - loss: 0.2998 - acc: 0.8766 - val_loss: 0.3663 - val_acc: 0.8400\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 161s - loss: 0.2457 - acc: 0.9006 - val_loss: 0.3863 - val_acc: 0.8362\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 152s - loss: 0.2022 - acc: 0.9207 - val_loss: 0.4124 - val_acc: 0.8312\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 163s - loss: 0.1677 - acc: 0.9354 - val_loss: 0.4591 - val_acc: 0.8298\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 146s - loss: 0.1406 - acc: 0.9466 - val_loss: 0.4790 - val_acc: 0.8295\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 156s - loss: 0.1242 - acc: 0.9520 - val_loss: 0.5096 - val_acc: 0.8275\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 151s - loss: 0.1053 - acc: 0.9608 - val_loss: 0.6006 - val_acc: 0.8204\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 155s - loss: 0.0941 - acc: 0.9648 - val_loss: 0.6269 - val_acc: 0.8225\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 153s - loss: 0.0841 - acc: 0.9689 - val_loss: 0.5690 - val_acc: 0.8228\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 160s - loss: 0.0815 - acc: 0.9705 - val_loss: 0.6177 - val_acc: 0.8248\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 150s - loss: 0.0728 - acc: 0.9714 - val_loss: 0.6212 - val_acc: 0.8218\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 185s - loss: 0.0659 - acc: 0.9764 - val_loss: 0.6772 - val_acc: 0.8191\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 191s - loss: 0.0643 - acc: 0.9776 - val_loss: 0.6434 - val_acc: 0.8148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee88c88>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and evaluate its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 39s    \n",
      "('Test loss:', 0.64337649493694304)\n",
      "('Test accuracy %:', 0.81484000000000001)\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test loss:', score)\n",
    "print('Test accuracy %:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model we have achieved 81% accuracy on the test dataset and 97% accuracy on the training dataset. And if we look at the above output actually epoch 3 had the best test accuracy. This feels like we might be overfitting the model on the training data so we can try making some changes to the model to avoid this. Following is a proposed an alternate model to avoid overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 163s - loss: 0.6086 - acc: 0.6569 - val_loss: 0.4334 - val_acc: 0.8064\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 167s - loss: 0.4629 - acc: 0.7868 - val_loss: 0.3897 - val_acc: 0.8294\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 169s - loss: 0.3805 - acc: 0.8357 - val_loss: 0.3553 - val_acc: 0.8442\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 159s - loss: 0.3283 - acc: 0.8588 - val_loss: 0.3480 - val_acc: 0.8456\n",
      "25000/25000 [==============================] - 46s    \n",
      "('Test loss:', 0.34797629148483278)\n",
      "('Test accuracy %:', 0.84560000000000002)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GaussianNoise\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.25))\n",
    "model.add(GaussianNoise(0.2))\n",
    "model.add(GRU(128, dropout_W=0.25, dropout_U=0.25))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size, nb_epoch=4,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test loss:', score)\n",
    "print('Test accuracy %:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seemed to have increased our accuracy on the testing dataset to 85%! We made several changes to our model. Gaussian noise applys the training input of the layer to an additive zero-centered gaussian noise with the standard deviation as the parameter - this means we dont learn on the training data too \"exactly\".  We also changed LSTM to GRU which is a Gated Recurrent Unit which you can read about here (https://arxiv.org/pdf/1412.3555v1.pdf), this is another type of layer that has just been shown to work well with this sort of data. And we increased dropout rates slightly to further mitigate overfitting.\n",
    "\n",
    "I'm no neural network expert but I was able to squeeze out 3% more accuracy on the test data with my changes. The takeaway here is that it is incrediblly easy to just grab and drop different layers into our network to try things out in order to try get better networks. With something as mathematically and representively powerful as neural networks with its complexity that often defies human understanding, it is very useful to have a tool that allows us to try different networks efficiently and with very little pain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Application: Diabetes in Pima Indians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for this example adapted from reference. We now show another example on classifying patients and whether they have onset of diabetes or not based on several variables including: no. of times pregnant, tricep skin fold thickness, bmi, and others. We begin by importing our required model and layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.data\", delimiter=\",\")\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define and compile a simple model with just 3 layers of densely connected units with different activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then fit our model on the dataset, but also use an optional parameter to split our data automatically into training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/150\n",
      "576/576 [==============================] - 2s - loss: 0.6892 - acc: 0.6458 - val_loss: 0.6857 - val_acc: 0.6354\n",
      "Epoch 2/150\n",
      "576/576 [==============================] - 2s - loss: 0.6781 - acc: 0.6563 - val_loss: 0.6726 - val_acc: 0.6354\n",
      "Epoch 3/150\n",
      "576/576 [==============================] - 2s - loss: 0.6617 - acc: 0.6563 - val_loss: 0.6607 - val_acc: 0.6354\n",
      "Epoch 4/150\n",
      "576/576 [==============================] - 2s - loss: 0.6454 - acc: 0.6563 - val_loss: 0.6501 - val_acc: 0.6354\n",
      "Epoch 5/150\n",
      "576/576 [==============================] - 2s - loss: 0.6362 - acc: 0.6563 - val_loss: 0.6470 - val_acc: 0.6354\n",
      "Epoch 6/150\n",
      "576/576 [==============================] - 2s - loss: 0.6266 - acc: 0.6562 - val_loss: 0.6487 - val_acc: 0.6354\n",
      "Epoch 7/150\n",
      "576/576 [==============================] - 2s - loss: 0.6210 - acc: 0.6562 - val_loss: 0.6411 - val_acc: 0.6354\n",
      "Epoch 8/150\n",
      "576/576 [==============================] - 2s - loss: 0.6117 - acc: 0.6563 - val_loss: 0.6398 - val_acc: 0.6354\n",
      "Epoch 9/150\n",
      "576/576 [==============================] - 2s - loss: 0.5999 - acc: 0.6563 - val_loss: 0.6278 - val_acc: 0.6354\n",
      "Epoch 10/150\n",
      "576/576 [==============================] - 2s - loss: 0.5974 - acc: 0.6563 - val_loss: 0.6273 - val_acc: 0.6354\n",
      "Epoch 11/150\n",
      "576/576 [==============================] - 2s - loss: 0.5822 - acc: 0.6563 - val_loss: 0.6244 - val_acc: 0.6354\n",
      "Epoch 12/150\n",
      "576/576 [==============================] - 2s - loss: 0.5754 - acc: 0.6563 - val_loss: 0.6191 - val_acc: 0.6354\n",
      "Epoch 13/150\n",
      "576/576 [==============================] - 2s - loss: 0.5578 - acc: 0.6563 - val_loss: 0.6084 - val_acc: 0.6354\n",
      "Epoch 14/150\n",
      "576/576 [==============================] - 2s - loss: 0.5392 - acc: 0.6563 - val_loss: 0.6198 - val_acc: 0.6354\n",
      "Epoch 15/150\n",
      "576/576 [==============================] - 2s - loss: 0.5270 - acc: 0.6563 - val_loss: 0.6244 - val_acc: 0.6354\n",
      "Epoch 16/150\n",
      "576/576 [==============================] - 2s - loss: 0.5196 - acc: 0.6563 - val_loss: 0.6335 - val_acc: 0.6354\n",
      "Epoch 17/150\n",
      "576/576 [==============================] - 2s - loss: 0.4944 - acc: 0.7361 - val_loss: 0.6446 - val_acc: 0.6563\n",
      "Epoch 18/150\n",
      "576/576 [==============================] - 2s - loss: 0.5049 - acc: 0.7760 - val_loss: 0.6170 - val_acc: 0.6823\n",
      "Epoch 19/150\n",
      "576/576 [==============================] - 2s - loss: 0.4843 - acc: 0.7899 - val_loss: 0.6205 - val_acc: 0.6979\n",
      "Epoch 20/150\n",
      "576/576 [==============================] - 2s - loss: 0.4891 - acc: 0.7708 - val_loss: 0.6160 - val_acc: 0.6927\n",
      "Epoch 21/150\n",
      "576/576 [==============================] - 2s - loss: 0.4682 - acc: 0.8090 - val_loss: 0.6596 - val_acc: 0.6823\n",
      "Epoch 22/150\n",
      "576/576 [==============================] - 2s - loss: 0.4696 - acc: 0.7830 - val_loss: 0.6344 - val_acc: 0.6875\n",
      "Epoch 23/150\n",
      "576/576 [==============================] - 2s - loss: 0.4580 - acc: 0.8142 - val_loss: 0.6215 - val_acc: 0.7083\n",
      "Epoch 24/150\n",
      "576/576 [==============================] - 2s - loss: 0.4736 - acc: 0.7986 - val_loss: 0.6537 - val_acc: 0.6979\n",
      "Epoch 25/150\n",
      "576/576 [==============================] - 2s - loss: 0.4665 - acc: 0.8125 - val_loss: 0.6565 - val_acc: 0.7083\n",
      "Epoch 26/150\n",
      "576/576 [==============================] - 2s - loss: 0.4470 - acc: 0.8264 - val_loss: 0.6343 - val_acc: 0.7083\n",
      "Epoch 27/150\n",
      "576/576 [==============================] - 2s - loss: 0.4413 - acc: 0.7934 - val_loss: 0.6458 - val_acc: 0.7083\n",
      "Epoch 28/150\n",
      "576/576 [==============================] - 2s - loss: 0.4463 - acc: 0.8125 - val_loss: 0.6520 - val_acc: 0.7031\n",
      "Epoch 29/150\n",
      "576/576 [==============================] - 2s - loss: 0.4357 - acc: 0.8142 - val_loss: 0.6598 - val_acc: 0.6927\n",
      "Epoch 30/150\n",
      "576/576 [==============================] - 2s - loss: 0.4386 - acc: 0.8212 - val_loss: 0.6339 - val_acc: 0.7083\n",
      "Epoch 31/150\n",
      "576/576 [==============================] - 2s - loss: 0.4418 - acc: 0.8177 - val_loss: 0.6443 - val_acc: 0.7031\n",
      "Epoch 32/150\n",
      "576/576 [==============================] - 2s - loss: 0.4405 - acc: 0.8038 - val_loss: 0.6246 - val_acc: 0.7083\n",
      "Epoch 33/150\n",
      "576/576 [==============================] - 2s - loss: 0.4077 - acc: 0.8542 - val_loss: 0.6645 - val_acc: 0.7031\n",
      "Epoch 34/150\n",
      "576/576 [==============================] - 2s - loss: 0.4152 - acc: 0.8455 - val_loss: 0.6736 - val_acc: 0.7031\n",
      "Epoch 35/150\n",
      "576/576 [==============================] - 2s - loss: 0.4213 - acc: 0.8351 - val_loss: 0.6471 - val_acc: 0.6875\n",
      "Epoch 36/150\n",
      "576/576 [==============================] - 2s - loss: 0.4241 - acc: 0.8247 - val_loss: 0.6568 - val_acc: 0.6875\n",
      "Epoch 37/150\n",
      "576/576 [==============================] - 2s - loss: 0.4281 - acc: 0.8472 - val_loss: 0.6614 - val_acc: 0.6875\n",
      "Epoch 38/150\n",
      "576/576 [==============================] - 4s - loss: 0.4324 - acc: 0.8125 - val_loss: 0.6685 - val_acc: 0.6875\n",
      "Epoch 39/150\n",
      "576/576 [==============================] - 2s - loss: 0.4109 - acc: 0.8403 - val_loss: 0.6556 - val_acc: 0.6823\n",
      "Epoch 40/150\n",
      "576/576 [==============================] - 2s - loss: 0.4219 - acc: 0.8229 - val_loss: 0.6691 - val_acc: 0.6979\n",
      "Epoch 41/150\n",
      "576/576 [==============================] - 2s - loss: 0.4211 - acc: 0.8299 - val_loss: 0.6920 - val_acc: 0.6927\n",
      "Epoch 42/150\n",
      "576/576 [==============================] - 3s - loss: 0.4161 - acc: 0.8299 - val_loss: 0.6548 - val_acc: 0.6771\n",
      "Epoch 43/150\n",
      "576/576 [==============================] - 2s - loss: 0.3682 - acc: 0.8715 - val_loss: 0.7044 - val_acc: 0.6875\n",
      "Epoch 44/150\n",
      "576/576 [==============================] - 2s - loss: 0.4080 - acc: 0.8420 - val_loss: 0.6741 - val_acc: 0.6771\n",
      "Epoch 45/150\n",
      "576/576 [==============================] - 2s - loss: 0.3984 - acc: 0.8437 - val_loss: 0.6692 - val_acc: 0.6823\n",
      "Epoch 46/150\n",
      "576/576 [==============================] - 2s - loss: 0.3913 - acc: 0.8351 - val_loss: 0.6671 - val_acc: 0.6823\n",
      "Epoch 47/150\n",
      "576/576 [==============================] - 2s - loss: 0.4003 - acc: 0.8368 - val_loss: 0.6489 - val_acc: 0.6927\n",
      "Epoch 48/150\n",
      "576/576 [==============================] - 2s - loss: 0.4111 - acc: 0.8281 - val_loss: 0.6635 - val_acc: 0.6823\n",
      "Epoch 49/150\n",
      "576/576 [==============================] - 2s - loss: 0.3849 - acc: 0.8507 - val_loss: 0.6729 - val_acc: 0.6667\n",
      "Epoch 50/150\n",
      "576/576 [==============================] - 2s - loss: 0.3821 - acc: 0.8559 - val_loss: 0.6952 - val_acc: 0.6667\n",
      "Epoch 51/150\n",
      "576/576 [==============================] - 2s - loss: 0.3547 - acc: 0.8594 - val_loss: 0.6732 - val_acc: 0.6979\n",
      "Epoch 52/150\n",
      "576/576 [==============================] - 2s - loss: 0.3553 - acc: 0.8646 - val_loss: 0.7139 - val_acc: 0.6927\n",
      "Epoch 53/150\n",
      "576/576 [==============================] - 2s - loss: 0.3809 - acc: 0.8385 - val_loss: 0.6714 - val_acc: 0.6563\n",
      "Epoch 54/150\n",
      "576/576 [==============================] - 2s - loss: 0.3787 - acc: 0.8576 - val_loss: 0.6865 - val_acc: 0.6719\n",
      "Epoch 55/150\n",
      "576/576 [==============================] - 2s - loss: 0.3780 - acc: 0.8472 - val_loss: 0.6862 - val_acc: 0.6563\n",
      "Epoch 56/150\n",
      "576/576 [==============================] - 2s - loss: 0.3574 - acc: 0.8663 - val_loss: 0.6718 - val_acc: 0.6771\n",
      "Epoch 57/150\n",
      "576/576 [==============================] - 2s - loss: 0.3571 - acc: 0.8542 - val_loss: 0.7316 - val_acc: 0.6927\n",
      "Epoch 58/150\n",
      "576/576 [==============================] - 2s - loss: 0.3459 - acc: 0.8715 - val_loss: 0.7147 - val_acc: 0.6667\n",
      "Epoch 59/150\n",
      "576/576 [==============================] - 2s - loss: 0.3816 - acc: 0.8576 - val_loss: 0.6530 - val_acc: 0.6823\n",
      "Epoch 60/150\n",
      "576/576 [==============================] - 2s - loss: 0.3606 - acc: 0.8646 - val_loss: 0.6628 - val_acc: 0.6771\n",
      "Epoch 61/150\n",
      "576/576 [==============================] - 2s - loss: 0.3364 - acc: 0.8733 - val_loss: 0.6657 - val_acc: 0.6719\n",
      "Epoch 62/150\n",
      "576/576 [==============================] - 2s - loss: 0.3757 - acc: 0.8507 - val_loss: 0.6936 - val_acc: 0.6927\n",
      "Epoch 63/150\n",
      "576/576 [==============================] - 2s - loss: 0.3407 - acc: 0.8715 - val_loss: 0.6849 - val_acc: 0.6823\n",
      "Epoch 64/150\n",
      "576/576 [==============================] - 2s - loss: 0.3573 - acc: 0.8576 - val_loss: 0.7165 - val_acc: 0.6875\n",
      "Epoch 65/150\n",
      "576/576 [==============================] - 2s - loss: 0.3302 - acc: 0.8854 - val_loss: 0.7267 - val_acc: 0.6875\n",
      "Epoch 66/150\n",
      "576/576 [==============================] - 2s - loss: 0.3560 - acc: 0.8750 - val_loss: 0.7020 - val_acc: 0.7031\n",
      "Epoch 67/150\n",
      "576/576 [==============================] - 2s - loss: 0.3444 - acc: 0.8611 - val_loss: 0.6774 - val_acc: 0.6771\n",
      "Epoch 68/150\n",
      "576/576 [==============================] - 2s - loss: 0.3354 - acc: 0.8715 - val_loss: 0.6992 - val_acc: 0.6823\n",
      "Epoch 69/150\n",
      "576/576 [==============================] - 2s - loss: 0.3440 - acc: 0.8594 - val_loss: 0.6974 - val_acc: 0.6979\n",
      "Epoch 70/150\n",
      "576/576 [==============================] - 2s - loss: 0.3624 - acc: 0.8663 - val_loss: 0.7106 - val_acc: 0.6927\n",
      "Epoch 71/150\n",
      "576/576 [==============================] - 2s - loss: 0.3499 - acc: 0.8785 - val_loss: 0.7024 - val_acc: 0.6875\n",
      "Epoch 72/150\n",
      "576/576 [==============================] - 2s - loss: 0.3364 - acc: 0.8733 - val_loss: 0.6950 - val_acc: 0.6979\n",
      "Epoch 73/150\n",
      "576/576 [==============================] - 2s - loss: 0.3362 - acc: 0.8715 - val_loss: 0.7123 - val_acc: 0.7031\n",
      "Epoch 74/150\n",
      "576/576 [==============================] - 2s - loss: 0.3070 - acc: 0.8889 - val_loss: 0.6779 - val_acc: 0.6927\n",
      "Epoch 75/150\n",
      "576/576 [==============================] - 2s - loss: 0.3178 - acc: 0.8819 - val_loss: 0.7224 - val_acc: 0.7083\n",
      "Epoch 76/150\n",
      "576/576 [==============================] - 2s - loss: 0.3228 - acc: 0.8715 - val_loss: 0.7229 - val_acc: 0.7083\n",
      "Epoch 77/150\n",
      "576/576 [==============================] - 2s - loss: 0.3171 - acc: 0.8941 - val_loss: 0.7269 - val_acc: 0.6927\n",
      "Epoch 78/150\n",
      "576/576 [==============================] - 2s - loss: 0.3135 - acc: 0.8767 - val_loss: 0.7565 - val_acc: 0.7031\n",
      "Epoch 79/150\n",
      "576/576 [==============================] - 2s - loss: 0.2690 - acc: 0.9097 - val_loss: 0.7660 - val_acc: 0.6979\n",
      "Epoch 80/150\n",
      "576/576 [==============================] - 2s - loss: 0.2771 - acc: 0.9080 - val_loss: 0.8004 - val_acc: 0.6979\n",
      "Epoch 81/150\n",
      "576/576 [==============================] - 2s - loss: 0.3034 - acc: 0.8819 - val_loss: 0.7578 - val_acc: 0.6771\n",
      "Epoch 82/150\n",
      "576/576 [==============================] - 2s - loss: 0.2625 - acc: 0.9010 - val_loss: 0.7868 - val_acc: 0.6823\n",
      "Epoch 83/150\n",
      "576/576 [==============================] - 2s - loss: 0.3153 - acc: 0.8872 - val_loss: 0.7650 - val_acc: 0.6823\n",
      "Epoch 84/150\n",
      "576/576 [==============================] - 2s - loss: 0.3037 - acc: 0.8924 - val_loss: 0.8138 - val_acc: 0.6823\n",
      "Epoch 85/150\n",
      "576/576 [==============================] - 2s - loss: 0.3136 - acc: 0.8715 - val_loss: 0.7478 - val_acc: 0.6823\n",
      "Epoch 86/150\n",
      "576/576 [==============================] - 2s - loss: 0.3142 - acc: 0.8750 - val_loss: 0.7977 - val_acc: 0.7083\n",
      "Epoch 87/150\n",
      "576/576 [==============================] - 2s - loss: 0.3220 - acc: 0.8837 - val_loss: 0.7327 - val_acc: 0.7188\n",
      "Epoch 88/150\n",
      "576/576 [==============================] - 2s - loss: 0.3088 - acc: 0.8767 - val_loss: 0.7274 - val_acc: 0.7083\n",
      "Epoch 89/150\n",
      "576/576 [==============================] - 3s - loss: 0.2900 - acc: 0.8889 - val_loss: 0.7525 - val_acc: 0.7344\n",
      "Epoch 90/150\n",
      "576/576 [==============================] - 3s - loss: 0.2911 - acc: 0.8941 - val_loss: 0.7249 - val_acc: 0.7031\n",
      "Epoch 91/150\n",
      "576/576 [==============================] - 2s - loss: 0.3072 - acc: 0.8663 - val_loss: 0.7311 - val_acc: 0.6927\n",
      "Epoch 92/150\n",
      "576/576 [==============================] - 2s - loss: 0.3120 - acc: 0.8715 - val_loss: 0.7098 - val_acc: 0.6979\n",
      "Epoch 93/150\n",
      "576/576 [==============================] - 2s - loss: 0.2992 - acc: 0.8924 - val_loss: 0.7146 - val_acc: 0.6979\n",
      "Epoch 94/150\n",
      "576/576 [==============================] - 2s - loss: 0.2940 - acc: 0.8906 - val_loss: 0.7419 - val_acc: 0.6823\n",
      "Epoch 95/150\n",
      "576/576 [==============================] - 2s - loss: 0.2788 - acc: 0.8837 - val_loss: 0.7653 - val_acc: 0.6979\n",
      "Epoch 96/150\n",
      "576/576 [==============================] - 2s - loss: 0.2952 - acc: 0.8941 - val_loss: 0.7313 - val_acc: 0.6979\n",
      "Epoch 97/150\n",
      "576/576 [==============================] - 3s - loss: 0.2987 - acc: 0.8872 - val_loss: 0.7233 - val_acc: 0.7031\n",
      "Epoch 98/150\n",
      "576/576 [==============================] - 2s - loss: 0.2989 - acc: 0.8889 - val_loss: 0.7299 - val_acc: 0.6979\n",
      "Epoch 99/150\n",
      "576/576 [==============================] - 2s - loss: 0.2724 - acc: 0.9010 - val_loss: 0.7658 - val_acc: 0.6979\n",
      "Epoch 100/150\n",
      "576/576 [==============================] - 2s - loss: 0.2979 - acc: 0.8854 - val_loss: 0.7710 - val_acc: 0.6823\n",
      "Epoch 101/150\n",
      "576/576 [==============================] - 2s - loss: 0.3005 - acc: 0.8698 - val_loss: 0.7318 - val_acc: 0.7031\n",
      "Epoch 102/150\n",
      "576/576 [==============================] - 2s - loss: 0.2715 - acc: 0.9045 - val_loss: 0.7568 - val_acc: 0.7031\n",
      "Epoch 103/150\n",
      "576/576 [==============================] - 2s - loss: 0.2775 - acc: 0.9045 - val_loss: 0.7303 - val_acc: 0.6979\n",
      "Epoch 104/150\n",
      "576/576 [==============================] - 2s - loss: 0.2804 - acc: 0.8889 - val_loss: 0.7531 - val_acc: 0.6875\n",
      "Epoch 105/150\n",
      "576/576 [==============================] - 3s - loss: 0.2652 - acc: 0.8958 - val_loss: 0.7900 - val_acc: 0.6823\n",
      "Epoch 106/150\n",
      "576/576 [==============================] - 2s - loss: 0.2972 - acc: 0.9045 - val_loss: 0.7722 - val_acc: 0.6927\n",
      "Epoch 107/150\n",
      "576/576 [==============================] - 2s - loss: 0.2737 - acc: 0.8976 - val_loss: 0.7569 - val_acc: 0.6875\n",
      "Epoch 108/150\n",
      "576/576 [==============================] - 3s - loss: 0.3041 - acc: 0.8785 - val_loss: 0.7457 - val_acc: 0.6823\n",
      "Epoch 109/150\n",
      "576/576 [==============================] - 3s - loss: 0.2615 - acc: 0.9010 - val_loss: 0.7349 - val_acc: 0.6875\n",
      "Epoch 110/150\n",
      "576/576 [==============================] - 3s - loss: 0.2480 - acc: 0.9115 - val_loss: 0.7796 - val_acc: 0.6875\n",
      "Epoch 111/150\n",
      "576/576 [==============================] - 3s - loss: 0.2521 - acc: 0.9062 - val_loss: 0.7627 - val_acc: 0.7083\n",
      "Epoch 112/150\n",
      "576/576 [==============================] - 3s - loss: 0.2644 - acc: 0.8976 - val_loss: 0.7559 - val_acc: 0.6979\n",
      "Epoch 113/150\n",
      "576/576 [==============================] - 3s - loss: 0.2576 - acc: 0.9115 - val_loss: 0.7683 - val_acc: 0.6927\n",
      "Epoch 114/150\n",
      "576/576 [==============================] - 3s - loss: 0.2608 - acc: 0.9149 - val_loss: 0.7787 - val_acc: 0.7083\n",
      "Epoch 115/150\n",
      "576/576 [==============================] - 3s - loss: 0.2700 - acc: 0.8976 - val_loss: 0.7583 - val_acc: 0.7083\n",
      "Epoch 116/150\n",
      "576/576 [==============================] - 3s - loss: 0.2768 - acc: 0.8854 - val_loss: 0.7862 - val_acc: 0.7083\n",
      "Epoch 117/150\n",
      "576/576 [==============================] - 3s - loss: 0.2532 - acc: 0.9028 - val_loss: 0.7606 - val_acc: 0.6875\n",
      "Epoch 118/150\n",
      "576/576 [==============================] - 3s - loss: 0.2716 - acc: 0.8958 - val_loss: 0.7800 - val_acc: 0.6927\n",
      "Epoch 119/150\n",
      "576/576 [==============================] - 3s - loss: 0.2596 - acc: 0.9149 - val_loss: 0.7901 - val_acc: 0.6823\n",
      "Epoch 120/150\n",
      "576/576 [==============================] - 3s - loss: 0.2900 - acc: 0.8976 - val_loss: 0.7349 - val_acc: 0.6927\n",
      "Epoch 121/150\n",
      "576/576 [==============================] - 3s - loss: 0.2218 - acc: 0.9340 - val_loss: 0.7707 - val_acc: 0.6719\n",
      "Epoch 122/150\n",
      "576/576 [==============================] - 3s - loss: 0.2626 - acc: 0.9010 - val_loss: 0.7997 - val_acc: 0.6771\n",
      "Epoch 123/150\n",
      "576/576 [==============================] - 3s - loss: 0.2531 - acc: 0.9080 - val_loss: 0.7972 - val_acc: 0.6667\n",
      "Epoch 124/150\n",
      "576/576 [==============================] - 3s - loss: 0.2461 - acc: 0.9132 - val_loss: 0.7914 - val_acc: 0.6615\n",
      "Epoch 125/150\n",
      "576/576 [==============================] - 3s - loss: 0.2445 - acc: 0.9184 - val_loss: 0.8558 - val_acc: 0.6719\n",
      "Epoch 126/150\n",
      "576/576 [==============================] - 3s - loss: 0.2335 - acc: 0.9167 - val_loss: 0.8425 - val_acc: 0.6979\n",
      "Epoch 127/150\n",
      "576/576 [==============================] - 3s - loss: 0.2310 - acc: 0.9167 - val_loss: 0.8216 - val_acc: 0.6771\n",
      "Epoch 128/150\n",
      "576/576 [==============================] - 3s - loss: 0.2885 - acc: 0.8941 - val_loss: 0.7997 - val_acc: 0.6667\n",
      "Epoch 129/150\n",
      "576/576 [==============================] - 3s - loss: 0.2648 - acc: 0.9010 - val_loss: 0.8090 - val_acc: 0.6719\n",
      "Epoch 130/150\n",
      "576/576 [==============================] - 3s - loss: 0.2651 - acc: 0.8976 - val_loss: 0.8053 - val_acc: 0.6667\n",
      "Epoch 131/150\n",
      "576/576 [==============================] - 3s - loss: 0.2491 - acc: 0.9028 - val_loss: 0.8653 - val_acc: 0.6771\n",
      "Epoch 132/150\n",
      "576/576 [==============================] - 3s - loss: 0.2347 - acc: 0.9236 - val_loss: 0.8418 - val_acc: 0.6615\n",
      "Epoch 133/150\n",
      "576/576 [==============================] - 3s - loss: 0.2613 - acc: 0.8906 - val_loss: 0.8137 - val_acc: 0.6615\n",
      "Epoch 134/150\n",
      "576/576 [==============================] - 3s - loss: 0.2570 - acc: 0.8854 - val_loss: 0.8593 - val_acc: 0.6719\n",
      "Epoch 135/150\n",
      "576/576 [==============================] - 3s - loss: 0.2141 - acc: 0.9184 - val_loss: 0.8394 - val_acc: 0.6615\n",
      "Epoch 136/150\n",
      "576/576 [==============================] - 3s - loss: 0.2636 - acc: 0.8941 - val_loss: 0.8778 - val_acc: 0.6771\n",
      "Epoch 137/150\n",
      "576/576 [==============================] - 3s - loss: 0.2015 - acc: 0.9410 - val_loss: 0.9275 - val_acc: 0.6667\n",
      "Epoch 138/150\n",
      "576/576 [==============================] - 3s - loss: 0.2197 - acc: 0.9253 - val_loss: 0.8640 - val_acc: 0.6823\n",
      "Epoch 139/150\n",
      "576/576 [==============================] - 5s - loss: 0.2089 - acc: 0.9306 - val_loss: 0.8654 - val_acc: 0.6719\n",
      "Epoch 140/150\n",
      "576/576 [==============================] - 3s - loss: 0.2237 - acc: 0.9149 - val_loss: 0.8741 - val_acc: 0.6458\n",
      "Epoch 141/150\n",
      "576/576 [==============================] - 3s - loss: 0.1974 - acc: 0.9306 - val_loss: 0.8616 - val_acc: 0.6510\n",
      "Epoch 142/150\n",
      "576/576 [==============================] - 3s - loss: 0.2482 - acc: 0.9062 - val_loss: 0.8682 - val_acc: 0.7083\n",
      "Epoch 143/150\n",
      "576/576 [==============================] - 3s - loss: 0.2459 - acc: 0.9132 - val_loss: 0.8405 - val_acc: 0.6823\n",
      "Epoch 144/150\n",
      "576/576 [==============================] - 3s - loss: 0.2225 - acc: 0.9184 - val_loss: 0.8456 - val_acc: 0.6823\n",
      "Epoch 145/150\n",
      "576/576 [==============================] - 3s - loss: 0.2193 - acc: 0.9271 - val_loss: 0.8659 - val_acc: 0.6771\n",
      "Epoch 146/150\n",
      "576/576 [==============================] - 3s - loss: 0.2297 - acc: 0.9132 - val_loss: 0.8380 - val_acc: 0.6615\n",
      "Epoch 147/150\n",
      "576/576 [==============================] - 3s - loss: 0.2179 - acc: 0.9132 - val_loss: 0.8432 - val_acc: 0.6563\n",
      "Epoch 148/150\n",
      "576/576 [==============================] - 3s - loss: 0.2464 - acc: 0.9028 - val_loss: 0.9323 - val_acc: 0.6667\n",
      "Epoch 149/150\n",
      "576/576 [==============================] - 3s - loss: 0.2342 - acc: 0.9080 - val_loss: 0.9352 - val_acc: 0.6875\n",
      "Epoch 150/150\n",
      "576/576 [==============================] - 3s - loss: 0.1953 - acc: 0.9236 - val_loss: 0.8740 - val_acc: 0.6875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20f0e4a8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, validation_split=0.25, nb_epoch=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply techniques we've learned in class such as k-fold cross validation, we can use a quick little addition from sci-kit learn to help with our kfold validation called StratifiedKFold. As you can see, we can very simply add and remove additional complexity and steps on top of all of our keras models without a care in the world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 75.32%\n",
      "acc: 74.03%\n",
      "acc: 76.62%\n",
      "acc: 83.12%\n",
      "acc: 80.52%\n",
      "acc: 62.34%\n",
      "acc: 71.43%\n",
      "acc: 80.52%\n",
      "acc: 82.89%\n",
      "acc: 65.79%\n",
      "75.26% (+/- 6.71%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "  # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "    model.add(Dense(8, init='uniform', activation='relu'))\n",
    "    model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "    model.fit(X[train], Y[train], nb_epoch=150, batch_size=10, verbose=0)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    " \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it is incredibly easy to make and modify Keras models, as well as build additional infrastructure and techniques on top of them. You can find more information about neural networks and Keras in the links below. I hope you have had as much fun and learned as much reading this tutorial as I did making it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- Keras tutorial videos: https://www.youtube.com/playlist?list=PLFxrZqbLojdKuK7Lm6uamegEFGW2wki6P\\\n",
    "- Keras examples: https://github.com/fchollet/keras/tree/master/examples\n",
    "- Neural Network reading: https://en.wikipedia.org/wiki/Artificial_neural_network\n",
    "- Additional NN reading: http://www.cs.cmu.edu/~epxing/Class/10701-10s/Lecture/lecture7.pdf\n",
    "- Old CMU Neural Net class: https://www.cs.cmu.edu/afs/cs/academic/class/15782-f06/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras: https://keras.io/\n",
    "- Iris Dataset: https://archive.ics.uci.edu/ml/datasets/Iris\n",
    "- IMDB Example: https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py\n",
    "- IMDB Alternate: https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py\n",
    "- Pima Indians Example: http://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "- Model Evaluation: http://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
