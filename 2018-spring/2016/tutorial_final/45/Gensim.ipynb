{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gensim\n",
    "Gensim is a useful python library for topic modelling and processling large corpora (collection of documents).\n",
    "To install Gensim first run the folloing command in terminal\n",
    "```\n",
    "pip install -U gensim\n",
    "```\n",
    "Alternatively see the gensim github page for instructions for building/installing from source [here](https://github.com/RaRe-Technologies/gensim).\n",
    "\n",
    "To get things started we must first import gensim and any other libraries we wish to use later on.\n",
    "\n",
    "Note that if you use pip to install gensim, you may see the following error if you're using Anaconda:\n",
    "\n",
    "```\n",
    "Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so.\n",
    "```\n",
    "And the python kernal may not start.\n",
    "\n",
    "This is caused by numpy being incorectly configured with MKL (Intel's Math Kernel Library) on some installations.\n",
    "To remedy  this run:\n",
    "```\n",
    "conda install mkl\n",
    "conda install -f numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim as gs\n",
    "from gensim import models\n",
    "from collections import defaultdict as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to define a corpus to use as an example. I use an excerpt from East of Eden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "our_corpus = [\"When a child first catches adults out when it first walks into his grave little head that adults do not always have divine intelligence, that their judgments are not always wise, their thinking true, their sentences just his world falls into panic desolation.\",\n",
    "              \"The gods are fallen and all safety gone.\",\n",
    "              \"And there is one sure thing about the fall of gods:\",\n",
    "              \"they do not fall a little;\",\n",
    "              \"they crash and shatter or sink deeply into green muck.\",\n",
    "              \"It is a tedious job to build them up again;\",\n",
    "              \"they never quite shine.\",\n",
    "              \"And the child world is never quite whole again.\",\n",
    "              \"It is an aching kind of growing.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can do some easy and simple text processing as we have done similarly using nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our self-defined stoplist, small for the purpose of example.\n",
    "stoplist = [\"the\", \"a\", \"it\", \"is\", \"an\", \"are\", \"and\", \"or\", \"to\", \"of\", \"that\"]\n",
    "# Then we make each word lowercase and \"tokenize\" words using whitespace\n",
    "tokens_list = [[word for word in document.lower().split() if word not in stoplist] for document in our_corpus]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we would want to clean up our corpus so we only have words that occur more than once, and so we can\n",
    "define a function to do so, but we wont use it for our example since our corpus is so small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A naive appraoch to cleaning corpus, including naive_tokenization.\n",
    "# In pracice we would need to process more of the tokens.\n",
    "def clean_corpus(corpus, stoplist=[]):\n",
    "    naive_tokens =  [[word for word in document.lower().split() if word not in stoplist] for document in corpus]\n",
    "    # Count, alternatively we can use Collections.counter\n",
    "    freq = dd(int)\n",
    "    for tokens in naive_tokens:\n",
    "        for token in tokens:\n",
    "            freq[token] += 1\n",
    "    return [[token for token in tokens if freq[token] > 1] for tokens in naive_tokens]\n",
    "\n",
    "# We wont really use cleaned_corpus later\n",
    "cleaned_corpus =  clean_corpus(our_corpus, stoplist=stoplist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gensim, we cas use the corpora class to determine how many unique words are in our corpus.\n",
    "\n",
    "Note that this uses gensim's dictionary to process words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(63 unique tokens: [u'all', u'adults', u'judgments', u'wise,', u'into']...)\n",
      "Total tokens: 82\n"
     ]
    }
   ],
   "source": [
    "our_dict = gs.corpora.Dictionary(tokens_list)\n",
    "print our_dict\n",
    "\n",
    "# Compared to the amount of tokens in tokens (counting repeats and similar words)\n",
    "print \"Total tokens: {}\".format(sum(sum(1 for token in tokens) for tokens in tokens_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, here are a couple of notes on the `gensim.corpora.dictionary.Dictionary` class.\n",
    "\n",
    "The `Dictionary` class is essentially a wrapper around the bag of words model, and thus keeping this in mind the following methods make sense. Consider that we have a dictionary that we initialized using \n",
    "```\n",
    "c_dict = gensim.corpora.Dictionary(documents, prune_at=20000)\n",
    "```\n",
    "* the prune_at arguement dictates the maximum number of unique words in the instance of the dictionary. Once this limit has been reached, it begins to expel the least frequent words\n",
    "* `c_dict.add_documents(documents, prune_at = 20000)` allows us to add more documents to our dictionary and change the prune_at parameter.\n",
    "* `c_dict.doc2bow(document, allow_update=False)` Creates a bag of word dictionary from a list of words (tokenized and normalized) and updates the given dictionary if allow_update is set to true. The bag of word dictionay is a list of 2-tuples which are (token_id, token_count). It should be noted that in gensim, this is thought of as a \"vector\" and these vectors are in a sparse format.\n",
    "* The Dictionary class implements the methods get(), items(), iteritems(), iterkeys(), and itervalues().\n",
    "* For a complete list of functions see [here](https://radimrehurek.com/gensim/corpora/dictionary.html)\n",
    "\n",
    "As an example, if we want to create a bag of words using a different example string we would do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty list since no update: []\n",
      "\n",
      "(token_id, token_count) list: \n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1)]\n",
      "\n",
      "A visual bag of words (word, count): \n",
      "[(u'dog.', 1), (u'brown', 1), (u'lazy', 1), (u'things.', 1), (u'jumped', 1), (u'over', 1), (u'modern', 1), (u'fox', 1), (u'all', 1), (u'radio', 2), (u'internet', 1), (u'quick', 1), (u'oldest.', 1), (u'television,', 1)]\n"
     ]
    }
   ],
   "source": [
    "test_doc = \"The quick brown fox jumped over the lazy dog. Television, Radio and the Internet are all modern things. Radio is oldest.\"\n",
    "test_doc_tokens = [word for word in test_doc.lower().split() if word not in stoplist]\n",
    "c_dict = gs.corpora.Dictionary()\n",
    "\n",
    "# bow1 will be empty since we're not updating the dictionary (False is default)\n",
    "bow1 = c_dict.doc2bow(test_doc_tokens, allow_update=False)\n",
    "print \"Empty list since no update: {}\".format(bow1)\n",
    "\n",
    "# bow2 will not be empty since we are updating the dictionary\n",
    "bow2 = c_dict.doc2bow(test_doc_tokens, allow_update=True)\n",
    "print \"\\n(token_id, token_count) list: \\n{}\".format(bow2)\n",
    "\n",
    "# If we want to replace the token_id with the token itself we can do the following for visual confirmation\n",
    "# we can do the following\n",
    "visual_bow2 = [(c_dict.get(k), v) for (k,v) in bow2]\n",
    "print \"\\nA visual bag of words (word, count): \\n{}\".format(visual_bow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the Dictionary is more useful than creating a frequency tracker as we did in clean_corpus above.\n",
    "Therefore a better implementation (still lacking) would replace the frequency dictionary with a `corpora.dictionary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'dog.': 0, u'brown': 1, u'lazy': 2, u'things.': 3, u'jumped': 4, u'over': 5, u'modern': 6, u'fox': 7, u'all': 8, u'radio': 9, u'internet': 10, u'quick': 11, u'oldest.': 12, u'television,': 13}\n",
      "\n",
      "\n",
      "{0: u'dog.', 1: u'brown', 2: u'lazy', 3: u'things.', 4: u'jumped', 5: u'over', 6: u'modern', 7: u'fox', 8: u'all', 9: u'radio', 10: u'internet', 11: u'quick', 12: u'oldest.', 13: u'television,'}\n"
     ]
    }
   ],
   "source": [
    "# As an aside we can get the dictionary for token2id or id2token  by doing the respective commands\n",
    "print c_dict.token2id\n",
    "print \"\\n\"\n",
    "print c_dict.id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "One of the reasons people would use gensim is because it supports streaming documents in.\n",
    "For example, If we had a corpus with many documents (on the order of millions), it would be way too intensive, and maybe impossible, to load all the documents into memory to process our corpus of vectors. Instead it is much more feasable to stream documents in and process them in a queue-like fashion. This is a possible because Gensim uses a sparce matrix implementations whenever it can.\n",
    "\n",
    "To make this simple to see, let's define an iterable that will take in a text file that will have documents seperated by newline characters. (We will be using Alice in Wonderland sourced from [here](https://www.gutenberg.org/ebooks/11))\n",
    "\n",
    "We will be using the concepts of a dictionary as discussed above in our class. Additionally we will be using the handy `gensim.parsing.preprocessing` library to process our text a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim.parsing.preprocessing as preproc\n",
    "\n",
    "class StreamCorpus:\n",
    "    def __init__(self, filename):\n",
    "        \"\"\"\n",
    "           Takes in a path to a file, and a gensim.corpora.Dictionary\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        self.dictionary = self.populate_dictionary()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.filename) as fd:\n",
    "            paragraphs = self.iter_paragraphs(fd)\n",
    "            for paragraph in paragraphs:\n",
    "                tokens = self.process_text(paragraph).lower().split(' ')\n",
    "                yield self.dictionary.doc2bow(tokens)\n",
    "                \n",
    "    # Create entries in our dictionary so we can generate a bag-of-words vector later on\n",
    "    def populate_dictionary(self):\n",
    "        class_dict = gs.corpora.Dictionary()\n",
    "        with open(self.filename) as fd:\n",
    "            paragraphs = self.iter_paragraphs(fd)\n",
    "            for paragraph in paragraphs:\n",
    "                tokens = self.process_text(paragraph).lower().split(' ')\n",
    "                class_dict.add_documents([tokens])\n",
    "        return class_dict\n",
    "   \n",
    "    # Function to give paragraph\n",
    "    def iter_paragraphs(self, fileobj):\n",
    "        lines = []\n",
    "        for line in fileobj:\n",
    "            if (line == \"\\n\") and lines:\n",
    "                yield ' '.join(lines)\n",
    "                lines = []\n",
    "            else:\n",
    "                lines.append(line);\n",
    "        yield ' '.join(lines)\n",
    "\n",
    "    # Feel free to change how we process text for different results\n",
    "    def process_text(self, text):\n",
    "        # Remove stopwords using gensim lib\n",
    "        text = preproc.remove_stopwords(text)\n",
    "        # Strip punctuation\n",
    "        text = preproc.strip_punctuation(text)\n",
    "        # Get rid of multiple whitespaces\n",
    "        text = preproc.strip_multiple_whitespaces(text)\n",
    "        return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can now create our corpus with vector representation with a memory-friendly approach!\n",
    "streamed = StreamCorpus('alice_in_wonderland.txt')\n",
    "alice_corpus = [vector for vector in streamed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Models\n",
    "You may be wondering at this point, why use gensim?\n",
    "Well instead of exiting using two libraries to create a model, we can stay within the gensim environment!\n",
    "\n",
    "One simple model we can train is the text frequency-inverse document frequency model, tfidf for short.\n",
    "\n",
    "We will want to take our corpus and make a vector for each document. In the end we will have a list of vectors which we will pass onto the tfidf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember we have already defined our_dict with the corpus, our_corpus\n",
    "# We just want to create the bag of words lists (list of vectors), and not update the dictionary.\n",
    "bow_corpus = [our_dict.doc2bow(text) for text in tokens_list]\n",
    "\n",
    "# Then we can pass in this bag-of-words corpus into out tfidf model.\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can transform the phrase \"tedious aching\" by tokenizing it, creating the bag of words vector for \"tedious aching\", then passing it to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is a list of (token_id, tfidf weight): \n",
      "[(49, 0.7071067811865475), (62, 0.7071067811865475)]\n"
     ]
    }
   ],
   "source": [
    "tfidf_pred = tfidf[our_dict.doc2bow(\"tedious aching\".lower().split())]\n",
    "print \"Output is a list of (token_id, tfidf weight): \\n{}\".format(tfidf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=831, num_nnz=14395)\n"
     ]
    }
   ],
   "source": [
    "# We can also use our example from before for Alice in Wonderland\n",
    "alice_tfidf = models.TfidfModel(alice_corpus)\n",
    "print alice_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim also supports a wide variety of other models to use, many of which are more complex than TF-IDF.\n",
    "\n",
    "\n",
    "You can check them out [here](https://radimrehurek.com/gensim/apiref.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Similarity\n",
    "Gensim can also help us determine similarities between texts which is often a useful tool.\n",
    "Gensim uses Cosine Similarity.\n",
    "\n",
    "Suppose we wanted to compare the sentence:\n",
    "\n",
    "\"Tiny alice is a mouse in wonderland, running from the red queen. She has a friend called the Mad Hatter and likes cards\"\n",
    "\n",
    "\n",
    "to all of the documents we defined in Alice in Wonderland. We will be using the Latent Semantic Indexing model as an example of Gensims support for other models while showing how this Similarity is \"queried\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our Streamed corpus of Alice in Wonderland already has a dictionary, so we will just use that.\n",
    "work_dict = streamed.dictionary\n",
    "query_doc = \"Tiny alice is a mouse in wonderland, running from the red queen. She has a friend called the Mad Hatter and likes cards\"\n",
    "# Create our LSI model\n",
    "alice_lsi = models.LsiModel(alice_corpus, id2word = work_dict, num_topics = 250)\n",
    "# Process our text using the code we wrote before\n",
    "tokens = streamed.process_text(query_doc).lower().split()\n",
    "# Turn our query into a vector using our dictionary (b.o.w style)\n",
    "query_vector_bow = work_dict.doc2bow(tokens)\n",
    "# Now use our LSI model to change vector types (almost there!)\n",
    "query_vector_lsi = alice_lsi[query_vector_bow]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim uses a class called `gensim.similarities.docsim.MatrixSimilarity` to work out similarity between a collection of documents and a query document in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim.similarities as sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(535, 0.39417034), (55, 0.37248236), (708, 0.35933661), (447, 0.35756776), (438, 0.32280397)]\n"
     ]
    }
   ],
   "source": [
    "# set up the Simularity query-handler\n",
    "sim_calc = sim.MatrixSimilarity(alice_lsi[alice_corpus])\n",
    "# perform a similarity query against the entirer alice_corpus\n",
    "similarities = sim_calc[query_vector_lsi]\n",
    "similarities = sorted(enumerate(similarities), key = lambda item: -item[1])\n",
    "print similarities[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output is a list of the highest ranked (by similarity ranking) document numbers. It is a list of (Document number, cosine_similarity) tuples. Recall that Cosine_Similarity is between -1 and 1.\n",
    "\n",
    "If you want to see what documents these are you would have to pull up the corresponding document for our corpus (the non-vector one) since our vector corpus is just a bag of words vector. We have included a (wasteful) function to find the document below for our document-per-paragraph model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DocFinder:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    # Function to give paragraph\n",
    "    def iter_paragraphs(self, fileobj):\n",
    "        lines = []\n",
    "        for line in fileobj:\n",
    "            if (line == \"\\n\") and lines:\n",
    "                yield ' '.join(lines)\n",
    "                lines = []\n",
    "            else:\n",
    "                lines.append(line);\n",
    "        yield ' '.join(lines)\n",
    "    def get_doc(self, num):\n",
    "        with open(self.filename) as fd:\n",
    "            paragraphs = self.iter_paragraphs(fd)\n",
    "            i = 0\n",
    "            for paragraph in paragraphs:\n",
    "                if i == num:\n",
    "                    return paragraph\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‘Let’s go on with the game,’ the Queen said to Alice; and Alice was\n",
      " too much frightened to say a word, but slowly followed her back to the\n",
      " croquet-ground.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finder = DocFinder('alice_in_wonderland.txt')\n",
    "print finder.get_doc(535)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A use for this type of similiarity ranking is to see how similar a users query is to a collection of documents. We could then suggest certain documents according to the users query. This isn't as good as Google's PageRank for web searching, but may be passable when searching through collections of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
