{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Tutorial on Model Ensembles: Maximum Margin Output Coding (MMOC)\n",
    "\n",
    "Author: **Emilio Esposito ([LinkedIn](https://www.linkedin.com/in/emilioespositousa) | [Portfolio](https://eesposito.com/) | [GitHub](https://github.com/EmilioEsposito))**\n",
    "\n",
    "## Table of Contents\n",
    "* [Introduction](#introduction)<br/>\n",
    "* [Data Prep](#Data Prep)<br/>\n",
    "* [Binarize the target](#Binarize the target)<br/>\n",
    "* [Create N Binary Classifiers](#Create N Binary Classifiers)<br/>\n",
    "* [Plotting Individual ROC curves](#Plotting Individual ROC curves)<br/>\n",
    "* [Picking the winning class: Applying MMOC](#Picking the winning class: Applying MMOC)<br/>\n",
    "* [Measuring Multiclassification Error: Confusion Matrix](#Measuring Multiclassification Error: Confusion Matrix)<br/>\n",
    "* [Measuring Multiclassification Error: Accuracy, TPR, and FPR](#Measuring Multiclassification Error: Accuracy, TPR, and FPR)<br/>\n",
    "* [Conclusion](#Conclusion)<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## Introduction\n",
    "\n",
    "The objective of this tutorial is to introduce the concept of model ensembles, and show one specific type of model ensemble in detail. The type of model ensemble we will cover is called Maximum Margin Output Coding (MMOC). \n",
    "\n",
    "So what exactly is a model ensemble? **A model ensemble is the process of using more than one model instead of a single model for prediction; the outputs of several models are combined together using a specified voting scheme to achieve a single prediction for each record.** \n",
    "\n",
    "MMOC is an approach that is used for multi-classification (i.e. binary classification is just between two classes A and B, but multi-classification can handle N number of classes A,B,C,D, etc). It does this by using N number of binary classifiers, and then combining their results into a single prediction for each record.\n",
    "\n",
    "MMOC is similar to Error Correcting Output Coding (ECOC), but is simpler because it takes advantage of the probabilistic real-valued output values from margin-based classifiers (in other words, it uses the probability output number between 0.0 and 1.0 ). ECOC requires individual cutoff values to be chosen before the outputs can be combined, but MMOC simply uses the probabilistic output as a proxy for \"confidence\". The simplest version of MMOC simply chooses the final class by selecting the model with the highest confidence score (highest probability of being in the class). An extremely simple example is shown below, but we'll also implement this in practice later:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://esporealestate.com/MMOC.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a detailed research paper on MMOC, refer to this publication: \n",
    "* [Yi Zhang & Jeff Schneider. Carnegie Mellon University. Maximum Margin Output Coding, ICML 2012.](http://www.cs.cmu.edu/~yizhang1/docs/MaxMarginCoding.pdf)\n",
    "\n",
    "And for a easier to understand slide-deck on margin based encoding:\n",
    "* [Erin Allwein, Rob Schapire, & Yoram Singer. Princeton University. Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers](http://www.cs.princeton.edu/~schapire/talks/ecoc-icml10.pdf)\n",
    "\n",
    "There are other equally valid approaches to tackle multi-classification besides MMOC that do not even use an ensemble (such as a Bayes Net or Softmax Regression), but there are some scenarios where it makes more sense to use an ensemble of several models rather than a single model. For example, if the different classes are driven by different and muttually exclusive features, then an ensemble would be more appropriate. Here is classic example where MMOC would be preferred over a single multinomial classfier:\n",
    "* class A is best predicted by features x1 and x2, but is completely independent of other features\n",
    "* class B is best predicted by features x3 and x4, but is completely independent of other features\n",
    "* class C is best predicted by features x5 and x6, but is completely independent of other features\n",
    "\n",
    "Also, MMOC ensembling allows the individual N binary classifers to use different features and even different models. Class \"A\" might use logistic regression, but class \"B\" might use Support Vector Machines (just to illustrate, this scenario would actually be unusual).\n",
    "\n",
    "\n",
    "Now let's get to our example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data Prep'></a>\n",
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import necessary libaries \n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import svm\n",
    "from matplotlib import colors\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we are going to use the standard Iris dataset found in the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['target_names', 'data', 'target', 'DESCR', 'feature_names']\n"
     ]
    }
   ],
   "source": [
    "iris_dict = datasets.load_iris()\n",
    "print iris_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris data has has 3 possible target classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa', 'versicolor', 'virginica']\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "target_names = list(iris_dict['target_names'])\n",
    "feature_names = iris_dict['feature_names']\n",
    "print target_names\n",
    "print feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "128                6.4               2.8                5.6               2.1   \n",
       "18                 5.7               3.8                1.7               0.3   \n",
       "130                7.4               2.8                6.1               1.9   \n",
       "105                7.6               3.0                6.6               2.1   \n",
       "107                7.3               2.9                6.3               1.8   \n",
       "\n",
       "     target target_name  \n",
       "128       2   virginica  \n",
       "18        0      setosa  \n",
       "130       2   virginica  \n",
       "105       2   virginica  \n",
       "107       2   virginica  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the X variables into a dataframe\n",
    "irisX = pd.DataFrame(iris_dict['data'], columns=iris_dict['feature_names'])\n",
    "\n",
    "# let's make a copy so we can add the target variable\n",
    "iris = irisX.copy()\n",
    "\n",
    "# add the numeric target\n",
    "iris['target'] = iris_dict['target']\n",
    "\n",
    "# let's shuffle the rows so we get a more representative sample of targets when we preview the data using head()\n",
    "np.random.seed(seed=4)\n",
    "iris = iris.iloc[np.random.choice(len(iris), len(iris), replace=False, ),:]\n",
    "\n",
    "# create mapping of numeric target to target actual name\n",
    "tn_dict = {i:target_names[i] for i in range(len(target_names))}\n",
    "\n",
    "# add target name as well\n",
    "iris['target_name'] = iris['target'].map(tn_dict)\n",
    "\n",
    "# preview final data\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's show some descriptive statistics for each column. Note the uniform distribution of the target. This uniform distribution will allow us to simplify our approach to MMOC later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) [datatype=float64]\n",
      "\tMean: 5.84333333333\n",
      "\tMin: 4.3\n",
      "\tMax: 7.9\n",
      "\tStd Dev: 0.825301291785\n",
      "sepal width (cm) [datatype=float64]\n",
      "\tMean: 3.054\n",
      "\tMin: 2.0\n",
      "\tMax: 4.4\n",
      "\tStd Dev: 0.432146580071\n",
      "petal length (cm) [datatype=float64]\n",
      "\tMean: 3.75866666667\n",
      "\tMin: 1.0\n",
      "\tMax: 6.9\n",
      "\tStd Dev: 1.75852918341\n",
      "petal width (cm) [datatype=float64]\n",
      "\tMean: 1.19866666667\n",
      "\tMin: 0.1\n",
      "\tMax: 2.5\n",
      "\tStd Dev: 0.760612618588\n",
      "target [datatype=int32]\n",
      "\tMean: 1.0\n",
      "\tMin: 0\n",
      "\tMax: 2\n",
      "\tStd Dev: 0.816496610641\n",
      "target_name [datatype=object]\n",
      "\tCounts:\n",
      "\tvirginica     50\n",
      "\tsetosa        50\n",
      "\tversicolor    50\n",
      "\tName: target_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prints descriptive stats on the data.\n",
    "# For numerics, it prints mean, min, max, and standard deviation\n",
    "# For categorical, it prints the count distribution\n",
    "def desc_statistics(df):\n",
    "    for col in df:\n",
    "        x = df[col]\n",
    "\n",
    "        # print name & datatype\n",
    "        print col, \"[datatype=\"+str(x.dtype)+\"]\"\n",
    "\n",
    "        # handle categorical \n",
    "        if x.dtype=='object':\n",
    "            print \"\\tCounts:\"\n",
    "            print \"\\t\",str(x.value_counts()).replace(\"\\n\",\"\\n\\t\")\n",
    "        # handle numeric\n",
    "        else:\n",
    "            print \"\\tMean:\",str(np.mean(x))\n",
    "            print \"\\tMin:\",str(np.min(x))\n",
    "            print \"\\tMax:\",str(np.max(x))\n",
    "            print \"\\tStd Dev:\",str(np.std(x))       \n",
    "\n",
    "desc_statistics(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Binarize the target'></a>\n",
    "## Binarize the target\n",
    "You may have thought we were ready to begin modeling, but remember since we have multiple classes (3 in this case), we need to create 3 binary variables to represent our targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "128                6.4               2.8                5.6               2.1   \n",
       "18                 5.7               3.8                1.7               0.3   \n",
       "130                7.4               2.8                6.1               1.9   \n",
       "105                7.6               3.0                6.6               2.1   \n",
       "107                7.3               2.9                6.3               1.8   \n",
       "\n",
       "     target target_name  setosa  versicolor  virginica  \n",
       "128       2   virginica     0.0         0.0        1.0  \n",
       "18        0      setosa     1.0         0.0        0.0  \n",
       "130       2   virginica     0.0         0.0        1.0  \n",
       "105       2   virginica     0.0         0.0        1.0  \n",
       "107       2   virginica     0.0         0.0        1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binarize the target\n",
    "dummies = pd.get_dummies(iris['target_name'])\n",
    "\n",
    "# add/update binarized cols to df\n",
    "for predcol in dummies.columns:\n",
    "    iris[predcol] = dummies[predcol]\n",
    "    \n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Create N Binary Classifiers'></a>\n",
    "## Create N Binary Classifiers\n",
    "Now it's time to build our binary models. Each model should be trained and cross-validated separately. We could use different sets of features for each class and even use different types of models. However, the purpose of this tutorial is not to teach feature selection, model selection, and cross-validation, so I won't explain the details here. I just want to show how to combine output from several models. For more background information on classification and cross-validation, please refer to [these lecture notes](http://www.datasciencecourse.org/nonlinear_modeling.pdf) from Carnegie Mellon University's [Practical Data Science](http://www.datasciencecourse.org/) course.\n",
    "\n",
    "\n",
    "Now let's create our 3 binary classifiers using SVM models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop through each target class\n",
    "for target_name in target_names:\n",
    "    \n",
    "    # split data into X and y\n",
    "    X = iris[feature_names]\n",
    "    y = iris[target_name]\n",
    "    \n",
    "    k = len(X)\n",
    "    kf = KFold(len(X), n_folds=k, shuffle=True, random_state=5)\n",
    "    #loop through each fold\n",
    "    for train_index, hold_index in kf:\n",
    "        \n",
    "        # split into train and hold\n",
    "        trainx = X.iloc[train_index,:]\n",
    "        trainy = y[train_index]\n",
    "        holdx = X.iloc[hold_index,:]\n",
    "        \n",
    "        # build SVM \n",
    "        model = svm.SVC(probability=True, C=1,gamma=.05,random_state=5)    \n",
    "    \n",
    "        # fit the model using train and return probabilistic predictions of holdout\n",
    "        fit = model.fit(trainx, trainy)\n",
    "        prob = fit.predict_proba(holdx)\n",
    "    \n",
    "        # add the probabilist output to the original dataframe\n",
    "        iris.loc[hold_index,'probclass_'+target_name] = prob[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see below, we added a probabilistic output for each classifer (the columns with the \"probclass_\" prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "      <th>probclass_setosa</th>\n",
       "      <th>probclass_versicolor</th>\n",
       "      <th>probclass_virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.635576</td>\n",
       "      <td>0.699851</td>\n",
       "      <td>0.826809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655379</td>\n",
       "      <td>0.642097</td>\n",
       "      <td>0.664245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.658594</td>\n",
       "      <td>0.661425</td>\n",
       "      <td>0.655054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.653835</td>\n",
       "      <td>0.650305</td>\n",
       "      <td>0.657817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.653676</td>\n",
       "      <td>0.662541</td>\n",
       "      <td>0.642726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "128                6.4               2.8                5.6               2.1   \n",
       "18                 5.7               3.8                1.7               0.3   \n",
       "130                7.4               2.8                6.1               1.9   \n",
       "105                7.6               3.0                6.6               2.1   \n",
       "107                7.3               2.9                6.3               1.8   \n",
       "\n",
       "     target target_name  setosa  versicolor  virginica  probclass_setosa  \\\n",
       "128       2   virginica     0.0         0.0        1.0          0.635576   \n",
       "18        0      setosa     1.0         0.0        0.0          0.655379   \n",
       "130       2   virginica     0.0         0.0        1.0          0.658594   \n",
       "105       2   virginica     0.0         0.0        1.0          0.653835   \n",
       "107       2   virginica     0.0         0.0        1.0          0.653676   \n",
       "\n",
       "     probclass_versicolor  probclass_virginica  \n",
       "128              0.699851             0.826809  \n",
       "18               0.642097             0.664245  \n",
       "130              0.661425             0.655054  \n",
       "105              0.650305             0.657817  \n",
       "107              0.662541             0.642726  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Plotting Individual ROC curves'></a>\n",
    "## Plotting Individual ROC curves\n",
    "Even though this is a multinomial classification problem, we still want to check the validity of the individual binary classifiers before we combine results. However, even if all of our binary classifiers have good AUCs, this tells us NOTHING about the error of our final output. In the next section we'll combine the binary classifiers and then measure the overall error using a confusion matrix. But for now, let's take a look at how our binary classifiers did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAGHCAYAAACJeOnXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+VCkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xba04d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take a dataframe and plot all of the binary classifier curves on a single figure\n",
    "def plotROC(df,pred_col_prefix):\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "    # get a list of the prediction columns\n",
    "    predcols = [col for col in df.columns if col[:10]==pred_col_prefix]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    mycolors = ['blue','red','green'] +list(colors.cnames)\n",
    "    \n",
    "    for i, (target_name, predcol) in enumerate(zip(target_names,predcols)):\n",
    "        \n",
    "        truth = df[target_name]\n",
    "        pred = df[predcol]\n",
    "\n",
    "        # calclulate fpr/tpr metrics\n",
    "        fpr, tpr, _ = metrics.roc_curve(truth, pred)\n",
    "\n",
    "        # calc the AUC\n",
    "        auc = metrics.roc_auc_score(truth, pred)\n",
    "\n",
    "        # set labels\n",
    "        plt.ylabel(\"TPR\")\n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.title(\"ROC Curve (using k-fold Cross Validation)\")\n",
    "\n",
    "        # plot the default model line\n",
    "        plt.plot([0,1])\n",
    "        \n",
    "        # plot tpr/fpr on ROC\n",
    "        leg_label = target_name + \", AUC=\" + str(round(auc, 2))\n",
    "        ax.plot(fpr, tpr, color=mycolors[i], label=leg_label)\n",
    "\n",
    "        # Begin citation for legend: http://matplotlib.org/1.3.0/examples/pylab_examples/legend_demo.html\n",
    "        # Now add the legend with some customizations.\n",
    "        legend = ax.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "plotROC(iris,\"probclass_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll discuss detailed interpretation of TPR and FPR later. For now, just recognize that a random choice model would produce the diagonal line. A perfect model (0 error) would be a line that goes straight up from (0,0) to (0,1), then takes a 90 degree turn right to (1,1). Any curve above the diagonal is doing better than random chance, and the closer it bulges towards (0,1) the better. AUC measures the area under the curve. AUC=1 indicates a perfect model, and AUC=0.50 indicates a random choice model (the diagonal line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Picking the winning class: Applying MMOC'></a>\n",
    "## Picking the winning class: Applying MMOC\n",
    "Now that we validated our individual binary classifiers, it's time to combine their results into a single final prediction for each record. \n",
    "\n",
    "As we eluded to earlier, we will do this by using the simplest approach of MMOC: pick the classifier that has the highest confidence (in this case, we are simply using the real-valued probability as the confidence). This approach is only valid if the distribution of our classes is relatively uniform. If the distribution was not uniform, we would either need another measure of confidence taking the distribution into account, or we would need to apply cutoff values and employ ECOC instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "      <th>probclass_setosa</th>\n",
       "      <th>probclass_versicolor</th>\n",
       "      <th>probclass_virginica</th>\n",
       "      <th>pred_target_name</th>\n",
       "      <th>predclass_setosa</th>\n",
       "      <th>predclass_versicolor</th>\n",
       "      <th>predclass_virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.635576</td>\n",
       "      <td>0.699851</td>\n",
       "      <td>0.826809</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655379</td>\n",
       "      <td>0.642097</td>\n",
       "      <td>0.664245</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.658594</td>\n",
       "      <td>0.661425</td>\n",
       "      <td>0.655054</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.653835</td>\n",
       "      <td>0.650305</td>\n",
       "      <td>0.657817</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.653676</td>\n",
       "      <td>0.662541</td>\n",
       "      <td>0.642726</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "128                6.4               2.8                5.6               2.1   \n",
       "18                 5.7               3.8                1.7               0.3   \n",
       "130                7.4               2.8                6.1               1.9   \n",
       "105                7.6               3.0                6.6               2.1   \n",
       "107                7.3               2.9                6.3               1.8   \n",
       "\n",
       "     target target_name  setosa  versicolor  virginica  probclass_setosa  \\\n",
       "128       2   virginica     0.0         0.0        1.0          0.635576   \n",
       "18        0      setosa     1.0         0.0        0.0          0.655379   \n",
       "130       2   virginica     0.0         0.0        1.0          0.658594   \n",
       "105       2   virginica     0.0         0.0        1.0          0.653835   \n",
       "107       2   virginica     0.0         0.0        1.0          0.653676   \n",
       "\n",
       "     probclass_versicolor  probclass_virginica pred_target_name  \\\n",
       "128              0.699851             0.826809        virginica   \n",
       "18               0.642097             0.664245        virginica   \n",
       "130              0.661425             0.655054       versicolor   \n",
       "105              0.650305             0.657817        virginica   \n",
       "107              0.662541             0.642726       versicolor   \n",
       "\n",
       "     predclass_setosa  predclass_versicolor  predclass_virginica  \n",
       "128               0.0                   0.0                  1.0  \n",
       "18                0.0                   0.0                  1.0  \n",
       "130               0.0                   1.0                  0.0  \n",
       "105               0.0                   0.0                  1.0  \n",
       "107               0.0                   1.0                  0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this function takes a dataframe, looks for columns that have probabilistic output (denoted by prefix probclass_)\n",
    "# it then picks the winning classifier based on which one had the highest confidence\n",
    "# it records the winner in both a pred_target_name column, and also in binarized predclass_ columns\n",
    "def applyMMOC(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # get a list of the prediction columns\n",
    "    predcols = [col for col in df.columns if col[:10]=='probclass_']\n",
    "    \n",
    "    # choose the winning classifier by picking the one with the highest probability/confidence\n",
    "    df['pred_target_name'] = df[predcols].idxmax(axis=1)\n",
    "    \n",
    "    # trim the predicted target name\n",
    "    df['pred_target_name'] = df['pred_target_name'].apply(lambda x: x.replace('probclass_',\"\"))\n",
    "    \n",
    "    # get dummies of pred as well, we can use them later to easily calc error metrics\n",
    "    pred_dummies = pd.get_dummies(df['pred_target_name'])\n",
    "    pred_dummies.columns = [\"predclass_\"+str(col) for col in pred_dummies.columns]\n",
    "    \n",
    "    # add the dummies to the df\n",
    "    for predcol in pred_dummies.columns:\n",
    "        df[predcol]=pred_dummies[predcol]\n",
    "\n",
    "    return df\n",
    "\n",
    "# preview final data with predictions\n",
    "iris = applyMMOC(iris)\n",
    "iris.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Measuring Multiclassification Error: Confusion Matrix'></a>\n",
    "## Measuring Multiclassification Error: Confusion Matrix\n",
    "Now that we have combined the output of our 3 binary models, we can now measure our error using a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred_target_name</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>4.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred_target_name  setosa  versicolor  virginica    All\n",
       "target_name                                           \n",
       "setosa              36.0         2.0       12.0   50.0\n",
       "versicolor           4.0        44.0        2.0   50.0\n",
       "virginica            2.0        21.0       27.0   50.0\n",
       "All                 42.0        67.0       41.0  150.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This funtion returns a nice df confusion matrix. Sklearn.metrics has it's own function but this is nicer.\n",
    "def getConfusionMatrix(df, y_truth, y_pred):\n",
    "    df = df.copy()\n",
    "    \n",
    "    labels = set(df[y_truth]).union(set(df[y_pred]))\n",
    "    \n",
    "    # create column of ones\n",
    "    df['ones'] = np.ones(len(df))\n",
    "    \n",
    "    # create pivoted df that shows truth vs predicted value counts\n",
    "    confmat = pd.pivot_table(df, values = 'ones', index=[y_truth], columns=[y_pred], aggfunc=np.sum, margins=True)\n",
    "    \n",
    "    return confmat\n",
    "\n",
    "confmat = getConfusionMatrix(iris,'target_name','pred_target_name')\n",
    "confmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix rows contain the true label counts, and the columns show the predicted label counts. A perfect model would result in a diagonal matrix (non-zero counts along the diagonal, with zero everywhere else). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Measuring Multiclassification Error: Accuracy, TPR, and FPR'></a>\n",
    "## Measuring Multiclassification Error: Accuracy, TPR, and FPR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Accuracy \n",
    "You can think of overall accuracy as the sum of the values along the diagonal in the confusion matrix, divided by the total record count:\n",
    "\n",
    "$$ Accuracy = \\frac{\\#\\ correct\\ predictions}{N} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71333333333333337"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sk.metrics.accuracy_score(iris['target_name'],iris['pred_target_name'])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate & False Positive Rate\n",
    "Now we will calculate the True Positive Rate (TPR) and the False Postive Rate (FPR) for each class. If you look at the ROC curves earlier, you'll notice these are the 2 metrics on each axis. A TPR measures how many times a model correctly classifies a record as being part of a given class. A FPR measures how many times a model classifies a record as being a member of the given class, when in reality it is NOT a member of that class. A perfect model would have a TPR of 1.00 (100%) and a FPR of 0.00 (0%). In practice, there is usually a tradeoff between these 2 metrics. They can also be expressed as the equations below:\n",
    "\n",
    "$$ TPR_{ClassA} = \\frac{\\#\\ Records\\ Model\\ Classified\\ Correctly\\ as\\ Class\\ A}{\\#\\ of\\ Class\\ A\\ Records} = \\frac{\\#\\ True\\ Positives}{\\#\\ Positives} $$\n",
    "\n",
    "$$ FPR_{ClassA} = \\frac{\\#\\ Records\\ Model\\ Classified\\ INCORRECTLY\\ as\\ Class\\ A}{\\#\\ of\\ NOT\\ Class\\ A\\ Records} = \\frac{\\#\\ False\\ Positives}{\\#\\ Negatives} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             FPR   TPR\n",
       "setosa      0.06  0.72\n",
       "versicolor  0.23  0.88\n",
       "virginica   0.14  0.54"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given a confusion matrix df, return a df of the TPR and FPR of each class\n",
    "def getTPRFPR(confusion_matrix_df):\n",
    "    \n",
    "    df = confusion_matrix_df.copy()\n",
    "    \n",
    "    # get target_names and remove \"All\"\n",
    "    target_names = list(df.index)\n",
    "    target_names.remove(\"All\")\n",
    "    \n",
    "    # take a subset of the confusion matrix, removing \"All\" rows/cols\n",
    "    df = df.loc[target_names,target_names]\n",
    "    \n",
    "    # create a dict to fill with other dictionaries of metrics for each label\n",
    "    label_metrics = {}\n",
    "\n",
    "    for label in target_names:\n",
    "        # add sub dictionary for metric\n",
    "        label_metrics[label] = {}\n",
    "\n",
    "        label_metrics[label][\"FPR\"] = sum(df.loc[((df.index!=label) ),label])/(sum(sum(df.values)) - sum(df.loc[label,target_names]))\n",
    "        label_metrics[label][\"TPR\"] = df.loc[label,label]/sum(df.loc[label,target_names])\n",
    "        \n",
    "    # make it into a nice dataframe for display\n",
    "    tprfpr_df = pd.DataFrame(label_metrics).transpose()\n",
    "    return tprfpr_df\n",
    "\n",
    "getTPRFPR(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not bad! Looks like our TPRs are relatively high yet our FPRs are relatively low.\n",
    "\n",
    "<a id='Conclusion'></a>\n",
    "## Conclusion\n",
    "Although this Iris dataset is not necessarily the ideal usage of a model ensemble using MMOC, my objective was to convey the methodology as simply as possible. This model ensemble framework can be greatly extended with other several other important data science techniques that were purposely glossed over in this tutorial (visualization, feature selection, cross validation, etc). \n",
    "\n",
    "For additional background on other aspects of data science, a terrific resource is the website for [Carnegie Mellon University's 15-688 Practical Data Science Course](http://www.datasciencecourse.org/). Hope you enjoyed my tutorial!\n",
    "\n",
    "Written by:\n",
    "\n",
    "**Emilio Esposito ([LinkedIn](https://www.linkedin.com/in/emilioespositousa) | [Portfolio](https://eesposito.com/) | [GitHub](https://github.com/EmilioEsposito))**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
