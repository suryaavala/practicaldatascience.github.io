{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist_loader\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Networks**\n",
    "\n",
    "In this tutorial, we will cover how to create a neural network that is capable of recognizing handwritten digits. We will be using Michael Nielson's neural network library to help us load our image dataset, as well as implementing the basics of its backpropagation algorithm. In addition, we will test our algorithm on the MNIST dataset, which is widely considered the gold standard in terms of datasets. \n",
    "\n",
    "\n",
    "The human brain is undoubtedly the single greatest, most complex computer in the known universe. Naturally, one would want to design algorithms that work more like human brains. Human brains are able to learn without being explicitly told a rigid set of mechanized instructions, and rapidly improve from experience. In many areas, humans easily outmatch computers in seemingly simple things, including image recognition, speech comprehension, and language processing. The concentrated efforts at creating computer programs that exhibited the human property of “learning” gave rise to what is now known as a neural network.\n",
    "\n",
    "Neural networks are motivated by their simplest counterpart, which are perceptrons. Perceptrons take multiple inputs x1, … xn, and produce a single binary output. Perceptions have both inputs, and weights. Perceptron’s will only fire if the sum of their inputs multiplied with their respective weights is greater than some threshold value. Perceptrons can model a multitude of things, including something like “is the weather good?”.\n",
    "\n",
    "The first question we will ask, is how will we choose a good function that determines the threshold? For the sake of simplicity, we will assume that our inputs are real numbers. The simplest answer to our question is that we can simply return our input as our output, i.e., the identity function. In this case, the neuron only fires if the input itself is greater than some threshold. \n",
    "\n",
    "![Image of neuron](http://www.sciencehq.com/image/Simple_graph_of_identity_function.JPG)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "identity(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, an immediate problem we run into with perceptrons is the fact that a small change in the input to a perceptron can lead to a large change in the output. We don’t want that, so we will devise a slightly better algorithm, called the sigmoid neuron, which is a modification of the perceptron algorithm, such that small changes to its input and its weight do not cause large changes in its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to note about the sigmoid function isn’t necessarily its algebraic form, but it’s shape. We notice that the sigmoid neuron starts off looking like the identity function, but tails off as the X-axis gets arbitrarily large or small . This means that small changes to its output don’t cause huge changes in its output, which is what we want: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of neuron](http://www.saedsayad.com/images/ANN_Sigmoid.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like how the human brain is composed of not one, but multiple neurons, our neural network should contain multiple neurons, as well as multiple layers, like so.\n",
    "![Image of neuron](http://neuralnetworksanddeeplearning.com/images/tikz10.png)\n",
    "\n",
    "Each layer will only fire if the sum of its previous neurons exceeds some threshold, so that the final output will be the forward-propagation of all the different neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then create a network class in the likeness of the above graph, which describes not just one but multiple sigmoid layers. In this code, sizes contains the number of neurons per layer, num_layers obviously contains the number of layers in our network, the biases are simply syntactic sugar for the negative of our threshold, and our weights correspond to the weights. \n",
    "\n",
    "What we want our network to truly do is to learn. We’ll do that by implementing a function called stochastic gradient descent, given below. \n",
    "\n",
    "The training data is a list of tuples, which represent the inputs desired outputs (the labeled data). In each epoch, the algorithm randomly shuffles the training data using numpy’s arrays, populating them with a series of random numbers that form a guassian distribution with mean 0 and standard deviation one, and then partitions it into mini-batches. For each mini-batch, we apply one step of gradient descent. The code for update mini batch is below. This function invokes something called the backpropogation algorithm, which relies on a heavy amount of calculus which will be beyond the scope of this tutuorial. \n",
    "\n",
    "In summary, our network takes a tuple of inputs, uses a layer of sigmoid neurons to return an output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(x, 1) for x in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add a feedforward algorithm to our network class, which applies our sigmoid function to each layer of our neural network. This feedforward applies the dot product of the sigmoid function with the weights, the threshold, and the biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "net = Network([3,2,3])\n",
    "print net.num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our _net_ object initializes a network, with 2 neurons in the first layer, 3 neurons in the second layer, and one in the third. As you can see by the use of np.random, we populate our biases and weights with random numpy floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045176659730911999"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "sigmoid_prime(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward(self, a):\n",
    "    \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        a = sigmoid(np.dot(w, a)+b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what our feedforward algorithm does is essentially return the output of every single neuron, starting from the leftmot layer until the rightmost layer, given an input. In this sense, it applies our sigmoid function to each layer.\n",
    "\n",
    "\n",
    "In order for our neural network to approximate a function, we will need an algorithm which lets us find the correct weights and biases that allow us to approximate our outputs for all inputs. Thus, we define a loss function, \n",
    "\n",
    "$$ C(w,b)  =  \\frac{1}{2n} \\sum_{x}^{n} ||y(x)-a||^2 $$\n",
    "\n",
    "\n",
    "w is the weights in the network, b is the biases, n is the number of training inputs, a is the vector of outputs. C is called the quadratic cost function because it is squared. We will see that the quadratic cost function is completely arbitrary: we can choose a different one if we’d like, but the quadratic is relatively nicer to differentiate. We note that we want to minimize this cost function: the smaller this cost is, the closer our weights and biases are to returning the correct output given an input. This is where gradient descent comes in. This topic was explained in lecture, so no further explanation will be given, other than the fact that gradient descent steps in the opposite direction of the gradient, over and over again until a local minimum is reached. \n",
    "\n",
    "We’ll do that by implementing a function called stochastic gradient descent, given below. Stochastic gradient descent is essentially the same as gradient descent that we learned in lecture, except for that fact that it only computes the gradient of our cost function for a small number of randomly chosen inputs. This allows us to get a relatively precise approximation of the true gradient, except for the fact that it is much much faster to calculate. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def update_mini_batch(self, mini_batch, alpha):\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(alpha/len(mini_batch))*nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(alpha/len(mini_batch))*nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update_mni_batch updates the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The \"mini_batch\" is a list of tuples \"(x, y)\", and \"alpha\" is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the full code for our simple neural network can be found below. Credit to michael nielson for the backpropogation algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes) \n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "    # stochastic gradient descent\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropogation** \n",
    "\n",
    "In this algorithm, we have omitted the details of backpropagation function, until now. While the true mathematical details of backpropogation are beyond the scope of this tutorial, at its core, the backpropogation algorithm is simply an extremely fast algorithm for computing gradients. This algorithm is divided into two main phases: \n",
    "1.\tOur first stage is propagation.\n",
    "In this phase, we propagate our network’s input throughout the neural network in order to generate the output activations. We then propagate our outputs backward through the neural network, which lets us generate the deltas between our expected and actual values. \n",
    "2.\tOur second step is to update the weights. \n",
    "In this stage, we multiply each weight connection’s output delta and input activation to get the gradient. We then subtract our learning rate from the gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**testing on the mnist dataset**\n",
    "\n",
    "Having implemented a simple neural network class, our next task will be to test our algorithm on the MNIST dataset. You can download it [Here](https://drive.google.com/file/d/0Bx25VRozHOqEcl95MVZ1cmY2T1k/view?usp=sharing), and put it in a file named \"data\", in the top level directory. Then we run the library which loads the MNIST dataset into our computer in a parsable format. (credit to Michael Nielson for supplying it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the MNIST dataset contains roughly ten thousand 28x28 images, we will create a network with 28x28 = 784 input neurons, 30 hidden layers, and 10 output layers (one corresponding to each digit 0-9). We will then learn the weights and the biases using stochastic gradient descent, with 30 iterations, a batch size of 10, and a learning rate of 3.0 Note that these hyperperameters can be fine-tuned with cross-validation which was explained in lecture, but we will not do so in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 7309 / 10000\n",
      "Epoch 1: 8399 / 10000\n",
      "Epoch 2: 8423 / 10000\n",
      "Epoch 3: 8498 / 10000\n",
      "Epoch 4: 8544 / 10000\n",
      "Epoch 5: 8535 / 10000\n",
      "Epoch 6: 8596 / 10000\n",
      "Epoch 7: 8583 / 10000\n",
      "Epoch 8: 8594 / 10000\n",
      "Epoch 9: 9464 / 10000\n",
      "Epoch 10: 9468 / 10000\n",
      "Epoch 11: 9506 / 10000\n",
      "Epoch 12: 9504 / 10000\n",
      "Epoch 13: 9489 / 10000\n",
      "Epoch 14: 9509 / 10000\n",
      "Epoch 15: 9521 / 10000\n",
      "Epoch 16: 9518 / 10000\n",
      "Epoch 17: 9512 / 10000\n",
      "Epoch 18: 9517 / 10000\n",
      "Epoch 19: 9529 / 10000\n",
      "Epoch 20: 9543 / 10000\n",
      "Epoch 21: 9538 / 10000\n",
      "Epoch 22: 9519 / 10000\n",
      "Epoch 23: 9513 / 10000\n",
      "Epoch 24: 9534 / 10000\n",
      "Epoch 25: 9531 / 10000\n",
      "Epoch 26: 9551 / 10000\n",
      "Epoch 27: 9528 / 10000\n",
      "Epoch 28: 9536 / 10000\n",
      "Epoch 29: 9538 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 30, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after just 30 iterations, our neural network achieves an accuracy rate of 95.3%! This is amazing, considering the full code is less than 100 lines long, and has not been optimized. \n",
    "\n",
    "**Summary and references for further reading** \n",
    "\n",
    "\n",
    "Those who are interested in learning more about the mathematics and detailed workings of neural networks should check out the draft version of “Deep Learning”, by Ian Goodfellow, et. All. \n",
    "\n",
    "\n",
    "Neural networks: http://www.deeplearningbook.org/\n",
    "\n",
    "Deep learning: http://neuralnetworksanddeeplearning.com/\n",
    "\n",
    "Neural nets and topology: https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
