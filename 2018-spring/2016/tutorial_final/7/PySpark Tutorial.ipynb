{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "This tutorial will introduce you to PySpark, a Spark python API, used for various analysis tasks with Apache Spark. It is a flexible tool for exploratory big data analysis because it integrates with the rest of the Python data analysis ecosystem, including pandas (DataFrames), NumPy (arrays), and Matplotlib (visualization). \n",
    "\n",
    "Spark is a fast and general purpose distributed computing framework (like MapReduce) that provides efficient in-memory computations for large scale data processing. Spark extends the MapReduce model to support more types of computations using a functional programming paradigm. Spark itself is written in Scala and offers support for three programming languages - Scala, Python and Java. \n",
    "\n",
    "PySpark is built on top of Spark's Java API. The overview of data flow in a Spark Application :\n",
    "\n",
    "\n",
    "[<img src=\"http://i.imgur.com/YlI8AqEl.png\" style=\"width: 400px;\">](http://i.imgur.com/YlI8AqEl.png)\n",
    "\n",
    "SparkContext object is created by the driver program at the start of Spark shell, which manages the cluster connections and coordinates the running processes. In the Python driver program, SparkContext uses Py4J to launch a JVM and create a JavaSparkContext. Py4J is only used on the driver for local communication between the Python and Java SparkContext objects. Data is processed in Python and cached/shuffled in the JVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "In this tutorial, we will get started by setting up Spark on our local system and then explore some basics of Spark programming i.e. RDDs, Transformations and Actions, lazy evaluation etc. We will also look into Spark SQL library, Spark Dataframes, and Machine Learning library (MLlib) with the help of examples.\n",
    "\n",
    "The following topics will be covered in the tutorial:\n",
    "    \n",
    "- [Installation and Configuration](#Installation-and-Configuration)\n",
    "- [Resilient Distributed Datasets (RDDs)](#Resilient-Distributed-Datasets)\n",
    "- [RDD Operations](#RDD-Operations)\n",
    " - [Creating RDDs](#Creating-RDDs)\n",
    " - [RDD Actions](#RDD-Actions)\n",
    " - [RDD Transformations](#RDD-Transformations)\n",
    "- [Using Spark SQL and DataFrames](#Using-Spark-SQL-and-DataFrames)\n",
    "- [Using Spark MLLib: Decision Trees](#Using-Spark-MLlib:-Decision-Trees)\n",
    "\n",
    "<br>\n",
    "Data Sets used for illustrations : \n",
    "- [UCI Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#)\n",
    "- [UCI Wine Quality Data Set](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Configuration\n",
    "\n",
    "For this tutorial, we will use the most recent and stable Spark release available i.e. 2.0.1 (at the time of this writing) in standalone mode configuration. This tutorial has been written using Ubuntu 16.04. Please note that some of the configuration commands might vary based on the user's operating system.\n",
    "\n",
    "Software Prequisites:\n",
    "- Apache Spark ([Download link](http://d3kbcqa49mib13.cloudfront.net/spark-2.0.1-bin-hadoop2.6.tgz)) \n",
    "- Python (v2.6+) \n",
    "- Java (v7.0+)\n",
    "\n",
    "<i>Note: Please enure to include java and python in your environment PATH and also set JAVA_HOME environment variable.</i>\n",
    "\n",
    "Unzip the downloaded Spark file and based on your extracted directory location, set SPARK_HOME environment variable using below command:\n",
    "\n",
    "    $ export SPARK_HOME = ~/spark-2.0.1-bin-hadoop2.6\n",
    "    \n",
    "    $ export PATH=$SPARK_HOME/bin:$PATH\n",
    "\n",
    "This completes our Spark installation and is ready to use on your local machine in \"standalone mode\". Spark comes with an interactive python shell which can be launched using below command:\n",
    "\n",
    "    $ $SPARK_HOME/bin/pyspark\n",
    "\n",
    "<img src=\"spark.png\">\n",
    "\n",
    "In order to interact with Spark from Jupyter notebook, execute the below command: \n",
    "\n",
    "    $ PYSPARK_DRIVER_PYTHON=jupyter $SPARK_HOME/bin/pyspark\n",
    "    \n",
    "<br>\n",
    "Refer the Spark documentation [here](http://spark.apache.org/docs/latest/), in case of any issues with the download link or launching pyspark. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets\n",
    "\n",
    "RDDs are the core data structure in Spark. RDDs are essentially a read-only and fault tolerant collection of objects that are partitioned across machines and can be operated in parallel. In local configuration, Spark simulates distributing the calculations over lots of machines by slicing computer's memory into partitions. RDDs can be created from and written to local file system, distributed storages like HDFS or S3, and other data sources. RDDs can be cached in memory which makes Spark very effective at iterative applications where the data is being reused throughout the course of an algorithm. \n",
    "\n",
    "RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. \n",
    "\n",
    "Spark applications are essentially the manipulation of RDDs through transformations and actions. \n",
    "\n",
    "\n",
    "<img src=\"http://blog.appliedinformaticsinc.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-03-at-3.34.57-PM.png\">\n",
    "\n",
    "An important thing to know is that all the transformations in Spark are lazy, i.e. they do not compute their results right away. Instead all the transformations on RDDs are stored in the memory and are executed all at once, when an action is called. This design enables Spark to run more efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext is created by the driver program in our case, Pyspark. It is usually referenced as the variable <b>sc</b>. We will use this SparkContext object to instantiate RDDs in the following sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME = /home/agoyal3/apps/spark-2.0.1\n",
      "SparkContext Object = <pyspark.context.SparkContext object at 0x7fcabdc5ef50>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#get the spark_home to check used spark version in case multiple versions are installed.\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "print \"SPARK_HOME = \" + spark_home\n",
    "\n",
    "#checks if the sparkContext object is available or not.\n",
    "print \"SparkContext Object = \" + str(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Operations\n",
    "\n",
    "Now, we will go through some most common RDD operations to under them better. To get complete list of available RDD operations, refer this [link](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)\n",
    "\n",
    "#### Creating RDDs \n",
    "\n",
    "From existing collection\n",
    "\n",
    "- <b>parallelize()</b> - Creates parallelized collections are created from an existing collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating RDD from python list. \n",
    "listData = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "rddList = sc.parallelize(listData)\n",
    "\n",
    "#creating RDD from python tuples. \n",
    "tupleData = [('a',1), ('b', 2), ('c', 3), ('a',4), ('c',3), ('d',2), ('a',1)]\n",
    "\n",
    "rddTuple = sc.parallelize(tupleData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From file\n",
    "\n",
    "- <b>textfile()</b> - Creates a RDD from a file on local or remote system\n",
    "\n",
    "In this example, we will use \"yelp_labelled.txt\" file obtained from Sentimental labelled sentences dataset with 1000 instances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting the file from URL and then extracting it\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "f = urllib.urlretrieve (\"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\", \"sentiment_labelled_sentences.zip\")\n",
    "\n",
    "dataset = zipfile.ZipFile('sentiment_labelled_sentences.zip')\n",
    "\n",
    "dataset.extractall()\n",
    "\n",
    "data_file = \"./sentiment labelled sentences/yelp_labelled.txt\"\n",
    "\n",
    "#creates a RDD from file\n",
    "rddFile = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD Actions\n",
    "\n",
    "Now, we will apply some actions on the 3 RDDs (rddList, rddTuple, rddFile) created above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>collect()</b> - Displays all the elements in RDD</i>             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddList.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>take(n)</b> - Return the first n lines from the dataset.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddList.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>first()</b> - Return the first element of the dataset (similar to take(1)).</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddList.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>reduce()</b> - Aggregate the elements of the dataset using a function func (which takes two arguments and returns one).</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sums up all the elements in the RDD\n",
    "rddList.reduce(lambda t1, t2: t1+t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>groupByKey()</b> - groups all the tuples in RDD by key value</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', <pyspark.resultiterable.ResultIterable at 0x7fcabc98e1d0>),\n",
       " ('d', <pyspark.resultiterable.ResultIterable at 0x7fcabc2ae350>),\n",
       " ('c', <pyspark.resultiterable.ResultIterable at 0x7fcabc2ae3d0>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x7fcabc2ae410>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddTuple.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>sortByKey( ascending=True|False )</b> - Sort the input RDD by the key value.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 4), ('a', 1), ('b', 2), ('c', 3), ('c', 3), ('d', 2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddTuple.sortByKey(True).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>countByValue()</b> - Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('a', 1): 2, ('a', 4): 1, ('b', 2): 1, ('c', 3): 2, ('d', 2): 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddTuple.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>count()</b> - Displays the count of elements in RDD.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of lines in the file\n",
    "rddFile.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>takeSample(withReplacement, num, [seed])</b> - This action will return n elements from the dataset, with or without replacement (true or false). Seed is an optional parameter that is used as a random generator.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A great touch.\\t1',\n",
       " u'I vomited in the bathroom mid lunch.\\t0',\n",
       " u'Check it out.\\t1',\n",
       " u'The problem I have is that they charge $11.99 for a sandwich that is no bigger than a Subway sub (which offers better and more amount of vegetables).\\t0',\n",
       " u'There is nothing authentic about this place.\\t0',\n",
       " u'It lacked flavor, seemed undercooked, and dry.\\t0',\n",
       " u'Nargile - I think you are great.\\t1',\n",
       " u'!....THE OWNERS REALLY REALLY need to quit being soooooo cheap let them wrap my freaking sandwich in two papers not one!\\t0']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddFile.takeSample(False, 8, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>saveAsTextFile()</b> - Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. </i>\n",
    "\n",
    "    Please note that multiple files will be created as computing is done in parallel. In case of an error while executing the below command again, make sure to delete the previously generated output directory. In default configuration, Spark does not allow file overwrite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rddFile.saveAsTextFile(\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD Transformations\n",
    "\n",
    "In this wordCount example, we will use the file loaded above and also look into some of the transformations operations.\n",
    "\n",
    "<i>\n",
    "- <b>map()</b> - Return a new distributed dataset formed by passing each element of the source through a function func. \n",
    "- <b>flatMap()</b> - Similar to map, but each input item can be mapped to 0 or more output items \n",
    "- <b>reduceByKey</b> - When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V.\n",
    "</i>\n",
    "\n",
    "Please note that due to lazy evaluation, no actual action is performed by Spark until take() command is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'better!', 1),\n",
       " (u'polite,', 1),\n",
       " (u'ever!', 2),\n",
       " (u'sushi.', 1),\n",
       " (u'yellow', 1),\n",
       " (u\"friend's\", 2),\n",
       " (u'hate', 2),\n",
       " (u'callings.', 1),\n",
       " (u'up.', 5),\n",
       " (u'over-hip', 1),\n",
       " (u'contained', 1),\n",
       " (u'Host', 1),\n",
       " (u'blanket', 1),\n",
       " (u'returning.', 1),\n",
       " (u'attentive,', 2),\n",
       " (u'attentive.', 2),\n",
       " (u'every', 8),\n",
       " (u'GO', 2),\n",
       " (u'selection.', 3),\n",
       " (u\"we'll\", 3)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns the word count in the file.\n",
    "rddWordCount = rddFile \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "#lazy evaluation, no action takes place until this line\n",
    "rddWordCount.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>filter()</b> - Return a new dataset formed by selecting those elements of the source on which func returns true.</i> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns the number of times the word occurred in the file\n",
    "rddWordFilter = rddFile.filter(lambda x: 'restaurant' in x)\n",
    "\n",
    "rddWordFilter.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>union()</b> - Return a new dataset that contains the union of the elements in the source dataset and the argument. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " ('a', 1),\n",
       " ('b', 2),\n",
       " ('c', 3),\n",
       " ('a', 4),\n",
       " ('c', 3),\n",
       " ('d', 2),\n",
       " ('a', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddUnion = rddList.union(rddTuple)\n",
    "\n",
    "rddUnion.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>intersection()</b> - Return a new RDD that contains the intersection of elements in the source dataset and the argument. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 1), ('d', 2), ('a', 4), ('c', 3)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddIntersection = rddUnion.intersection(rddTuple)\n",
    "\n",
    "rddIntersection.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>distinct()</b> - Return a new dataset that contains the distinct elements of the source dataset.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('d', 2), ('c', 3), ('a', 4), ('b', 2)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddDistinct = rddTuple.distinct()\n",
    "\n",
    "rddDistinct.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <i><b>join()</b> - Returns a new dataset joining two RDDs based on a common key.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 1)),\n",
       " ('a', (1, 4)),\n",
       " ('a', (4, 1)),\n",
       " ('a', (4, 4)),\n",
       " ('a', (1, 1)),\n",
       " ('a', (1, 4)),\n",
       " ('c', (3, 3)),\n",
       " ('c', (3, 3)),\n",
       " ('b', (2, 2)),\n",
       " ('d', (2, 2))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddJoin = rddTuple.join(rddIntersection)\n",
    "\n",
    "rddJoin.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark SQL and DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at Spark's SQL library and it's ability to handle data in a structured way. Like a table in a relational database or a data frame in R or Pandas, Spark also has the concept of Dataframes which can be queried using SQL language. A Spark DataFrame is a distributed collection of data organized into named columns.\n",
    "\n",
    "To get started, we will first download the white wine quality dataset file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "wine_dataset_file = urllib.urlretrieve (\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", \"white_wine.csv\")\n",
    "\n",
    "raw_data = sc.textFile(\"white_wine.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry point into all SQL functionality in Spark is the SQLContext class. We will use the global context object sc to create a SQLContext instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL can convert an RDD of Row objects to a DataFrame. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys define the column names, and the types are inferred by looking at the first row. Therefore, it is important that there is no missing data in the first row of the RDD in order to properly infer the schema.\n",
    "\n",
    "The data in our dataset is delimited by \";\" and includes headers. We will now create the RDD from the given data and specify the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splits the data based on the delimiter.\n",
    "data = raw_data.map(lambda x: x.split(\";\"))\n",
    "\n",
    "header = data.first()\n",
    "\n",
    "# removes the header from the RDD\n",
    "data = data.filter(lambda l: l!=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# specify the schema    \n",
    "row_data = data.map(lambda p: Row(\n",
    "    fixed_acidity=float(p[0]),\n",
    "    volatile_acidity=float(p[1]),\n",
    "    citric_acid=float(p[2]),\n",
    "    residual_sugar=float(p[3]),\n",
    "    chlorides=float(p[4]),\n",
    "    free_fulfur_dioxide=float(p[5]),\n",
    "    total_sulfur_dioxide=float(p[6]),\n",
    "    density=float(p[7]),\n",
    "    pH=float(p[8]),\n",
    "    sulphates=float(p[9]),\n",
    "    alcohol=float(p[10]),\n",
    "    quality=int(p[11]))   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our RDD of Row, we can infer and register the schema to create the Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wineDataFrame = sqlContext.createDataFrame(row_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at our dataframe schema using printSchema. In addition, we can get baisc statistics for numerical columns using describe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- chlorides: double (nullable = true)\n",
      " |-- citric_acid: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- fixed_acidity: double (nullable = true)\n",
      " |-- free_fulfur_dioxide: double (nullable = true)\n",
      " |-- pH: double (nullable = true)\n",
      " |-- quality: long (nullable = true)\n",
      " |-- residual_sugar: double (nullable = true)\n",
      " |-- sulphates: double (nullable = true)\n",
      " |-- total_sulfur_dioxide: double (nullable = true)\n",
      " |-- volatile_acidity: double (nullable = true)\n",
      "\n",
      "Number of records: 4898\n",
      "Number of columns: 12\n",
      "+-------+------------------+\n",
      "|summary|           alcohol|\n",
      "+-------+------------------+\n",
      "|  count|              4898|\n",
      "|   mean| 10.51426704777462|\n",
      "| stddev|1.2306205677573196|\n",
      "|    min|               8.0|\n",
      "|    max|              14.2|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|     fixed_acidity|\n",
      "+-------+------------------+\n",
      "|  count|              4898|\n",
      "|   mean|  6.85478766843609|\n",
      "| stddev|0.8438682276875117|\n",
      "|    min|               3.8|\n",
      "|    max|              14.2|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|           quality|\n",
      "+-------+------------------+\n",
      "|  count|              4898|\n",
      "|   mean|  5.87790935075541|\n",
      "| stddev|0.8856385749678303|\n",
      "|    min|                 3|\n",
      "|    max|                 9|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the schema\n",
    "wineDataFrame.printSchema()\n",
    "\n",
    "# Number of records/columns\n",
    "print \"Number of records: %s\" % wineDataFrame.count()\n",
    "print \"Number of columns: %s\" % len(wineDataFrame.columns)\n",
    "\n",
    "#view some summary of the columns\n",
    "wineDataFrame.describe(\"alcohol\").show()\n",
    "wineDataFrame.describe(\"fixed_acidity\").show()\n",
    "wineDataFrame.describe(\"quality\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will register the DataFrame as a SQL table and use SQL language to query the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a table.\n",
    "wineDataFrame.registerTempTable(\"white_wine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+----+--------------+---------+-------+\n",
      "|alcohol|chlorides|citric_acid|  pH|residual_sugar|sulphates|quality|\n",
      "+-------+---------+-----------+----+--------------+---------+-------+\n",
      "|   10.4|    0.035|       0.45| 3.2|          10.6|     0.46|      9|\n",
      "|   12.4|    0.021|       0.29|3.41|           1.6|     0.61|      9|\n",
      "|   12.5|    0.031|       0.36|3.28|           2.0|     0.48|      9|\n",
      "|   12.7|    0.018|       0.34|3.28|           4.2|     0.36|      9|\n",
      "|   12.9|    0.032|       0.49|3.37|           2.2|     0.42|      9|\n",
      "+-------+---------+-----------+----+--------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #run sql queries\n",
    "queryResult = sqlContext.sql(\"SELECT alcohol,chlorides,citric_acid,pH,residual_sugar,sulphates,quality FROM white_wine where quality=9\")\n",
    "queryResult.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Spark uses the dataframes in distributed mode, it makes the querying process very efficient and fast for large sets of data. \n",
    "\n",
    "For detailed information about the Dataframe operations and data sources, please refer [this](https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframe-operations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark MLlib: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will see how to use MLlib to perform classification task using decision trees. The standard Spark package comes with an in-built Machine Learning library (MLlib) which supports many high-quality alogrithms and utilities. Due to distributed computing and ability to do in-memory computation, Spark excels at iterative computation thus enabling MLlib to run fast.\n",
    "\n",
    "Some of the available algorithms are:\n",
    "- classification:  logistic regression, linear support vector machine(SVM), naive Bayes \n",
    "- regression: generalized linear regression (GLM) \n",
    "- collaborative filtering: alternating least squares (ALS) \n",
    "- clustering: k-means \n",
    "- decomposition: singular value decomposition (SVD), principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the classification problem to the white wine quality dataset used in above example.\n",
    "\n",
    "\n",
    "#### Preparing the data\n",
    "\n",
    "Get the dataset file and store the \";\" separated data into a RDD without the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "wine_dataset_file = urllib.urlretrieve (\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", \"white_wine.csv\")\n",
    "\n",
    "raw_data = sc.textFile(\"white_wine.csv\")\n",
    "\n",
    "# splits the data based on the delimiter.\n",
    "data = raw_data.map(lambda x: x.split(\";\"))\n",
    "\n",
    "header = data.first()\n",
    "\n",
    "# removes the header from the RDD\n",
    "data = data.filter(lambda l: l!=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Decision trees in MLlib, we need to translate our data into a list of labelled points, which are key-value pairs. The key or label is the target classification or class of the observation and the values is a feature vector stored as an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree feature vector: [7.0,0.27,0.36,20.7,0.045,45.0,170.0,1.001,3.0,0.45,8.8]\n",
      "Decision Tree feature vector length: 11\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "# first 11 columns include the features data\n",
    "def extract_features_dt(record):\n",
    "    return np.array(map(float, record[0:11]))\n",
    "\n",
    "# last column is our class label \n",
    "def extract_label(record):\n",
    "    return float(record[-1])\n",
    "\n",
    "# create labelled points from the data RDD\n",
    "data_dt = data.map(lambda r: LabeledPoint(extract_label(r), extract_features_dt(r)))\n",
    "\n",
    "print \"Decision Tree feature vector: \" + str(data_dt.first().features)\n",
    "print \"Decision Tree feature vector length: \" + str(len(data_dt.first().features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do 70-30 split of our data for training and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into training and test sets\n",
    "train_data_dt, test_data_dt = data_dt.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a classifier\n",
    "\n",
    "We will now proceed with building our decision tree model. For this, we will use train classifier method of the DecisionTree which takes labelled training data, number of classes, categorical features information (none in our case), impurity, maximum tree depth and maximum bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained in 2.096 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "from time import time\n",
    "\n",
    "startTime = time()\n",
    "\n",
    "#build the model\n",
    "dt_model = DecisionTree.trainClassifier(train_data_dt,11,{},\"gini\",4,42)\n",
    "\n",
    "print \"Classifier trained in {} seconds\".format(round(time()-startTime,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the model\n",
    "\n",
    "Using the toDebugString method in our three model we can obtain a lot of information regarding splits, nodes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model:\n",
      "\n",
      "DecisionTreeModel classifier of depth 4 with 31 nodes\n",
      "  If (feature 10 <= 10.8)\n",
      "   If (feature 1 <= 0.27)\n",
      "    If (feature 1 <= 0.205)\n",
      "     If (feature 10 <= 9.1)\n",
      "      Predict: 6.0\n",
      "     Else (feature 10 > 9.1)\n",
      "      Predict: 6.0\n",
      "    Else (feature 1 > 0.205)\n",
      "     If (feature 10 <= 9.8)\n",
      "      Predict: 5.0\n",
      "     Else (feature 10 > 9.8)\n",
      "      Predict: 6.0\n",
      "   Else (feature 1 > 0.27)\n",
      "    If (feature 10 <= 10.3)\n",
      "     If (feature 8 <= 3.24)\n",
      "      Predict: 5.0\n",
      "     Else (feature 8 > 3.24)\n",
      "      Predict: 5.0\n",
      "    Else (feature 10 > 10.3)\n",
      "     If (feature 5 <= 17.0)\n",
      "      Predict: 5.0\n",
      "     Else (feature 5 > 17.0)\n",
      "      Predict: 6.0\n",
      "  Else (feature 10 > 10.8)\n",
      "   If (feature 10 <= 11.7)\n",
      "    If (feature 5 <= 10.0)\n",
      "     If (feature 1 <= 0.35)\n",
      "      Predict: 5.0\n",
      "     Else (feature 1 > 0.35)\n",
      "      Predict: 4.0\n",
      "    Else (feature 5 > 10.0)\n",
      "     If (feature 6 <= 152.0)\n",
      "      Predict: 6.0\n",
      "     Else (feature 6 > 152.0)\n",
      "      Predict: 6.0\n",
      "   Else (feature 10 > 11.7)\n",
      "    If (feature 5 <= 21.0)\n",
      "     If (feature 8 <= 3.29)\n",
      "      Predict: 6.0\n",
      "     Else (feature 8 > 3.29)\n",
      "      Predict: 7.0\n",
      "    Else (feature 5 > 21.0)\n",
      "     If (feature 3 <= 1.2)\n",
      "      Predict: 6.0\n",
      "     Else (feature 3 > 1.2)\n",
      "      Predict: 7.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Learned classification tree model:\\n\"\n",
    "print dt_model.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model\n",
    "\n",
    "In order to measure the classification error on our test data, we use map on the test_data RDD and the model to predict each test point class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree depth: 4\n",
      "Decision Tree number of nodes: 31\n",
      "Decision Tree predictions: \n",
      "[(6.0, 5.0), (6.0, 6.0), (7.0, 7.0), (5.0, 5.0), (6.0, 6.0), (5.0, 6.0), (5.0, 5.0), (5.0, 5.0), (5.0, 5.0), (5.0, 6.0), (7.0, 6.0), (6.0, 6.0), (6.0, 5.0), (6.0, 6.0), (6.0, 5.0), (6.0, 6.0), (6.0, 5.0), (8.0, 6.0), (6.0, 5.0), (6.0, 5.0), (5.0, 5.0), (5.0, 5.0), (7.0, 7.0), (7.0, 7.0), (6.0, 5.0), (4.0, 5.0), (6.0, 5.0), (5.0, 5.0), (5.0, 6.0), (6.0, 5.0), (5.0, 5.0), (5.0, 5.0), (5.0, 5.0), (5.0, 5.0), (6.0, 6.0), (5.0, 5.0), (6.0, 6.0), (5.0, 5.0), (6.0, 5.0), (6.0, 6.0), (6.0, 5.0), (6.0, 5.0), (8.0, 7.0), (7.0, 7.0), (6.0, 5.0), (6.0, 5.0), (4.0, 4.0), (7.0, 6.0), (6.0, 6.0), (6.0, 6.0)]\n"
     ]
    }
   ],
   "source": [
    "preds = dt_model.predict(test_data_dt.map(lambda p: p.features))\n",
    "actual_and_preds = test_data_dt.map(lambda p: p.label).zip(preds)\n",
    "\n",
    "print \"Decision Tree depth: \" + str(dt_model.depth())\n",
    "print \"Decision Tree number of nodes: \" + str(dt_model.numNodes())\n",
    "print \"Decision Tree predictions: \\n\" + str(actual_and_preds.take(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification results are returned in pairs, with the actual test label and the predicted one. This is used to calculate the classification error by using filter and count as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction made in 0.809 seconds\n",
      "Test accuracy is 0.49112\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "test_accuracy = actual_and_preds.filter(lambda (v, p): v != p).count() / float(test_data_dt.count())\n",
    "\n",
    "print \"Prediction made in {} seconds\".format(round(time()-t0,3))\n",
    "\n",
    "print \"Test accuracy is {}\".format(round(test_accuracy,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy obtained from our model is around 49%. You can further experiment with different depths and maxBins to see how it impacts the complexity, time required to train and accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary and References\n",
    "\n",
    "This tutorial highlighted some of the basic operations and analysis tasks that can be performed with Apache Spark using PySpark. Much more detailed information about the libraries, packages and operations supported in Apache Spark is available at below links:\n",
    "\n",
    "- [Apche Spark](http://spark.apache.org/docs/latest/index.html)\n",
    "- [PySpark (API Documentation)](http://spark.apache.org/docs/2.0.1/api/python/index.html)\n",
    "- [RDDs](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)\n",
    "- [SPARK SQL and Dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframe-operations)\n",
    "- [Machine Learning Library (MLlib)](http://spark.apache.org/docs/latest/ml-guide.html)\n",
    "- [PySpark Internals](https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals)\n",
    "- [Decision Tree Example](https://xscale10.blogspot.com/2015/08/machine-learning-using-spark-mllib-part.html)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
