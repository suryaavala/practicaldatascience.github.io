{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce on AWS\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The penetration of the Internet and the increased usage of mobile devices have dramatically increased the data available in this world. The questions is, how are we going to process such a magnificent amount of data? Yes, we do have super computers in some places, but not everyone has access to them, especially normal developers. Is there a way for an individual to perform big data analysis efficiently?\n",
    "\n",
    "Thanks to all the open-source communities, we do. One of the most popular model is MapReduce. We will talk about it in this tutorial. You'll learn how to process a dataset of dozens of gigabytes with the help of cloud services. It means you can do it on a normal machine, anywhere, anytime. \n",
    "\n",
    "#### Tutorial Content\n",
    "\n",
    "- [MapReduce](#MapReduce)\n",
    "- [Understanding MapReduce: Wordcount](#Understanding-MapReduce:-Wordcount)\n",
    "- [Meet mrjob](#Meet-mrjob)\n",
    "- [Let's Talk About Shakespeare](#Let's-Talk-About-Shakespeare)\n",
    "- [Elastic MapReduce](#Elastic-MapReduce)\n",
    "- [Streaming Hadoop MapReduce](#Streaming-Hadoop-MapReduce)\n",
    "- [Build Your Own Hadoop Cluster](#Build-Your-Own-Hadoop-Cluster)\n",
    "- [Summary](#Summary)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "\n",
    "The MapReduce programming model is designed to process big dataset using a large number of machines in-parallel. It could be illustrated as a \"server farm\". The main advantage of MapReduce is to process data on a flexible scale over many machines. Imagine you are processing a dataset of 100TB, it may take days to complete this task on a single machine, even it's a very powerful one, but perhaps only a few hours on a cluster of twenty regular machines, as they are working simultaneously and collaboratively on one dataset. \n",
    "\n",
    "#### Hadoop MapReduce\n",
    "\n",
    "[Hadoop MapReduce](http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.0/hadoop-2.7.0.tar.gz) is an open-source implementation of Google's MapReduce. Though Hadoop MapReduce is written in Java, its implementation can be realzied through other languages, such as Python and C. Hadoop MapReduce uses the Hadoop Distributed File System (HDFS) as the underlying file system. \n",
    "\n",
    "#### How it works\n",
    "\n",
    "A standard MapReduce would follow these six steps:  \n",
    "\n",
    "1. **Map-Spliting**: The dataset is firstly splited into HDFS blocks chunks. If a file is smaller than the size limit, it remains intact, otherwise it will be split. The default size of each chunk is 100MB, but this can be determined by user configuraion. For example, if we have two files for the MapReduce job, one at 87MB and the other at 123MB, we will have three HDFS chunks: 87MB, 100MB, 23MB. This process will be completed automatically by Hadoop MapReduce and is not controled by our code; \n",
    "\n",
    "2. **Map-Map**: This task is where our code for Map phase gets executed. For each line of the data, our map() function is invoked to process the line according to the desired patterns. The output of our map() function would be key-value pairs. For example, if we analyze an article, we may break lines into words, and generate a key-value pair for each word as ['word':1]. This is a process where we \"extend\" our data with redundancy;\n",
    "\n",
    "3. **Map-Partition**: The output of Map function is exported directly to a in-memory buffer using \"print\" in Python (System.out.println() in Java). Each time when the buffer is almost full, 80% by default, it created a sorted partition. The output of this task are partitions with various features;\n",
    "\n",
    "4. **Reduce-Shuffle**: The data partitions from Map phase will be shuffled. Partitions share certain charactistics will be recognized as one group.This is a process controlled by MapReduce's Resource Manager;\n",
    "\n",
    "5. **Reduce-Merge&Sort**: Those partitions that have been categorized as one group will together form a single, large partition, which will be used to feed the Reduce task. This is a process controlled by MapReduce's Resource Manager;\n",
    "\n",
    "6. **Reduce-Reduce**: A partiion, a group of data, will be processed by our reduce() function in this task. Ususally, it should summarize the output, leading to a significant reduce in the data size, as this task removes redundancy;\n",
    "\n",
    "(Step 3, 4 and 5 are often recogized as one \"Shuffle\" stage in many cases, as they are closed related and are not controlled by user functions)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding MapReduce: Wordcount\n",
    "\n",
    "Wordcount is the simplest implementation of MapReduce task. Let's get our hands dirty now. Given the sentence below:\n",
    "\n",
    "<img src=\"ppap.png\" width=\"300px\" height=\"300px\" />\n",
    "<center>__\"I have a pan, I have an applie, ah, apple-pan.\"__</center>\n",
    "\n",
    "We firstly clean up the text line using this clean function. We will implement similar cleaning process inside our mapper function later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"I have a pan, I have an applie, ah, apple-pan.\"\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "def clean(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    result = []\n",
    "    processed = text.lower()\n",
    "    processed = processed.replace(\"'s\", \"\")\n",
    "    processed = processed.replace(\"'\", \"\")\n",
    "    remove_punct = string.maketrans(string.punctuation,\" \"*len(string.punctuation))\n",
    "    processed = processed.translate(remove_punct)\n",
    "    for element in nltk.word_tokenize(processed):\n",
    "        try:\n",
    "            result.append(str(lemmatizer.lemmatize(element)))\n",
    "        except:\n",
    "            continue\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall haveï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have a pan i have an applie ah apple pan\n"
     ]
    }
   ],
   "source": [
    "print clean(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Steps Explained:\n",
    "\n",
    "##### Split\n",
    "The Map phase will firstly split the data into HDFS chunk and distribute them to several different machines, let's say three machines:  \n",
    "\n",
    "Assume each chunk can have maximum four words (corresponding to 100MB):  \n",
    "HDFS 1: i have a pan  \n",
    "HDFS 2: i have an apple  \n",
    "HDFS 3: ah apple pan  \n",
    "\n",
    "Machine 1: HDFS1(i have a pan)  \n",
    "Machine 2: HDFS2(i have an apple)  \n",
    "Machine 3: HDFS3(ah apple pan)  \n",
    "\n",
    "##### Map  \n",
    "Now we execute a map() function for each chunk. After the Map function, we shall have many uncategorized partitions in each machine:  \n",
    "\n",
    "Machine 1: {\"i\":1}  \n",
    "Machine 1: {\"have\":1}  \n",
    "Machine 1: {\"a\":1}  \n",
    "Machine 1: {\"pan\":1}  \n",
    "  \n",
    "Machine 2: {\"i\":1}  \n",
    "Machine 2: {\"have\":1}  \n",
    "Machine 2: {\"an\":1}  \n",
    "Machine 2: {\"apple\":1}   \n",
    "  \n",
    "Machine 3: {\"ah\":1}   \n",
    "Machine 3: {\"apple\":1}    \n",
    "Machine 3: {\"pan\":1}\n",
    "\n",
    "##### Shuffle    \n",
    "We categorize and sort similar partitions and make them new partitions, which we used to feed our Reducer.     \n",
    "  \n",
    "New Partition 1: {\"i\":1}  \n",
    "New Partition 1: {\"i\":1}  \n",
    "New Partition 1: {\"have\":1}  \n",
    "New Partition 1: {\"have\":1}  \n",
    "  \n",
    "New Partition 2: {\"a\":1}  \n",
    "New Partition 2: {\"an\":1}  \n",
    "New Partition 2: {\"ah\":1}  \n",
    "  \n",
    "New Partition 3: {\"pan\":1}    \n",
    "New Partition 3: {\"pan\":1}  \n",
    "New Partition 3: {\"apple\":1}     \n",
    "New Partition 3: {\"apple\":1}     \n",
    "\n",
    "##### Reduce   \n",
    "Finally we invode reduce() function on each newly-formed partition and get the word counts for each word.  \n",
    "Reducer 1: {\"i\":2}   \n",
    "Reducer 1: {\"have\":2}  \n",
    "  \n",
    "Reducer 2: {\"a\":1}  \n",
    "Reducer 2: {\"an\":1}  \n",
    "Reducer 2: {\"ah\":1}  \n",
    "  \n",
    "Reducer 3: {\"pan\":2}   \n",
    "Reducer 3: {\"apple\":2}  \n",
    "\n",
    "The final results of word counts will be put to target HDFS directory at the user's disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meet mrjob\n",
    "\n",
    "[mrjob](https://pythonhosted.org/mrjob/index.html) is a python library to write Hadoop MapReduce programs. Although we can also use no library with simple sys.in, mrjob is more integrated and allows us to run and debug locally. Each mrjob has to have at least one mapper, one combiner(shuffle), and one reducer, included in one or multiple \"steps\". \n",
    "\n",
    "Below is a simple example using mrjob to implement the \"apple-pan\" example we described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    # Assign one count to each word\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield word, 1\n",
    "    \n",
    "    # Sum up the frequency of each word\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "    \n",
    "    # Generate the results\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S: You may have concern regarding the space this cleaning process takes, but recall that Hadoop MapReduce split large files into HDFS pieces at 100MB, so it is very unlikely to suffer from stake overflow in reality. In case that happens, simply decrease the size of default HDFS chunks, say, to 50MB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1\n",
      "ah 1\n",
      "an 1\n",
      "apple 1\n",
      "applie 1\n",
      "have 2\n",
      "i 2\n",
      "pan 2\n"
     ]
    }
   ],
   "source": [
    "# This cell displays the results of mrjob in Jupyter. This is not working in Hadoop\n",
    "import wordcount\n",
    "reload(wordcount)\n",
    "\n",
    "# With in example.txt, we have:\n",
    "# \"i have a pan i have an applie ah apple pan\"\n",
    "mr_job = wordcount.MRWordFreqCount(args=['example.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        key, value = mr_job.parse_output_line(line)\n",
    "        print key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Talk About Shakespeare\n",
    "\n",
    "In this part, we will count the number of words in \"shakespeare.txt\", which we used in a previous project, and fine out the top 100 words used. Though it is not a huge file(4MB), it is plain text and contains very rich contents. The file can thus generate a very representative output of a Hadoop MapReduce job. If you are looking for a larger dataset, you could tend to [Twitter](https://dev.twitter.com/docs), [Wikipedia](https://www.mediawiki.org/wiki/API:Main_page), or [this guide](http://kevinchai.net/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the mrjob program we will use on Hadoop Mapreduce\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "class MRTopWords(MRJob):\n",
    "    \n",
    "    # Store the key-value pair\n",
    "    topdict = {}\n",
    "        \n",
    "    # Clean up the text and assign one count to each word\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # Clean up text before further processing\n",
    "        result = []\n",
    "        processed = line.lower()\n",
    "        processed = processed.replace(\"'s\", \"\")\n",
    "        processed = processed.replace(\"'\", \"\")\n",
    "        remove_punct = string.maketrans(string.punctuation,\" \"*len(string.punctuation))\n",
    "        processed = processed.translate(remove_punct)\n",
    "        lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        for element in nltk.word_tokenize(processed):\n",
    "            try:\n",
    "                result.append(str(lemmatizer.lemmatize(element)))\n",
    "            except:\n",
    "                continue\n",
    "        result = ' '.join(result)\n",
    "        \n",
    "        for word in result.split():\n",
    "            yield word, 1\n",
    "    \n",
    "    # Sum up the frequency of each word\n",
    "    def combiner_count_frequency(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "    \n",
    "    # Populate the dictionary with word occurances\n",
    "    def reducer_populate_frequency(self, word, counts):\n",
    "        frequency = sum(counts)\n",
    "        \n",
    "        # Eliminate words with only one occurance\n",
    "        if frequency > 1:\n",
    "            self.topdict[word] = frequency\n",
    "        dummy = self.topdict\n",
    "        yield None, dummy\n",
    "    \n",
    "    # Second reducer used to print out \n",
    "    def reducer_find_top_word(self, _, dummy):\n",
    "        count = 0\n",
    "        sorted_list = sorted(self.topdict, key=self.topdict.get, reverse=True)\n",
    "        for key in sorted_list:\n",
    "            yield key, self.topdict[key]\n",
    "            count += 1\n",
    "            if count > 99:\n",
    "                break\n",
    "    \n",
    "    # Steps allow us to flexibly construct the MapReduce processes\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                    combiner=self.combiner_count_frequency,\n",
    "                    reducer=self.reducer_populate_frequency),\n",
    "            MRStep(reducer=self.reducer_find_top_word)\n",
    "        ]\n",
    "\n",
    "# These lines are required if we run the program on EMR, but not locally.\n",
    "if __name__ == '__main__':\n",
    "    MRTopWords.run()\n",
    "\n",
    "# We may need to changed these two lines to if the above lines does not yield the desired results:\n",
    "if __name__ == '__main__':\n",
    "    job = MRTopWords(args=['-r', 'emr'])\n",
    "    with job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            key, value = job.parse_output_line(line)\n",
    "            print key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot run and display the result here, as shakespeare is a large file and thus there are too many arguments for input, leading to exception: \"[Errno 22] Invalid argument\". In addition, it would also be a challenge for our memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic MapReduce\n",
    "Amazon Web Services (AWS) is one of the most popular cloud computing services available for all parties. AWS offers various services that support all kinds of missions developers may have. In our scope, we will use the Elastic MapReduce (EMR) service from AWS.\n",
    "\n",
    "#### Get Our AWS Ready  \n",
    "  \n",
    "1. Log into our AWS account, go to \"Security Credentials\", create a new credential, and download it(automatically);  \n",
    "2. Create a configuation file named \"mrjob.conf\", copy the following contents inside:  \n",
    "runners:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;emr:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aws_access_key_id: AKIAISOAS3434(Your credentials)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aws_secret_access_key: AKIAISOAS3434(Your credentials)  \n",
    "3. Launch an EMR cluster. Click \"Create cluster\" on the upper left corner and go to advanced options.  \n",
    "  \n",
    "    Uncheck all other software that we will not use.  \n",
    "![title](emr1.png)  \n",
    "  \n",
    "    Pick three m1.large instace with spot price. The size of the instances is large enough for our small file and the spot price is way much cheaper than on-demand price (at least ten times!).    \n",
    "![title](emr2.png)  \n",
    "\n",
    "    Change the security group to an all-open one, so we will not encounter any weird exception due to permission denial.  \n",
    "![title](emr3.png)\n",
    "\n",
    "    Keep clicking next and create the cluster. It may take a while for the spot instances to be launched.   \n",
    "    \n",
    "4. Once the cluster is up and running, SSH into the master node(very important) and put our .py file, .txt file, and also this .conf file in one dictionary. You can also upload them to S3 storage, but we're not here for cloud computing, so let save them locally.  \n",
    "5. Download the packages we need:  \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo pip install mrjob`\n",
    "6. Run the following command to start our MapReduce quest:  \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`python mapr.py -r emr shakespeare.txt --conf-path mrjob.conf`\n",
    "7. Sit back and wait for its completion.  \n",
    "8. Once it's finished, it generates an output file in a temperory dictionary in our S3 storage. We could go and collect our output file(usually a .txt file) in this newly-generated folder.  \n",
    "9. Don't forget to close your EMR cluster. Yes I lost $5 for forgetting this. It could be worse    \n",
    "  \n",
    "#### Below is our output:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"the\"   27052\n",
    "# \"and\"   25083\n",
    "# \"i\"     20142\n",
    "# \"to\"    18993\n",
    "# \"of\"    15867\n",
    "# \"a\"     14196\n",
    "# \"you\"   13347\n",
    "# \"my\"    11875\n",
    "# \"that\"  10847\n",
    "# \"in\"    10568\n",
    "# \"is\"    8851\n",
    "# \"not\"   8235\n",
    "# \"it\"    7512\n",
    "# \"me\"    7489\n",
    "# \"with\"  7403\n",
    "# \"for\"   7372\n",
    "# \"be\"    6670\n",
    "# \"his\"   6518\n",
    "# \"your\"  6507\n",
    "# \"he\"    6454\n",
    "# \"this\"  6448\n",
    "# \"but\"   6055\n",
    "# \"have\"  5746\n",
    "# \"as\"    5535\n",
    "# \"thou\"  5194\n",
    "# \"him\"   5070\n",
    "# \"will\"  4838\n",
    "# \"so\"    4823\n",
    "# \"what\"  4712\n",
    "# \"her\"   3838\n",
    "# \"thy\"   3727\n",
    "# \"all\"   3709\n",
    "# \"no\"    3676\n",
    "# \"do\"    3640\n",
    "# \"by\"    3606\n",
    "# \"shall\" 3472\n",
    "# \"if\"    3437\n",
    "# \"are\"   3324\n",
    "# \"we\"    3275\n",
    "# \"thee\"  3024\n",
    "# \"our\"   3016\n",
    "# \"on\"    2948\n",
    "# \"good\"  2724\n",
    "# \"now\"   2722\n",
    "# \"lord\"  2651\n",
    "# \"o\"     2559\n",
    "# \"from\"  2537\n",
    "# \"well\"  2499\n",
    "# \"sir\"   2452\n",
    "# \"come\"  2451\n",
    "# \"at\"    2448\n",
    "# \"they\"  2391\n",
    "# \"she\"   2384\n",
    "# \"enter\" 2338\n",
    "# \"or\"    2332\n",
    "# \"here\"  2290\n",
    "# \"let\"   2280\n",
    "# \"would\" 2248\n",
    "# \"more\"  2227\n",
    "# \"which\" 2192\n",
    "# \"was\"   2164\n",
    "# \"there\" 2147\n",
    "# \"how\"   2118\n",
    "# \"then\"  2101\n",
    "# \"am\"    2100\n",
    "# \"love\"  1996\n",
    "# \"their\" 1991\n",
    "# \"ill\"   1983\n",
    "# \"man\"   1940\n",
    "# \"them\"  1935\n",
    "# \"when\"  1914\n",
    "# \"hath\"  1845\n",
    "# \"than\"  1803\n",
    "# \"one\"   1760\n",
    "# \"like\"  1751\n",
    "# \"an\"    1738\n",
    "# \"go\"    1693\n",
    "# \"upon\"  1671\n",
    "# \"king\"  1650\n",
    "# \"know\"  1635\n",
    "# \"us\"    1632\n",
    "# \"say\"   1624\n",
    "# \"may\"   1600\n",
    "# \"make\"  1586\n",
    "# \"did\"   1571\n",
    "# \"were\"  1538\n",
    "# \"yet\"   1526\n",
    "# \"should\"        1510\n",
    "# \"must\"  1465\n",
    "# \"why\"   1455\n",
    "# \"had\"   1392\n",
    "# \"out\"   1385\n",
    "# \"tis\"   1384\n",
    "# \"see\"   1378\n",
    "# \"such\"  1351\n",
    "# \"where\" 1314\n",
    "# \"give\"  1297\n",
    "# \"who\"   1288\n",
    "# \"these\" 1282\n",
    "# \"some\"  1281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Hadoop MapReduce\n",
    "\n",
    "If you have low demand on local tests or do not want to use mrjob library and set up the EMR environment, I get a good news for you. There is a way to generate output file without even logging in to your node: Streaming. We simply tell the cluster where the mapper, reducer, input file, output file are and the cluster will finish all tasks automatically. Using streaming, we have to upload all the required to S3 stroage and download the output from it as well. Noteice that there is no combiner, so we have to make some changes to our programs, because reducer will now take the output of mapper as input. To create a streaming MapReduce program, we need to launch an EMR cluster with these configurations changed:  \n",
    "\n",
    "![title](streaming1.png)\n",
    "<img src=\"streaming2.png\" width=\"800px\" height=\"600px\"/>  \n",
    "  \n",
    "  \n",
    "Here are sample mapper and reducer we can use for the same purpose:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "for line in sys.stdin:\n",
    "    processed = line.lower()\n",
    "    processed = processed.replace(\"'s\", \"\")\n",
    "    processed = processed.replace(\"'\", \"\")\n",
    "    remove_punct = string.maketrans(string.punctuation,\" \"*len(string.punctuation))\n",
    "    processed = processed.translate(remove_punct)\n",
    "    lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    for element in nltk.word_tokenize(processed):\n",
    "        try:\n",
    "            result.append(str(lemmatizer.lemmatize(element)))\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    for word in result:\n",
    "        print '%s\\t%s' % (word, 1) # print formatted results, not yield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "counter = collections.Counter()\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.strip().split(\"\\t\")\n",
    "    counter[key] += int(value)\n",
    "\n",
    "result = counter.most_common(100)\n",
    "for each in result:\n",
    "    print '%s\\t%s' % (each[0], each[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Your Own Hadoop Cluster\n",
    "\n",
    "To avoid the high costs generated by AWS EMR, we could run Hadoop MapReduce tasks locally with Hadoop installed in our machines. We could either run it on a single-node or a cluster of multiple nodes, it makes no difference in our operation but in efficiency. I have successfully set up the Hadoop, but got challenged when I tried to run my program on a cluster of several virtual machines with Ubuntu 14.04. However, this is a valid way to execute a Hadoop MapReduce program and should be concerned when you have limited budget.  \n",
    "\n",
    "If you have a PC, you can download [VirtualBox](https://www.virtualbox.org/wiki/Downloads) with an [Ubuntu 14.04](http://releases.ubuntu.com/14.04/) image to launch linux machines. These two links provide very comprehensive tutorials about how to set up Hadoop environment in your local machine.\n",
    "\n",
    "1. [Running Hadoop on Ubuntu Linux (Single-Node Cluster)](http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/)  \n",
    "2. [Setting up a Apache Hadoop 2.7 single node on Ubuntu 14.04](http://thepowerofdata.io/setting-up-a-apache-hadoop-2-7-single-node-on-ubuntu-14-04/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Now, you see at least three ways to perform MapReduce tasks: with regular EMR, with streaming EMR, and with local machines. Though we only talked about word count here, there are other tasks you can perform using MapReduce and their idea is the same: map and then reduce. One example is to find mutual friends, based on which you can make friend recommendation. This is a common model used by many organizations such as Facebook and Linkedin.  \n",
    "  \n",
    "In a word, there is a lot more to explore about MapReduce, a mature and practical model for big data analysis.  \n",
    "<img src=\"recommendation.jpg\"/>  \n",
    "<center>__Using MapReduce to Find Common Friend__</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References   \n",
    "[Apache Hadoop](http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.0/hadoop-2.7.0.tar.gz)  \n",
    "[mrjob v0.5.6 documentation](https://pythonhosted.org/mrjob/index.html)  \n",
    "[Anatomy of a MapReduce Job](http://ercoppa.github.io/HadoopInternals/AnatomyMapReduceJob.html)     \n",
    "[mrjob and S3](https://www.classes.cs.uchicago.edu/archive/2013/spring/12300-1/labs/lab5/)  \n",
    "[Elastic Map Reduce with Amazon S3, AWS, EMR, Python, MrJob and Ubuntu 14.04](http://meshfields.de/elastic-map-reduce/)  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
