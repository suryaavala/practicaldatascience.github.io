{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "============\n",
    "This tutorial will introduce you to the field of reinforcement learning, a subfield of machine learning conserned with designing algorithms which learn from experience. Specifically this tutorial will consist of a introductory overview of Markov Decision Processes, and the Q-learning algorithm, followed by a simple Q-learning example using OpenAI's Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial content\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Review of prerequisite theoretical frameworks for the second unit.\n",
    "    - [Markov Decission Process](#Markov-Decission-Process)\n",
    "    - [Reinforcement Learning](#Reinforcement-Learning)\n",
    "    - [Artificial Neural Networks](#Artificial-Neural-Networks)\n",
    "2. Using **[tqdm](https://pypi.python.org/pypi/tqdm)**, **[opencv](http://opencv.org)**, **[tensorflow](https://www.tensorflow.org)**, and **[gym](https://gym.openai.com)** to apply reinforcement learning to *FlappyBird*.\n",
    "    - [Installing the libraries](#Installing-the-libraries)\n",
    "        - [Installing tqdm](#Installing-tqdm)\n",
    "        - [Installing OpenCV](#Installing-OpenCV)\n",
    "        - [Installing TensorFlow](#Installing-TensorFlow)\n",
    "        - [Installing Gym](#Installing-Gym)\n",
    "        - [Installing PLE](#Installing-PLE)\n",
    "        - [Installing Gym-PLE](#Installing-Gym-PLE)\n",
    "    - [Using the libraries](#Using-the-libraries)\n",
    "        - [Using TensorFlow](#Using-TensorFlow)\n",
    "        - [Using Gym](#Using-Gym)\n",
    "    - [Tabular Q-Learning Example](#Tabular-Q-Learning-Example)\n",
    "    - [Deep Q-Learning](#Deep-Q-Learning)\n",
    "3. Topics for further reading.\n",
    "    - [Machine Learning](#Machine-Learning)\n",
    "    - [Computer Vision](#Computer-Vision)\n",
    "    - [Reinforcement Learning](#Reinforcement-Learning)\n",
    "    - [Deep Learning](#Deep-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br><br/>\n",
    "<center>\n",
    "        <h1>Theory</h1>\n",
    "</center>\n",
    "<br><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decission Proccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Decission Process**\n",
    "- set of states $s \\in S$\n",
    "- set of actions $a \\in A$\n",
    "- a transition function $T(s,a,s^{\\prime}) = \\mathbf{P}(s^{\\prime} \\mid s,a)$, i.e. the probability of $S^{\\prime}$ being the next state given action $a$ taken at state $s$.\n",
    "- a reward function $R$\n",
    "..* we can define the reward function to depend on any of:\n",
    "...* the current state: $R(s)$\n",
    "...* the current state-action pair: $R(s,a)$ \n",
    "...* the current state, action, and next state: $R(s,a,s^{\\prime})$\n",
    "- terminal states (either a goal state with positive reward, or a non-goal state with negative reward)\n",
    "- discount factor $\\gamma$. A discount factor of $0$ results in a memoryless reward function, where the reward of the current state (and/or action) is only considered. Conversely, a discount factor of $1$ results in a reward function that approximates the expected value of future rewards (weighted by the probability of each possible future state/action).\n",
    "\n",
    "**Markov Property**\n",
    "simply put, the outcome of any action $a$ depends only on the current state and not on any of the previous state-action pairs.\n",
    "$$\\mathbf{P}(s_{t+1} \\mid s_{t}, a_{t}) = \\mathbf{P}(s_{t+1} \\mid s_{1},a_{1},s_{2},a_{2},\\ldots,s_{t},a_{t})$$\n",
    "\n",
    "**Policies**\n",
    "In MDPs instead of plans we have policies.\n",
    "A policy $\\pi^{*}\\ :\\ S \\rightarrow A$ (a function mapping states to actions)\n",
    "specifies what action to take in each state\n",
    "\n",
    "\n",
    "**State Value given policy $\\pi$**\n",
    "We can evaluate the state value of a policy by calculating the expectation over the reward function. (i.e. the sum of future rewards of each possible future state weighted by the probability of reaching that future state from the current state -- where future rewards are discounted by the discount factor $\\gamma$),\n",
    "$$\n",
    "    V^{\\pi}(s) = \\sum_{s^{\\prime} \\in S} \\mathbf{P}(s^{\\prime} \\mid s, \\pi(s)) \\left[ R(s,\\pi(s),s^{\\prime}) + \\gamma V^{\\pi}(s^{\\prime})\\right]\n",
    "$$\n",
    "\n",
    "**State-Action Value given policy $\\pi$**\n",
    "We can evaluate the state-action pair value of a policy with a simliar expectation. (i.e. the sum of future rewards of each possible future state-action pair weighted by the probability of reaching that future state-action pair -- with future reward discounted by the discount factor $\\gamma$),\n",
    "$$\n",
    "    Q^{\\pi}(s,a) = \\sum_{s^{\\prime} \\in S} \\mathbf{P}(s^{\\prime} \\mid s,a) \\left[ R(s,a,s^{\\prime}) + \\gamma V^{\\pi}(s^{\\prime})\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "**Optimal State Value**\n",
    "For any given state $s$ we can determine the value of the optimal action to take by simply applying the $max$ function instead of a summation,\n",
    "$$\n",
    "    [V^{*}(S_{i}) = \\text{max}_{a}\\ \\left( \\mathbf{P}(s_{j} \\mid s_{i}, a) \\left[ R(s_{i}, a, s^{\\prime}) + \\gamma V^{*}(s_{j}) \\right] \\right) \n",
    "$$\n",
    "\n",
    "**Optimal State-Action Value**\n",
    "Similiarly, for any given state-action pair, we can determine the opitmal action to take for any given state (i.e. the optimal state-action pair for some state) by iteration the $argmax$ function),\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\pi^{*} &= \\text{argmax}_{a}\\ Q(s_{i}, a)\\\\\n",
    "            &= \\text{argmax}_{a}\\ \\left( \\sum_{s_{j} \\in S} \\mathbf{P}(s_{j} \\mid s_{i}, a) \\left[ R(s_{i}, a, s^{\\prime}) + \\gamma V^{*}(s_{j}) \\right] \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "It is worth pointing out that in a deterministic environment (i.e. the transition function always has a probability of $1$ or $0$) and when the reward is always $1$, the $V^{\\pi}$ and $Q^{\\pi}$ functions take on the form of the sum of a geometric series. $$S_{n} = \\lim_{C \\rightarrow \\infty} \\sum_{k=0}^{C} \\gamma^{k} = \\frac{1}{1 - \\gamma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some ways Reinforcement Learning is an extension of the Markov Decision Process. While in a MDP the algorithm is given a model of the reward and transition functions, with RL the algorithm must approximate the reward and transition functions by sampling events in the environment.\n",
    "\n",
    "[<img src=\"https://gym.openai.com/static/img/tutorial/aeloop.svg\">](https://gym.openai.com/static/img/tutorial/aeloop.svg)\n",
    "\n",
    "There are several different flavors of RL, in this tutorial we are going to cover a specific model-free reinforcment learning algorithm, Q-learning.\n",
    "\n",
    "Simply put, the Q-learning algorithm, so named because it learns an optimal state-action policy for a finite MDP by approximating the optimal state-action value function $Q(s,a)$.\n",
    "\n",
    "This approximation is accomplished using direct sampling, \n",
    "$$\n",
    "\\begin{align*}\n",
    "Q_{\\text{sample}}(s,a) &= R(s,a,s^{\\prime}) + \\gamma \\max_{a^{\\prime}} Q(s^{\\prime}, a^{\\prime}) \\\\\n",
    "Q(s,a) &= (1 - \\alpha) Q(s,a) + (\\alpha) Q_{\\text{sample}}(s,a)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"http://www.global-warming-and-the-climate.com/images/Neuron-input.GIF\">](http://www.global-warming-and-the-climate.com/images/Neuron-input.GIF)\n",
    "[<img src=\"http://tensorfly.cn/special/deeplearning/images/tikz35.png\">](http://tensorfly.cn/special/deeplearning/images/tikz35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br><br/>\n",
    "<center>\n",
    "        <h1>Tutorial</h1>\n",
    "</center>\n",
    "<br><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Installing the libraries\n",
    "Before getting started, you'll need to install tensorflow, gym.openai, PLE (pygame learning environment), and gym-ple.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing tqdm\n",
    "A simple library for displaying a progress bar for loops, useful for graphical representation of DQN training loop.\n",
    "\n",
    "    $ pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing OpenCV\n",
    "If you haven't already install <code> cmake </code>\n",
    "\n",
    "    $ brew install -v cmake\n",
    "    \n",
    "Now, we install OpenCV with homebrew.\n",
    "\n",
    "    $ brew install homebrew/science/opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing tensorflow\n",
    "On OSX you can install the CPU only version of tensorflow for python 2.7 with 'pip':\n",
    "    \n",
    "    $ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc0-py2-none-any.whl\n",
    "\n",
    "    $ sudo pip install --upgrade $TF_BINARY_URL\n",
    "   \n",
    "If you want a different version, check out the other [tf binaries](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing gym\n",
    "We will be using PLE's 'FlappyBird' environment, so we cannot just do a minimal install of gym. Doing a full install entails installing some dependencies first, which we can do with 'brew' on OSX:\n",
    "\n",
    "    $ brew install cmake boost boost-python sdl2 swig wget\n",
    "\n",
    "\n",
    "Now, we are ready to install the gym module with 'pip':\n",
    "\n",
    "    $ pip install 'gym[all]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing ple\n",
    "Before installing Pygame Learning Environment, we need to install its dependencies. Note although PLE requires a Numpy installation, tensorflow has already installed it for us. We can use a combination of 'brew' and 'pip' to install the remaining dependencies:\n",
    "\n",
    "       $ pip install pillow\n",
    "       \n",
    "       $ brew install sdl sdl_ttf sdl_image sdl_mixer portmidi\n",
    "       \n",
    "       $ pip install pygame\n",
    "    \n",
    "\n",
    "We are now ready to install PLE:\n",
    "\n",
    "    $ git clone https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
    "    \n",
    "    $ cd PyGame-Learning-Environment/\n",
    "    \n",
    "    $ pip install -e . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note\n",
    "Currently PLE does not support its Doom environment on OSX (you can get around this, but we wont bother). However, annoyingly importing PLE will throw an error asking the user to install the doom environment.\n",
    "\n",
    "We can get around this by commenting out a few lines of code (since we won't be using the Doom Env):\n",
    "\n",
    "Navigate to wherever you cloned 'Pygame-Learning-Environment'.\n",
    "\n",
    "- Open up '/Pygame-Learning-Environment/ple/ple.py', and comment out the lines on 127-128\n",
    "\n",
    "\n",
    "        if isinstance(self.game, base.DoomWrapper):\n",
    "            self.rng = rng\n",
    "\n",
    "- Next, open up '/Pygame-Learning-Environment/ple/games/\\__init\\__.py', and comment out the import on line 1:\n",
    "\n",
    "        from .doom import Doom\n",
    "\n",
    "- Lastly, open up '/Pygame-Learning-Environment/ple/games/base/\\__init\\__.py', and comment out the import on line 2:\n",
    "\n",
    "        from .doomwrapper import DoomWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing gym-ple\n",
    "Now we just need to install 'gym-ple', so that we can use PLE environments in 'gym'. We'll just clone the git repo, and install with 'pip':\n",
    "\n",
    "    $ git clone https://github.com/lusob/gym-ple.git\n",
    "    \n",
    "    $ cd gym-ple/\n",
    "    \n",
    "    $ pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_ple\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import logging, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Using the libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will implement logistic regression, in TensorFlow, for the MNIST dataset -- installed with tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import MNIST\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"\n",
    "    perform logistic regression, using gradient descent.\n",
    "    \"\"\"\n",
    "    def __init__(self, **userconfig=None):\n",
    "        mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "        self.mnist = mnist\n",
    "        \n",
    "        self.X_train = mnist.train.images\n",
    "        self.y_train = mnist.train.labels\n",
    "        self.X_test = mnist.test.images\n",
    "        self.y_test = mnist.test.labels\n",
    "        \n",
    "        self.config = {\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"n_epochs\" : 20,\n",
    "            \"batch_size\" : 100,\n",
    "            \"n_batches\" : int(mnist.train.num_examples / 100)\n",
    "        if userconfig: \n",
    "            self.config.update(userconfig)\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 10])\n",
    "        \n",
    "        self.W = tf.Variable(tf.zeroes([784, 10])) # weight matrix\n",
    "        self.b = tf.Variable(tf.zeroes([10]))      # bias array\n",
    "    \n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(tf.matmul(self.x, self.W) + self.b)\n",
    "    \n",
    "    def cost(self):\n",
    "        \"\"\"\n",
    "        use the cross-enthropy loss function to minimize error\n",
    "        \"\"\"\n",
    "        return tf.reduce_mean(-tf.reduce_sum(y*tf.log(self.predict()), reduce_indices=1))\n",
    "    \n",
    "    def gradient(self):\n",
    "        \"\"\"\n",
    "        Gradient Descent\n",
    "        \"\"\"\n",
    "        return tf.train.GradientDescentOptimizer(self.config[\"learning_rate\"]).minimize(cost)\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        init = tf.initialize_all_variables()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            \n",
    "            epoch_costs = []\n",
    "            for epoch in tqdm(range(self.config[\"n_epochs\"])):\n",
    "                batch_costs = []\n",
    "                for t in range(self.config[\"n_batches\"]):\n",
    "                    batch_xs, batch_ys = self.mnist.train.next_batch(self.config[\"batch_size\"])\n",
    "                    _, batch_cost = sess.run([self.gradient(), self.cost()], \n",
    "                                             feed_dict={x: batch_xs, y: batch_ys})\n",
    "                    batch_costs.append(batch_cost)\n",
    "                epoch_costs.append(sum(batch_costs) / self.config[\"n_batches\"])\n",
    "        return epoch_costs\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "_ = LR.train()\n",
    "model_correct = tf.equal(tf.argmax(LR.predict(), 1), tf.argmax(LR.y, 1))\n",
    "model_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI's Gym is an open source toolkit for reinforcement learning which provides a standardized environment so that one can meaningfully compare different reinforcement learning algorithms.\n",
    "\n",
    "Gym is composed of Environments, and Spaces.\n",
    "\n",
    "Environments define the actual MDP to be learned, including Spaces, and the reward function.\n",
    "\n",
    "Spaces define the representation of actions, and observations.\n",
    "\n",
    "For a more indepth introduction, you can consult the library [documentation](https://gym.openai.com/docs) (about one page).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define an agent\n",
    "class Random_Agent(object):\n",
    "    \"\"\" Simple agent example, which acts randomly \"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "    \n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "outdir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'results/Random-Agent')\n",
    "\n",
    "# setup the environment\n",
    "env = gym.make('Taxi-v1')\n",
    "env.monitor.start(outdir, force=True)\n",
    "\n",
    "# assign the agent\n",
    "agent = Random_Agent(env.action_space)\n",
    "\n",
    "episodes = 100\n",
    "steps = 200\n",
    "reward = 0\n",
    "done = False\n",
    "\n",
    "# train our random agent on the Flappy Bird environment\n",
    "for episode in tqdm(range(episodes)):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    for step in range(steps):\n",
    "        action = agent.act(observation, reward, done)\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.monitor.close()\n",
    "logger.info(\"Successfully ran Random-Agent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Q-Learning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TQ_Agent(object):\n",
    "    \"\"\"\n",
    "    A simple agent implementing Epsilon Greedy Q-learning, which uses a 2d-dict to store Q-values.\n",
    "    (i.e. self.q = dict[observation] -> dict[action] -> Q-val for (observation-action) pair).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, observation_space, action_space, **userconfig=None):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.action_n = action_space.n\n",
    "        self.config = {\n",
    "            \"mean\" : 0.0,           # Initialize Q values with this mean\n",
    "            \"std\" : 0.0,            # Initialize Q values with this standard deviation\n",
    "            \"alpha\" : 0.1,          # Learning rate\n",
    "            \"epsilon\": 0.05,        # Epsilon in epsilon greedy policies\n",
    "            \"gamma\": 0.95,          # Discount factor\n",
    "            \"n_iter\": 10000}        # Number of iterations\n",
    "        if userconfig:\n",
    "            self.config.update(userconfig)\n",
    "        # allow for random initialization of Q-values -- following a Normal Distribution.\n",
    "        self.q = defaultdict(lambda: self.config[\"std\"] * np.random.randn(self.action_n) + self.config[\"mean\"])\n",
    "\n",
    "    def act(self, observation):\n",
    "        \"\"\" \n",
    "        E-Greedy: \n",
    "            - with probability (1-epsilon) choose argmax{ Q(s,a) for all a}\n",
    "            - with probability (epsilon) choose a random action\n",
    "        \"\"\"\n",
    "        random_action = self.action_space.sample()\n",
    "        best_action = np.argmax(self.q[observation])\n",
    "        return best_action if np.random.random() > self.config[\"epsilon\"] else random_action\n",
    "\n",
    "    def learn(self, env):\n",
    "        \"\"\"\n",
    "        sampleQ(s,a) = R(s,a,s') + gamma * max{ Q(s',a') for all a'} \n",
    "\n",
    "        updatedQ = (1-alpha) * Q(s,a)  +  (alpha) * sampleQ(s,a)\n",
    "                 = ( Q(s,a) - alpha * Q(s,a) )  +  ( alpha * sampleQ(s,a) )\n",
    "                 = Q(s,a) + ( - alpha * Q(s,a) )  -  ( -alpha * sampleQ(s,a) )\n",
    "                 = Q(s,a) - alpha * ( Q(s,a) - sampleQ(s,a) )\n",
    "            => self.q[s][a] -= alpha * ( self.q[s][a] - sampleQ(s,a) ) \n",
    "        \"\"\"\n",
    "        observation = env.reset()\n",
    "        for t in range(self.config[\"n_iter\"]):\n",
    "            action = self.act(observation)\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "\n",
    "            sampleQ = reward + self.config[\"gamma\"] * (np.max(self.q[observation]) if not done else 0.0)\n",
    "            self.q[observation][action] -= self.config[\"alpha\"] * (self.q[observation][action] - sampleQ)\n",
    "\n",
    "            observation = next_observation\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "outdir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'results/TQ-Agent')\n",
    "print outdir\n",
    "\n",
    "# setup the environment\n",
    "env = gym.make('Taxi-v1')\n",
    "env.monitor.start(outdir, force=True)\n",
    "\n",
    "# assign the agent\n",
    "agent = TQ_Agent(env.observation_space, env.action_space)\n",
    "\n",
    "# train the agent on the flappy bird environment\n",
    "episodes = 100\n",
    "for episode in tqdm(range(episodes)):\n",
    "\tagent.learn(env)\n",
    "\n",
    "env.monitor.close()\n",
    "logger.info(\"Successfully ran TQ-Agent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"http://www.nature.com/nature/journal/v518/n7540/images/nature14236-f1.jpg\">](http://www.nature.com/nature/journal/v518/n7540/images/nature14236-f1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure is the general layout of Google Deepmind's DQN used in its atari paper. It is outside of the scope of this tutorial, however if you are interested, feel free to check out this excellent [blog post](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/) covering the deepmind paper -- specifically check out the section on the Deep Q-Network. Or just read the [paper](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html).\n",
    "\n",
    "Here is a similiar algorithm for applying Deep Q-Network to *FlappyBird*, covered [here](http://cs229.stanford.edu/proj2015/362_report.pdf).\n",
    "\n",
    "Lets begin by considering the FlappyBird environment.\n",
    "\n",
    "The action space is Discrete, which is pretty straight forward.\n",
    "However, the observation space is a Box defined as <code> spaces.Box(low=0, high=255, shape=(self.screen_width, self.screen_height, 3)) </code>, so what we have here is a stack of four images of the screen -- since <code> spaces.Box.shape </code> is zero indexed.\n",
    "\n",
    "Here is some pseudocode covering what is going on behind the scenes: \n",
    "[<img src=\"https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.43-AM-1.png\">](https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.43-AM-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br><br/>\n",
    "<center>\n",
    "        <h1>Additional Resources</h1>\n",
    "</center>\n",
    "<br><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- introductory resources:\n",
    "    - [stanford machine learning (coursera)](https://www.coursera.org/learn/machine-learning)\n",
    "    - [Introductory chapter of deep learning book](http://www.deeplearningbook.org/contents/intro.html)\n",
    "    \n",
    "- advanced resources:\n",
    "    - [10-601 S15 lecture videos (youtube)](https://www.youtube.com/playlist?list=PLAJ0alZrN8rD63LD0FkzKFiFgkOmEtltQ)\n",
    "    - [10-701 F14 lecture videos (youtube)](https://www.youtube.com/playlist?list=PLAJ0alZrN8rC-QCaaZ0Z-brjoWyIO8CKd)\n",
    "\n",
    "- textbooks:\n",
    "    - [tom michell's ml book](http://www.cs.cmu.edu/~tom/mlbook.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Simon Prince's cv book](http://www.computervisionmodels.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Reinforcement Learning: An Introduction](http://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [neural nets and deep learning](http://neuralnetworksanddeeplearning.com)\n",
    "- [deep learning book](http://www.deeplearningbook.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
