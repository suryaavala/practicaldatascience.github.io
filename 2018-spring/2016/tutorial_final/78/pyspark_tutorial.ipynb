{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Tutorial\n",
    "\n",
    "By: Julien Naegeli \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is a powerful open source cluster computing framework that provides simplistic uses of complex analytics processes. Apache Spark is an in-memory data processing engine that has well documented and frequently updated APIs to execute streaming, machine learning or SQL workloads on large datasets. PySpark allows us to make use of this Big Data processing framework in Python. What makes Spark a common selection for Big Data processing is its speed, due to the in-memory data processing engine that many alternatives do not provide. Spark computes everything in-nemory while Hadoop's MapReduce, a popular alternative, persists the data on the disk after every map or reduce job. For the purposes of this tutorial, we will focus on its uses.\n",
    "\n",
    "This tutorial will teach the basic uses of PySpark, covering the basic data structures, statistics applications, and its Machine Learning Library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "1. Download Spark from http://spark.apache.org/downloads.html\n",
    "2. Unzip the download to this location: ~/spark-2.0.1/\n",
    "3. Follow these instructions in the command line:\n",
    "    \n",
    "```\n",
    "cd ~/spark-2.0.1/\n",
    "\n",
    "brew install sbt\n",
    "\n",
    "sbt assembly\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "1. Create a PySpark Profile for jupyter notebook:\n",
    "```\n",
    "ipython profile create pyspark\n",
    "```\n",
    "Follow the instructions listed here for setup if you encounter any issues: http://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Resilient Distributed Datasets (RDD) are central to Spark's main functionalities. RDDs are distributed collections of items that allow Spark to process data in a distributed fashion, as the name entails. RDDs let you save data on memory and preserve it to the disc if and only if it is required. This has a big role in the speed of Spark's execution engine.\n",
    "\n",
    "There are two ways to create an RDD:\n",
    "\n",
    "1. Parallelize an existing collection of data into an RDD. \n",
    "\n",
    "2. Reference an external dataset from storage.\n",
    "\n",
    "Below you will find an example of both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize Spark Context\n",
    "sc = SparkContext()\n",
    "\n",
    "# Parallelize\n",
    "nums = [1,2,3,4,5,6,7,8,9,10]\n",
    "dist_nums = sc.parallelize(nums)\n",
    "hamlet = sc.textFile(\"hamlet.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to print out the contents of an RDD.\n",
    "\n",
    "    1. Print out a specified number of elements with RDD.take(n)\n",
    "    2. Print out the entire RDD with RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'', u'1604', u'', u'', u'THE TRAGEDY OF HAMLET, PRINCE OF DENMARK', u'', u'', u'by William Shakespeare', u'', u'']\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "print hamlet.take(10)\n",
    "print dist_nums.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After printing the first 10 elements of the hamlet RDD, we see that there are a number of empty strings that represent empty lines, to account for this we can easily filter them out by using ```RDD.filter()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1604',\n",
       " u'THE TRAGEDY OF HAMLET, PRINCE OF DENMARK',\n",
       " u'by William Shakespeare',\n",
       " u'Dramatis Personae',\n",
       " u'  Claudius, King of Denmark.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet = hamlet.filter(lambda x: len(x) > 0)\n",
    "hamlet.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs have operations called actions and transformations.\n",
    "\n",
    "    1. Actions - Operations that return values.\n",
    "    2. Transformations - Operations that create new RDDs.\n",
    "\n",
    "```Filter``` was an example of a Transformation, as it returned a new RDD, with a filtered result. \n",
    "\n",
    "```Take``` and ```Collect``` are examples of actions, as they returned the values of the RDDs.\n",
    "\n",
    "One of the most powerful, yet simple RDD actions is ```RDD.reduce(f)```.\n",
    "\n",
    "```RDD.reduce(f)``` allows us to aggregate the elements of the RDD using a function f. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum:  55\n",
      "product:  3628800\n"
     ]
    }
   ],
   "source": [
    "total = dist_nums.reduce(lambda x, y: x + y)\n",
    "prod  = dist_nums.reduce(lambda x, y: x * y)\n",
    "\n",
    "print \"sum: \", total\n",
    "print \"product: \", prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most powerful RDD transformations is ```RDD.map(f)```.\n",
    "\n",
    "```RDD.map(f)``` returns a new RDD with each element, ```x```, passed through the function ```f```, as ```f(x)```.\n",
    "\n",
    "The best way to understand is by example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD with values doubled:  [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
      "RDD of if even:  [False, True, False, True, False, True, False, True, False, True]\n"
     ]
    }
   ],
   "source": [
    "doubled = dist_nums.map(lambda x: 2*x)\n",
    "isEven  = dist_nums.map(lambda x: x % 2 == 0)\n",
    "\n",
    "print \"RDD with values doubled: \", doubled.collect()\n",
    "print \"RDD of if even: \", isEven.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting ```Map``` and ```Reduce``` together, we have a powerful aggregation tool.\n",
    "\n",
    "Looking at our hamlet text file, let's calculate the total size of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total size:  187271\n"
     ]
    }
   ],
   "source": [
    "total_size = hamlet.map(lambda line: len(line)).reduce(lambda x, y: x + y)\n",
    "\n",
    "print \"total size: \", total_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we mapped the lines to their respective lengths, and then reduced by summing up those lengths.  As you can see, the syntax is simple while the process is powerful.\n",
    "\n",
    "Let's look into some additional useful transformations:\n",
    "\n",
    "1. ```RDD.reduceByKey(f)``` operates on a dataset of key-value pairs, and reduces using the specified function ```f``` to return a new dataset of key-value pairs.\n",
    "\n",
    "2. ```RDD.groupByKey(f)``` operates on a dataset of key-value pairs, and groups by each key, returning a dataset of keys mapped to a collection of their respective grouped values.\n",
    "\n",
    "3. ```RDD.sortByKey(f)```, as the name suggests, orders the RDD by key. You can specify in what order with the (boolean) ascending parameter.\n",
    "\n",
    "4.  ```RDD.sortBy(f)```, orders the RDD by the given function f. You can specify in what order with the (boolean) ascending parameter.\n",
    "\n",
    "#### Using these tranformations, let's figure out which character speaks the most amount of times!\n",
    "\n",
    "Here's an example of a line spoken by Francisco:\n",
    "    \n",
    "    \"  Fran. You come most carefully upon your hour.\"\n",
    "    \n",
    "Looking carefully, we see that each line has two spaces ```\"  \"```, the shortened character name, followed by their line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"  Ber. Who's there.?\",\n",
       " u'  Fran. Nay, answer me. Stand and unfold yourself.',\n",
       " u'  Ber. Long live the King!',\n",
       " u'  Fran. Bernardo?',\n",
       " u'  Ber. He.']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# First filter to only have the lines with actual speech!\n",
    "filtered = hamlet.filter(lambda line: re.match(\"(^  \\w{3,6}\\. )\", line))\n",
    "filtered.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'  Ber.', 1),\n",
       " (u'  Fran.', 1),\n",
       " (u'  Ber.', 1),\n",
       " (u'  Fran.', 1),\n",
       " (u'  Ber.', 1)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, let's map the character name to 1 (so we can add up each occurence):\n",
    "paired = filtered.map(lambda line: (re.match(\"(^  \\w{3,6}\\.)\",line).group(0),1))\n",
    "paired.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'  Ham.', 358), (u'  Hor.', 108), (u'  King.', 102), (u'  Pol.', 86), (u'  Queen.', 69)]\n",
      "[(u'  Ham.', 358), (u'  Hor.', 108), (u'  King.', 102), (u'  Pol.', 86), (u'  Queen.', 69)]\n"
     ]
    }
   ],
   "source": [
    "# Using reduceByKey, let's determine the amount of times each character has spoken:\n",
    "reduced_1 = paired.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# We now expect each element to have this structure: (Character, # of lines)\n",
    "ordered_1 = reduced.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# In another fashion:\n",
    "\n",
    "reduced_2 = paired.countByKey().items()\n",
    "reduced_2.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print ordered_1.take(5)\n",
    "print reduced_2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, we see that Hamlet has spoken the most. Using RDDs made the process in retrieving this information extremely simple, and can do this at scale with enormous amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Important Spark Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'  Ham.', 358), 0), ((u'  Hor.', 108), 1), ((u'  King.', 102), 2), ((u'  Pol.', 86), 3), ((u'  Queen.', 69), 4)]\n",
      "[6, 15, 24]\n"
     ]
    }
   ],
   "source": [
    "# Indexing an RDD\n",
    "indexed = ordered.zipWithIndex()\n",
    "\n",
    "# Partitioning an RDD\n",
    "nums = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "dist_nums = sc.parallelize(nums,3) # 3 Partitions \n",
    "\n",
    "# mapPartitions() allows us to map separately on each partition\n",
    "def f(iterator):\n",
    "    yield sum(iterator)\n",
    "    \n",
    "print indexed.take(5)\n",
    "print dist_nums.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, you can see the ease at which we can index each element within an RDD.\n",
    "\n",
    "We can also see the usefulness in taking advantage of Spark's distributed nature, and apply functions to each partition. Above, we summed up the values in each of the three partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark Machine Learning Library\n",
    "\n",
    "Now that we've learned the basic uses of spark and its data structures, let's get into some more complex tasks. Many of the models that we have applied in class are readily available within Spark's Machine Learning Library. In the below cells, we will learn how to use some basic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this we will look at a more interesting dataset. The dataset we will be using has just one predictor and one response variable.  We will be looking at the relationship between a human's weight and their systolic blood pressure. \n",
    "\n",
    "To prepare for modeling, we must understand a variety of new data types that are introduce with this Library:\n",
    "\n",
    "1. Local Vector\n",
    "2. Labeled Point\n",
    "3. Local Matrix\n",
    "\n",
    "We will show the use of vectors and matrices in the cell below to better understand them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors, Matrix, Matrices\n",
    "\n",
    "# Local Vectors\n",
    "dense_v  = [1.0, 2.0, 3.0]\n",
    "sparse_v = Vectors.sparse(3, [1, 2], [3.0, 4.0])\n",
    "\n",
    "# Local Matrices\n",
    "dense_m  = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "sparse_m = Matrices.sparse(3, 2, [0, 1, 2], [1, 2, 3], [2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific example, weight is our only predictor and systolic blood pressure is our response. Let's start by reading in and structuring the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'165', u'130'], [u'167', u'133'], [u'180', u'150'], [u'155', u'128'], [u'212', u'151'], [u'175', u'146'], [u'190', u'150'], [u'210', u'140'], [u'200', u'148'], [u'149', u'125'], [u'158', u'133'], [u'169', u'135'], [u'170', u'150'], [u'172', u'153'], [u'159', u'128'], [u'168', u'132'], [u'174', u'149'], [u'183', u'158'], [u'215', u'150'], [u'195', u'163'], [u'180', u'156'], [u'143', u'124'], [u'240', u'170'], [u'235', u'165'], [u'192', u'160'], [u'187', u'159']]\n"
     ]
    }
   ],
   "source": [
    "data = sc.textFile(\"weightVsBP.csv\") # Read in data\n",
    "\n",
    "data = data.map(lambda line: line.split(\",\")) # Break into rows\n",
    "\n",
    "header = data.first()\n",
    "sbp = data.filter(lambda line: line != header) # Take out the header from the csv\n",
    "\n",
    "print sbp.collect() # Look at our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ```LabeledPoint``` is a combination of a label/response and a local vector which can be either dense or sparse. The label/response will be the value of interest that the model will look to predict, whereas the values within the vectors are the predictors. LabeledPoints are heavily used by the learning algorithms within Spark's Machine Learning Library (MLlib).\n",
    "\n",
    "Now that we know what our data looks like, we will create our LabeledPoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors:  [1.0,2.0,3.0,4.0]\n",
      "Response:  0.0\n",
      "[LabeledPoint(1.0, [10.0]), LabeledPoint(2.0, [20.0]), LabeledPoint(5.0, [33.0])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Creating just one LabeledPoint as follows\n",
    "practice_LP = LabeledPoint(0,[1,2,3,4])   # dense\n",
    "practice_LP_2 = LabeledPoint(4, sparse_v) # sparse\n",
    "\n",
    "# The predictors can be extracted as follows\n",
    "predictors  = practice_LP.features\n",
    "response    = practice_LP.label\n",
    "print \"Predictors: \", predictors\n",
    "print \"Response: \", response\n",
    "\n",
    "# Aggregate across the entire dataset\n",
    "lps = data.map(lambda row: LabeledPoint(row[0],[row[1]]))\n",
    "\n",
    "print lps.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an RDD of LabeledPoints and everything we need to start making some simple predictions through Spark's MLlib. Let's start by using a Linear Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (Actual, Prediction) [(165.0, 163.0348896074737), (167.0, 166.79723321380004), (180.0, 188.11718031631582), (155.0, 160.5266605365895), (212.0, 189.37129485175794)] \n",
      "\n",
      "Mean Squared Error:  241.765928817\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "\n",
    "# Create the model\n",
    "model = LinearRegressionWithSGD.train(lps, iterations=100, step=0.0001)\n",
    "\n",
    "# Predict\n",
    "predictions = lps.map(lambda p: (p.label, float(model.predict(p.features))))\n",
    "print \"(Actual, Prediction)\", predictions.take(5), \"\\n\"\n",
    "\n",
    "# Calculate the MSE\n",
    "square = predictions.map(lambda (v, p): (v - p)**2)\n",
    "summed  = squared.reduce(lambda x, y: x + y)\n",
    "MSE = summed / predictions.count()\n",
    "\n",
    "print \"Mean Squared Error: \", MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have very simply and successfully created a Linear Regression model with Spark. LabeledPoints help simplify the process immensely, especially when there is a large amount of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "In the above linear regression model, we calculated the Mean Squared error by hand using ```map``` and ```reduce```. Spark also provides something called Evaluation Metrics, where these types of metrics are calculated for us.\n",
    "\n",
    "The following metrics are available to be calculated using the Linear Regression Model we created above:\n",
    "\n",
    "1. Mean Squared Error (MSE)\n",
    "2. Root Mean Squared Error (RMSE)\n",
    "3. Mean Absolute Error (MAE)\n",
    "4. Coefficient of Determination (R2)\n",
    "5. Explained Variance\n",
    "\n",
    "These are executed below using Evaluation Metrics. The necessary input to the metrics object is an RDD of (label,predictions) as we made above in the variable ```predictions```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 241.765928817\n",
      "RMSE: 15.5488240332\n",
      "R-squared: 0.112284688151\n",
      "MAE: 12.4803510021\n",
      "Explained variance: 588.974530086\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# Create metrics object\n",
    "metrics = RegressionMetrics(predictions)\n",
    "\n",
    "print \"MSE:\", metrics.meanSquaredError\n",
    "print \"RMSE:\", metrics.rootMeanSquaredError\n",
    "print \"R-squared:\", metrics.r2\n",
    "print \"MAE:\", metrics.meanAbsoluteError\n",
    "print \"Explained variance:\", metrics.explainedVariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Statistics on RDDs with MLlib\n",
    "\n",
    "Spark's Machine Learning Library also provides an API for calculating statistics with RDDs. Let's take a look at the possible options we have.\n",
    "\n",
    "#### Column Statistics:\n",
    "\n",
    "```.colStats()``` provides various statistics for the columns of a matrix. This matrix can be represented by an RDD. Let's revisit our dataset with systolic blood pressure and calculate some interesting statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum:  [ 240.  170.]\n",
      "Minimum:  [ 143.  124.]\n",
      "Mean:  [ 182.42307692  145.61538462]\n",
      "Variance:  [ 612.49384615  180.08615385]\n",
      "Amount of Nonzeroes:  [ 26.  26.]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "matrix = sbp # same data that was used above\n",
    "\n",
    "summary = Statistics.colStats(matrix)\n",
    "\n",
    "print \"Maximum: \", summary.max()\n",
    "print \"Minimum: \", summary.min()\n",
    "print \"Mean: \", summary.mean()\n",
    "print \"Variance: \", summary.variance()\n",
    "print \"Amount of Nonzeroes: \", summary.numNonzeros()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Correlation:\n",
    "\n",
    "Here we will look at how we can calculate the correlation between two sequences of data.\n",
    "\n",
    "```.corr()``` is the provided function to calculate correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation:  0.773490300531 \n",
      "\n",
      "Correlation Matrix:\n",
      "[[ 1.          0.97888347  0.99038957]\n",
      " [ 0.97888347  1.          0.99774832]\n",
      " [ 0.99038957  0.99774832  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "weight = sbp.map(lambda x: x[0])\n",
    "blood_pressure = sbp.map(lambda x: x[1])\n",
    "\n",
    "print \"Correlation: \", Statistics.corr(weight, blood_pressure, method=\"pearson\"), \"\\n\"\n",
    "\n",
    "# If we had an RDD of vectors, .corr() would calculate the correlation between each.\n",
    "\n",
    "mat = sc.parallelize(\n",
    "    [[1.0, 10.0, 100.0], \n",
    "     [2.0, 20.0, 200.0], \n",
    "     [5.0, 33.0, 366.0]])\n",
    "\n",
    "print \"Correlation Matrix:\" \n",
    "print Statistics.corr(mat, method=\"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Real World Example\n",
    "\n",
    "For this example, we will be working with a dataset that contains every single shot Kobe Bryant (Los Angeles Lakers Basketball Player) took throughout his career. Each shot has a number of attributes including shot type, distance, location, time remaining, opponent, and more. We will use this data to predict whether or not his shots went in! Now, let's put together the concepts we have learned so far and make some predictions.\n",
    "\n",
    "This is clearly a classification problem, and for this reason we will use Spark's Logistic Regression Model.  Yet, since this is a real-word dataset, the data requires a bit of cleaning and structuring to prepare for the model. Below, I have simply converted each string categorical variable to number format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row from pre-cleaned dataset:  [[u'Jump Shot', u'Left Side(L)', u'8-16 ft.', u'10', u'2000-01', u'0', u'0']]\n",
      "Row from new dataset:  [[43, 2, 0, u'10', 6, u'0', u'0']]\n"
     ]
    }
   ],
   "source": [
    "k_data = sc.textFile(\"kobe-shots.csv\") # Read in data\n",
    "k_data = k_data.map(lambda line: line.split(\",\")) # Break into rows\n",
    "\n",
    "header = k_data.first()\n",
    "k_data = k_data.filter(lambda line: line != header) # Take out the header from the csv\n",
    "k_data = k_data.map(lambda row: row[:7])\n",
    "\n",
    "# Distinct values in each attribute\n",
    "\n",
    "distinct_shot_type = k_data.map(lambda x: x[0]).distinct()\n",
    "distinct_area = k_data.map(lambda x: x[1]).distinct()\n",
    "distinct_distance = k_data.map(lambda x: x[2]).distinct()\n",
    "distinct_season = k_data.map(lambda x: x[4]).distinct()\n",
    "\n",
    "# Create a dictionary of attributes mapped to their category number\n",
    "\n",
    "shot_type_map = dict(distinct_shot_type.zipWithIndex().collect()) # Remember zipWithIndex?\n",
    "area_map = dict(distinct_area.zipWithIndex().collect())\n",
    "distance_map = dict(distinct_distance.zipWithIndex().collect())\n",
    "season_map = dict(distinct_season.zipWithIndex().collect())\n",
    "\n",
    "kobe = k_data.map(lambda row: [shot_type_map[row[0]], area_map[row[1]], distance_map[row[2]], \n",
    "                                 row[3], season_map[row[4]], row[5], row[6]])\n",
    "\n",
    "print \"Row from pre-cleaned dataset: \", k_data.take(1)\n",
    "print \"Row from new dataset: \", kobe.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all of the categorical variables are indexed and ready to be given to the model. As we did with our Linear Regression Model, we will create our RDD of LabeledPoints, train the model, and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "labeled_data = kobe.map(lambda row: LabeledPoint(row[6],row[:6]))\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(labeled_data)\n",
    "\n",
    "predictions = labeled_data.map(lambda p: (float(p.label), float(model.predict(p.features))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've made our predictions, let's calculate our metrics. Let's put the skills that we've learned thus far to use:\n",
    "\n",
    "1. Calculate the training error using RDD actions and transformations.\n",
    "2. Calculate metrics using MLlib's Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error:  0.375218897148\n",
      "Area under precision-recall curve:  0.557574019457\n",
      "Area under ROC curve:  0.626559120217\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "trainingError = preds.filter(lambda (val, pred): val != pred).count() / float(labeled_data.count())\n",
    "\n",
    "metrics = BinaryClassificationMetrics(predictions) # Metrics Object\n",
    "\n",
    "print \"Training Error: \", trainingError\n",
    "print \"Area under precision-recall curve: \", metrics.areaUnderPR\n",
    "print \"Area under ROC curve: \", metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This real world example gives a great idea of how Spark can make complex tasks rather simple to accomplish. Spark's powerful RDD actions, RDD transformations, LabeledPoints, models, and evaluation metrics all came to fruition in this real-world example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Now that we have seen the various data structures, data types, applications, and uses of PySpark, you should feel confident applying these skills to more complicated tasks! The Linear and Logisitic Regression Models provided in this notebook were simple to help in the understanding of Spark's applications as a whole, and more complicated ones shouldn't be all that much harder to implement. With the basics down, you are prepared to start processing massive amounts of data and providing meaningful information from that data using Spark and Spark MLlib!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
