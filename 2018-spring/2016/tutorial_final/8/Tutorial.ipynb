{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>How To Retrieve Unstructred Web Data In a Structured Manner with Riko</center>\n",
    "## <center>A Riveting 15-688 Tutorial</center><br/><center>*by* Ahmet Emre Unal (ahmetemu)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have heard about [Google Reader](https://en.wikipedia.org/wiki/Google_Reader). It was a free [RSS](https://en.wikipedia.org/wiki/RSS) reader that brought RSS reading to the masses. It was a great product and I, personally, was a very heavy user. Google Reader allowed me to follow many websites that publish things infrequently. This, though, was only possible through the RSS feeds published by the websites.\n",
    "\n",
    "It's great when a website admin takes the time to create the necessary RSS feeds (or implement the tool that does it) but every so often, you come across websites that you want to follow but don't have an RSS feed. How can you now make use of this beautiful system? Can you somehow parse the plain HTML web page to retrieve data in an ordered fashion?\n",
    "\n",
    "[The Riko library](https://github.com/nerevu/riko/) is a library that allows you to do exactly that. By using Riko, we can parse the plain HTML of a website and retrieve the elements in a website in an orderly fashion, like iterating through ```<li>``` elements with a for-loop, for example. \n",
    "\n",
    "I personally believe in walking through examples to learn something so let's jump right in (If you would like to follow along, you can [install Riko](https://github.com/nerevu/riko/blob/master/README.rst#installation) on your local environment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from riko.collections.sync import SyncPipe\n",
    "\n",
    "def get_test_site_url(test_site_name):\n",
    "    return 'file://' + os.getcwd() + '/test_sites/' + test_site_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "#\n",
    "# Note: You can use the following section to create the test sites' files:\n",
    "#\n",
    "##########################################################################################\n",
    "\n",
    "test_site_1_contents = '''<!DOCTYPE html>\\n<html>\\n<body>\\n\\n<h4>This is a simple example</h4>\n",
    "<div class=\"container\">\\n    <ul>\\n      <li class=\"drink hot\">Coffee</li>\n",
    "      <li class=\"drink hot\">Green Tea</li>\\n      <li class=\"hot drink\">Black Tea</li>\n",
    "      <li class=\"drink cold\">Milk</li>\\n      <li class=\"food\">Chocolate</li>\n",
    "      <li class=\"food\">Marshmallow</li>\n",
    "    </ul>\\n</div>\\n\\n</body>\\n</html>\\n'''\n",
    "\n",
    "test_site_2_contents = '''<!DOCTYPE html>\\n<html>\\n<body>\\n\\n<h4>This is a slightly more complex example</h4>\n",
    "<div class=\"container\">\\n    <ul>\\n      <li class=\"drink hot\">Coffee</li>\n",
    "      <li class=\"drink hot\">Green Tea\\n          <p>Oolong Tea</p>\n",
    "          <a href=\"https://en.wikipedia.org/wiki/Oolong\"></a>\\n      </li>\\n      <li class=\"hot drink\">Black Tea\n",
    "          <p>Rize Tea</p>\\n          <a href=\"https://en.wikipedia.org/wiki/Rize_Tea\"></a>\\n      </li>\n",
    "      <li class=\"drink cold\">Milk</li>\\n      <li class=\"food\">Chocolate</li>\n",
    "      <li class=\"food\">Marshmallow</li>\\n    </ul>\\n</div>\\n\\n</body>\\n</html>\\n'''\n",
    "\n",
    "# You can use the following functions to create the test sites' files:\n",
    "\n",
    "path = os.getcwd() + '/test_sites/'\n",
    "\n",
    "# Check if 'test_sites' folder exists\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)  # Create the 'test_sites' folder\n",
    "    \n",
    "# Check if 'test1.html' file exists\n",
    "if not os.path.exists(path + 'test1.html'):\n",
    "    with open(path + 'test1.html', \"w\") as test_site_1:\n",
    "        test_site_1.write(test_site_1_contents)\n",
    "    \n",
    "# Check if 'test2.html' file exists\n",
    "if not os.path.exists(path + 'test2.html'):\n",
    "    with open(path + 'test2.html', \"w\") as test_site_2:\n",
    "        test_site_2.write(test_site_2_contents)\n",
    "\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ```test_sites``` folder, you will find some number of HTML files that are simple website examples. The first one, ```test1.html```, is as follows:\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<h4>This is a simple example</h4>\n",
    "<div class=\"container\">\n",
    "    <ul>\n",
    "      <li class=\"drink hot\">Coffee</li>\n",
    "      <li class=\"drink hot\">Green Tea</li>\n",
    "      <li class=\"hot drink\">Black Tea</li>\n",
    "      <li class=\"drink cold\">Milk</li>\n",
    "      <li class=\"food\">Chocolate</li>\n",
    "      <li class=\"food\">Marshmallow</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "Riko sees things through what's called a 'pipe'. By fetching a webpage through a URL and pointing Riko to the appropriate part of said webpage, we can obtain 'streams' coming from those 'pipe's that can be iterated. Let's start with a very simple act of retrieveing the webpage in its entirety. We can achieve this with the very simple [```fetchpage```](https://github.com/nerevu/riko/blob/master/riko/modules/fetchpage.py) module, which will literally just fetch a page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = get_test_site_url('test1.html')          # The URL of our test website\n",
    "fetch_conf = {'url': url}                      # A configuration dictionary for Riko\n",
    "pipe = SyncPipe('fetchpage', conf=fetch_conf)  # A pipe that streams 'test1.html'\n",
    "stream = pipe.output                           # The stream being output from the pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did was to tell Riko to create a synchronous pipe (using the [SyncPipe class](https://github.com/nerevu/riko/blob/master/riko/collections/sync.py)) that uses the webpage fetching module (called ```fetchpage```) to fetch the URL specified in the ```fetch_conf``` configuration dictionary. \n",
    "\n",
    "We could've created the stream driectly by simply using the ```fetchpage``` module directly:\n",
    "```python\n",
    "from riko.modules import fetchpage\n",
    "stream = fetchpage.pipe(conf=fetch_conf)\n",
    "```\n",
    "but we'll see in a bit why we're using the ```SyncPipe``` class.\n",
    "\n",
    "You might've wondered when did Riko even have the time to go fetch the page? Well, pipes in Riko are *lazy*. That means it won't start fetching (or processing) a URL before we start iterating. So let's iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'content': '<!DOCTYPE html>\\n\\n<html>\\n\\n<body>\\n\\n\\n\\n<h4>This is a simple example</h4>\\n\\n<div class=\"container\">\\n\\n    <ul>\\n\\n      <li class=\"drink hot\">Coffee</li>\\n\\n      <li class=\"drink hot\">Green Tea</li>\\n\\n      <li class=\"hot drink\">Black Tea</li>\\n\\n      <li class=\"drink cold\">Milk</li>\\n\\n      <li class=\"food\">Chocolate</li>\\n\\n      <li class=\"food\">Marshmallow</li>\\n\\n    </ul>\\n\\n</div>\\n\\n\\n\\n</body>\\n\\n</html>\\n'}\n"
     ]
    }
   ],
   "source": [
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I told you it would literally just fetch the entire page:\n",
    "```python\n",
    "{u'content': '<!DOCTYPE html>\\n\\n<html>\\n\\n<body>\\n\\n\\n\\n<h4>This is a simple example</h4>\\n\\n<div class=\"container\">\\n\\n    <ul>\\n\\n      <li class=\"drink hot\">Coffee</li>\\n\\n      <li class=\"drink hot\">Green Tea</li>\\n\\n      <li class=\"hot drink\">Black Tea</li>\\n\\n      <li class=\"drink cold\">Milk</li>\\n\\n      <li class=\"food\">Chocolate</li>\\n\\n      <li class=\"food\">Marshmallow</li>\\n\\n    </ul>\\n\\n</div>\\n\\n\\n\\n</body>\\n\\n</html>\\n\\n\\n'}\n",
    "```\n",
    "The whole webpage being printed is not really that useful; there is nothing special about this. We could've at least specified a start and end tag for Riko to fetch only that part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'content': '\\n\\n\\n\\n<h4>This is a simple example</h4>\\n\\n<div class=\"container\">\\n\\n    <ul>\\n\\n      <li class=\"drink hot\">Coffee</li>\\n\\n      <li class=\"drink hot\">Green Tea</li>\\n\\n      <li class=\"hot drink\">Black Tea</li>\\n\\n      <li class=\"drink cold\">Milk</li>\\n\\n      <li class=\"food\">Chocolate</li>\\n\\n      <li class=\"food\">Marshmallow</li>\\n\\n    </ul>\\n\\n</div>\\n\\n\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "fetch_conf = {   # The same config as above, but with the start and end tags to fetch specified\n",
    "    'url': url,\n",
    "    'start': '<body>',\n",
    "    'end': '</body>'\n",
    "}\n",
    "pipe = SyncPipe('fetchpage', conf=fetch_conf)  # A pipe that streams 'test1.html' according to the config above\n",
    "stream = pipe.output                           # The stream being output from the pipe\n",
    "\n",
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't very useful either, honestly:\n",
    "```python\n",
    "{u'content': '\\n\\n\\n\\n<h4>This is a simple example</h4>\\n\\n<div class=\"container\">\\n\\n    <ul>\\n\\n      <li class=\"drink hot\">Coffee</li>\\n\\n      <li class=\"drink hot\">Green Tea</li>\\n\\n      <li class=\"hot drink\">Black Tea</li>\\n\\n      <li class=\"drink cold\">Milk</li>\\n\\n      <li class=\"food\">Chocolate</li>\\n\\n      <li class=\"food\">Marshmallow</li>\\n\\n    </ul>\\n\\n</div>\\n\\n\\n\\n'}\n",
    "```\n",
    "To get to the list items we want, we'd need to do some weird string processing. We don't want to do that and that's why we have Riko!\n",
    "***\n",
    "Let's take a side step and ask ourselves a question: a URL is a string that points to a webpage (or a file in the filesystem), but what could point to an element *inside* a webpage? The answer is [XPath](https://en.wikipedia.org/wiki/XPath). 'XPath' is very similar to a URL, only that it denotes a path inside a markup file. For example, the XPath of the ```<ul>``` element in the website structure above is ```/html/body/div/ul```. In turn, each ```<li>``` element under that ```<ul>``` element could be pointed to using the XPath ```/html/body/div/ul/li[<index>]```, where ```<index>``` is the 1-based index (index = 1 is the first element) or all ```<li>``` elements with the XPath ```/html/body/div/ul/li```.\n",
    "***\n",
    "Riko has an alternate module called [```xpathfetchpage```](https://github.com/nerevu/riko/blob/master/riko/modules/xpathfetchpage.py) that can take a URL, as well as an XPath, and can pipe the element pointed by that XPath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'{http://www.w3.org/1999/xhtml}li': [{u'content': u'Coffee', u'class': u'drink hot'}, {u'content': u'Green Tea', u'class': u'drink hot'}, {u'content': u'Black Tea', u'class': u'hot drink'}, {u'content': u'Milk', u'class': u'drink cold'}, {u'content': u'Chocolate', u'class': u'food'}, {u'content': u'Marshmallow', u'class': u'food'}]}\n"
     ]
    }
   ],
   "source": [
    "xpath = '/html/body/div/ul'                         # The XPath of the <ul> element\n",
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside 'test1.html'\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, now this seems interesting:\n",
    "```python\n",
    "{u'{http://www.w3.org/1999/xhtml}li': [{u'content': u'Coffee', u'class': u'drink hot'}, {u'content': u'Green Tea', u'class': u'drink hot'}, {u'content': u'Black Tea', u'class': u'hot drink'}, {u'content': u'Milk', u'class': u'drink cold'}, {u'content': u'Chocolate', u'class': u'food'}, {u'content': u'Marshmallow', u'class': u'food'}]}\n",
    "```\n",
    "The pipe seems to have retrieved a dictionary with a single key, ```u'{http://www.w3.org/1999/xhtml}li'``` (weird key, I know), which points to a list of dictionaries, like ```{u'content': u'Coffee', u'class': u'drink hot'}```, that look eerily similar to our list elements! But it's still tedious at this point to unwrap that outer dictionary. Let's try pointing Riko to an XPath that matches all multiple `<li>` elements, which is ```/html/body/div/ul/li```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'content': u'Coffee', u'class': u'drink hot'}\n",
      "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
      "{u'content': u'Black Tea', u'class': u'hot drink'}\n",
      "{u'content': u'Milk', u'class': u'drink cold'}\n",
      "{u'content': u'Chocolate', u'class': u'food'}\n",
      "{u'content': u'Marshmallow', u'class': u'food'}\n"
     ]
    }
   ],
   "source": [
    "xpath = '/html/body/div/ul/li'                      # The XPath of the <li> element(s)\n",
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside 'test1.html'\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're talking:\n",
    "```python\n",
    "{u'content': u'Coffee', u'class': u'drink hot'}\n",
    "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
    "{u'content': u'Black Tea', u'class': u'hot drink'}\n",
    "{u'content': u'Milk', u'class': u'drink cold'}\n",
    "{u'content': u'Chocolate', u'class': u'food'}\n",
    "{u'content': u'Marshmallow', u'class': u'food'}\n",
    "```\n",
    "We have retrieved each ```<li>``` element as a seperate item through the stream we created.\n",
    "\n",
    "As mentioned above, we could've retrieved a specific ```<li>``` element by specifying its index on the XPath; adding '`[1]`' to the end of the XPath above will return:\n",
    "```python\n",
    "{u'content': u'Coffee', u'class': u'drink hot'}\n",
    "```\n",
    "\n",
    "Let's say we are only interested in the drinks. How do we only get the drinks? Do we check for and do some weird string matching with the ```class``` of each element *while* iterating over the stream elements and only add ones that match our criteria? Nope!\n",
    "\n",
    "The point of having streams and pipes is to *filter* the streams and prevent the unwanted objects from going through the stream in the first place. Riko has a way to filter streams, by using the very handy [```filter```](https://github.com/nerevu/riko/blob/master/riko/modules/filter.py) pipe module. The gist of thinking in Riko's terms is to think of chaining pipes together. The first pipe will be carrying a flow of ```<li>``` elements we pointed to. The second pipe, the ```filter``` pipe, will only let through elements that match a certain criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'content': u'Coffee', u'class': u'drink hot'}\n",
      "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
      "{u'content': u'Black Tea', u'class': u'hot drink'}\n",
      "{u'content': u'Milk', u'class': u'drink cold'}\n"
     ]
    }
   ],
   "source": [
    "url = get_test_site_url('test1.html')               # The URL of our test website\n",
    "xpath = '/html/body/div/ul/li'                      # The XPath of the <li> element(s)\n",
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside 'test1.html'\n",
    "filter_rule = {                                     # A 'filter' rule that tells the 'filter'\n",
    "    'field': 'class',                               # pipe to perform the 'contains' operation on the 'class'\n",
    "    'op': 'contains',                               # field, to check wether the value 'drink' exists, and\n",
    "    'value': 'drink'                                # only let through the items that do match the rule\n",
    "}\n",
    "filter_conf = {'rule': filter_rule}                 # The 'filter' pipe configuration created from the rule\n",
    "pipe = pipe.filter(conf=filter_conf)                # A chained pipe that filters acording to the configuration\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting really cool:\n",
    "```python\n",
    "{u'content': u'Coffee', u'class': u'drink hot'}\n",
    "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
    "{u'content': u'Black Tea', u'class': u'hot drink'}\n",
    "{u'content': u'Milk', u'class': u'drink cold'}\n",
    "```\n",
    "We seemed to have retrieved all the drinks, and only the drinks! A similar operation can be performed to only retrieve the hot drinks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'content': u'Coffee', u'class': u'drink hot'}\n",
      "{u'content': u'Green Tea', u'class': u'drink hot'}\n"
     ]
    }
   ],
   "source": [
    "url = get_test_site_url('test1.html')               # The URL of our test website\n",
    "xpath = '/html/body/div/ul/li'                      # The XPath of the <li> element(s)\n",
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside 'test1.html'\n",
    "filter_rule = {                                     # A 'filter' rule that tells the 'filter'\n",
    "    'field': 'class',                               # pipe to perform the 'contains' operation on the 'class'\n",
    "    'op': 'contains',                               # field, to check whether the value 'drink hot' exists, and\n",
    "    'value': 'drink hot'                            # only let through the items that do match the rule\n",
    "}\n",
    "filter_conf = {'rule': filter_rule}                 # The 'filter' pipe configuration created from the rule\n",
    "pipe = pipe.filter(conf=filter_conf)                # A chained pipe that filters acording to the configuration\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this even cooler:\n",
    "```python\n",
    "{u'content': u'Coffee', u'class': u'drink hot'}\n",
    "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
    "```\n",
    "but it seems like we have a problem: the fact that the '```value```' key in the rule above has a '```drink hot```' value means that it's not matching an ```<li>``` element with the class '```hot drink```', which is perfectly valid and equal to the class '```drink hot```'. It looks like having a long, more specific value can get pretty unwieldy. It seems to me like it would make more sense if we could apply shorter, more general, *multiple* rules to the ```filter``` pipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'content': u'Coffee', u'class': u'drink hot'}\n",
      "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
      "{u'content': u'Black Tea', u'class': u'hot drink'}\n"
     ]
    }
   ],
   "source": [
    "url = get_test_site_url('test1.html')               # The URL of our test website\n",
    "xpath = '/html/body/div/ul/li'                      # The XPath of the <li> element(s)\n",
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside 'test1.html'\n",
    "filter_rule_drink = {                               # A 'filter' rule that tells the 'filter'\n",
    "    'field': 'class',                               # pipe to perform the 'contains' operation on the 'class'\n",
    "    'op': 'contains',                               # field, to check whether the value 'drink' exists, and\n",
    "    'value': 'drink'                                # only let through the items that do match the rule\n",
    "}\n",
    "filter_rule_hot = {                                 # A 'filter' rule that tells the 'filter'\n",
    "    'field': 'class',                               # pipe to perform the 'contains' operation on the 'class'\n",
    "    'op': 'contains',                               # field, to check whether the value 'hot' exists, and\n",
    "    'value': 'hot'                                  # only let through the items that do match the rule\n",
    "}\n",
    "filter_conf = {                                     # The 'filter' pipe configuration created from the two\n",
    "    'rule': [filter_rule_drink, filter_rule_hot]    # rules specified above\n",
    "}\n",
    "pipe = pipe.filter(conf=filter_conf)                # A chained pipe that filters acording to the configuration\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you heard? They're saying you're the coolest kid on the block:\n",
    "```python\n",
    "{u'content': u'Coffee', u'class': u'drink hot'}\n",
    "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
    "{u'content': u'Black Tea', u'class': u'drink hot'}\n",
    "```\n",
    "It seems to be pretty clear how you can apply different filters to get the elements you want. You can use the filter pipe to filter based on the content as well, to, for example, print only the teas:\n",
    "```python\n",
    "filter_rule = {          # A 'filter' rule that tells the 'filter'\n",
    "    'field': 'content',  # pipe to perform the 'contains' operation on the 'content'\n",
    "    'op': 'contains',    # field, to check whether the value 'tea' exists, and\n",
    "    'value': 'tea'       # only let through the items that do match the rule\n",
    "}\n",
    "```\n",
    "which, when used in the ways above, would print:\n",
    "```python\n",
    "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
    "{u'content': u'Black Tea', u'class': u'hot drink'}\n",
    "```\n",
    "You can notice that the rule was applied case-insensitively.\n",
    "***\n",
    "Through all of these streams, you can use the items, which are plain old Python objects, in any way you want. You can go ahead and print the list of hot drinks you have with the following for-loop:\n",
    "```python\n",
    "for item in stream:\n",
    "    print item['content']  # 'item' object is a regular Python dictionary\n",
    "```\n",
    "which would print:\n",
    "```\n",
    "Coffee\n",
    "Green Tea\n",
    "Black Tea\n",
    "```\n",
    "***\n",
    "Let's look at the following, more complicated webpage structure, which is `test2.html`:\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<h4>This is a slightly more complex example</h4>\n",
    "<div class=\"container\">\n",
    "    <ul>\n",
    "      <li class=\"drink hot\">Coffee</li>\n",
    "      <li class=\"drink hot\">Green Tea\n",
    "          <p>Oolong Tea</p>\n",
    "          <a href=\"https://en.wikipedia.org/wiki/Oolong\"></a>\n",
    "      </li>\n",
    "      <li class=\"hot drink\">Black Tea\n",
    "          <p>Rize Tea</p>\n",
    "          <a href=\"https://en.wikipedia.org/wiki/Rize_Tea\"></a>\n",
    "      </li>\n",
    "      <li class=\"drink cold\">Milk</li>\n",
    "      <li class=\"food\">Chocolate</li>\n",
    "      <li class=\"food\">Marshmallow</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "How would we access the URLs nested under the teas in the list? If you thought of 'XPath', you can congratulate yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Oolong\n",
      "https://en.wikipedia.org/wiki/Rize_Tea\n"
     ]
    }
   ],
   "source": [
    "url = get_test_site_url('test2.html')               # The URL of our test website\n",
    "xpath = '/html/body/div/ul/li/a'                    # The XPath of the <a> element(s)\n",
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside 'test2.html'\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in stream:\n",
    "    print item['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we got both of the URLs:\n",
    "```\n",
    "https://en.wikipedia.org/wiki/Oolong\n",
    "https://en.wikipedia.org/wiki/Rize_Tea\n",
    "```\n",
    "Notice how Riko didn't raise an error for ```<li>``` tags that lack ```<a>``` tags underneath them. This is because the XPath only matches those that do have the ```<a>``` tags. This is very handy for unstructured web data, where some tags might have nested elements, while some might not.\n",
    "***\n",
    "Finally, let's apply what we learned to a real world example: a prominent Turkish writer by the name 'Yılmaz Özdil' publishes an article every day on the newspaper 'Sözcü', talking about the current affairs of Turkey. The newspaper lists his articles under the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.sozcu.com.tr/kategori/yazarlar/yilmaz-ozdil/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this page, you can see a list of article titles (that link to the articles themselves), along with the date it was published. The XPath of the list elements are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xpath = '/html/body/div[5]/div[6]/div[3]/div[1]/div[2]/div[1]/div[1]/div[2]/ul/li/a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's go ahead and set up a pipe to fetch these list entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'href': u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/ilelebet-payidar-2-1477851/', u'{http://www.w3.org/1999/xhtml}p': u'\\u0130lelebet payidar', u'{http://www.w3.org/1999/xhtml}span': {u'content': u'30 Ekim 2016', u'class': u'date'}, u'title': u'\\u0130lelebet payidar'}\n",
      "\n",
      "{u'href': u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/cumhuriyet-mucizedir-1475895/', u'{http://www.w3.org/1999/xhtml}p': u'Cumhuriyet, mucizedir', u'{http://www.w3.org/1999/xhtml}span': {u'content': u'29 Ekim 2016', u'class': u'date'}, u'title': u'Cumhuriyet, mucizedir'}\n",
      "\n",
      "{u'href': u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/yarin-bayram-1473877/', u'{http://www.w3.org/1999/xhtml}p': u'Yar\\u0131n bayram...', u'{http://www.w3.org/1999/xhtml}span': {u'content': u'28 Ekim 2016', u'class': u'date'}, u'title': u'Yar\\u0131n bayram...'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside the web page\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in itertools.islice(stream, 3):            # itertools.islice will allow us to get only\n",
    "    print item                                      # the first n elements, which is 3 in this case\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It seems like we retrieved the first 3 articles (the exact articles you retrieve will be different when ran on a different day):\n",
    "```python\n",
    "{u'href': u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/ilelebet-payidar-2-1477851/', u'{http://www.w3.org/1999/xhtml}p': u'\\u0130lelebet payidar', u'{http://www.w3.org/1999/xhtml}span': {u'content': u'30 Ekim 2016', u'class': u'date'}, u'title': u'\\u0130lelebet payidar'}\n",
    "\n",
    "{u'href': u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/cumhuriyet-mucizedir-1475895/', u'{http://www.w3.org/1999/xhtml}p': u'Cumhuriyet, mucizedir', u'{http://www.w3.org/1999/xhtml}span': {u'content': u'29 Ekim 2016', u'class': u'date'}, u'title': u'Cumhuriyet, mucizedir'}\n",
    "\n",
    "{u'href': u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/yarin-bayram-1473877/', u'{http://www.w3.org/1999/xhtml}p': u'Yar\\u0131n bayram...', u'{http://www.w3.org/1999/xhtml}span': {u'content': u'28 Ekim 2016', u'class': u'date'}, u'title': u'Yar\\u0131n bayram...'}\n",
    "```\n",
    "You can notice that each element has a title, a URL and a date. Let's say that we want to parse all of this and return it as a list of tuples, where each entry is of form: `(title, date, url)`. We can do this the old fashioned way, where we iterate through each of those dictionaries and get the data we want. Instead, let's do something a bit different: let's set up two pipes for two different XPaths, and iterate through them synchronously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'\\u0130lelebet payidar', u'30 Ekim 2016', u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/ilelebet-payidar-2-1477851/')\n",
      "\n",
      "(u'Cumhuriyet, mucizedir', u'29 Ekim 2016', u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/cumhuriyet-mucizedir-1475895/')\n",
      "\n",
      "(u'Yar\\u0131n bayram...', u'28 Ekim 2016', u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/yarin-bayram-1473877/')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top-level <a> elements stream\n",
    "xpath_conf_top = {'xpath': xpath, 'url': url}                   # The XPath config. for the top-level <a> elements\n",
    "pipe_top = SyncPipe('xpathfetchpage', conf=xpath_conf_top)      # A pipe that streams the top-level <a> elements \n",
    "stream_top = pipe_top.output                                    # The stream being output from the pipe\n",
    "  \n",
    "# The child <span> element stream\n",
    "xpath_date = xpath + '/span'                                    # XPath of the <span> children\n",
    "xpath_conf_date = {'xpath': xpath_date, 'url': url}             # The XPath config. for the top-level <a> elements\n",
    "pipe_date = SyncPipe('xpathfetchpage', conf=xpath_conf_date)    # A pipe that streams the top-level <a> elements \n",
    "stream_date = pipe_date.output                                  # The stream being output from the pipe\n",
    "\n",
    "sync_iterator = zip(stream_top, stream_date)                    # Create a synchronous iterator from the two pipes\n",
    "for top_item, date_item in itertools.islice(sync_iterator, 3):  # itertools.islice will allow us to get only\n",
    "    article = (top_item['title'],                               # the first n elements, which is 3 in this case\n",
    "               date_item['content'], \n",
    "               top_item['href'])\n",
    "    print article\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is awesome!\n",
    "```python\n",
    "(u'\\u0130lelebet payidar', u'30 Ekim 2016', u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/ilelebet-payidar-2-1477851/')\n",
    "\n",
    "(u'Cumhuriyet, mucizedir', u'29 Ekim 2016', u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/cumhuriyet-mucizedir-1475895/')\n",
    "\n",
    "(u'Yar\\u0131n bayram...', u'28 Ekim 2016', u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/yarin-bayram-1473877/')\n",
    "```\n",
    "You can go to the website and see the list elements for yourself. For a website that is mostly auto-generated (disastrously, might I say), this was relatively easy to achieve!\n",
    "\n",
    "Let's look at one last example: let's fetch this list and dynamically fetch the articles it points to and get the full article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'\\u0130lelebet payidar', u\"17 Kas\\u0131m 1938.\\n*\\nMaalesef, izdihamdan dalgalanma oldu, durun ittirmeyin demeye kalmad\\u0131, giri\\u015f kap\\u0131s\\u0131n\\u0131n \\xf6n\\xfcnde, saat kulesinin \\xe7evresinde \\xe7\\u0131\\u011fl\\u0131klar y\\xfckseldi, atl\\u0131 polisler arkadan y\\xfcklenen kalabal\\u0131\\u011f\\u0131 da\\u011f\\u0131tana kadar i\\u015f i\\u015ften ge\\xe7ti, insanlar s\\u0131k\\u0131\\u015ft\\u0131, ezildi, 11 ki\\u015fi hayat\\u0131n\\u0131 kaybetti.\\n*\\nErtesi g\\xfcnk\\xfc gazeteler, h\\xfck\\xfcmetin resmi tebli\\u011fini yaz\\u0131yordu\\u2026 Denizyollar\\u0131 i\\u015fletmesi m\\xfcd\\xfcr\\xfc Raufi Manyas'\\u0131n k\\u0131z\\u0131 Bilun, 16 ya\\u015f\\u0131ndayd\\u0131. \\u0130stiklal caddesi 236 numarada oturan bayan Anna, 58 ya\\u015f\\u0131ndayd\\u0131. Bayan Rona Ki\\u015fnir ve k\\u0131z\\u0131 Bella, istiklal caddesi y\\u0131ld\\u0131z apartman\\u0131nda oturuyorlard\\u0131. Bak\\u0131rk\\xf6y'den Hatice han\\u0131m, a\\u015f\\xe7\\u0131yd\\u0131, 55 ya\\u015f\\u0131ndayd\\u0131. Kurtulu\\u015f semtinden Diyamandi, s\\xfct\\xe7\\xfcyd\\xfc, 40 ya\\u015f\\u0131ndayd\\u0131. Topkap\\u0131 arpaemini yoku\\u015funda oturan Abd\\xfclhamid, 50 ya\\u015f\\u0131ndayd\\u0131. Aksaray Laleli'de oturan bayan Kevser Mehmet, 35 ya\\u015f\\u0131ndayd\\u0131. Tarlaba\\u015f\\u0131 19 numarada oturan Satenik Ohannes, 35 ya\\u015f\\u0131ndayd\\u0131. Saint Benoit Lisesi \\xf6\\u011frencisi Paul Kuto, hen\\xfcz 15 ya\\u015f\\u0131ndayd\\u0131. Ve, Beyo\\u011flu L\\xfcksemburg otelinde kalan Leon.\\n*\\nM\\xfcsl\\xfcman, h\\u0131ristiyan, musevi, T\\xfcrk, Rum, Ermeni\\u2026 \\u201cNe Mutlu T\\xfcrk\\xfcm Diyene\\u201dye dua etmek i\\xe7in, sayg\\u0131lar\\u0131n\\u0131 sunmak i\\xe7in kuyru\\u011fa girmi\\u015flerdi. Memleketin ortak paydas\\u0131'na ortak g\\xf6zya\\u015f\\u0131 d\\xf6k\\xfcyorlard\\u0131.\\n*\\nAtat\\xfcrk'\\xfcn naa\\u015f\\u0131 \\u0130stanbul'dan Ankara'ya getirildi, 10 Kas\\u0131m 1953'te An\\u0131tkabir'e defnedildi, dokuz g\\xfcn sonra ziyarete a\\xe7\\u0131ld\\u0131, ilk g\\xfcn \\xe7ok k\\u0131sa s\\xfcre a\\xe7\\u0131k kald\\u0131, 70 bin ki\\u015fi geldi. 75'inci \\xf6l\\xfcm y\\u0131ld\\xf6n\\xfcm\\xfcnde, sadece 10 Kas\\u0131m g\\xfcn\\xfc, sadece bir g\\xfcnde, 1 milyon 89 bin ki\\u015fi vard\\u0131.\\n*\\nAn\\u0131tkabir'e her y\\u0131l 4 ila 9 milyon yurtta\\u015f ko\\u015fuyor, dua ediyor, devrimlerine ba\\u011fl\\u0131l\\u0131\\u011f\\u0131n\\u0131 sunuyor. Ortalama 6 milyon kabul etsek\\u2026 1953'ten bu yana 378 milyon ki\\u015fi An\\u0131tkabir'i ziyaret etti. ABD'nin n\\xfcfusundan neredeyse 100 milyon daha fazla, Avrupa Birli\\u011fi'ne \\xfcye 28 \\xfclkenin toplam n\\xfcfusu kadar\\u2026 Y\\u0131llar ge\\xe7tik\\xe7e k\\xfcllenece\\u011fine, giderek alevlenen b\\xf6ylesine bir sevginin, d\\xfcnyada \\xf6rne\\u011fi yok.\\n*\\nD\\xfcn gene bak\\u0131yoruz\\u2026\\n*\\nT\\xfcrk Silahs\\u0131z Kuvvetleri'dir bu.\\n*\\nDahili bedhahlara ra\\u011fmen\\u2026\")\n",
      "\n",
      "(u'Cumhuriyet, mucizedir', u\"*\\nYanm\\u0131\\u015f bina say\\u0131s\\u0131 115 bin, hasarl\\u0131 bina say\\u0131s\\u0131 12 bindi, komple k\\xfcl edilmi\\u015f k\\xf6y say\\u0131s\\u0131 binin \\xfczerindeydi, \\xfclkeyi yeniden in\\u015fa etmek gerekiyordu, kiremit bile ithaldi. Limanlar, madenler yabanc\\u0131ya aitti, demiryollar\\u0131n\\u0131n bir metresi bile bize ait de\\u011fildi. Toplam sermayenin sadece y\\xfczde 15'i T\\xfcrk't\\xfc. Osmanl\\u0131'dan ayakta kala kala d\\xf6rt fabrika kalm\\u0131\\u015ft\\u0131, Hereke ipek, Feshane y\\xfcn, Bak\\u0131rk\\xf6y bez, Beykoz deri\\u2026 \\u201cSanayi\\u201d denilen i\\u015fletmelerin y\\xfczde 96's\\u0131nda motor yoktu. 10 i\\u015f\\xe7iden fazla i\\u015f\\xe7i \\xe7al\\u0131\\u015ft\\u0131ran, sadece 280 i\\u015fyeri vard\\u0131, bunlar\\u0131n da 250'si yabanc\\u0131lar\\u0131nd\\u0131. Ki\\u015fi ba\\u015f\\u0131na milli gelir 45 dolard\\u0131. Elektrik sadece \\u0130stanbul, \\u0130zmir ve Tarsus'ta vard\\u0131, g\\xfcya vard\\u0131 demek daha do\\u011fru olur, \\xe7\\xfcnk\\xfc, elektrik \\xfcretimi sadece 50 kilovatsaatt\\u0131, yanl\\u0131\\u015f okumad\\u0131n\\u0131z, sadece 50 kilovatsaatt\\u0131. D\\xf6rt mevsim kullan\\u0131labilen karayolu yoktu, otomobil say\\u0131s\\u0131 bin 490'd\\u0131, sadece d\\xf6rt \\u015fehirde \\xf6zel otomobil vard\\u0131.\\n*\\nZaten peri\\u015fan\\u0131z, \\xfcst\\xfcne, m\\xfcbadeleyle 400 bin insan geldi. Ceplerinde para yok, i\\u015f yok, ba\\u015flar\\u0131n\\u0131 sokacak ev yoktu, s\\u0131\\u011f\\u0131nabilecekleri akraba yoktu, \\xe7o\\u011funlu\\u011fu hastayd\\u0131. Gelen her iki \\xe7ocuktan biri, yollarda, at arabalar\\u0131n\\u0131n s\\u0131rt\\u0131nda, ilk iki ay i\\xe7inde hayat\\u0131n\\u0131 kaybetti. Kendi ailemden biliyorum, \\xe7aresizlikten ma\\u011farada kalanlar oldu, ma\\u011farada.\\n*\\nKad\\u0131n, insan de\\u011fildi.\\n*\\nTiyatro yok, m\\xfczik yok, resim yok, heykel yok, spor yoktu. Arkeolojik eserler, padi\\u015fahlar\\u0131n hediye olarak, trenlerle Avrupa'ya ka\\xe7\\u0131r\\u0131lm\\u0131\\u015ft\\u0131.\\n*\\nKimisi alaturka saat'i kullan\\u0131yor, g\\xfcne\\u015fin batt\\u0131\\u011f\\u0131 an\\u0131 12.00 kabul ediyordu. Kimisi zevalli saat'i kullan\\u0131yor, g\\xfcne\\u015fin en tepede oldu\\u011fu an\\u0131 12.00 kabul ediyordu. Kimisi g\\xfcne\\u015f batarken grubi saat'i esas al\\u0131yordu. Kimisi g\\xfcne\\u015fin tamamen batt\\u0131\\u011f\\u0131 ezani saat'i esas al\\u0131yordu. \\u201cSaat ka\\xe7 birader?\\u201d diye sordu\\u011funda, her kafadan ayr\\u0131 ses \\xe7\\u0131k\\u0131yordu.\\n*\\nKimisi hicri takvim kullan\\u0131yordu, kimisi rumi takvim kullan\\u0131yordu. Kimisinin \\u015fubat'\\u0131 kimisinin aral\\u0131k'\\u0131na denk geliyordu. Herkes ayn\\u0131 zaman dilimindeydi ama, farkl\\u0131 aylarda ya\\u015f\\u0131yordu!\\n*\\nDirhem, okka, \\xe7eki vard\\u0131.\\n*\\n600 sene boyunca T\\xfcrk\\xe7e'nin \\u0131rz\\u0131na ge\\xe7ilmi\\u015f, Arap\\xe7a-Fars\\xe7a harmanlamas\\u0131na Osmanl\\u0131ca denilmi\\u015fti. Frans\\u0131zca, \\u0130talyanca kelimeler, Levanten terimler dilimizi istila etmi\\u015fti. Kar\\u015f\\u0131l\\u0131kl\\u0131 sesli-sessiz harfleri olmayan Arap\\xe7a'yla T\\xfcrk\\xe7e yazmaya \\xe7al\\u0131\\u015f\\u0131yorlard\\u0131.\\n*\\n\\u201cHarf devrimi yap\\u0131ld\\u0131, bir gecede cahille\\u015ftirildik\\u201d filan deniyor\\u2026 Halbuki, \\u0130brahim M\\xfcteferrika'dan itibaren 150 sene boyunca bas\\u0131lan kitap say\\u0131s\\u0131, alt taraf\\u0131 417 adetti. Bunlar\\u0131n da \\xe7o\\u011fu gayrim\\xfcslimlerin matbaas\\u0131ndan \\xe7\\u0131km\\u0131\\u015ft\\u0131. Ki zaten, M\\xfcteferrika da dev\\u015firmeydi.\\n*\\nBu topraklara kitap gelene kadar, Avrupa'da 2.5 milyon farkl\\u0131 kitap bas\\u0131lm\\u0131\\u015f, be\\u015f milyar adet sat\\u0131lm\\u0131\\u015ft\\u0131. Voltaire bir kitab\\u0131nda maalesef \\u201c\\u0130stanbul'da bir y\\u0131lda yaz\\u0131lanlar, Paris'te bir g\\xfcnde yaz\\u0131lanlardan azd\\u0131r\\u201d demi\\u015fti! Gazete sadece \\u0130stanbul ve \\u0130zmir'de vard\\u0131.\\n*\\nErkeklerin sadece y\\xfczde yedisi, kad\\u0131nlar\\u0131n sadece binde d\\xf6rd\\xfc okuma yazma biliyordu. Okur yazar erkeklerin ezici \\xe7o\\u011funlu\\u011fu, subay veya gayrim\\xfcslimdi. Okul ya\\u015f\\u0131 gelen her d\\xf6rt \\xe7ocu\\u011fumuzdan \\xfc\\xe7\\xfc okula gitmiyordu. Toplam 4 bin 894 ilkokul, sadece 72 ortaokul, sadece 23 lise vard\\u0131. Ba\\u015fkent Ankara'da mesela, sadece iki lise vard\\u0131. T\\xfcrkiye'nin t\\xfcm liselerinde sadece 230 k\\u0131z \\xf6\\u011frenci kay\\u0131tl\\u0131yd\\u0131. \\xd6\\u011fretmenlerin \\xfc\\xe7te birinin \\xf6\\u011fretmenlik e\\u011fitimi yoktu. B\\xfct\\xfcn memlekette tek \\xfcniversite vard\\u0131, dar\\xfclf\\xfcnun, medreseden halliceydi. Memleket bilimden \\xe7oook uzakt\\u0131. Medreselerde T\\xfcrk\\xe7e yasakt\\u0131, ba\\u011fnazl\\u0131k yuvas\\u0131yd\\u0131, din diye hurafe \\xf6\\u011fretiyorlard\\u0131.\\n*\\n*\\nCumhuriyet devrimi, mucizedir.\\n*\")\n",
      "\n",
      "(u'Yar\\u0131n bayram...', u\"An\\u0131tkabir'e gitti\\u011finde seni en \\xe7ok etkileyen nedir derseniz\\u2026\\n*\\nBiri beyaz sapl\\u0131, di\\u011ferleri siyah sapl\\u0131, sekiz ustura, seramik tabak, madeni tas, b\\u0131y\\u0131k makas\\u0131, tarak, s\\u0131f\\u0131r numara sa\\xe7 kesme makinesi, sakal f\\u0131r\\xe7as\\u0131 ve bileme ta\\u015f\\u0131\\u2026 Ayr\\u0131ca, arkas\\u0131na K.A. harfleri kaz\\u0131nm\\u0131\\u015f g\\xfcm\\xfc\\u015f el aynas\\u0131, kapa\\u011f\\u0131na ay-y\\u0131ld\\u0131z i\\u015flenmi\\u015f metal esans \\u015fi\\u015fesi, \\xe7i\\xe7ek motifli cam krem kab\\u0131 ve t\\u0131rnak t\\xf6rp\\xfcs\\xfc.\\n*\\nHayat\\u0131 cephelerden cephelere s\\xfcr\\xfcklenerek ge\\xe7ti, yata\\u011f\\u0131ndan \\xe7ok arazide yatt\\u0131, siperlerde sabahlad\\u0131\\u2026 Bak\\u0131ms\\u0131z tek kare foto\\u011fraf\\u0131 yok.\\n*\\nD\\xfcnyan\\u0131n gelmi\\u015f ge\\xe7mi\\u015f en \\u015f\\u0131k giyinen lideridir o.\\n*\\nAram\\u0131zdan ayr\\u0131lal\\u0131 78 sene oldu, bizimkileri zaten bo\\u015fver, bug\\xfcnk\\xfc Fransa cumhurba\\u015fkan\\u0131ndan, ABD ba\\u015fkan\\u0131ndan bile daha \\u015f\\u0131k\\u2026 En \\xf6nemli moda markalar\\u0131 \\u0130talyan ama, \\u0130talya ba\\u015fbakan\\u0131 giyiyor, gen\\xe7, tarz bir adam olmas\\u0131na ra\\u011fmen, \\xfcst\\xfcnde Mustafa Kemal'deki gibi durmuyor. \\xc7\\xfcnk\\xfc, kuma\\u015f, diki\\u015f, tasar\\u0131m, fizik yetmiyor, karizma istiyor.\\n*\\nHep fit'ti.\\n*\\n(Ortal\\u0131kta f\\u0131r\\u0131nc\\u0131 k\\xfcre\\u011fi gibi dola\\u015fmaya benzemiyor yani bu i\\u015f.)\\n*\\n\\xc7o\\u011funlukla beyaz, daima a\\xe7\\u0131k renk g\\xf6mlek tercih ederdi, bebe yaka, ata yaka, i\\u011fneli yaka kullan\\u0131rd\\u0131, man\\u015fetlerine ya da kalbinin \\xfczerine K.A veya G.M.K. armas\\u0131 i\\u015fletirdi. Kol d\\xfc\\u011fmesi sever, yaka i\\u011fnesi takard\\u0131. Sayfideyse, Savarona'daysa, k\\u0131sa kollu keten g\\xf6mlek giyerdi. Laciverti pek sevmez, kruvaze'den hi\\xe7 ho\\u015flanmaz, genellikle yelekli, \\xfc\\xe7 par\\xe7al\\u0131 siyah tak\\u0131m elbiseler diktirirdi. Her daim \\xfct\\xfcl\\xfc olmas\\u0131na b\\xfcy\\xfck \\xf6zen g\\xf6sterirdi, buru\\u015fmas\\u0131na, k\\u0131vr\\u0131lmas\\u0131na, oras\\u0131ndan buras\\u0131ndan sarkma yapmas\\u0131na asla tahamm\\xfcl edemezdi. \\xc7apraz \\xe7izgili, desenli, tak\\u0131m\\u0131na kontrast renkli kravatlar kullan\\u0131rd\\u0131. En sevdi\\u011fi kravat i\\u011fnesi, g\\xf6vdesi burgulu, alt\\u0131n, devlet demiryollar\\u0131 amblemi olan\\u0131yd\\u0131. K\\xf6stekli saati ve ceket cebi mendili, vazge\\xe7ilmez aksesuarlar\\u0131yd\\u0131; ipek mendillerinin kenarlar\\u0131 zikzak motifli olurdu. Seyahatlerinde t\\xfcvit tak\\u0131m, g\\xfcderi ceket, riding coat tarz\\u0131 jokey pantolonlar\\u0131 giyerdi. D\\xfcz kemerden s\\u0131k\\u0131l\\u0131rd\\u0131, \\xf6rg\\xfcl\\xfc, illa ki tokal\\u0131 yapt\\u0131r\\u0131rd\\u0131. Baston deyip ge\\xe7me\\u2026 Hobisiydi, kimisi fildi\\u015fiydi, kimisi l\\xfcleta\\u015f\\u0131 topuzluydu; tek mermi atabilen, tetikli olan\\u0131 me\\u015fhurdur ama, asl\\u0131nda en \\xe7ok sap\\u0131nda tav\\u015fan yakalam\\u0131\\u015f aslan fig\\xfcr\\xfc bulunan, ucu metal halkal\\u0131, ah\\u015fap bastonunu severdi. Smokin ve frak'ta beyaz papyon takard\\u0131. Maharet isteyen pelerin'i de\\u011fme akt\\xf6rlere ta\\u015f \\xe7\\u0131kart\\u0131rcas\\u0131na ta\\u015f\\u0131rd\\u0131, omuzlar\\u0131na illa siyah de\\u011fil, bazen mavi fular atard\\u0131. Ba\\u011fc\\u0131kl\\u0131 siyah rugan ayakkab\\u0131 severdi, astar\\u0131n\\u0131 k\\u0131rm\\u0131z\\u0131 kadifeyle kaplat\\u0131rd\\u0131, \\xe7izgili siyah \\xe7orap kullan\\u0131rd\\u0131, yerine g\\xf6re, gerekirse tozluk kullan\\u0131rd\\u0131, \\xe7izme'yi sadece arazide de\\u011fil, \\u015fehir hayat\\u0131nda da giyerdi, Sirkeci'de sipari\\u015fle yapt\\u0131r\\u0131rd\\u0131. Yazl\\u0131k k\\u0131yafetlerinin alt\\u0131na beyaz veya lacivert-beyaz ayakkab\\u0131lar giyerdi, bu ayakkab\\u0131larla \\xe7orap giymezdi, hatta bazen ten rengi sandalet giyerdi. Paltoyu hantal bulurdu, m\\xfcmk\\xfcn oldu\\u011funca giymemeye \\xe7al\\u0131\\u015f\\u0131rd\\u0131, mecbur kal\\u0131rsa, koyu renk yerine, gri veya kahverengi tonlar\\u0131 tercih ederdi. Desenli ka\\u015fkollar\\u0131 k\\u0131\\u015f aylar\\u0131n\\u0131n olmazsa olmaz\\u0131yd\\u0131. Ve eldiven tabii\\u2026 \\u0130\\xe7i ve bile\\u011fi kurt k\\xfcrk\\xfcnden olanlar\\u0131 severdi. Ak\\u015famlar\\u0131 pijama \\xfczerine mavi-lacivert \\xe7izgili, kirli beyaz, \\u015fal yakal\\u0131 robd\\xf6\\u015fambr al\\u0131rd\\u0131. Ceketli pantolonlu, ku\\u015fa\\u011f\\u0131 p\\xfcsk\\xfcll\\xfc ipek pijamalar giyerdi; yakas\\u0131, kola\\u011fz\\u0131 ve cep kapa\\u011f\\u0131 mutlaka farkl\\u0131 renk \\u015feritli olurdu.\\n*\\nBir insan hem kalpa\\u011f\\u0131, hem silindir \\u015fapkay\\u0131, hem panama \\u015fapkay\\u0131, hem melon \\u015fapkay\\u0131, hem f\\xf6tr\\xfc, hem kasketi, b\\xf6ylesine e\\u015fde\\u011fer yak\\u0131\\u015f\\u0131kl\\u0131l\\u0131kla ta\\u015f\\u0131yabilir mi karde\\u015fim\\u2026 O ta\\u015f\\u0131yordu.\\n*\\n\\u015e\\u0131k, \\xf6zenli, bak\\u0131ml\\u0131 olmay\\u0131 elitizm zanneden, se\\xe7kincilik zanneden\\u2026 Anadolu \\xe7ocu\\u011fu olmay\\u0131, masa \\xf6rt\\xfcs\\xfcn\\xfc and\\u0131ran tuhaf ceketlerle, yer sofras\\u0131nda ba\\u011fda\\u015f kurmak zanneden\\u2026 Halk adam\\u0131 olmay\\u0131, lang\\u0131r lungurlukla kar\\u0131\\u015ft\\u0131ran\\u2026 \\u201cGard\\u0131rop Atat\\xfcrk\\xe7\\xfcs\\xfc\\u201d laf\\u0131na s\\u0131r\\u0131tanlar\\u2026 O'nun gard\\u0131robunda kullan\\u0131lm\\u0131\\u015f k\\xfclot torbas\\u0131 bile olabilir mi?\\n*\\nHer 29 Ekim.\\n*\\nSevgili anneler, memleketimin g\\xfczel kad\\u0131nlar\\u0131\\u2026\\n*\\nO sizi nas\\u0131l bekliyorsa\\u2026\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Article list elements stream\n",
    "xpath_conf_list = {'xpath': xpath, 'url': url}                 # The XPath configuration for the article list\n",
    "pipe_list = SyncPipe('xpathfetchpage', conf=xpath_conf_list)   # A pipe that streams the article list elements\n",
    "stream_list = pipe_list.output                                 # The stream being output from the pipe\n",
    "\n",
    "# The article stream\n",
    "xpath_article = '/html/body/div[5]/div[6]/div[3]/div/div[2]/div[1]/div/div[2]/div[2]'  # XPath of article body\n",
    "\n",
    "xpath_conf_article = {                                         # The XPath configuration for the articles\n",
    "    'url': {'subkey': 'href'},                                 # Notice how we can refer to a 'subkey' as the\n",
    "    'xpath': xpath_article                                     # URL of this configuration\n",
    "}\n",
    "pipe_article = pipe_list.xpathfetchpage(                       # A pipe that streams the articles linked to\n",
    "    conf=xpath_conf_article                                    # by the list stream\n",
    ")                                                              # Notice how we create this pipe by chaining a\n",
    "                                                               # pipe on top of the list pipe; how this one is\n",
    "                                                               # 'dependent' on the list pipe\n",
    "stream_article = pipe_article.output                           # The stream being output from the article pipe\n",
    "\n",
    "sync_iterator = zip(stream_list, stream_article)               # Create a synchronous iterator from the two pipes\n",
    "for list_item, article in itertools.islice(sync_iterator, 3):  # itertools.islice will allow us to get only\n",
    "                                                               # the first n elements, which is 3 in this case\n",
    "    p_elements = article['{http://www.w3.org/1999/xhtml}p']    # Get the list of <p> elements under this XPath\n",
    "    article_body = [paragraph                                  # Grab only the strings under the <p> elements\n",
    "                    for paragraph in p_elements\n",
    "                    if type(paragraph) in [str, unicode]]\n",
    "    article_body = '\\n'.join(article_body)                     # Join strings to create the whole article\n",
    "    article = (list_item['title'], article_body)               # Create the article's (title, body) tuple\n",
    "    print article\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a script to fetch the articles and read them easily, without needing to go to the website: \n",
    "```python\n",
    "(u'\\u0130lelebet payidar', u\"17 Kas\\u0131m 1938.\\n*\\nMaalesef, izdihamdan dalga ...\")\n",
    "\n",
    "(u'Cumhuriyet, mucizedir', u\"*\\nYanm\\u0131\\u015f bina say\\u0131s\\u0131 115 bin, ...\")\n",
    "\n",
    "(u'Yar\\u0131n bayram...', u\"An\\u0131tkabir'e gitti\\u011finde seni en \\xe7ok etk ...\")\n",
    "```\n",
    "The article body looks nicely formatted when you print it. Can this be the most effective ad blocker?\n",
    "***\n",
    "The power of Riko and its pipes may not be immediately visible through parsing just a website but as you explore different options, you can appreciate the power it gives you, the developer, over the mess that is HTML and the World Wide Web. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
