{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, you will learn some basic ideas about k-Neareset Neighbors algorithm (or kNN for short) and apply kNN using available packages. Also, you will implement the kNN algorithm from the scratch for classification analysis. As one of the top 10 data mining algorithms identified by IEEE International Conference on Data Mining (ICDM), kNN is very popular in data analysis for its simplicity and good performance in application. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is kNN\n",
    "\n",
    "The kNN algorithm is instance-based which can be used for classification and regression. The idea behind the algorithm is very simple: use the characteristics of an object's k-nearest neighbors to evalute itself. It is a supervised learning algorithm which means it learns a model from samples with known labels or values. \n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e7/KnnClassification.svg\">](https://upload.wikimedia.org/wikipedia/commons/e/e7/KnnClassification.svg)\n",
    "\n",
    "The graph above is a classic graph from wiki to explain the basic idea of kNN. The samples shown in the graph has two classes: one is blue square and the other is red triangle. The green circle at the center location is a sample waiting to be decided which class it belongs to. The result depends on the parameter k by applying the majority vote rule.\n",
    "\n",
    "If k = 3 which means the class of the green circle depends on the three nearest neighbors, the green circle is a red triangle.\n",
    "\n",
    "If k = 5 which means the class of the green circle depends on the five nearest neighbors, the green circle is a blue square.\n",
    "\n",
    "Concerning how to choose a best k, it depends largely on the dataset. A larger k means deciding the label or value of a unknown test based on more neighbors, and thus could reduce the effect of outliers and noise to a certain degree. But the bad side is that the distinction between different classes may not be that clear. \n",
    "\n",
    "The kNN algorithm is also a lazy-learning algorithm since the model will be constructed when a prediction is needed to make.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "kNN can be applied to both classification and regression problems and will have different output values based on which type the problem belongs to. \n",
    "\n",
    "In a classification problem, an output for an object is a class label for the object determined by holding a majority vote of the labels of the object's k number of neighbors. The attribute types decide the measure of similarity. The Euclidean distance is used for real-valued data and Hamming distance is used for categorical data.\n",
    "\n",
    "In a regression problem, an output for an object is a certain value determined by the average value of the object's k number of neighbors.\n",
    "\n",
    "Training data using kNN for analysis purposes has a feature space, either scalar or multi-dimensional, indicating that a certain distance can be calculated to compare different objects to find the k-nearest neighbors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## sklearn.neighbors\n",
    "\n",
    "If you want to use available package to apply kNN, then sklearn.neighbors can be your choice.\n",
    "\n",
    "There are two main classess in sklearn.neighbors for k-Nearest Neighbors: one is sklearn.neighbors.KNeighborsRegressor for regression analysis and the other is sklearn.neighbors.KNeighborsClassifier for classfication analysis. Let's start from classification first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier\n",
    "\n",
    "Actually besides KNeighborsClassifier, scikit-learn provides another class called RadiusNeighborsClassifier for nearest neighbors classification. This class works well especially when the data sample is not well-uniform sampled because the user can appoint a specific R which is the radius to decide a field of reference instances. Thus data points in sparse distribution use fewer neighbors for classification. However, it is not that effective for a dataset with high-dimension spaces and the reason could be referred to a term called \"curse of dimensionality\" meaning various phenomena that happen only in high-dimensional spaces during the data analysis. \n",
    "\n",
    "KNeighborsClassifier is the more popular used one and you will try to use it. The dataset to be used here is the iris dataset from sklearn.datasets. By the way, there are many available datasets in scikit-learn. If you want to try kNN using other datasets, you can download and try the similar commands below.\n",
    "\n",
    "You can download the data using load_iris(), and the default value for the parameter return_X_y is \"False\" under which the return value is a Bunch type. The Bunch type is a quite useful object like dictionary, providing information about 'target_names'(label names), 'data'(data without labels), 'target'(labels), 'feature_names'(names of the features) and 'DESCR'(description of the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  ['setosa' 'versicolor' 'virginica']\n",
      "featur:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "first five rows of data:  [[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]]\n",
      "labels:  [0 1 2]\n",
      "number of samples:  150\n",
      "number of samples for setosa:  50\n",
      "number of samples for versicolor:  50\n",
      "number of samples for virginica:  50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris_data = load_iris()\n",
    "# print the basic information of this dataset\n",
    "print \"class: \", iris_data['target_names']\n",
    "print \"featur: \", iris_data['feature_names']\n",
    "print \"first five rows of data: \", iris_data['data'][:5]\n",
    "print \"labels: \", np.unique(iris_data['target']) # Labels are the numeric way to represent classes correspondingly.\n",
    "print \"number of samples: \", len(iris_data['data'])\n",
    "print \"number of samples for \"+ iris_data['target_names'][0] +\": \", len([iris_data['data'][i] for i in range(len(iris_data['target'])) if iris_data['target'][i] == 0])\n",
    "print \"number of samples for \"+ iris_data['target_names'][1] +\": \", len([iris_data['data'][i] for i in range(len(iris_data['target'])) if iris_data['target'][i] == 1])\n",
    "print \"number of samples for \"+ iris_data['target_names'][2] +\": \", len([iris_data['data'][i] for i in range(len(iris_data['target'])) if iris_data['target'][i] == 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to split the dataset into a training dataset and a test dataset. We can do the split randomly with a ratio of 66% and 34% respectively. Before that, let's combine data with labels so that it will be easier to randomly select training data and test data. You can do it by inserting the label column to the last column of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add labels to the dataset\n",
    "data = np.array(iris_data['data'])\n",
    "data = np.insert(data, 4, iris_data['target'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning splitting data, you can create a random selector and then select training and test rows by index. Also, you can split attributes and label at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a random selector\n",
    "selector = range(len(data))\n",
    "np.random.shuffle(selector)\n",
    "\n",
    "# select the training dataset\n",
    "train = data[selector[:99]][:, :-1]\n",
    "train_label = data[selector[:99]][:,-1]\n",
    "\n",
    "# select the test dataset\n",
    "test = data[selector[99:]][:, :-1]\n",
    "test_label = data[selector[99:]][:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with datasets ready, you can initiate a KNeighborsClassifier. All the parameters are optional and the default value for n_neighbors is 5. Another interesting parameter called \"weights\" is for you to choose whether to assign same weights to each neighbor ('uniform') or to assign different weights based on the inverse distance from the unknown data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knnClassifier = KNeighborsClassifier(n_neighbors = 10, weights = 'uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to use the training dataset to train the classifier, and the method is \"fit(X, y)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnClassifier.fit(train, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to use the classifier to predict labels of the test dataset and calculate the error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate:  0.0196078431373\n"
     ]
    }
   ],
   "source": [
    "# predict the label of test data\n",
    "predict_label1 = knnClassifier.predict(test)\n",
    "\n",
    "# print the error rate\n",
    "error = 0\n",
    "for i in range(len(test_label)):\n",
    "    if test_label[i] != predict_label1[i]:\n",
    "        error += 1\n",
    "\n",
    "error = error * 1.0 / len(test)\n",
    "print \"error rate: \", error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can do kNN using KNeighborsClassifier! Let's look at KNeighborsRegressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor\n",
    "\n",
    "Similar to the case in classification, besides KNeighborsRegressor, scikit-learn provides another class called RadiusNeighborsRegressor for doing regression analysis. RediusNeighborsRegressor works well when the data is not well-uniform sampled and it lets the user to select a radius r to check the conditions of neighbors.\n",
    "\n",
    "The way that we use KNeighborsRegressor is similar to that of KNeighborsClassifier. Thus you can try it by using a small test example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the predicted value of [[5, 6]]:  [ 2.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "\n",
    "# create a small example\n",
    "x = [[1, 2], [2, 2], [2, 3], [5, 7], [6, 8], [8, 8], [8, 12], [10, 14]]\n",
    "variables = np.array(x)\n",
    "value = [10, 8, 1, 1, 3, 2, 4, 3]\n",
    "\n",
    "# initiate the regressor with a neighbor number of 2\n",
    "neighbor = KNeighborsRegressor(2)\n",
    "\n",
    "# fit the regressor with training data and value\n",
    "neighbor.fit(variables, value)\n",
    "\n",
    "# use the regressor to predict \n",
    "predicted_value = neighbor.predict([[5, 6]])\n",
    "\n",
    "print \"for the predicted value of [[5, 6]]: \", predicted_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement kNN \n",
    "\n",
    "Instead of using available packages, you may be intereseted in writing your own kNN. Let's start to write our own kNN from the scratch for classfication analysis!\n",
    "\n",
    "One thing to mention about is that I think it's better to keep things complete. So I keep the whole codes in one cell below and add comments in between. Sorry for the inconvenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapdict import heapdict\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "class kNNClassfication():\n",
    "    \n",
    "    \"\"\"To initiate, we can pass in the training data, the corresponding labels, \n",
    "                    and k which is the number of neighbors we choose to decide the unknown instance \"\"\"\n",
    "    def __init__(self, train_data, label_data, k_data):\n",
    "        self.train = np.array(train_data)        \n",
    "        self.label = label_data\n",
    "        self.k = k_data\n",
    "        pass\n",
    "    \n",
    "    \"\"\"To predict, we pass in the test data and return the predicted labels.\n",
    "       Based on what we have discussed above, for an unknown instance, we need to find its k neighbors.\n",
    "       For the classification problem, we decide its label based on the majority vote rule\"\"\"\n",
    "    def predict(self, test_data):\n",
    "        predict_label = []\n",
    "        neighbors = []\n",
    "        label_test = ''\n",
    "        \n",
    "        # We find each instance's neighbors and append to the neighbors array\n",
    "        for instance in test_data:\n",
    "            kneighbor = self.find_neighbors(instance)\n",
    "            neighbors.append(kneighbor)\n",
    "        \n",
    "        # We find each instance's majority label and append to the predict_label array\n",
    "        for kneighbor in neighbors:\n",
    "            label_test = self.find_majority(kneighbor)\n",
    "            predict_label.append(label_test)\n",
    "\n",
    "        return predict_label    \n",
    "        pass\n",
    "    \n",
    "    \"\"\"We use a method to find neighbors of an instance and return the labels of neighbors as a dictionary.\n",
    "       We call the calculate_distance method to get the distance between the unknown instance and each training data.\n",
    "       \"\"\"\n",
    "    def find_neighbors(self, instance):\n",
    "        # use a heapdict() to find neighbors with the small distance\n",
    "        container = heapdict()\n",
    "        \n",
    "        # calculate the distance between the unknown instance and each training data\n",
    "        for i in range(len(self.train)):\n",
    "            distance = self.calculate_distance(instance, self.train[i])\n",
    "            container[i] = distance\n",
    "\n",
    "        # add labels of k nearest neighbors to the dictionary        \n",
    "        neighbors = {}\n",
    "        for i in range(self.k):\n",
    "            neighbor = container.popitem()\n",
    "            key = neighbor[0]\n",
    "            value = neighbor[1]\n",
    "            neighbors[key] = self.label[key]\n",
    "        \n",
    "        return neighbors\n",
    "        pass\n",
    "    \n",
    "    \"\"\"calculate the Euclidean distance between two instances\"\"\"\n",
    "    def calculate_distance(self, x, y):\n",
    "        distance = 0\n",
    "        sqr_distance = 0\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            distance_i = (x[i] - y[i]) ** 2\n",
    "            distance = distance + distance_i\n",
    "                       \n",
    "        sqr_distance = math.sqrt(distance)\n",
    "        \n",
    "        return sqr_distance\n",
    "\n",
    "        pass\n",
    "                       \n",
    "    \"\"\"We pass in an instance's k nearest neighbors, find the majority label and return it.\"\"\"                   \n",
    "    def find_majority(self, kneighbor):\n",
    "        label = []\n",
    "        for item in kneighbor:\n",
    "            label.append(kneighbor[item])\n",
    "        \n",
    "        # use Counter() to get the count of each label\n",
    "        c = Counter(label)\n",
    "        test_label = c.most_common(1)[0][0]\n",
    "        return test_label\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finish our own kNN class! Next, let's write a simple test. I make up three classes with different distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predicted value:  ['dog', 'tiger', 'cat']\n",
      "error rate:  0\n"
     ]
    }
   ],
   "source": [
    "train_small = np.array([[1, 2, 3], [2, 2, 1], [2, 3, 4], [5, 7, 6], [6, 8, 7], [8, 8, 4], [8, 6, 10], [10, 16, 8]])\n",
    "label_small = np.array(['dog', 'dog', 'dog', 'cat', 'cat','cat', 'tiger', 'tiger'])\n",
    "test_small = np.array([[4,3,2], [12, 10, 10], [5,8,7]])\n",
    "test_label_small = np.array(['dog', 'tiger', 'cat'])\n",
    "\n",
    "k = 2\n",
    "\n",
    "# initiate the classifier and predict the test data\n",
    "knnClassifier = kNNClassfication(train_small, label_small, 2)\n",
    "label_test_small = knnClassifier.predict(test_small)\n",
    "print \"the predicted value: \", label_test_small\n",
    "\n",
    "# calculate the error rate\n",
    "error_small = 0\n",
    "for i in range(len(test_small)):\n",
    "    if (test_label_small[i] != label_test_small[i]):\n",
    "        error_small += 1\n",
    "\n",
    "error_small = error_small / len(test_small)\n",
    "print \"error rate: \", error_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also test on the iris dataset which we have used when learning how to use KNeighborsClassifier from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  versicolor    predicted class:  virginica\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  setosa    predicted class:  setosa\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  versicolor    predicted class:  versicolor\n",
      "actual class:  virginica    predicted class:  virginica\n",
      "error rate:  0.0196078431373\n"
     ]
    }
   ],
   "source": [
    "# use the training data split from iris \n",
    "knnClassifier3 = kNNClassfication(train, train_label, 10)\n",
    "\n",
    "# predict the test data\n",
    "predict_label2 = knnClassifier3.predict(test)\n",
    "\n",
    "# replace the integer number of output with specific class name\n",
    "test_class = []\n",
    "for i in test_label:\n",
    "    if i == 0:\n",
    "        test_class.append('setosa')\n",
    "    elif i == 1:\n",
    "        test_class.append('versicolor')\n",
    "    else:\n",
    "        test_class.append('virginica')\n",
    "\n",
    "predict_class = []\n",
    "for i in predict_label2:\n",
    "    if i == 0:\n",
    "        predict_class.append('setosa')\n",
    "    elif i == 1:\n",
    "        predict_class.append('versicolor')\n",
    "    else:\n",
    "        predict_class.append('virginica')\n",
    "\n",
    "# print the actual labels and the predicted labels\n",
    "for i in range(len(test_label)):\n",
    "    print \"actual class: \", test_class[i], \"   predicted class: \", predict_class[i]\n",
    "\n",
    "# print the error rate\n",
    "error2 = 0\n",
    "for i in range(len(test_label)):\n",
    "    if test_label[i] != predict_label2[i]:\n",
    "        error2 += 1\n",
    "\n",
    "error2 = error2 * 1.0 / len(test)\n",
    "print \"error rate: \", error2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you implement knnClassifier! One point is that we can normalize all the attributes meaning scaling them between 0 and 1 before we calculate the Euclidean distance. You can try it and see whether it improves the model or not!\n",
    "\n",
    "You can also try knnRegressior to predict a real-valued attribute and the basic ideas are quite similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications \n",
    "\n",
    "Since kNN is very easy to understand and implement, it is applied in many areas in the actual world.\n",
    "\n",
    "For example, kNN can be applied in the text mining to conduct tasks of text classification. Based on applying kNN to the training data, some rules could be found about the characteristics of the text which could be used to predict the category of a future word, sentence or paragraph. A special case of the text mining topic is to filter spam mail. Through building a model based on a training dataset of spam emails, when a new email comes in, kNN could based on the model set on the training dataset to predict whether this is a spam mail or not, thus providing convenience to people's daily life.\n",
    "\n",
    "Besides, kNN can be applied in marketing areas. For a target attribute of a customer, with a training dataset of other informaiton about the customer together with the target attribute, the prediction of the target attribute of a future cusotmer can be gotten from comparing the customer's attributes with these attributes from customers in the training dataset. \n",
    "\n",
    "kNN can also be applied together with algorithms like linear regression and ridge regression to solve practical tasks. There is an example of predicting a face's lower half part given the upper half part by using kNN, linear regression, randomized trees and so on from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Resources to Learn More\n",
    "\n",
    "The k-Nearest Neighbors algorithm is a powerful non-parametric method which could be applied in practical classification and regression problems. You can use available kNN classes in scikit-learn, or you can implement your own kNN algorithm from the scratch. \n",
    "\n",
    "What I have introduced above are some basic ideas about k-Nearest Neighbors. I want to share with you some additional sources about kNN so that you can learn more about it if you are interested in this topic.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Here are some implementations about kNN for your reference:\n",
    "\n",
    "The kNN algorithm in scikit-learn: https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/neighbors\n",
    "\n",
    "The kNN algorithm in python package: https://pypi.python.org/pypi/KNN\n",
    "\n",
    "### Examples\n",
    "\n",
    "Here are some examples from scikit-learn about applying kNN to solve practical tasks:\n",
    "\n",
    "Classifying documents based on a 20 news groups dataset : \n",
    "http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py\n",
    "\n",
    "Predicting people's lower half face given the upper half from scikit-learn: \n",
    "http://scikit-learn.org/stable/auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py\n",
    "\n",
    "### Papers\n",
    "\n",
    "#### More on theory:\n",
    "Shemim Begum, Debasis Chakraborty, Ram Sarkar, \"Data Classification Using Feature Selection and kNN Machine Learning Approach\" (http://ieeexplore.ieee.org/document/7546208/).\n",
    "\n",
    "Lei Wang, Latifur Khan, Bhavani Thuraisingham, \"An Effective Evidence Theory Based K-Nearest Neighbor (KNN) Classification\" (http://ieeexplore.ieee.org/document/4740552/).\n",
    "\n",
    "#### More on practice:\n",
    "Q. Peter He, Jin Wang, \"Fault Detection Using the k-Nearest Neighbor Rule for Semiconductor Manufacturing Processes\" (http://ieeexplore.ieee.org/document/4369338/).\n",
    "\n",
    "Anand Upadhyay, Aditya Shetty, etc, \"Land use and land cover classification of LISS-III satellite image using KNN and decision tree\"(http://ieeexplore.ieee.org/document/7724471/).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
