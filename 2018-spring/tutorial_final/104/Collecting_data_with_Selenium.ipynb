{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup \n",
    "\n",
    "Selenium is essentially a package for automatic testing. It is also vastly used for collecting data from dynamic generated web pages, which we will talk about later. To begin with, run following scripts to setup Selenium environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Selenium in d:\\anaconda35\\lib\\site-packages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pip\n",
    "pip.main(['install', 'Selenium'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver        \n",
    "from selenium.webdriver.common.keys import Keys     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to download a driver according to the browser you will use and the operating system of your computer. Here we are going to use Chrome. So download the driver from https://chromedriver.storage.googleapis.com/index.html?path=2.36/. And you will need to put the driver in the same folder as this Jupyter Notebook. To test whether you have setup the environment correctly, please run the code snippet in **Getting Started**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of HTML of Google homepge: 222774\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.google.com/\")\n",
    "html = driver.page_source\n",
    "print(\"Length of HTML of Google homepge: \" + str(len(html)))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You just get the source code from Google homepage. Here you may wonder why the browser is opened after you start the code. Well, this is an important feature, or advantage of Selenium. Selenium is essentially a automatic testing tool. It imitates the operations of a real user. So this piece of code just imitates a real user opening the browser and visiting the Google homepage. You can see here is that you do not need to deal with those annoying HTTP requests but focusing on the web page and the data in it. If you want to collect data from a series of web page, such as lists of products, you only need to think about how you collect data from those pages as a normal user, then automate your behavior as a real user. Cool, right?  \n",
    "  \n",
    "Then we will going to talk about how to use Selenium scrape webpages in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Yahoo Finance\n",
    "Now let's try to collect something. How about the current values of Cryptocurrencies? Let's try to collect all the values of cryptocurrencies from https://finance.yahoo.com/cryptocurrencies.  \n",
    "  \n",
    "Open the page, and think about how to collect data as a normal user:  \n",
    "1. Open the browser\n",
    "2. Open the web page\n",
    "3. Copy the data row by row\n",
    "4. Go to the next page\n",
    "5. Stop collecting data after finishing the 10th page  \n",
    "6. Close the browser   \n",
    "  \n",
    "So let's try to automate it with Selenium. And we will go through those important concepts related to Selenium with this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Web driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web driver is the core object of Selenium. It is created with specified browser. For example, if you wants to visit the website with Chrome, you need to create a web driver of Chrome. Also, remember to close your driver after you finish. Otherwise the browser will stay there. You can just easily regard creating web driver as opening up the browser and closing the driver as closing the browser. \n",
    "  \n",
    "Selenium also supports browser like Internet Explorer and Firefox. And later we will talk about a headless browser, which allows you to run Selenium without opening a seeable browser. The following piece of code, which we have used at the begining, creates a driver of Chrome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Locating elements\n",
    "Let's just try to collect a single row from the first page of https://finance.yahoo.com/cryptocurrencies. Easy enough for a new starter, right?  \n",
    "  \n",
    "#### Web element \n",
    "Web elements are the HTML objects captured by Selenium selector. The object itself provides you ways to parse it. \n",
    "  \n",
    "You can get following attributes from elements:\n",
    "```\n",
    "element.tag_name\n",
    "\n",
    "```\n",
    "```\n",
    "element.text\n",
    "```\n",
    "It also provides method to access its attribute\n",
    "```\n",
    "element.get_attribute(name)\n",
    "```\n",
    "Example:\n",
    "Check if the \"active\" CSS class is applied to an element:\n",
    "```\n",
    "is_active = \"active\" in target_element.get_attribute(\"class\")\n",
    "```\n",
    "  \n",
    "#### XPath & Selector\n",
    "So how Selenim locate elements? Selenium supports multiple ways to locate elements. The most common way is to use XPath. XPath is a method to identify the location of an element in an HTML page. It looks like this:  \n",
    "```\n",
    "//*[@id=\"scr-res-table\"]/table/tbody/tr[1]/td[2]/a\n",
    "```\n",
    "You may wonder what this complex string means and how to write it. Well, this XPath means the location of the element is under the element with id \"scr-res-table\", the table, the tbody, the first row and second table data (it uses one indexing). And actually you really do not need to know how to write it by yourself. You can easily get it by Chrome developer tool as the figure shows.\n",
    "![title](figure 1.png)\n",
    "<center>Figure 1</center>  \n",
    "  \n",
    "Then we will use Selenium **selector** to select the element by XPath. In the following code, we firstly obtained the **web elements** by XPath using ```driver.find_element_by_xpath()``` method and then extract information from the ** web elements**. The row we are going to scrape looks like this:\n",
    "![title](figure 2.png)\n",
    "<center>Figure 2</center>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'symbol': 'BTC-USD', 'name': 'Bitcoin USD', 'price': '7,909.70', 'change': '+101.22', 'percentage_change': '+1.30%', 'market_cap': '134.025B', 'volume_in_currency': '518.386M', 'total_volume_all_currencies': '749.978M', 'circulating_supply': '3.687B'}\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "row = {\n",
    "    \"symbol\":\"\",\n",
    "    \"name\":\"\",\n",
    "    \"price\":\"\",\n",
    "    \"change\":\"\",\n",
    "    \"percentage_change\":\"\",\n",
    "    \"market_cap\":\"\",\n",
    "    \"volume_in_currency\":\"\",\n",
    "    \"total_volume_all_currencies\":\"\",\n",
    "    \"circulating_supply\":\"\",\n",
    "}\n",
    "\n",
    "driver.get(\"https://finance.yahoo.com/cryptocurrencies\")\n",
    "element = driver.find_element_by_xpath('//*[@id=\"scr-res-table\"]/table/tbody/tr[1]/td[2]')\n",
    "row[\"symbol\"] = element.text\n",
    "\n",
    "element = driver.find_element_by_xpath('//*[@id=\"scr-res-table\"]/table/tbody/tr[1]/td[3]')\n",
    "row[\"name\"] = element.text\n",
    "\n",
    "element = driver.find_element_by_xpath('//*[@id=\"scr-res-table\"]/table/tbody/tr[1]/td[4]')\n",
    "row[\"price\"] = element.text\n",
    "\n",
    "element = driver.find_element_by_xpath('//*[@id=\"scr-res-table\"]/table/tbody/tr[1]/td[5]')\n",
    "row[\"change\"] = element.text\n",
    "\n",
    "element = driver.find_element_by_xpath('//*[@id=\"scr-res-table\"]/table/tbody/tr[1]/td[6]')\n",
    "row[\"percentage_change\"] = element.text\n",
    "\n",
    "element = driver.find_element_by_xpath('//*[@id=\"scr-res-table\"]/table/tbody/tr[1]/td[7]')\n",
    "row[\"market_cap\"] = element.text\n",
    "\n",
    "element = driver.find_element_by_xpath('//*[@id=\"scr-res-table\"]/table/tbody/tr[1]/td[8]')\n",
    "row[\"volume_in_currency\"] = element.text\n",
    "\n",
    "element = driver.find_element_by_xpath('//*[@id=\"scr-res-table\"]/table/tbody/tr[1]/td[9]')\n",
    "row[\"total_volume_all_currencies\"] = element.text\n",
    "\n",
    "element = driver.find_element_by_xpath('//*[@id=\"scr-res-table\"]/table/tbody/tr[1]/td[10]')\n",
    "row[\"circulating_supply\"] = element.text\n",
    "\n",
    "print(row)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data of the first row has been successfully crawled by the crawler. Then let's try to scrape the whole page. You may find it annoyed to copy those XPaths. But by carefully observing, you can find that only the index after ```td``` changes and actually this feature happens to most websites. Using this feature, we can crawl all the rows on a single page with a loop. And you do not need to specify how many rows are there, because the number of rows may change(for example, the number of rows on the last page may different from the first page). You only need to use a infinite loop break when there is an ```ElementNotFoundException```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrapePage(driver):\n",
    "    rows = []\n",
    "    keys = [\n",
    "        \"symbol\", \"name\", \"price\", \"change\",\n",
    "        \"percentage_change\", \"market_cap\", \"volume_in_currency\",\n",
    "        \"total_volume_all_currencies\", \"circulating_supply\",\n",
    "    ]\n",
    "    row_index = 1\n",
    "    while True:\n",
    "        try:\n",
    "            row = dict()\n",
    "            for data_index in range(2, 11):\n",
    "                element = driver.find_element_by_xpath('//*[@id=\"scr-res-table\"]/table/tbody/tr[' + str(row_index) + ']/td[' + str(data_index) + ']')\n",
    "                row[keys[data_index - 2]] = element.text\n",
    "            rows.append(row)\n",
    "#             print(row)\n",
    "            row_index = row_index + 1\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows of the page: \n",
      "{'symbol': 'BTC-USD', 'name': 'Bitcoin USD', 'price': '7,909.70', 'change': '+101.22', 'percentage_change': '+1.30%', 'market_cap': '134.025B', 'volume_in_currency': '518.386M', 'total_volume_all_currencies': '749.978M', 'circulating_supply': '3.687B'}\n",
      "{'symbol': 'ETH-USD', 'name': 'Ethereum USD', 'price': '447.68', 'change': '-1.10', 'percentage_change': '-0.25%', 'market_cap': '44.083B', 'volume_in_currency': '149.164M', 'total_volume_all_currencies': '203.905M', 'circulating_supply': '667.67M'}\n",
      "{'symbol': 'XRP-USD', 'name': 'Ripple USD', 'price': '0.5698', 'change': '-0.0005', 'percentage_change': '-0.0877%', 'market_cap': '21.827B', 'volume_in_currency': '18.882M', 'total_volume_all_currencies': '29.799M', 'circulating_supply': '182.39M'}\n",
      "{'symbol': 'BCH-USD', 'name': 'Bitcoin Cash / BCC USD', 'price': '867.71', 'change': '-9.98', 'percentage_change': '-1.14%', 'market_cap': '14.788B', 'volume_in_currency': '21.05M', 'total_volume_all_currencies': '28.408M', 'circulating_supply': '202.654M'}\n",
      "{'symbol': 'LTC-USD', 'name': 'Litecoin USD', 'price': '133.14', 'change': '-1.74', 'percentage_change': '-1.29%', 'market_cap': '7.433B', 'volume_in_currency': '52.858M', 'total_volume_all_currencies': '80.827M', 'circulating_supply': '239.296M'}\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://finance.yahoo.com/cryptocurrencies\")\n",
    "rows = scrapePage(driver)\n",
    "driver.close()\n",
    "print(\"First five rows of the page: \")\n",
    "for r in rows[:5]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to collecting data from page to page using the ```scrapePage()``` function. The mechanism Selenium operating a page is called **'navigation'**. The operation may include opening a page, interacting with a page, moving between different windows and etc. Now let's try to scrape from page to page using navigation. \n",
    "  \n",
    "So how can we navigate to the next page? There are multiple ways to do it:\n",
    "1. Try to get the url of the next page, and open it with web driver\n",
    "2. Click the **'next'** button using interaction function provided by Selenium  \n",
    "  \n",
    "A problem here is how Selenium know when to stop crawling. This requires us to study the web page carefully. When reaching the end of the lists, the **'next'** button is disabled. And in the HTML code of the the **'next'** button, a 'disabled' string shows up. So after finishing crawling each page, we need to check whether 'disabled' string exists in **'next'** button. If so, we need to stop crawling, otherwise we go to the next page. The HTML code can be obtained by ```element.get_attribute(\"outerHTML\")``` function.  \n",
    "  \n",
    "Let's try to get the url of next page first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First method\n",
    "By studying the website, we can find that it uses a GET request with ```offset``` and ```count```. For example, ```https://finance.yahoo.com/cryptocurrencies?offset=0&count=25``` means to show 0 - 25 results. Then we can obtain the URL of the next page by setting the ```offset``` and ```count```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPages(driver):\n",
    "    rows = []\n",
    "    offset = 0\n",
    "    while True:\n",
    "        driver.get(\"https://finance.yahoo.com/cryptocurrencies?offset=\" + str(offset) + \"&count=25\")\n",
    "        next_button_element = driver.find_element_by_xpath('//*[@id=\"fin-scr-res-table\"]/div[2]/div[2]/button[3]')\n",
    "        rows.extend(scrapePage(driver))\n",
    "        outer_html = next_button_element.get_attribute(\"outerHTML\")\n",
    "        if \"disabled\" in outer_html:\n",
    "            break\n",
    "        offset = offset + 25\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: \n",
      "112\n",
      "\n",
      "First row of data: \n",
      "{'symbol': 'BTC-USD', 'name': 'Bitcoin USD', 'price': '7,909.70', 'change': '+101.22', 'percentage_change': '+1.30%', 'market_cap': '134.025B', 'volume_in_currency': '518.386M', 'total_volume_all_currencies': '749.978M', 'circulating_supply': '3.687B'}\n",
      "\n",
      "Last row of data: \n",
      "{'symbol': 'PAY-USD', 'name': 'TenX USD', 'price': '1.04077', 'change': '-0.02215', 'percentage_change': '-2.08388%', 'market_cap': '0', 'volume_in_currency': '0', 'total_volume_all_currencies': '0', 'circulating_supply': '4.529M'}\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "rows = getPages(driver)\n",
    "driver.close()\n",
    "print(\"Number of entries: \")\n",
    "print(len(rows))\n",
    "print(\"\\nFirst row of data: \")\n",
    "print(rows[0])\n",
    "print(\"\\nLast row of data: \")\n",
    "print(rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second method\n",
    "Then let's try the second method, where we are going to use **'navigation'**.  \n",
    "   \n",
    "The following code realized it with the second method. The crawler will click the **'next'** button and get the next page. After each click, remember to **refresh** the page using ```driver.refresh()```, otherwise the driver will lose its focus and it will throws exception if you continue scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPages2(driver):\n",
    "    rows = []\n",
    "    while True:\n",
    "        next_button_element = driver.find_element_by_xpath('//*[@id=\"fin-scr-res-table\"]/div[2]/div[2]/button[3]')\n",
    "        rows.extend(scrapePage(driver))\n",
    "        outer_html = next_button_element.get_attribute(\"outerHTML\")\n",
    "        if \"disabled\" in outer_html:\n",
    "            break\n",
    "        next_button_element.click()\n",
    "        driver.refresh()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: \n",
      "112\n",
      "\n",
      "First row of data: \n",
      "{'symbol': 'BTC-USD', 'name': 'Bitcoin USD', 'price': '7,907.94', 'change': '+99.46', 'percentage_change': '+1.2737%', 'market_cap': '134.007B', 'volume_in_currency': '518.895M', 'total_volume_all_currencies': '750.487M', 'circulating_supply': '3.689B'}\n",
      "\n",
      "Last row of data: \n",
      "{'symbol': 'PAY-USD', 'name': 'TenX USD', 'price': '1.0441', 'change': '-0.0182', 'percentage_change': '-1.7114%', 'market_cap': '0', 'volume_in_currency': '0', 'total_volume_all_currencies': '0', 'circulating_supply': '4.552M'}\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://finance.yahoo.com/cryptocurrencies\")\n",
    "rows = getPages2(driver)\n",
    "driver.close()\n",
    "\n",
    "print(\"Number of entries: \")\n",
    "print(len(rows))\n",
    "print(\"\\nFirst row of data: \")\n",
    "print(rows[0])\n",
    "print(\"\\nLast row of data: \")\n",
    "print(rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you may find another advantage of Selenium. By clicking the button **'next'**, new contents will be generated by Javascript in the original frame, rather than direct the user to a new page. Data like this which generates dynamically is impossible to collect with HTTP requests, if the url does not change with the content. We need to mention two important concepts, which are **static crawler** and **dynamic crawler**.\n",
    "* Static crawler: Static crawler is only able to crawl static information. It is not able to get dynamic information by interacting with the web page.Crawlers developed by Scrapy or requests are static crawler.\n",
    "* Dynamic crawler: Unlinke static crawler, dynamic crawler is able to interact with the web page and capture dynamic information. Crawler developed by Selenium is dynamic crawler.  \n",
    "  \n",
    "But one shortage for dynamic crawler is the speed is very slow, because it needs to load a real web page before collecting data. And it needs to refresh the page if there are some interactions changes the content of the web page. \n",
    "  \n",
    "Comparing the first method and the second method, the first method is a bit more faster than the second, because there are less interations with the website. So if you can find the regularity in the URLs, using the first method is a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced techniques & further studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Headless browse and cloud deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a Linux operating system without a graphical user interface, where you cannot have a real browser with UI, you need to use a headless browser. This feature allows you to deploy Selenium crawler on cloud environment such as docker. Headless browser is especially useful when you want to deploy your Selenium browser onto the cloud services, such as AWS EC2, whose OS is a Linux without UI. Following codes shows how to run a Selenium crawler with headless Chrome. You do not need extra set up if you already have a Chrome on you device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "\n",
    "def headlessChromeCrawl():\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "    driver.get(\"http://www.datasciencecourse.org/\")\n",
    "    html = driver.page_source\n",
    "    driver.close()\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of HTML of course homepge: 12368\n"
     ]
    }
   ],
   "source": [
    "html = headlessChromeCrawl()\n",
    "print(\"Length of HTML of course homepge: \" + str(len(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further readings, please refer to https://developers.google.com/web/updates/2017/04/headless-chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wait\n",
    "Wait is a special mechanism provided by Selenium, which allows Selenium crawler to stop running for a while until something happens or or stop for a fixed time before next action. For example, it takes time for a browser to load some data on the web page. If the Selenium crawler starts to collect those data before browser finishes loading those data, the crawler will get nothing or throws an exception. Wait allows the work of crawler more stable.  \n",
    "  \n",
    "For further reference please refer to http://selenium-python.readthedocs.io/waits.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nowadays most websites have both PC version and mobile version. An interesting thing is that most websites has less forbidden on their mobile version while more forbiddens on their PC version, although the data is the same. Therefore, you may want to collect the data from a mobile version. To do so, you need to install a plugin on Firefox, called **User Agent Overrider**, which allows you to open the web page like a mobile browser. To use that plugin in Selenium, you need to use **Profile**. Profile is a setting in browser, which allows Selenium to retrieve the plugin installed on Firefox, otherwise by default, Selenium will use a browser without any plugin.  \n",
    "  \n",
    "For further reading, please refer to:   \n",
    "* User agnet overrider on Firefox: https://addons.mozilla.org/en-US/firefox/addon/user-agent-overrider/\n",
    "* Set up profile on Firefox: https://support.mozilla.org/en-US/kb/profile-manager-create-and-remove-firefox-profiles\n",
    "* Set up Selenium webdriver with profile: http://toolsqa.com/selenium-webdriver/custom-firefox-profile/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should I use Selenium?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as concerned, there are two types of crawler, static and dynamic. And Selenium belongs to the second class. Compared with static crawler such as Requests or Scrapy, Selenium has following advantages:\n",
    "* Be able to interact and support javascript very well\n",
    "* Workflow is easy to understand  \n",
    "  \n",
    "However, Selenium also has huge disadvabtages:\n",
    "* Low speed, not suitable for large dataset\n",
    "* Not very easy to extend the project\n",
    "* Not many related projects or plugins  \n",
    "  \n",
    "Therefore, in most situations, if the website can be crawled by both Selenium and static crawler, static crawler is always a better choice. However, nowadays as the prevailing of single-page application, increasing number of websites are using Javascript to generate their content dynamically to improve the user experience. In such circumstances, static crawler may **not able to** collect data from it. This is when you should use Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "Official documents for Selenium, Python edition: http://selenium-python.readthedocs.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
