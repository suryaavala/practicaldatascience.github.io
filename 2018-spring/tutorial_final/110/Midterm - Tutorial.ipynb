{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Text Analysis on Data Corpus Written in Multiple Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was obtained from an online survey deployed in a MOOC called conversational English. The MOOC (Massive Open Online Course) was deployed on the OpenEdX plaform, and is targetted to students who want to learn how to speak English in conversation settings. \n",
    "\n",
    "The survey asked students about their motivation to engage in an English speaking MOOC.\n",
    "\n",
    "\n",
    "Our survey drew 34,100 respondents from 166 countries; the top 10 countries represented include India (13.63%), Brazil(9.48%), Colombia (8.04%), Mexico (6.76%), Egypt (5.76%), China (3.5%), Spain (3.15%), Vietnam (2.21%), Pakistan (2.18%), and Russia (2.17%). Among the total survey respondents, 1.64% stated that English was their native language, and 1.20% indicated that English was the language they spoke the best; these participants were removed from the sample. 35% of the students indicated that they had spent some time in a region where English is spoken often. The table below shows student demographic information for survey participants including age, gender, and education level.\n",
    "\n",
    "\n",
    "At the end of the survey, we asked the students the following question:\n",
    "\n",
    "\"How can we change MOOCs to help students who are non-native English speakers?\"\n",
    "\n",
    "I will be analyzing the response to that question in this tutorial today. About 8041 students entered at least 1 character in the text box for the question above. The dataset is messy in several ways:\n",
    "\n",
    "1. Some students entered punctuation only, so it needs to be cleaned\n",
    "2. Some of the responses are in other languages (e.g Spanish), so they will need to be translated to English\n",
    "3. Many of the responses have random ascii characters, by virtue of students who live abroad and have non-English computer keyboards, so that needs to be dealt with\n",
    "4. A lot of the English responses are grammatically incorrect so that may limit strength of the inferences we can draw from the data\n",
    "5. Most importantly, we need to analyze the text corpus using different ML python libraries to gain some insight about the kinds of design interventions that English Language Learners feel are beneficial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To begin, we will start by importing the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['text', 'english_speaker'])\n",
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AUTOLAB_IGNORE_START\n",
    "#print(df)\n",
    "### AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is imported, it gives us an opportunity to get an overview of what the data looks like. It looks like some students entered only numbers, punctuation only, etc as their responses. As a first step in the data cleaning process, we will change all responses to lowercase, delete all ' characters (e.g don't becomes dont), and remove all responses that don't contain at least one alphabet character. Finally, remove all the leading and trailing spaces.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    text = str(row['text'])\n",
    "\n",
    "    if(re.search('[a-zA-Z]', text)):\n",
    "        row['text'] = text.lower().replace('\\'', '').strip()\n",
    "        \n",
    "    else:\n",
    "        row['text'] = \"\"\n",
    "\n",
    "### AUTOLAB_IGNORE_START\n",
    "#print(df)\n",
    "### AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another really nice way of cleaning data is by converting multiple white spaces into a single space, and separating all punctuation from letters/numbers by adding a space before and after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the dataframe, get each text response, remove additional spaces, then add space before and after punctuation\n",
    "for index, row in df.iterrows():\n",
    "    text = str(row['text'])\n",
    "    \n",
    "    # replace all whitespace with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # then add spaces before all punctuation except hyphen, so they are separate tokens\n",
    "    punctuation = set(re.findall(r\"[^\\w\\s]+\", text)) - {\"-\"}\n",
    "    for c in punctuation:\n",
    "        text = text.replace(c, \" \"+c+\" \")\n",
    "    \n",
    "    #replace old text with formatted text\n",
    "    row['text'] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Every data set is unique - I always find it useful to export my data to a csv, sort the data alphabetically, and identify phrases that are obviously not useful. This next step exports the code to a file called cleaned_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AUTOLAB_IGNORE_START\n",
    "df.to_csv(\"cleaned_dataset1.csv\", sep=',', encoding='utf-8')\n",
    "### AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we asked we expect for the students to give us actual design recommendations, we expect that they give fairly lengthy responses. Also, based on our analysis of the csv file, we observed that the vast majority of the responses with less than 20 characters were not informative enough for our reseach purposes. Therefore, we will elimimate all responses less than 21 characters, and create a smaller dataframe without the blank responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    text = str(row['text'])\n",
    "\n",
    "    if(len(text)<21):        \n",
    "        row['text'] = \"\"\n",
    "        \n",
    "df = df[df['text'] != '']\n",
    "\n",
    "### AUTOLAB_IGNORE_START\n",
    "#print(df)\n",
    "### AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step eliminated over 2000 rows in the data which increasing the likelihood of obtaining insights that are more detailed and actionable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Text Corpus with Multiple Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that our respondents are non native English speakers, there is a very high probability that some of their responses are in languages other than English. I found two different python libraries that allow for language translation. Google's [langdetect](https://pypi.python.org/pypi/langdetect) library provides functionality that reads a string, and returns the likely language of the text (e.g. English). This library can currently detect 55 languages (same languages as google translate). To install this library, run \"pip install langdetect\" in your Anaconda shell (there is no conda install equivalent). In this next step, I am going to create a new field in my dataframe called \"text_language\" that holds the returned language of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "from langdetect import detect\n",
    "\n",
    "#create a new column in the dataframe\n",
    "df['text_language'] = None\n",
    "\n",
    "#iterate through the dataframe, detect the language and save it. \n",
    "for index, row in df.iterrows():\n",
    "    text = str(row['text'])\n",
    "    \n",
    "    #detect the language if possible and save it in text_language\n",
    "    row['text_language'] = detect(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AUTOLAB_IGNORE_START\n",
    "#print(df)\n",
    "### AUTOLAB_IGNORE_STOP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a general picture of the language variety in our corpus, we will do a language count in the entire dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty dictionary to store the language counts\n",
    "language_count = {}\n",
    "english_count = 0\n",
    "other_language_count = 0\n",
    "\n",
    "#iterate through the dataframe, and count the number of languages the tool detected. \n",
    "for index, row in df.iterrows():\n",
    "    text = str(row['text_language'])\n",
    "    \n",
    "    if text in language_count:\n",
    "        language_count[text] = language_count[text] + 1\n",
    "    else:\n",
    "        language_count[text] = 1\n",
    "    \n",
    "    if text == \"en\":\n",
    "        english_count += 1\n",
    "    else:\n",
    "        other_language_count += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AUTOLAB_IGNORE_START\n",
    "print(language_count)\n",
    "print(\"\\n\\nEnglish: \" + str(english_count))\n",
    "print(\"Other Languages: \"+ str(other_language_count))\n",
    "df.to_csv(\"cleaned_dataset.csv\", sep=',', encoding='utf-8')\n",
    "### AUTOLAB_IGNORE_STOP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All together, this library shows that there are 392 text responses that need to be translated. Potential solutions to this problem includes finding native speakers for all languages, putting all the responses on Mechanical Turk and have crowd workers translate it, copy and paste each one into Google translate, or find a python library that can automatically provide translations (which thankfully exists). \n",
    "\n",
    "Google also has a [translation API](https://pypi.python.org/pypi/googletrans) that allows for automatic translation of language if you provide a source and target language. Without specifying a source language, it tries to detect the source language before it translates the text. To install, run \"pip install googletrans\" in your conda shell. The library can be used as follows: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and initialize translator\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "#first, save the original survey responses before translation\n",
    "df['old_text'] = df['text']\n",
    "\n",
    "#then create a column to hold if it it gets translated\n",
    "df['translated'] = None\n",
    "\n",
    "original = 0\n",
    "translated = 0\n",
    "\n",
    "#iterate through the dataframe, check if text is not english,  translate it.\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    lang = str(row['text_language'])\n",
    "    \n",
    "    txt = str(row['text'])\n",
    "    \n",
    "    if lang == \"en\":\n",
    "        original += 1\n",
    "        row['translated'] = \"Original\"\n",
    "    else:\n",
    "        new_text = translator.translate(txt, src=lang, dest='en').text\n",
    "        if detect(new_text) == 'en': \n",
    "            translated += 1\n",
    "            row['translated'] = \"Translated\"\n",
    "            row['text'] = new_text      \n",
    "        else:\n",
    "            original += 1\n",
    "            row['translated'] = \"Original\"\n",
    "            row['text_language'] = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AUTOLAB_IGNORE_START\n",
    "print(\"\\n\\nOriginal Count: \" + str(original))\n",
    "print(\"Translated Count: \"+ str(translated))\n",
    "df.to_csv(\"cleaned_dataset.csv\", sep=',', encoding='utf-8')\n",
    "\n",
    "df2 = df[df['translated'] == \"Translated\"]\n",
    "\n",
    "for index, row in df2.iterrows():\n",
    "    print(str(row['old_text']), ' -> ', str(row['text']))\n",
    "### AUTOLAB_IGNORE_STOP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the results above, it looks like responses in other languages not only got translated, but a few English responses with typos also got \"translated\" to fix the typos. This almost makes me want to run the entire text corpus through the translator as most of the respondents are non-native English speakers therefore most of the corpus is likely to be covered in typos. Otherwise, the results are very impressive. For the rest of this tutorial, I am going to be utilizing some the free text mining tools that were covered in class to try to gain insights on the design recommendations that the students shared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Homework 3, we covered different methods of analyzing free text data including the bag-of-words approach, n-grams, cosine similarity, perplexity etc. A simple bag of words approach is not useful in this case because regardless of the word frequency, we cannot extract any insights from the corpus with individual words. However, for my purposes, I wanted to really wanted to extract the common topics that emerged from the corpus to gain insights on the interventions that are potentially helpful. \n",
    "\n",
    "I decided to use a topic extraction algorithm to get more comprehensible insights from the data. After some research, I found that the Python library ‘Scikit Learn’ (we have used this severally in past assignments), has two different topic modelling algorithms called Latent Dirichlet Allocation (LDA), and Non-negative Matrix Factorization (NMF).\n",
    "\n",
    "For a nice summary on how these work, please visit this blog [post](https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730). \n",
    "\n",
    "Although both algorithms perform topic modeling, NDM has been shown to produce more use insights in smaller data sets. This library requires that you specify both the number of topics and the words per topic to be extracted. I found that 5 words per topic, and 30 topic total produced meaningful results without being extremely repetitive for my particular text. The section below shows how I used these libraries to extract the topics that were common in the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def preprocessing():\n",
    "    \n",
    "    #take into English stop words \n",
    "    tf_vectorizer = CountVectorizer(stop_words='english')\n",
    "    \n",
    "    tf = tf_vectorizer.fit_transform(df[\"text\"].tolist())\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    #set number of topics to 20 total, and number of top words that make up a topic to 10. Play with these numbers to see different results\n",
    "    no_topics = 30\n",
    "    no_top_words = 5\n",
    "\n",
    "    #build the topic model with the parameters and your text corpus\n",
    "    lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "    #return the model and its features\n",
    "    return lda, tf_feature_names, no_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, feature_names, no_top_words = preprocessing()\n",
    "\n",
    "### AUTOLAB_IGNORE_START\n",
    "\n",
    "#print out each topic produced by the model\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx))\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    \n",
    "### AUTOLAB_IGNORE_STOP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is almost impossible to manually go through really large text responses and infer their meaning without a very large team, and a well defined coding dictionary. These topic modeling algorithms are extremely important in data science as they at least give us enough insight about the kinds of topic that might emerge, and can help with the creation of a coding dictionary, before the entire corpus is dispersed to the team for analysis. \n",
    "\n",
    "Some of these topics that the model produced are more insightful that others. The most obvious things I learned from this model are as follows: \n",
    "\n",
    "1. A lot of these students express appreciation and complements for courses as is (Topic 1)\n",
    "2. Students say they can benefit from instructors speaking more slowly and enunciating clearly (Topic 2,22)\n",
    "3. They can benefit from clearer and more practical instructions and examples (8)\n",
    "4. There is a highly expressed need for better translators and dictionaries (11,12,17,19)\n",
    "5. Language subtitles are beneficial (14)\n",
    "6. They can benefit for an increase in face-face interactions in person or through video tools (24,25) - there is already research evidence for this. Please see [Kulkarni et al. Talkabout](https://hci.stanford.edu/publications/2015/PeerStudio/cscw237-kulkarni.pdf) system. \n",
    "7. There is room for other kind of multimedia instruction (18)\n",
    "8. They can benefit from conversation with native English speakers (27)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
