{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How  to write a Web Crawler in Python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In data science, the very basic is to collect data. In some cases, the data is well documented and we know the quantity of the data well, like the reviews we collected from Yelp, which only requires multiple requests. However, when we want to collect data from social network, we don't know how much data is there or where exactly we can find data we want. In this case, we need web crawlers to scan across the Internet to  collect all the data we want. \n",
    "\n",
    "This tutorial will first give a basic understanding of a web crawler with writeup and easy Python 3 code. We will then introduce several important issues in writing a crawler. At last we will introduce a popular Python sraping library/framework 'Scrapy', which helps us to solve the problems mentioned and buidl a web crawler/scraper with little work. \n",
    "\n",
    "### prerequisites\n",
    "\n",
    "#### Basic knowlegde of HTTP requests\n",
    "\n",
    "Due to the limit of words, people who are intereted can read NTU's introduction to HTTP, which is very helpful. \n",
    "\n",
    "- [HTTP (HyperText Transfer Protocol)](https://www.ntu.edu.sg/home/ehchua/programming/webprogramming/HTTP_Basics.html)\n",
    "\n",
    "#### Basic knowlegde of HTML \n",
    "\n",
    "If you don't know this topic well, you can refer to w3school's HTML tutorial which is given below.\n",
    "\n",
    "- [HTML5 Tutorial](https://www.w3schools.com/html)\n",
    "\n",
    "#### Some Python libraries\n",
    "\n",
    "You should have experience with Python `Requests` and `Beautiful Soup` libraries. Here are the official documents:\n",
    "- [Requests: HTTP for Humans](http://docs.python-requests.org/en/master/#requests-http-for-humans)\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#)\n",
    "\n",
    "#### Xpath language\n",
    "\n",
    "Xpath is a language used to search for elements in XML document. You will need Xpath basics when using Scrapy. Here is a good tutorial provided by W3Schools.\n",
    "\n",
    "- [XPath Tutorial](https://www.w3schools.com/xml/xpath_intro.asp)\n",
    "\n",
    "### Environment specification: \n",
    "- Python: 3.6.4\n",
    "- Beautiful Soup: 4.6.0\n",
    "- Requests: 2.18.4\n",
    "- Scrapy: 1.5.0\n",
    "- Linux Shell is used in this tutorial, thus run this jupyter notebook on Linux/MacOS only. Yet Scrapy command is not OS-denpendent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "\n",
    "- [What is a web crawler](#What-is-a-web-crawler)\n",
    "- [Write your first web crawler](#Write-your-first-web-crawler)\n",
    "- [Issues to consider in a crawler](#Issues-to-consider-in-a-crawler)\n",
    "- [Using Scrapy to build a web crawler](#Using-Scrapy-to-build-a-web-crawler)\n",
    "- [Reference](#Reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a web crawler\n",
    "\n",
    "A [web crawler](https://en.wikipedia.org/wiki/Web_crawler), also refered as a `Web spider`, or an `ant`. It is a program that sends requests to websites and then downloads web content. Usually, it starts with a list/queue of URLs to visit. When the crawler visits a website, it collects page content and identify the hyperlinks in the web page, add them (or first filter them by user-specified rules) to the list/queue of URLs to visit. Doing visiting websites and adding new urls recursively, web crawlers traverse the websites across the Internet or on a specific host, collecting web page data.\n",
    "\n",
    "[<img src=\"500px-WebCrawlerArchitecture.png\">](https://upload.wikimedia.org/wikipedia/commons/d/df/WebCrawlerArchitecture.svg)\n",
    "<center>Figure 1: Web Crawler Architecture (from [WikiPedia](https://en.wikipedia.org/wiki/Web_crawler))</center>\n",
    "\n",
    "Here is some cool stuff achieved with a web crawler:\n",
    "\n",
    "###  Google search engine\n",
    "Google uses crawlers to record the info of web pages and make an index of them and store the index locally.\n",
    "[<img src=\"hqdefault.jpg\">](https://youtu.be/BNHR6IQJGZs \"How Search Works\")\n",
    "<center>Video 1: How Search Works</center>\n",
    "Or you can read Google's introduction if you are interested:\n",
    "- [Google search engine|Crawling and indexing](https://www.google.com/intl/ALL/search/howsearchworks/crawling-indexing) \n",
    "### More web crawler case studies\n",
    "This website collected web crawler use cases in ecommerce, travel, restaurant, etc, which should be a good resource if you are very interested in web crawler application and need more information. \n",
    "\n",
    "- [Web crawlers use cases](http://promptcloud.dev.onpressidium.com/web-crawl-use-cases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your first web crawler\n",
    "\n",
    "Let's first implement a very basic version to consolidate our basic understanding of a web crawler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from types import MethodType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This naive crawler class\n",
    "# @url_queue:   the queue which stores urls to be crawled\n",
    "# @visited_url: the set which stores urls already added to url_queue\n",
    "# @pg_cng:      the max count of web pages to be downloaded and stored \n",
    "# @saved_pg:    the dictionary storing downloaded pages, format:{url:content}\n",
    "# @max_qsize:   the max size of url_queue\n",
    "class NaiveCrawler:\n",
    "    url_queue = queue.Queue()\n",
    "    visited_url = set()\n",
    "    pg_cng = 0\n",
    "    saved_pg = {}\n",
    "    max_qsize = 0\n",
    "    \n",
    "    # @init_pg:     list of initial pages for crawling\n",
    "    def __init__(self, init_pg, max_qsize=10000):\n",
    "        self.pg_cng = 0\n",
    "        self.url_queue = queue.Queue()\n",
    "        self.saved_pg = {}\n",
    "        for url in init_pg:\n",
    "            self.url_queue.put(url)\n",
    "            self.visited_url.add(url)\n",
    "        self.max_qsize = max_qsize\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.saved_pg.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a spider class `NaiveCrawler`, which contains basic parameters for a crawler, a queue storing urls to be scraped, a set storing urls already visited, web page contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawling function\n",
    "# @pg_limit:  max number of pages to crawl\n",
    "# @max_qsize: max number of pages stored in the url_queue\n",
    "# @headers:   the http GET request headers\n",
    "# @params:    the http GET request params\n",
    "# @interval:  interval between two consecutive requests\n",
    "def crawl(self, pg_limit=100, \n",
    "    headers={}, params={},\n",
    "    interval=0.1):\n",
    "    #only download pg_limit number of web pages\n",
    "    while(self.pg_cng < pg_limit):\n",
    "        time.sleep(interval)\n",
    "        if self.url_queue.qsize()>0:\n",
    "            #get the first url in the queue\n",
    "            current_url = self.url_queue.get() \n",
    "            try:\n",
    "                #request web pages  \n",
    "                response = requests.get(current_url, headers=headers, params=params)\n",
    "                html = response.text\n",
    "                self.process_page(html,current_url)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            pass\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `crawl` function is the main function called by our crawler. In each cycle it picks the first url in `url_queue`, sends a request to download the corresponding web page contents with `requests.get`, store the web page contents & add new url links to `url_queue` with the function `process_page`. The loop stops when there are enough pages downloaded or url_queue is full (This is an exception). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the downloaded web page, including store/filter content \n",
    "# and add new url to url_queue\n",
    "# @html: the html doc downloaded\n",
    "# @url:  the url of the downloaded web page \n",
    "def process_page(self,html,url):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        #simply store the whole html doc\n",
    "        self.saved_pg[url] = html\n",
    "        self.pg_cng += 1\n",
    "        #identify all possible externel links appearing in the html doc\n",
    "        #example externel link: \"//facebook.com\", \"www.google.com\"\n",
    "        for a in soup.find_all(\"a\", href=re.compile(\"^(http|www|//)*$\")):\n",
    "            #only if one url is not seen before and queue size does\n",
    "            #not exceeed max limit, we add it to url_queue\n",
    "            ex_url = a['href']\n",
    "            if ex_url is not None and ex_url[0]=='/':\n",
    "                if self.url_queue.qsize() < self.max_qsize:\n",
    "                    if ex_url not in self.visited_url:\n",
    "                        self.url_queue.put(ex_url)\n",
    "                        #once we add a url to url_queue, we consider it 'visited'\n",
    "                        self.visited_url.add(ex_url)\n",
    "\n",
    "        #identify all possible internel links \n",
    "        #example internel links: \"/ebooks/56580\"\n",
    "        for a in soup.find_all(\"a\", href=re.compile(\"^/[^/].*$\")):\n",
    "            # concatenate the present web page's url with the relative url \n",
    "            in_url = a['href']\n",
    "            if in_url is not None:\n",
    "                in_url = url + in_url\n",
    "                if self.url_queue.qsize() < self.max_qsize:\n",
    "                    if in_url not in self.visited_url:\n",
    "                        self.url_queue.put(in_url)\n",
    "                        #once we add a url to url_queue, we consider it 'visited'\n",
    "                        self.visited_url.add(in_url)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `process_page` first parse the downloaded html file with BeautifulSoup library, which we are quite familiar with. It then adds the web page content to the dictionary `saved_pg` and extract all hyperlinks in the html file and store them into `url_queue`, the queue storing urls to be crawled. Here we deal with internel links and externel links separately.\n",
    "\n",
    "You may need some knowledge of internel link and externel link in HTML (similar to relative path and absolute path in OS), and here is a good introduction if you don't know them before.\n",
    "\n",
    "[Creating Internal & External HTML Links](https://clearlydecoded.com/creating-internal-external-html-links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawler dump result:  20  pages in total\n",
      "http://www.gutenberg.org/\n",
      "http://www.gutenberg.org//wiki/Category:Bookshelf\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Contact_Information#Electronic_Mail\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Terms_of_Use\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Project_Gutenberg_Needs_Your_Donation\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Feeds\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Offline_Catalogs\n",
      "http://www.gutenberg.org//wiki/Gutenberg:MobileReader_Devices_How-To\n",
      "http://www.gutenberg.org//wiki/Category:How-To\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Contact_Information\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Readers%27_FAQ#R.26._I.27ve_found_some_obvious_typos_in_a_Project_Gutenberg_text._How_should_I_report_them.3F\n",
      "http://www.gutenberg.org//wiki/Category:Volunteering\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Promote_Project_Gutenberg\n",
      "http://www.gutenberg.org//wiki/Gutenberg:About\n",
      "http://www.gutenberg.org//wiki/Gutenberg:No_Cost_or_Freedom%3F\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Permission_How-To\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Information_About_Linking_to_our_Pages\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Information_About_Robot_Access_to_our_Pages\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Partners,_Affiliates_and_Resources\n",
      "http://www.gutenberg.org//wiki/Gutenberg:Subscribe_How-To\n",
      "http://www.gutenberg.org/ <!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\" xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:og=\"http://opengraphprotocol.org/schema/\" xmlns:fb=\"http://www.facebook.com/2008/fbml\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Gutenberg</title>\n",
      "<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );</script>\n",
      "<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSp ....\n"
     ]
    }
   ],
   "source": [
    "#Test case\n",
    "naive_crawler = NaiveCrawler([\"http://www.gutenberg.org/\"])\n",
    "# add defined functions to NaiveCrawler instance\n",
    "# this is not a good code style, I only do this to\n",
    "# make it easier to cut the code into chunks to \n",
    "# insert writeup between chunks\n",
    "naive_crawler.crawl = MethodType(crawl,naive_crawler)\n",
    "naive_crawler.process_page = MethodType(process_page,naive_crawler)\n",
    "\n",
    "naive_crawler.crawl(pg_limit=20) \n",
    "print(\"crawler dump result: \",len(naive_crawler.saved_pg),\" pages in total\")\n",
    "for url in naive_crawler.saved_pg:\n",
    "    print(url)\n",
    "    \n",
    "key = list(naive_crawler.saved_pg.keys())[0] \n",
    "print(key, naive_crawler.saved_pg[key][:500], \"....\")\n",
    "\n",
    "del naive_crawler\n",
    "#print(naive_crawler.saved_pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues to consider in a crawler\n",
    "\n",
    "#### Avoid visiting the same urls: \n",
    "\n",
    "If you do not exclude the urls already visited from the request/url queue, you r program may get very slow or even generate request loops. However, in the code above, we can't recognize \"https://www.gutenberg.org/ebooks/56857\", \"www.gutenberg.org/ebooks/56857\", \"//gutenberg.org/ebooks/56857\", \"/ebooks/56857\"(embeded on www.gutenberg.org) as one url.\n",
    "\n",
    "\n",
    "#### Avoid being blocked: \n",
    "\n",
    "Website hosts always have many techniques to prevent cyberattacks. To avoid a server crash, hosts will usually block an IP if it sends a flood of requests in a short time. This is why we should add an interval between requests. Besides, if you want to scrape trillions of bytes of data from one host (e.g. You want all reviews on Yelp), you should use many different IPs to scrape data, like using proxy servers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### User agent:\n",
    "\n",
    "In http request packet, the [UserAgent](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent) is a key-value pair in a request header that allows hosts to identify the application type, operating system, software vendor, version of the software user agent sending the request.\n",
    "\n",
    "Sometimes website hosts will parse the 'UserAgent' in a request header and deny the request if it is not sent by a browser. In this case, we should add 'UserAgent' entry in our request headr to disguise the request packet we send. \n",
    "\n",
    "#### Others:\n",
    "- cookies\n",
    "- fast method to find duplicate urls (Bloom Filter)\n",
    "- parallel spiders, distributed spiders\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scrapy to build a web crawler\n",
    "\n",
    "If we write our own code to deal with all the issues mentioned above, that will be a big project. Luckily, there is a Python web scraping library, which is able to handle framwork and generates part of spider code with little configuration code. \n",
    "\n",
    "Here I'd like to introduce to you how to build a web crawler with Scrapy. BTW, Installation info is given here:\n",
    "- [Installation Guide](https://doc.scrapy.org/en/latest/intro/install.html#installation-guide)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The framework/pipeline of Scrapy is a little bit different from the code of `NaiveCrawler` shown above though the basic ideas are the same. We can still use Scrapy to implement our web crawler without knowing its inner data flow. If you are quite interested in its structure and data flow, you can refer to Scrapy's offical document:\n",
    "\n",
    "- [Architecture overview](https://doc.scrapy.org/en/latest/topics/architecture.html#architecture-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a project\n",
    "\n",
    "Srapy has a very powerful command-line tool which provides several easy but very powerful commands. With several lines of these commands, you can create your crawler framework project, generate simple spider code, launch a crawlers and write output files in the specified format. The command usage is `scrapy <command> [options] [args]`. If you want to know more about Scrapy's command-line tool, you can refer to its official document:\n",
    "\n",
    "- [Command line tool](https://doc.scrapy.org/en/latest/topics/commands.html#command-line-tool)\n",
    "\n",
    "Now we first create a project using its `startproject` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: scrapy.cfg already exists in /home/vincent/Documents/practical_data/tutorial/web_crawler\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#rm -r ./web_crawler\n",
    "scrapy startproject web_crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With command `scrapy startproject web_crawler`, scrapy automatically generates a working directory `web_crawler`, which contains a scraping framework. The directory structure is like this:\n",
    "<img src=\"create_project.png\">\n",
    "In the working directory, `scrapy.cfg` is theproject configuration file and the inner `spiders` directory is where we should put our web crawler files, which I will cover later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a spider\n",
    "\n",
    "In Scrapy, spiders are classes that Scrapy framework uses to scrape information from a website. A spider class you create must inherits from `scrapy.Spider`. You must define the initial requests/urls in the spider class.\n",
    "You should put your spider class in the `spiders` directory. \n",
    "\n",
    "Optionally, you can define how your crawler will follow links in the pages, and how to parse the downloaded page content/how to extract data in your spider class.\n",
    "\n",
    "You can of course write your own spider or you can use Scrapy commands to generate a spider and adjust the code generated. Let's first generate a spider automatically and see how to write a spider class.\n",
    "\n",
    "You can refer to official document for more information:\n",
    "- [Scrapy commands | genspider](https://doc.scrapy.org/en/latest/topics/commands.html#genspider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spider 'guntenberg_crawler' already exists in module:\n",
      "  web_crawler.spiders.guntenberg_crawler\n",
      "# -*- coding: utf-8 -*-\n",
      "import scrapy\n",
      "\n",
      "\n",
      "class GuntenbergCrawlerSpider(scrapy.Spider):\n",
      "    name = 'guntenberg_crawler'\n",
      "    allowed_domains = ['www.gutenberg.org']\n",
      "    start_urls = ['http://www.gutenberg.org/']\n",
      "\n",
      "    def parse(self, response):\n",
      "        pass\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ./web_crawler\n",
    "\n",
    "#scrapy genspider [-t template] <name> <domain>\n",
    "scrapy genspider guntenberg_crawler www.gutenberg.org\n",
    "cat ./web_crawler/spiders/guntenberg_crawler.py\n",
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With command `scrapy genspide guntenberg_crawler www.gutenberg.org`, we generate a spider class  `guntenberg_crawler`, which is located in `spiders` directory. In class `GuntenbergCrawlerSpider`, `name` is the   name of the spider, which is needed when we execute `crawl` command. `allowed_domains` is an optional list of strings containing domains that this spider is allowed to crawl. `start_urls` is a list of urls where the spider  begins to crawl from by default.\n",
    "\n",
    "For more spider attributes and details about how scraping cycle goes, you can refer to this page:\n",
    "- [Spiders](https://doc.scrapy.org/en/latest/topics/spiders.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a spider \n",
    "Now we can run the spider just generated easily with Scrapy command `scrapy crawl <name_spider>`, but pay attention that all Scrapy commands except from `startproject` should run in a Scrapy working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ./web_crawler\n",
    "scrapy crawl guntenberg_crawler\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we launch a spider, Scrapy will generate an initial queue of requests from `start_urls` or you can specify your own initial queue of requests with function `start_requests()`.\n",
    "\n",
    "These two classes have same initial queue of requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class Spider_1(scrapy.Spider):\n",
    "    name = 'spider_1'\n",
    "    start_urls = ['http://quotes.toscrape.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        pass    \n",
    "    \n",
    "class Spider_2(scrapy.Spider):\n",
    "    name = 'spider_2'\n",
    "    \n",
    "    def start_requests(self):\n",
    "        urls = ['http://quotes.toscrape.com']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "        \n",
    "    def parse(self, response):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`start_requests()` provides us with a more flexible initialization strategy, but in our tutorial simply using `start_urls` is enough. \n",
    "\n",
    "If you want to know more about `start_requests()`, you can refer to:\n",
    "- [start_requests()](https://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.start_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time a Scrapy receives a response, it will execute `parse()` by default to parse the response. Yet `parse()` has a unique feature that you can generate an iterator of `scrapy.Request` object in `parse()`, which will be added to Scrapy's requests queue. With this feature, after we extract urls in `parse()`, we can easily add them to queue of urls waiting to be crawled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a web crawler\n",
    "\n",
    "The following code is in file `web_crawler.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#web_crawler.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class WebCrawler(scrapy.Spider):\n",
    "    name = \"web_crawler\"\n",
    "    #allowed_domains = ['http://quotes.toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # record the scraped urls\n",
    "        filename = 'web_crawler_log.txt'\n",
    "        with open(filename, 'a+') as f:\n",
    "            f.writeln(response.url)\n",
    "        #extract all urls and crawl them later\n",
    "        next_pages = response.xpath('//a/@href').extract()\n",
    "        for next_page in next_pages:\n",
    "            if next_page is not None:\n",
    "                yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `parse()` function, we use [XPATH SELECTOR](https://doc.scrapy.org/en/latest/topics/selectors.html#selectors) `response.xpath` to select all `href`s in tag `a` in the html document just downloaded. Then we extract the texts of hrefs using `.extract()` method. \n",
    "\n",
    "We use `response.follow(next_page, callback=self.parse)` to generate a Scrapy request. Remember we have to parse internal links and external links separately when we use `Beautiful Soup` and `Requests`, but now Scrapy will deal with them both automatically. At last, we generate an iterator of Scrapy requests, which will be caught by Scrapy framework and be sent to requests queue.\n",
    "\n",
    "See there are around 20 lines of Python code for a crawler, much shorter than our 'original version'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://quotes.toscrape.com/author/Andre-Gide/tein/eedbackct-privacy/http://quotes.toscrape.comhttp://quotes.toscrape.com/tag/value/page/1/http://quotes.toscrape.com/tag/success/page/1/http://quotes.toscrape.com/tag/adulthood/page/1/https://scrapinghub.comhttp://quotes.toscrape.com/tag/be-yourself/page/1/https://www.goodreads.com/quoteshttps://scrapinghub.com/privacy-policyhttps://scrapinghub.com/abuse-reporthttp://quotes.toscrape.com/tag/humor/page/1/https://scrapinghub.com/terms-of-servicehttp://quotes.toscrape.com/tag/classic/page/1/https://www.goodreads.com/about/privacyhttps://www.goodreads.com/helphttps://scrapinghub.wufoo.com/forms/m1f2hw8b00tckmh/http://quotes.toscrape.com/tag/books/page/1/https://www.goodreads.com/about/privacy1http://quotes.toscrape.com/tag/aliteracy/page/1/https://www.goodreads.com/questions/guidelineshttp://quotes.toscrape.com/tag/miracles/page/1/https://www.goodreads.com/help/show/108-how-do-i-add-a-goodreads-tab-to-my-facebook-pagehttp://quotes.toscrape.com/tag/miracle/page/1/http://quotes.toscrape.com/tag/live/page/1/http://quotes.toscrape.com/tag/life/page/1/https://www.goodreads.com/group/show/31471-goodreads-author-feedback-grouphttp://quotes.toscrape.com/tag/inspirational/page/1/http://quotes.toscrape.com/author/Andre-Gide/http://quotes.toscrape.com/tag/simile/http://quotes.toscrape.com/tag/truth/http://quotes.toscrape.com/author/Marilyn-Monroe/http://quotes.toscrape.com/tag/humor/page/2/http://quotes.toscrape.com/tag/reading/page/1/http://quotes.toscrape.com/author/Jane-Austen/http://quotes.toscrape.com\n",
      "http://quotes.toscrape.com/tag/thinking/page/1/\n",
      "http://quotes.toscrape.com/tag/deep-thoughts/page/1/\n",
      "http://quotes.toscrape.com/tag/change/page/1/\n",
      "https://scrapinghub.com\n",
      "http://quotes.toscrape.com/login\n",
      "https://www.goodreads.com/quotes\n",
      "https://scrapinghub.com/privacy-policy\n",
      "http://quotes.toscrape.com/\n",
      "http://quotes.toscrape.com/tag/adulthood/page/1/\n",
      "https://www.goodreads.com/\n",
      "https://scrapinghub.com/terms-of-service\n",
      "https://scrapinghub.com/abuse-report\n",
      "http://quotes.toscrape.com/tag/be-yourself/page/1/\n",
      "http://quotes.toscrape.com/tag/humor/page/1/\n",
      "https://www.goodreads.com/help\n",
      "https://www.goodreads.com/about/privacy\n",
      "http://quotes.toscrape.com/tag/classic/page/1/\n",
      "https://www.goodreads.com/list/show/11.Best_Crime_Mystery_Books\n",
      "http://quotes.toscrape.com/tag/books/page/1/\n",
      "https://www.goodreads.com/book/show/17899948-rebecca\n",
      "https://scrapinghub.wufoo.com/forms/m1f2hw8b00tckmh/\n",
      "http://quotes.toscrape.com/tag/aliteracy/page/1/\n",
      "https://www.goodreads.com/book/show/960.Angels_Demons\n",
      "https://www.goodreads.com/book/show/16299.And_Then_There_Were_None\n",
      "http://quotes.toscrape.com/tag/miracles/page/1/\n",
      "https://www.goodreads.com/book/show/2429135.The_Girl_with_the_Dragon_Tattoo\n",
      "https://www.goodreads.com/giveaway/terms\n",
      "http://quotes.toscrape.com/tag/miracle/page/1/\n",
      "http://quotes.toscrape.com/tag/open-mind/page/1/\n",
      "https://www.goodreads.com/about/privacy1\n",
      "http://quotes.toscrape.com/author/Albert-Einstein/\n",
      "http://quotes.toscrape.com/author/Marilyn-Monroe/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-30 22:50:53 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: web_crawler)\n",
      "2018-03-30 22:50:53 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 18:10:19) - [GCC 7.2.0], pyOpenSSL 17.5.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.1.4, Platform Linux-4.13.0-37-generic-x86_64-with-debian-stretch-sid\n",
      "2018-03-30 22:50:53 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'web_crawler', 'CLOSESPIDER_PAGECOUNT': '20', 'DOWNLOAD_DELAY': 0.25, 'NEWSPIDER_MODULE': 'web_crawler.spiders', 'SPIDER_MODULES': ['web_crawler.spiders']}\n",
      "2018-03-30 22:50:53 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.closespider.CloseSpider',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2018-03-30 22:50:53 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2018-03-30 22:50:53 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2018-03-30 22:50:53 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2018-03-30 22:50:53 [scrapy.core.engine] INFO: Spider opened\n",
      "2018-03-30 22:50:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2018-03-30 22:50:53 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2018-03-30 22:50:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com> (referer: None)\n",
      "2018-03-30 22:50:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://quotes.toscrape.com> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vincent/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/home/vincent/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py\", line 30, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/home/vincent/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/home/vincent/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/vincent/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/vincent/Documents/practical_data/tutorial/web_crawler/web_crawler/spiders/web_crawler.py\", line 16, in parse\n",
      "    f.writeln(response.url)\n",
      "AttributeError: '_io.TextIOWrapper' object has no attribute 'writeln'\n",
      "2018-03-30 22:50:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2018-03-30 22:50:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 218,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 2333,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2018, 3, 31, 2, 50, 53, 926753),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 7,\n",
      " 'memusage/max': 53604352,\n",
      " 'memusage/startup': 53604352,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/AttributeError': 1,\n",
      " 'start_time': datetime.datetime(2018, 3, 31, 2, 50, 53, 488549)}\n",
      "2018-03-30 22:50:53 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ./web_crawler\n",
    "scrapy crawl web_crawler --set CLOSESPIDER_PAGECOUNT=20\n",
    "cat ./web_crawler_log.txt\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `--set CLOSESPIDER_PAGECOUNT=20` arguments to set the page count limit, but the exact page count is not 20. This is because when scraped page count is 20, Scrapy starts its `CloseSpider` process, and Scrapy first waits all requests in the queue to finish and then terminates. Thus there will few more pages downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using UserAgent with Scrapy\n",
    "\n",
    "As has been mentioned, some websites identify robots/scripts by UserAgent and deny/drop requests sent by them. Now let's see how we can easily handle this with scrapy.\n",
    "\n",
    "I wrote a naive scraper called `zhihu.py`, which only requests a web page from 'www.zhihu.com' and write the returned content to `zhihu.html`. Zhihu is a Quora-like forum website where people raise and answer questions. Zhihu uses UserAgent to identify robots/scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: utf-8 -*-\n",
      "import scrapy\n",
      "\n",
      "\n",
      "class ZhihuSpider(scrapy.Spider):\n",
      "    name = 'zhihu'\n",
      "    #allowed_domains = ['www.zhihu.com']\n",
      "    start_urls = ['http://www.zhihu.com']\n",
      "\n",
      "    def parse(self, response):\n",
      "        filename = 'zhihu.html'\n",
      "        with open(filename, 'wb') as f:\n",
      "            f.write(response.body)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-30 22:56:24 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: web_crawler)\n",
      "2018-03-30 22:56:24 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 18:10:19) - [GCC 7.2.0], pyOpenSSL 17.5.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.1.4, Platform Linux-4.13.0-37-generic-x86_64-with-debian-stretch-sid\n",
      "2018-03-30 22:56:24 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'web_crawler', 'DOWNLOAD_DELAY': 0.25, 'NEWSPIDER_MODULE': 'web_crawler.spiders', 'SPIDER_MODULES': ['web_crawler.spiders']}\n",
      "2018-03-30 22:56:24 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2018-03-30 22:56:24 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2018-03-30 22:56:24 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2018-03-30 22:56:24 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2018-03-30 22:56:24 [scrapy.core.engine] INFO: Spider opened\n",
      "2018-03-30 22:56:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2018-03-30 22:56:24 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2018-03-30 22:56:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.zhihu.com/> from <GET http://www.zhihu.com>\n",
      "2018-03-30 22:56:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.zhihu.com/> (failed 1 times): 500 Internal Server Error\n",
      "2018-03-30 22:56:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.zhihu.com/> (failed 2 times): 500 Internal Server Error\n",
      "2018-03-30 22:56:40 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://www.zhihu.com/> (failed 3 times): 500 Internal Server Error\n",
      "2018-03-30 22:56:40 [scrapy.core.engine] DEBUG: Crawled (500) <GET https://www.zhihu.com/> (referer: None)\n",
      "2018-03-30 22:56:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <500 https://www.zhihu.com/>: HTTP status code is not handled or not allowed\n",
      "2018-03-30 22:56:40 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2018-03-30 22:56:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 848,\n",
      " 'downloader/request_count': 4,\n",
      " 'downloader/request_method_count/GET': 4,\n",
      " 'downloader/response_bytes': 1103,\n",
      " 'downloader/response_count': 4,\n",
      " 'downloader/response_status_count/301': 1,\n",
      " 'downloader/response_status_count/500': 3,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2018, 3, 31, 2, 56, 40, 917621),\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/500': 1,\n",
      " 'log_count/DEBUG': 6,\n",
      " 'log_count/INFO': 8,\n",
      " 'memusage/max': 53166080,\n",
      " 'memusage/startup': 53166080,\n",
      " 'response_received_count': 1,\n",
      " 'retry/count': 2,\n",
      " 'retry/max_reached': 1,\n",
      " 'retry/reason_count/500 Internal Server Error': 2,\n",
      " 'scheduler/dequeued': 4,\n",
      " 'scheduler/dequeued/memory': 4,\n",
      " 'scheduler/enqueued': 4,\n",
      " 'scheduler/enqueued/memory': 4,\n",
      " 'start_time': datetime.datetime(2018, 3, 31, 2, 56, 24, 630575)}\n",
      "2018-03-30 22:56:40 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "cat: ./zhihu.html: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#example WITHOUT user_agent \n",
    "cd ./web_crawler\n",
    "cat ./web_crawler/spiders/zhihu.py\n",
    "echo -e '\\n\\n'\n",
    "scrapy crawl zhihu\n",
    "echo -e '\\n\\n'\n",
    "cat ./zhihu.html\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log lines `'downloader/response_status_count/500': 3` shows that we have HTTP 500 Error (Internal Server Error) in scraping. Since there is no content downloaded, there is no `zhihu.html` file.\n",
    "\n",
    "To use user agent, we can set the default headers in `settings.py`:\n",
    "   \n",
    "    DEFAULT_REQUEST_HEADERS = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:59.0) Gecko/',\n",
    "        'Accept-Language': 'en',\n",
    "    }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "<!doctype html>\n",
      "<html lang=\"zh\" data-hairline=\"true\" data-theme=\"light\"><head><meta charset=\"utf-8\"/><title data-react-helmet=\"true\">知乎 - 发现更大的世界</title><meta name=\"viewport\" content=\"width=device-width,initial-scale=1,maximum-scale=1\"/><meta name=\"renderer\" content=\"webkit\"/><meta name=\"force-rendering\" content=\"webkit\"/><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"/><meta name=\"google-site-verification\" content=\"FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg\"/><link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"https://static.zhihu.com/static/favicon.ico\"/><link rel=\"dns-prefetch\" href=\"//static.zhimg.com\"/><link rel=\"dns-prefetch\" href=\"//pic1.zhimg.com\"/><link rel=\"dns-prefetch\" href=\"//pic2.zhimg.com\"/><link rel=\"dns-prefetch\" href=\"//pic3.zhimg.com\"/><link rel=\"dns-prefetch\" href=\"//pic4.zhimg.com\"/><link href=\"https://static.zhihu.com/heifetz/main.app.a427cd79f99bfefcc7d0.css\" rel=\"stylesheet\"/></head><body class=\"EntrySign-body\"><div id=\"root\"><div data-zop-userToken=\"{}\" data-reactroot=\"\" data-reactid=\"1\" data-react-checksum=\"-527650967\"><!-- react-empty: 2 --><div class=\"LoadingBar\" data-reactid=\"3\"></div><!-- react-empty: 4 --><div data-reactid=\"5\"><header role=\"banner\" class=\"Sticky AppHeader\" data-za-module=\"TopNavBar\" data-reactid=\"6\"><!-- react-empty: 7 --><div class=\"AppHeader-inner\" data-reactid=\"8\"><a href=\"//www.zhihu.com\" aria-label=\"知乎\" data-reactid=\"9\"><svg viewBox=\"0 0 200 91\" class=\"Icon ZhihuLogo Icon--logo\" style=\"height:30px;width:64px;\" width=\"64\" height=\"30\" aria-hidden=\"true\" data-reactid=\"10\"><title data-reactid=\"11\"></title><g data-reactid=\"12\"><path d=\"M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z\" fill-rule=\"evenodd\"/></g></svg></a><nav role=\"navigation\" class=\"AppHeader-nav\" data-reactid=\"13\"><a class=\"AppHeader-navItem\" href=\"//www.zhihu.com/\" data-reactid=\"14\">首页</a><a class=\"AppHeader-navItem\" href=\"//www.zhihu.com/explore\" data-reactid=\"15\">发现</a><a href=\"//www.zhihu.com/topic\" class=\"AppHeader-navItem\" data-reactid=\"16\">话题</a></nav><div class=\"SearchBar\" role=\"search\" data-reactid=\"17\"><div class=\"SearchBar-toolWrapper\" data-reactid=\"18\"><form class=\"SearchBar-tool\" data-reactid=\"19\"><div data-reactid=\"20\"><div class=\"Popover\" data-reactid=\"21\"><div class=\"SearchBar-input Input-wrapper Input-wrapper--grey\" data-reactid=\"22\"><input type=\"text\" maxlength=\"100\" value=\"\" autocomplete=\"off\" role=\"combobox\" aria-expanded=\"false\" aria-autocomplete=\"list\" aria-activedescendant=\"null--1\" id=\"null-toggle\" aria-haspopup=\"true\" aria-owns=\"null-content\" class=\"Input\" placeholder=\"搜索你感兴趣的内容…\" data-reactid=\"23\"/><div class=\"Input-after\" data-reactid=\"24\"><button class=\"Button SearchBar-searchIcon Button--primary\" aria-label=\"搜索\" type=\"button\" data-reactid=\"25\"><svg viewBox=\"0 0 16 16\" class=\"Icon Icon--search\" style=\"height:16px;width:16px;\" width=\"16\" height=\"16\" aria-hidden=\"true\" data-reactid=\"26\"><title data-reactid=\"27\"></title><g data-reactid=\"28\"><path d=\"M12.054 10.864c.887-1.14 1.42-2.57 1.42-4.127C13.474 3.017 10.457 0 6.737 0S0 3.016 0 6.737c0 3.72 3.016 6.737 6.737 6.737 1.556 0 2.985-.533 4.127-1.42l3.103 3.104c.765.46 1.705-.37 1.19-1.19l-3.103-3.104zm-5.317.925c-2.786 0-5.053-2.267-5.053-5.053S3.95 1.684 6.737 1.684 11.79 3.95 11.79 6.737 9.522 11.79 6.736 11.79z\"/></g></svg></button></div></div><!-- react-empty: 29 --></div></div></form></div></div><div class=\"AppHeader-userInfo\" data-reactid=\"30\"><!-- react-empty: 31 --><div class=\"AppHeader-profile\" data-reactid=\"32\"><div data-reactid=\"33\"><button class=\"Button AppHeader-login Button--blue\" type=\"button\" data-reactid=\"34\"><!-- react-text: 35 -->登录<!-- /react-text --></button><button class=\"Button Button--primary Button--blue\" type=\"button\" data-reactid=\"36\"><!-- react-text: 37 -->加入知乎<!-- /react-text --></button></div></div></div></div><!-- react-empty: 38 --></header></div><!-- react-empty: 39 --><main role=\"main\" class=\"App-main\" data-reactid=\"40\"><div class=\"SignFlowHomepage\" style=\"background-image:url(https://static.zhihu.com/heifetz/bg.8ca8122d44fc9a0f7b04.png);\" data-reactid=\"41\"><!-- react-empty: 42 --><div class=\"SignFlowHomepage-content\" data-reactid=\"43\"><div class=\"Card SignContainer-content\" data-reactid=\"44\"><div class=\"SignFlowHeader\" style=\"padding-bottom:5px;\" data-reactid=\"45\"><svg viewBox=\"0 0 200 91\" class=\"Icon ZhihuLogo Icon--logo\" style=\"height:65.625px;width:140px;\" width=\"140\" height=\"65.625\" aria-hidden=\"true\" data-reactid=\"46\"><title data-reactid=\"47\"></title><g data-reactid=\"48\"><path d=\"M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z\" fill-rule=\"evenodd\"/></g></svg><div class=\"SignFlowHeader-slogen\" data-reactid=\"49\"><!-- react-text: 50 -->注册<!-- /react-text --><!-- react-text: 51 -->知乎，发现更大的世界<!-- /react-text --></div></div><div class=\"SignContainer-inner\" data-reactid=\"52\"><div class=\"Register\" data-reactid=\"53\"><div data-reactid=\"54\"><div class=\"Register-content\" data-reactid=\"55\"><form novalidate=\"\" data-reactid=\"56\"><div class=\"SignFlow-account\" data-reactid=\"57\"><div class=\"SignFlow-supportedCountriesSelectContainer\" data-reactid=\"58\"></div><div class=\"SignFlowInput SignFlow-accountInputContainer\" data-reactid=\"59\"><div class=\"SignFlow-accountInput Input-wrapper\" data-reactid=\"60\"><input type=\"tel\" value=\"\" name=\"phoneNo\" class=\"Input\" placeholder=\"手机号\" data-reactid=\"61\"/></div><div class=\"SignFlowInput-errorMask SignFlowInput-requiredErrorMask SignFlowInput-errorMask--hidden\" data-reactid=\"62\"></div></div></div><div class=\"Captcha SignFlow-captchaContainer Register-captcha Captcha-chinese\" style=\"width:0px;height:0px;opacity:0;overflow:hidden;margin:0px;padding:0px;border:0px;\" data-reactid=\"63\"><div data-reactid=\"64\"><div class=\"Captcha-chineseOperator\" data-reactid=\"65\"><span class=\"Captcha-info\" data-reactid=\"66\">请点击图中倒立的文字</span><button class=\"Button Captcha-chineseRefreshButton Button--plain\" type=\"button\" data-reactid=\"67\"><svg class=\"Zi Zi--Refresh\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"20\" height=\"20\" data-reactid=\"68\"><path d=\"M20 12.878C20 17.358 16.411 21 12 21s-8-3.643-8-8.122c0-4.044 3.032-7.51 6.954-8.038.034-1.185.012-1.049.012-1.049-.013-.728.461-1.003 1.057-.615l3.311 2.158c.598.39.596 1.026 0 1.418l-3.31 2.181c-.598.393-1.08.12-1.079-.606 0 0 .006-.606-.003-1.157-2.689.51-4.675 2.9-4.675 5.708 0 3.21 2.572 5.822 5.733 5.822 3.163 0 5.733-2.612 5.733-5.822 0-.633.51-1.148 1.134-1.148.625 0 1.133.515 1.133 1.148\" fill-rule=\"evenodd\" data-reactid=\"69\"></path></svg></button></div><div class=\"Captcha-chineseContainer\" data-reactid=\"70\"><img data-tooltip=\"看不清楚？换一张\" class=\"Captcha-chineseImg\" src=\"data:image/jpg;base64,null\" alt=\"图形验证码\" data-reactid=\"71\"/></div></div></div><div class=\"Register-SMSInput\" data-reactid=\"72\"><div class=\"SignFlow SignFlow-smsInputContainer\" data-reactid=\"73\"><div class=\"SignFlowInput SignFlow-smsInput\" data-reactid=\"74\"><div class=\"Input-wrapper\" data-reactid=\"75\"><input type=\"number\" value=\"\" name=\"digits\" class=\"Input\" placeholder=\"输入 6 位短信验证码\" data-reactid=\"76\"/></div><div class=\"SignFlowInput-errorMask SignFlowInput-requiredErrorMask SignFlowInput-errorMask--hidden\" data-reactid=\"77\"></div></div><button class=\"Button CountingDownButton SignFlow-smsInputButton Button--plain\" type=\"button\" data-reactid=\"78\"><!-- react-text: 79 -->获取短信验证码<!-- /react-text --></button></div><div class=\"Register-smsBackUp\" data-reactid=\"80\"><span data-reactid=\"81\"><!-- react-text: 82 -->接收<!-- /react-text --><!-- react-text: 83 -->语音<!-- /react-text --><!-- react-text: 84 -->验证码<!-- /react-text --></span></div></div><button class=\"Button Register-submitButton Button--primary Button--blue\" type=\"submit\" data-reactid=\"85\"><!-- react-text: 86 -->注册<!-- /react-text --></button></form><div class=\"Register-footer\" data-reactid=\"87\"><span class=\"Register-declaration\" data-reactid=\"88\"><!-- react-text: 89 -->注册即代表你同意<!-- /react-text --><a href=\"https://www.zhihu.com/terms\" data-reactid=\"90\">《知乎协议》</a></span><a class=\"Register-org\" href=\"https://www.zhihu.com/org/signup\" data-reactid=\"91\">注册机构号</a></div></div></div></div><div class=\"SignContainer-switch\" data-reactid=\"92\"><!-- react-text: 93 -->已有帐号？<!-- /react-text --><span data-reactid=\"94\">登录</span></div><div class=\"SignFlowHomepage-qrImage SignFlowHomepage-qrImageHidden\" data-reactid=\"95\"><div data-reactid=\"96\"></div></div></div></div><button class=\"Button SignFlowHomepage-downloadBtn\" type=\"button\" data-reactid=\"97\"><!-- react-text: 98 -->下载知乎 App<!-- /react-text --></button></div><footer class=\"SignFlowHomepage-footer\" data-reactid=\"99\"><div class=\"ZhihuLinks\" data-reactid=\"100\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://zhuanlan.zhihu.com\" data-reactid=\"101\">知乎专栏</a><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/roundtable\" data-reactid=\"102\">圆桌</a><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/explore\" data-reactid=\"103\">发现</a><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/app\" data-reactid=\"104\">移动应用</a><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/contact\" data-reactid=\"105\">联系我们</a><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/careers\" data-reactid=\"106\">来知乎工作</a><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/org/signup\" data-reactid=\"107\">注册机构号</a></div><div class=\"ZhihuRights\" data-reactid=\"108\"><span data-reactid=\"109\">© 2018 知乎</span><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"http://www.miibeian.gov.cn/\" data-reactid=\"110\">京 ICP 证 110745 号</a><span data-reactid=\"111\"><!-- react-text: 112 -->京公网安备 11010802010035 号<!-- /react-text --><a href=\"http://zhstatic.zhihu.com/assets/zhihu/publish-license.jpg\" target=\"_blank\" rel=\"noopener noreferrer\" data-reactid=\"113\">出版物经营许可证</a></span></div><div class=\"ZhihuReports\" data-reactid=\"114\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://zhuanlan.zhihu.com/p/28852607\" data-reactid=\"115\">侵权举报</a><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"http://www.12377.cn\" data-reactid=\"116\">网上有害信息举报专区</a><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/jubao\" data-reactid=\"117\">儿童色情信息举报专区</a><span data-reactid=\"118\">违法和不良信息举报：010-82716601</span></div><div class=\"ZhihuIntegrity\" data-reactid=\"119\"><div data-reactid=\"120\"><img src=\"https://static.zhihu.com/static/revved/img/index/chengxing_logo@2x.65dc76e8.png\" alt=\"诚信网站示范企业\" data-reactid=\"121\"/><a href=\"https://credit.szfw.org/CX20170607038331320388.html\" data-reactid=\"122\">诚信网站示范企业</a></div></div></footer></div></main><!-- react-empty: 123 --><!-- react-empty: 124 --><!-- react-empty: 125 --><!-- react-empty: 126 --></div></div><div id=\"data\" style=\"display:none;\" data-state=\"{&quot;privacy&quot;:{&quot;showPrivacy&quot;:false},&quot;loading&quot;:{&quot;global&quot;:{&quot;count&quot;:0},&quot;local&quot;:{&quot;token/&quot;:false,&quot;env/getExperiments/&quot;:false}},&quot;entities&quot;:{&quot;users&quot;:{},&quot;questions&quot;:{},&quot;answers&quot;:{},&quot;articles&quot;:{},&quot;columns&quot;:{},&quot;topics&quot;:{},&quot;roundtables&quot;:{},&quot;favlists&quot;:{},&quot;comments&quot;:{},&quot;notifications&quot;:{},&quot;ebooks&quot;:{},&quot;activities&quot;:{},&quot;feeds&quot;:{},&quot;pins&quot;:{},&quot;promotions&quot;:{}},&quot;currentUser&quot;:&quot;&quot;,&quot;token&quot;:{&quot;xUDID&quot;:&quot;ADDvZNKSXg2PTmd22FKfXu-4p_adB2RSPYM=&quot;},&quot;account&quot;:{&quot;lockLevel&quot;:{},&quot;locakTicketStatus&quot;:false,&quot;challenge&quot;:[],&quot;errorStatus&quot;:false,&quot;message&quot;:&quot;&quot;,&quot;isFetching&quot;:false},&quot;notification&quot;:{},&quot;people&quot;:{&quot;profileStatus&quot;:{},&quot;activitiesByUser&quot;:{},&quot;answersByUser&quot;:{},&quot;answersSortByVotesByUser&quot;:{},&quot;answersIncludedByUser&quot;:{},&quot;votedAnswersByUser&quot;:{},&quot;thankedAnswersByUser&quot;:{},&quot;voteAnswersByUser&quot;:{},&quot;thankAnswersByUser&quot;:{},&quot;topicAnswersByUser&quot;:{},&quot;articlesByUser&quot;:{},&quot;articlesSortByVotesByUser&quot;:{},&quot;articlesIncludedByUser&quot;:{},&quot;pinsByUser&quot;:{},&quot;questionsByUser&quot;:{},&quot;commercialQuestionsByUser&quot;:{},&quot;favlistsByUser&quot;:{},&quot;followingByUser&quot;:{},&quot;followersByUser&quot;:{},&quot;mutualsByUser&quot;:{},&quot;followingColumnsByUser&quot;:{},&quot;followingQuestionsByUser&quot;:{},&quot;followingFavlistsByUser&quot;:{},&quot;followingTopicsByUser&quot;:{},&quot;publicationsByUser&quot;:{},&quot;columnsByUser&quot;:{},&quot;allFavlistsByUser&quot;:{},&quot;brands&quot;:null},&quot;env&quot;:{&quot;experiment&quot;:{&quot;ge3&quot;:&quot;ge3_9&quot;,&quot;ge2&quot;:&quot;ge2_1&quot;,&quot;navi&quot;:&quot;1&quot;,&quot;growthSearch&quot;:&quot;s2&quot;,&quot;sEI&quot;:&quot;c&quot;,&quot;nwebQAGrowth&quot;:&quot;experiment&quot;,&quot;qawebRelatedReadingsContentControl&quot;:&quot;close&quot;,&quot;nwebSearch&quot;:&quot;nweb_search_heifetz&quot;,&quot;rt&quot;:&quot;y&quot;,&quot;isOffice&quot;:&quot;false&quot;,&quot;enableTtsPlay&quot;:&quot;post&quot;,&quot;newLiveFeedMediacard&quot;:&quot;new&quot;,&quot;newMobileAppHeader&quot;:&quot;true&quot;,&quot;androidPassThroughPush&quot;:&quot;all&quot;,&quot;hybridZhmoreVideo&quot;:&quot;yes&quot;,&quot;nwebGrowthPeople&quot;:&quot;default&quot;,&quot;nwebSearchSuggest&quot;:&quot;default&quot;,&quot;qrcodeLogin&quot;:&quot;qrcode&quot;,&quot;enableVoteDownReasonMenu&quot;:&quot;enable&quot;,&quot;isShowUnicomFreeEntry&quot;:&quot;unicom_free_entry_off&quot;,&quot;growthBanner&quot;:&quot;default&quot;,&quot;newMobileColumnAppheader&quot;:&quot;new_header&quot;,&quot;rows&quot;:&quot;1&quot;,&quot;androidDbRecommendAction&quot;:&quot;open&quot;,&quot;zcmLighting&quot;:&quot;zcm&quot;,&quot;info&quot;:&quot;0&quot;,&quot;androidDbFeedHashTagStyle&quot;:&quot;button&quot;,&quot;appStoreRateDialog&quot;:&quot;close&quot;,&quot;mobileFeedGuide&quot;:&quot;block&quot;,&quot;default&quot;:&quot;None&quot;,&quot;isNewNotiPanel&quot;:&quot;no&quot;,&quot;adR&quot;:&quot;b&quot;,&quot;wechatShareModal&quot;:&quot;wechat_share_modal_show&quot;,&quot;uRe&quot;:&quot;1&quot;,&quot;androidProfilePanel&quot;:&quot;panel_b&quot;},&quot;experimentOrigin&quot;:{&quot;ge3&quot;:&quot;ge3_9&quot;,&quot;ge2&quot;:&quot;ge2_1&quot;,&quot;navi&quot;:&quot;1&quot;,&quot;growth_search&quot;:&quot;s2&quot;,&quot;SE_I&quot;:&quot;c&quot;,&quot;nwebQAGrowth&quot;:&quot;experiment&quot;,&quot;qaweb_related_readings_content_control&quot;:&quot;close&quot;,&quot;nweb_search&quot;:&quot;nweb_search_heifetz&quot;,&quot;rt&quot;:&quot;y&quot;,&quot;is_office&quot;:&quot;false&quot;,&quot;enable_tts_play&quot;:&quot;post&quot;,&quot;new_live_feed_mediacard&quot;:&quot;new&quot;,&quot;new_mobile_app_header&quot;:&quot;true&quot;,&quot;android_pass_through_push&quot;:&quot;all&quot;,&quot;hybrid_zhmore_video&quot;:&quot;yes&quot;,&quot;nweb_growth_people&quot;:&quot;default&quot;,&quot;nweb_search_suggest&quot;:&quot;default&quot;,&quot;qrcode_login&quot;:&quot;qrcode&quot;,&quot;enable_vote_down_reason_menu&quot;:&quot;enable&quot;,&quot;is_show_unicom_free_entry&quot;:&quot;unicom_free_entry_off&quot;,&quot;growth_banner&quot;:&quot;default&quot;,&quot;new_mobile_column_appheader&quot;:&quot;new_header&quot;,&quot;rows&quot;:&quot;1&quot;,&quot;android_db_recommend_action&quot;:&quot;open&quot;,&quot;zcm-lighting&quot;:&quot;zcm&quot;,&quot;info&quot;:&quot;0&quot;,&quot;android_db_feed_hash_tag_style&quot;:&quot;button&quot;,&quot;app_store_rate_dialog&quot;:&quot;close&quot;,&quot;mobile_feed_guide&quot;:&quot;block&quot;,&quot;default&quot;:&quot;None&quot;,&quot;is_new_noti_panel&quot;:&quot;no&quot;,&quot;ad_r&quot;:&quot;b&quot;,&quot;wechat_share_modal&quot;:&quot;wechat_share_modal_show&quot;,&quot;u_re&quot;:&quot;1&quot;,&quot;android_profile_panel&quot;:&quot;panel_b&quot;},&quot;userAgent&quot;:{&quot;Edge&quot;:false,&quot;Wechat&quot;:false,&quot;Weibo&quot;:false,&quot;QQ&quot;:false,&quot;Mobile&quot;:false,&quot;Android&quot;:false,&quot;iOS&quot;:false,&quot;isAppleDevice&quot;:false,&quot;Zhihu&quot;:false,&quot;ZhihuHybrid&quot;:false,&quot;isBot&quot;:false,&quot;Tablet&quot;:false,&quot;isWebView&quot;:false,&quot;origin&quot;:&quot;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:59.0) Gecko/&quot;},&quot;trafficSource&quot;:&quot;production&quot;,&quot;edition&quot;:{&quot;baidu&quot;:false,&quot;yidianzixun&quot;:false,&quot;sogou&quot;:false},&quot;theme&quot;:&quot;light&quot;,&quot;referer&quot;:&quot;&quot;,&quot;conf&quot;:{&quot;unblockContacts&quot;:{&quot;mobile&quot;:&quot;1069 02098 21212&quot;,&quot;telecom&quot;:&quot;1069 0920 21212&quot;,&quot;unicom&quot;:&quot;1069 0920 21212&quot;}},&quot;ipInfo&quot;:{}},&quot;me&quot;:{&quot;organizationProfileStatus&quot;:{}},&quot;comments&quot;:{&quot;pagination&quot;:{},&quot;collapsed&quot;:{},&quot;reverse&quot;:{},&quot;reviewing&quot;:{},&quot;conversation&quot;:{},&quot;parent&quot;:{}},&quot;pushNotifications&quot;:{&quot;default&quot;:{&quot;isFetching&quot;:false,&quot;isDrained&quot;:false,&quot;ids&quot;:[]},&quot;follow&quot;:{&quot;isFetching&quot;:false,&quot;isDrained&quot;:false,&quot;ids&quot;:[]},&quot;vote-thank&quot;:{&quot;isFetching&quot;:false,&quot;isDrained&quot;:false,&quot;ids&quot;:[]},&quot;currentTab&quot;:&quot;default&quot;,&quot;notificationsCount&quot;:{&quot;default&quot;:0,&quot;follow&quot;:0,&quot;vote-thank&quot;:0}},&quot;messages&quot;:{&quot;data&quot;:{},&quot;currentTab&quot;:&quot;common&quot;,&quot;messageCount&quot;:0},&quot;register&quot;:{&quot;registerValidateSucceeded&quot;:null,&quot;registerValidateErrors&quot;:{},&quot;registerConfirmError&quot;:null,&quot;sendDigitsError&quot;:null,&quot;registerConfirmSucceeded&quot;:null},&quot;login&quot;:{&quot;loginUnregisteredError&quot;:false,&quot;loginBindWechatError&quot;:false,&quot;loginConfirmError&quot;:null,&quot;sendDigitsError&quot;:null,&quot;validateDigitsError&quot;:false,&quot;loginConfirmSucceeded&quot;:null,&quot;qrcodeLoginToken&quot;:&quot;&quot;,&quot;qrcodeLoginScanStatus&quot;:0,&quot;qrcodeLoginError&quot;:null,&quot;qrcodeLoginReturnNewToken&quot;:false},&quot;active&quot;:{&quot;sendDigitsError&quot;:null,&quot;activeConfirmSucceeded&quot;:null,&quot;activeConfirmError&quot;:null},&quot;coupon&quot;:{&quot;isRedeemingCoupon&quot;:false},&quot;question&quot;:{&quot;followers&quot;:{},&quot;concernedFollowers&quot;:{},&quot;answers&quot;:{},&quot;hiddenAnswers&quot;:{},&quot;createdAnswers&quot;:{},&quot;collapsedAnswers&quot;:{},&quot;notificationAnswers&quot;:{},&quot;invitationCandidates&quot;:{},&quot;inviters&quot;:{},&quot;invitees&quot;:{},&quot;similarQuestions&quot;:{},&quot;relatedCommodities&quot;:{},&quot;recommendReadings&quot;:{},&quot;bio&quot;:{},&quot;brand&quot;:{},&quot;permission&quot;:{},&quot;advancedStyle&quot;:{},&quot;commonAnswerCount&quot;:0,&quot;hiddenAnswerCount&quot;:0,&quot;meta&quot;:{},&quot;autoInvitation&quot;:{}},&quot;shareTexts&quot;:{},&quot;answers&quot;:{&quot;voters&quot;:{},&quot;copyrightApplicants&quot;:{},&quot;favlists&quot;:{},&quot;newAnswer&quot;:{},&quot;concernedUpvoters&quot;:{}},&quot;banner&quot;:{},&quot;topic&quot;:{&quot;bios&quot;:{},&quot;hot&quot;:{},&quot;newest&quot;:{},&quot;top&quot;:{},&quot;unanswered&quot;:{},&quot;questions&quot;:{},&quot;followers&quot;:{},&quot;parent&quot;:{},&quot;children&quot;:{},&quot;bestAnswerers&quot;:{},&quot;index&quot;:{},&quot;intro&quot;:{}},&quot;captcha&quot;:{&quot;captchaNeeded&quot;:false,&quot;captchaValidated&quot;:false,&quot;captchaBase64String&quot;:null,&quot;captchaValidationMessage&quot;:null,&quot;loginCaptchaExpires&quot;:false},&quot;sms&quot;:{&quot;supportedCountries&quot;:[]},&quot;explore&quot;:{&quot;recommendations&quot;:{},&quot;hotfeeds&quot;:{}},&quot;articles&quot;:{&quot;voters&quot;:{}},&quot;favlists&quot;:{&quot;relations&quot;:{}},&quot;pins&quot;:{&quot;voters&quot;:{}},&quot;topstory&quot;:{&quot;topstorys&quot;:{&quot;isFetching&quot;:false,&quot;isDrained&quot;:false,&quot;afterId&quot;:0,&quot;items&quot;:[],&quot;next&quot;:null},&quot;sidebar&quot;:null,&quot;announcement&quot;:{},&quot;hotList&quot;:[]},&quot;upload&quot;:{},&quot;video&quot;:{&quot;data&quot;:{}},&quot;guide&quot;:{&quot;guide&quot;:{&quot;isFetching&quot;:false,&quot;isShowGuide&quot;:false}},&quot;switches&quot;:{},&quot;reward&quot;:{&quot;answer&quot;:{},&quot;article&quot;:{},&quot;question&quot;:{}},&quot;search&quot;:{&quot;recommendSearch&quot;:[],&quot;topSearch&quot;:{},&quot;attachedInfo&quot;:{},&quot;nextOffset&quot;:{},&quot;generalByQuery&quot;:{},&quot;generalByQueryInADay&quot;:{},&quot;generalByQueryInAWeek&quot;:{},&quot;generalByQueryInThreeMonths&quot;:{},&quot;peopleByQuery&quot;:{},&quot;topicByQuery&quot;:{},&quot;columnByQuery&quot;:{},&quot;liveByQuery&quot;:{},&quot;albumByQuery&quot;:{},&quot;eBookByQuery&quot;:{}}}\" data-config=\"{&quot;apiAddress&quot;:&quot;/api/v4/&quot;,&quot;deployEnv&quot;:&quot;production&quot;}\"></div><script src=\"https://static.zhihu.com/heifetz/vendor.d063778094ddaa35854a.js\"></script><script src=\"https://static.zhihu.com/heifetz/main.raven.f3d7c6e55fc98c076a11.js\" async=\"\"></script><script src=\"https://static.zhihu.com/heifetz/main.app.6092c4c532f38b2e5737.js\"></script><script></script></body></html>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-30 22:57:00 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: web_crawler)\n",
      "2018-03-30 22:57:00 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 18:10:19) - [GCC 7.2.0], pyOpenSSL 17.5.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.1.4, Platform Linux-4.13.0-37-generic-x86_64-with-debian-stretch-sid\n",
      "2018-03-30 22:57:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'web_crawler', 'DOWNLOAD_DELAY': 0.25, 'NEWSPIDER_MODULE': 'web_crawler.spiders', 'SPIDER_MODULES': ['web_crawler.spiders']}\n",
      "2018-03-30 22:57:00 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2018-03-30 22:57:00 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2018-03-30 22:57:00 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2018-03-30 22:57:00 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2018-03-30 22:57:00 [scrapy.core.engine] INFO: Spider opened\n",
      "2018-03-30 22:57:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2018-03-30 22:57:00 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2018-03-30 22:57:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.zhihu.com/> from <GET http://www.zhihu.com>\n",
      "2018-03-30 22:57:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.zhihu.com/signup?next=%2F> from <GET https://www.zhihu.com/>\n",
      "2018-03-30 22:57:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.zhihu.com/signup?next=%2F> (referer: None)\n",
      "2018-03-30 22:57:01 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2018-03-30 22:57:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 495,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 9073,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/301': 1,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2018, 3, 31, 2, 57, 1, 401265),\n",
      " 'log_count/DEBUG': 4,\n",
      " 'log_count/INFO': 7,\n",
      " 'memusage/max': 53538816,\n",
      " 'memusage/startup': 53538816,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2018, 3, 31, 2, 57, 0, 158216)}\n",
      "2018-03-30 22:57:01 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd ./web_crawler\n",
    "# write DEFAULT_REQUEST_HEADERS to settings.py\n",
    "echo -e \"DEFAULT_REQUEST_HEADERS = {\\n'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:59.0) Gecko/',\\n'Accept-Language': 'en',\\n}\" >> ./web_crawler/settings.py\n",
    "\n",
    "#execute crawl command again!\n",
    "scrapy crawl zhihu \n",
    "echo -e '\\n\\n'\n",
    "cat ./zhihu.html\n",
    "#scrapy crawl zhihu \n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with user agent, we get the page content we want. LOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] https://en.wikipedia.org/wiki/Web_crawler  \n",
    "[2] Typical Uses For Web Crawlers:https://blog.datafiniti.co/typical-uses-for-web-crawlers-c0860c5863ca  \n",
    "[3] Scrapy 1.5 documentation:https://doc.scrapy.org/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
