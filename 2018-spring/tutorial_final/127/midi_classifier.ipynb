{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Neural Networks and SVM to Classify Music Genres\n",
    "\n",
    "## Introduction\n",
    "Many people like listening to music, and some might be interested in determining\n",
    "the genre of the music that they are listening to. The midi format is a popular music\n",
    "format that can capture many aspects of given music, such as instrument, chord\n",
    "signatures and so on and it is also easy to create midi files using instruments\n",
    "like a portable piano or stage piano, as you could just record your playing and export\n",
    "it in the midi format. Therefore, in this tutorial, I would like to\n",
    "walk through the procedure of extracting features from midi data, matching them with genre labels\n",
    "from the million song dataset and training them on neural networks and SVMs to predict the\n",
    "genres of music, given any midi file.\n",
    "\n",
    "## Tutorial Content\n",
    "* [Package Installation and Imports](#step-1)\n",
    "* [Download and Parse Genre Labels](#step-2)\n",
    "* [Download, Parse and Match Midi Files](#step-3)\n",
    "* [Extract Features from Matched Midi Files](#step-4)\n",
    "* [Partition Dataset and Format Features](#step-5)\n",
    "* [Construct Models to Fit Labeled Features](#step-6)\n",
    "* [Calculate Test Accuracy](#step-7)\n",
    "* [Make a Prediction on a Midi File](#step-8)\n",
    "* [Analysis, Next Steps and Additional Information](#next-steps)\n",
    "* [Author and Credits](#credits)\n",
    "\n",
    "## Required Files and Libraries\n",
    "The following will be required for this tutorial:\n",
    "* The **`numpy`** module.\n",
    "* The **`pandas`** module.\n",
    "* The **`scikit-learn`** module.\n",
    "* The **`pretty_midi`** module for which documentation can be found at https://craffel.github.io/pretty-midi/.\n",
    "* The midi file dataset that can be found and downloaded from http://colinraffel.com/projects/lmd/.\n",
    "* The genre labels that can be found and downloaded from http://www.tagtraum.com/msd_genre_datasets.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-1\"></a>\n",
    "\n",
    "## Step 1: Package Installation and Imports\n",
    "The **`pretty_midi`** module can be installed using pip:\n",
    "    \n",
    "    $ pip install pretty_midi\n",
    "\n",
    "The other packages like `numpy`, `pandas` and `scikit-learn` can also be installed using pip:\n",
    "    \n",
    "    $ pip install numpy\n",
    "\n",
    "    $ pip install pandas\n",
    "\n",
    "    $ pip install scikit-learn\n",
    "\n",
    "We may also install these modules using the package manager of our choice, like `apt`, `homebrew`, `pacman`, `yaourt`, etc.\n",
    "\n",
    "After we have these installed, we need import these three modules and make sure we can run the following\n",
    "lines of code, otherwise we will need to try re-installing the modules or use another installation method\n",
    "instead. \n",
    "\n",
    "Aside from the modules mentioned above, we will also be using the `warnings` package to raise errors instead of \n",
    "warnings when a corrupted midi file is read, so that we can catch these errors and skip these files. The `os` \n",
    "package is used to traverse through the folder stucture of midi files to find each one and extract the track ID \n",
    "data from the working directory string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pretty_midi\n",
    "import warnings\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-2\"></a>\n",
    "\n",
    "## Step 2: Download and Parse Genre Labels\n",
    "In this step we will download and parse the genre labels and construct a pandas dataframe to store the data.\n",
    "Do the following:\n",
    "* Go to http://www.tagtraum.com/msd_genre_datasets.html\n",
    "* Scroll down to the Genre Ground Truth section to download the zip file under the label \"CD1\".\n",
    "* Then unzip the file in this folder.\n",
    "\n",
    "Now we can write a function to read the file and create a dataframe to store the contents.\n",
    "After writing the function, we can call it on the path to the genre label file, and if we\n",
    "print the first couple lines of the output, we should get a dataframe with the Track IDs and the corresponding \n",
    "Genres.\n",
    "\n",
    "We can also construct a list of genre labels that we have seen as well as a dictionary mapping genres to the\n",
    "index of that genre in the list to make it easier and faster to convert between a number and a genre. This will\n",
    "facilitate training and the prediction in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Genre             TrackID\n",
      "0  Pop_Rock  TRAAAAK128F9318786\n",
      "1       Rap  TRAAAAW128F429D538\n",
      "2  Pop_Rock  TRAAABD128F429CF47\n",
      "3      Jazz  TRAAAED128E0783FAB\n",
      "4  Pop_Rock  TRAAAEF128F4273421\n",
      "\n",
      "['Jazz', 'Folk', 'Electronic', 'Latin', 'Vocal', 'Blues', 'Rap', 'International', 'RnB', 'Reggae', 'New Age', 'Pop_Rock', 'Country']\n",
      "\n",
      "{'Jazz': 0, 'Folk': 1, 'Electronic': 2, 'Latin': 3, 'Vocal': 4, 'Blues': 5, 'Rap': 6, 'International': 7, 'RnB': 8, 'Reggae': 9, 'New Age': 10, 'Pop_Rock': 11, 'Country': 12}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_genres(path):\n",
    "    \"\"\"\n",
    "    This function reads the genre labels and puts it into a pandas DataFrame.\n",
    "    \n",
    "    @input path: The path to the genre label file.\n",
    "    @type path: String\n",
    "    \n",
    "    @return: A pandas dataframe containing the genres and midi IDs.\n",
    "    @rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    genres = []\n",
    "    with open(path) as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            if line[0] != '#':\n",
    "                [x, y, *_] = line.strip().split(\"\\t\")\n",
    "                ids.append(x)\n",
    "                genres.append(y)\n",
    "            line = f.readline()\n",
    "    genre_df = pd.DataFrame(data={\"Genre\": genres, \"TrackID\": ids})\n",
    "    return genre_df\n",
    "\n",
    "# Get the Genre DataFrame\n",
    "genre_path = \"msd_tagtraum_cd1.cls\"\n",
    "genre_df = get_genres(genre_path)\n",
    "\n",
    "# Create Genre List and Dictionary\n",
    "label_list = list(set(genre_df.Genre))\n",
    "label_dict = {lbl: label_list.index(lbl) for lbl in label_list}\n",
    "\n",
    "# Print to Visualize\n",
    "print(genre_df.head(), end=\"\\n\\n\")\n",
    "print(label_list, end=\"\\n\\n\")\n",
    "print(label_dict, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-3\"></a>\n",
    "\n",
    "## Step 3: Download, Parse and Match Midi Files\n",
    "In this step we will download the midi files, parse them then match them with the tracks for which we have\n",
    "genre labels for.\n",
    "\n",
    "We can follow the procedure below:\n",
    "* Go to http://colinraffel.com/projects/lmd/\n",
    "* Scroll down and click on the link in the section that says \"LMD-matched\" to download a midi dataset where each file is matched to an entry in the million song dataset.\n",
    "* After the download you can untar the file in this folder using the command\n",
    "    \n",
    "    \n",
    "    $ tar -xvf lmd_matched.tar.gz\n",
    "    \n",
    "Now we can write a function to traverse the folder structure of the midi files, create a dataframe\n",
    "to hold the contents, then join with the genre label dataframe in order to make sure that we only read files\n",
    "that have a label in genre dataframe that we created in [step 2](#step-2). At the end, we can again print out\n",
    "a part of our dataframe of matched midi file paths along with their corresponding genre to see the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Path     Genre\n",
      "0  lmd_matched/I/I/I/TRIIIMY128F4259A8E/1347136e2...  Pop_Rock\n",
      "1  lmd_matched/I/I/I/TRIIIMY128F4259A8E/e77645a3b...  Pop_Rock\n",
      "2  lmd_matched/I/I/U/TRIIUGA128F42AD432/861fea2b6...  Pop_Rock\n",
      "3  lmd_matched/I/I/U/TRIIUGA128F42AD432/e9af70dd5...  Pop_Rock\n",
      "4  lmd_matched/I/I/U/TRIIUGA128F42AD432/93cbbf687...  Pop_Rock\n"
     ]
    }
   ],
   "source": [
    "def get_matched_midi(midi_folder, genre_df):\n",
    "    \"\"\"\n",
    "    This function loads in midi file paths that are found in the given folder, puts this data into a\n",
    "    pandas DataFrame, then matches each entry with a genre described in get_genres.\n",
    "    \n",
    "    @input midi_folder: The path to the midi files.\n",
    "    @type midi_folder: String\n",
    "    @input genre_df: The genre label dataframe generated by get_genres.\n",
    "    @type genre_df: pandas.DataFrame\n",
    "    \n",
    "    @return: A dataframe of track id and path to a midi file with that track id.\n",
    "    @rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    # Get All Midi Files\n",
    "    track_ids, file_paths = [], []\n",
    "    for dir_name, subdir_list, file_list in os.walk(midi_folder):\n",
    "        if len(dir_name) == 36:\n",
    "            track_id = dir_name[18:]\n",
    "            file_path_list = [\"/\".join([dir_name, file]) for file in file_list]\n",
    "            for file_path in file_path_list:\n",
    "                track_ids.append(track_id)\n",
    "                file_paths.append(file_path)\n",
    "    all_midi_df = pd.DataFrame({\"TrackID\": track_ids, \"Path\": file_paths})\n",
    "    \n",
    "    # Inner Join with Genre Dataframe\n",
    "    df = pd.merge(all_midi_df, genre_df, on='TrackID', how='inner')\n",
    "    return df.drop([\"TrackID\"], axis=1)\n",
    "\n",
    "# Obtain DataFrame with Matched Genres to File Paths\n",
    "midi_path = \"lmd_matched\"\n",
    "matched_midi_df = get_matched_midi(midi_path, genre_df)\n",
    "\n",
    "# Print to Check Correctness\n",
    "print(matched_midi_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-4\"></a>\n",
    "\n",
    "## Step 4: Extract Features from Matched Midi Files\n",
    "In this step we will use the Python package called **`pretty_midi`**, and write a function to extract\n",
    "features and relevant information from midi files. This step will setup a design matrix for training.\n",
    "After we write our function, we can call it and store the design matrix in a variable for further use.\n",
    "Parsing the data from each midi file will take some time, so with the amount of data that we have in\n",
    "from our dataframe, this step will take about an hour to run.\n",
    "\n",
    "We will write a function called `normalize_features` to normalize a feature vector such that each value\n",
    "will approximately fall in the range (-1, 1) which will make it easier and faster for the machine\n",
    "learning modelto converge.\n",
    "\n",
    "After that, we need a function `get_features` to get some useful features from a given midi file. Over\n",
    "here we need to decide which features that `pretty_midi` can give us are more likely to be important\n",
    "factors that contribute to classifying the music genre. As an example I will extract the tempo, number\n",
    "of chord signature changes, the resolution as well as the time signature. In this function we will also\n",
    "have to detect corrupted file and make sure not to include them in our training.\n",
    "\n",
    "Then we will write a function to construct a matrix of all the features of the entire set of data that\n",
    "we obtained in [step 3](#step-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31833658 -0.35        0.125       0.125      11.        ]\n",
      " [ 0.32477658  0.55        0.125       0.125      11.        ]\n",
      " [ 0.0560317  -0.53        0.125       0.125      11.        ]\n",
      " ...\n",
      " [ 0.2000007  -0.17        0.125       0.125      11.        ]\n",
      " [ 0.24371827  0.31        0.125       0.125       3.        ]\n",
      " [ 0.22364894  0.31        0.125       0.125      11.        ]]\n",
      "CPU times: user 51min 10s, sys: 1min 10s, total: 52min 21s\n",
      "Wall time: 53min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def normalize_features(features):\n",
    "    \"\"\"\n",
    "    This function normalizes the features to the range [-1, 1]\n",
    "    \n",
    "    @input features: The array of features.\n",
    "    @type features: List of float\n",
    "    \n",
    "    @return: Normalized features.\n",
    "    @rtype: List of float\n",
    "    \"\"\"\n",
    "    tempo = (features[0] - 150) / 300\n",
    "    num_sig_changes = (features[1] - 2) / 10\n",
    "    resolution = (features[2] - 260) / 400\n",
    "    time_sig_1 = (features[3] - 3) / 8\n",
    "    time_sig_2 = (features[4] - 3) / 8\n",
    "    return [tempo, resolution, time_sig_1, time_sig_2]\n",
    "\n",
    "\n",
    "def get_features(path):\n",
    "    \"\"\"\n",
    "    This function extracts the features from a midi file when given its path.\n",
    "    \n",
    "    @input path: The path to the midi file.\n",
    "    @type path: String\n",
    "    \n",
    "    @return: The extracted features.\n",
    "    @rtype: List of float\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Test for Corrupted Midi Files\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"error\")\n",
    "            file = pretty_midi.PrettyMIDI(path)\n",
    "            \n",
    "            tempo = file.estimate_tempo()\n",
    "            num_sig_changes = len(file.time_signature_changes)\n",
    "            resolution = file.resolution\n",
    "            ts_changes = file.time_signature_changes\n",
    "            ts_1 = 4\n",
    "            ts_2 = 4\n",
    "            if len(ts_changes) > 0:\n",
    "                ts_1 = ts_changes[0].numerator\n",
    "                ts_2 = ts_changes[0].denominator\n",
    "            return normalize_features([tempo, num_sig_changes, resolution, ts_1, ts_2])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_midi_features(path_df):\n",
    "    \"\"\"\n",
    "    This function takes in the path DataFrame, then for each midi file, it extracts certain\n",
    "    features, maps the genre to a number and concatenates these to a large design matrix to return.\n",
    "    \n",
    "    @input path_df: A dataframe with paths to midi files, as well as their corresponding matched genre.\n",
    "    @type path_df: pandas.DataFrame\n",
    "    \n",
    "    @return: A matrix of features along with label.\n",
    "    @rtype: numpy.ndarray of float\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    for index, row in path_df.iterrows():\n",
    "        features = get_features(row.Path)\n",
    "        genre = label_dict[row.Genre]\n",
    "        if features is not None:\n",
    "            features.append(genre)\n",
    "            all_features.append(features)\n",
    "    return np.array(all_features)\n",
    "\n",
    "labeled_features = extract_midi_features(matched_midi_df)\n",
    "print(labeled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-5\"></a>\n",
    "\n",
    "## Step 5: Partition Dataset and Format Features\n",
    "In this step, we will partition the data set so that 60% of it is used for training the data, 20% is used for\n",
    "validation (to tune the hyperparameters of the neural network), and the remaining 20% is used as the testing\n",
    "set to report our accuracy.\n",
    "\n",
    "When writing the code to partition, it is nice to randomly permute the whole set of data first so that we can\n",
    "get a more random sets of data for each of the partitions.\n",
    "\n",
    "After splitting the data, we also need to separate the features from the labels, and we also need to write a\n",
    "function to encode the labels using one-hot encoding for multi-class classification with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15499788 -0.35        0.125       0.125     ]\n",
      " [ 0.07662544 -0.17        0.125       0.125     ]\n",
      " [ 0.11022558 -0.53        0.125       0.125     ]\n",
      " [ 0.1958105  -0.41        0.125       0.125     ]\n",
      " [ 0.15969576 -0.35        0.125       0.125     ]\n",
      " [ 0.09223524  0.55        0.125       0.125     ]\n",
      " [ 0.28787879 -0.05        0.125       0.125     ]\n",
      " [ 0.05369878  0.31        0.125       0.125     ]\n",
      " [ 0.1066389   0.31        0.125       0.125     ]\n",
      " [ 0.36558283  0.31        0.125       0.125     ]]\n",
      "[11 11 11 11 11  6 11 11  3 11]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle Entire Dataset to Make Random\n",
    "labeled_features = np.random.permutation(labeled_features)\n",
    "\n",
    "# Partition into 3 Sets\n",
    "num = len(labeled_features)\n",
    "num_training = int(num * 0.6)\n",
    "num_validation = int(num * 0.8)\n",
    "training_data = labeled_features[:num_training]\n",
    "validation_data = labeled_features[num_training:num_validation]\n",
    "test_data = labeled_features[num_validation:]\n",
    "\n",
    "# Separate Features from Labels\n",
    "num_cols = training_data.shape[1] - 1\n",
    "training_features = training_data[:, :num_cols]\n",
    "validation_features = validation_data[:, :num_cols]\n",
    "test_features = test_data[:, :num_cols]\n",
    "\n",
    "# Format Features for Multi-class Classification\n",
    "num_classes = len(label_list)\n",
    "training_labels = training_data[:, num_cols].astype(int)\n",
    "validation_labels = validation_data[:, num_cols].astype(int)\n",
    "test_labels = test_data[:, num_cols].astype(int)\n",
    "\n",
    "# Function for One-Hot Encoding\n",
    "def one_hot(labels):\n",
    "    \"\"\"\n",
    "    This function encodes the labels using one-hot encoding.\n",
    "    \n",
    "    @input num_classes: The number of genres/classes.\n",
    "    @type num_classes: int\n",
    "    @input labels: The genre labels to encode.\n",
    "    @type labels: numpy.ndarray of int\n",
    "    \n",
    "    @return: The one-hot encoding of the labels.\n",
    "    @rtype: numpy.ndarray of int\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels].astype(int)\n",
    "\n",
    "# Print to Check Dimentions and to Visualize\n",
    "print(test_features[:10])\n",
    "print(test_labels[:10])\n",
    "print(one_hot(test_labels)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"step-6\"></a>\n",
    "\n",
    "## Step 6: Construct Models to Fit Labeled Features\n",
    "\n",
    "In this step we will use `scikit-learn` to fit our data to neural networks with different\n",
    "configurations and an SVM. `scikit-learn` has a Multi-layer Perceptron model as well as\n",
    "an Support Vector Classifier model that we can use for supervised learning. We will in this\n",
    "step, use the validation data set to tune our hyperparameters, pick the best model and\n",
    "return the classifier that performed best on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.7511980830670927\n"
     ]
    }
   ],
   "source": [
    "def train_model(t_features, t_labels, v_features, v_labels):\n",
    "    \"\"\"\n",
    "    This function trains a neural network using a couple different configurations.\n",
    "    \n",
    "    @input t_features: The training features.\n",
    "    @type t_features: numpy.ndarray of float\n",
    "    @input t_labels: The training labels.\n",
    "    @type t_labels: numpy.ndarray of int\n",
    "    @input v_features: The validation features.\n",
    "    @type v_features: numpy.ndarray of float\n",
    "    @input v_labels: The validation labels.\n",
    "    @type v_labels: numpy.ndarray of int\n",
    "    \n",
    "    @return: The classifier that achieved the best validation accuracy.\n",
    "    @rtype: sklearn.neural_network.multilayer_perceptron.MLPClassifier\n",
    "    \"\"\"\n",
    "    # Neural Network and SVM Configurations\n",
    "    clf_1 = MLPClassifier(solver='adam', alpha=1e-4, hidden_layer_sizes=(5,), random_state=1)\n",
    "    clf_2 = MLPClassifier(solver='adam', alpha=1e-4, hidden_layer_sizes=(5, 5), random_state=1)\n",
    "    clf_3 = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(10, 10), random_state=1)\n",
    "    clf_4 = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(100, 100), random_state=1)\n",
    "    clf_svm = SVC()\n",
    "    \n",
    "    # Keep Track of the Best Model\n",
    "    best_clf = None\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    # Test the Accuracies of the Models and Get Best\n",
    "    for clf in [clf_1, clf_2, clf_3, clf_4, clf_svm]:\n",
    "        t_labels_hot = one_hot(t_labels)\n",
    "        v_labels_hot = one_hot(v_labels)\n",
    "        if (type(clf) == SVC):\n",
    "            clf = clf.fit(t_features, t_labels)\n",
    "        else:\n",
    "            clf = clf.fit(t_features, t_labels_hot)\n",
    "        predictions = clf.predict(v_features)\n",
    "        count = 0\n",
    "        for i in range(len(v_labels)):\n",
    "            if (type(clf) != SVC):\n",
    "                if np.array_equal(v_labels_hot[i], predictions[i]):\n",
    "                    count += 1\n",
    "            else:\n",
    "                if v_labels[i] == predictions[i]:\n",
    "                    count += 1\n",
    "        accuracy = count / len(v_labels_hot)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_clf = clf\n",
    "\n",
    "    print(\"Best Accuracy:\", best_accuracy)\n",
    "    return best_clf\n",
    "\n",
    "classifier = train_model(training_features, training_labels, validation_features, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"step-7\"></a>\n",
    "\n",
    "## Step 7: Calculate Test Accuracy\n",
    "In this step we will calculate the test accuracy of our trained model. Now we can use\n",
    "our test data set that we created in [step-5](#step-5). This will be an accurate\n",
    "reporting since our classification model has never seen this test set before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7469061876247505\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(clf, t_features, t_labels):\n",
    "    \"\"\"\n",
    "    This function takes a trained model as well as the test features and its\n",
    "    corresponding labels, and reports the accuracy of the model.\n",
    "    \n",
    "    @input clf: The trained classifier.\n",
    "    @type model: sklearn.neural_network.multilayer_perceptron.MLPClassifier\n",
    "    @input t_features: The features from the test set.\n",
    "    @type f_features: numpy.ndarray of float\n",
    "    @input t_labels: The labels of the test set features.\n",
    "    @type t_labels: numpy.ndarray of int\n",
    "    \n",
    "    @return: The accuracy.\n",
    "    @rtype: float\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    predictions = clf.predict(t_features)\n",
    "    t_labels_hot = one_hot(t_labels)\n",
    "    for i in range(len(t_features)):\n",
    "        if (type(clf) == SVC):\n",
    "            if t_labels[i] == predictions[i]:\n",
    "                count += 1\n",
    "        else:\n",
    "            if np.array_equal(t_labels_hot[i], predictions[i]):\n",
    "                count += 1\n",
    "    return count / len(t_features)\n",
    "\n",
    "# Print the Test Accuracy\n",
    "print(calculate_accuracy(classifier, test_features, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-8\"></a>\n",
    "\n",
    "## Step 8: Make a Prediction on a Midi File\n",
    "\n",
    "In this step we will make a prediction on a midi file and print out the genre that\n",
    "the classifier predicts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop_Rock\n"
     ]
    }
   ],
   "source": [
    "def make_prediction(clf, midi_path):\n",
    "    \"\"\"\n",
    "    This function uses the classifier to predict the genre of a midi file.\n",
    "    \n",
    "    @input clf: The trained classifier.\n",
    "    @type clf: sklearn.neural_network.multilayer_perceptron.MLPClassifier\n",
    "    @input midi_path: The path to the midi file that we are trying to classify.\n",
    "    @type midi_path: String\n",
    "    \n",
    "    @return: The predicted genre of the midi file.\n",
    "    @rtype: String\n",
    "    \"\"\"\n",
    "    features = get_features(midi_path)\n",
    "    prediction_ind = list(clf.predict([features])[0]).index(1)\n",
    "    prediction = label_list[prediction_ind]\n",
    "    return prediction\n",
    "    \n",
    "# Make a Prediction\n",
    "test_midi_path =\"lmd_matched/B/F/E/TRBFELB128F426BFF2/289270d85c81802d912c9907c645dc2d.mid\"\n",
    "print(make_prediction(classifier, test_midi_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"next-steps\"></a>\n",
    "\n",
    "## Analysis, Next Steps and Additional Information\n",
    "\n",
    "For this classification, we achieved a test accuracy of 75% which is not very high. However, if we\n",
    "use the `calculate_accuracy` function to check our training accuracy, we will see that it is fairly\n",
    "similiar to our test accuracy, which means that we are not over-fitting our data. Therefore, one\n",
    "reason for our low accuracy may be that our features are not relevant enough to our genre predicion.\n",
    "For example, different genres of music may have different tempos, or the same genre might have a wide\n",
    "range of possible tempos. Another possible reason could be that we have messy data, and that some of\n",
    "the labels of midi files are wrong. We did, however, walk through the entire pipeline of parsing the\n",
    "raw midi data, matching them with genres and training the parsed features using different machine\n",
    "learning models and making a correct prediction on an unlabeled file!\n",
    "\n",
    "Given our analysis, some of the next things that we can do are:\n",
    "* Take a deeper look into music theory and decide which features of music are more relevant to a certain genre.\n",
    "* Analyze the data to see if some extracted features do not differ a lot between different genres.\n",
    "* Listen to some matched midi files and make a classification by ear to see if it matches the label.\n",
    "* Use different machine learning models to see if they perform better.\n",
    "\n",
    "\n",
    "It might be useful to look up on some additional information:\n",
    "* [F1 score](https://en.wikipedia.org/wiki/F1_score) (This might be useful since our data is not stratified and we have a large number of some label, and fewer others)\n",
    "* [scikit-learn classifiers](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) might help us choose different classifiers that could possible give a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"credits\"></a>\n",
    "\n",
    "## Author and Credits\n",
    "The author of this tutorial is Sander Shi, a Computer Science undergraduate at Carnegie Mellon University. This\n",
    "tutorial is created as a class project for Practical Data Science (15-388) taught by Zico Kolter during the\n",
    "Spring of 2018.\n",
    "\n",
    "The midi dataset that I used in this tutorial is created by Colin Raffel, and the website for the dataset can\n",
    "be found [here](http://colinraffel.com/projects/lmd). The Python module called `pretty_midi` that I used in\n",
    "this tutorial is also created by Colin Raffel, and the link to it can be found\n",
    "[here](http://craffel.github.io/pretty-midi/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
