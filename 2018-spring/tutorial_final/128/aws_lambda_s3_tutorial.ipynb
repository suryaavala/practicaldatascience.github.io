{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AWS S3 and AWS Lambda for Data Science\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/320px-Amazon_Web_Services_Logo.svg.png\">\n",
    "\n",
    "## Introduction\n",
    "This tutorial will introduce you to AWS S3 and AWS Lambda. More specifically, I will walk you through the AWS software development kit for Python called Boto 3 as well as the Python [function handler](https://docs.aws.amazon.com/lambda/latest/dg/python-programming-model-handler-types.html) for AWS Lambda.\n",
    "\n",
    "AWS S3 is a storage service running on AWS, Amazon's cloud computing service. This service is particularly useful in the context of Data Science and especially Big Data as it will allow us to easily manipulate large amounts of data without having to store them locally on our own machine.\n",
    "\n",
    "AWS Lambda is a [serverless computing](https://en.wikipedia.org/wiki/Serverless_computing) service also provided by Amazon. It lets you run code on the cloud without having to worry about setting up any kind of server. Our code will be executed only when needed and AWS Lambda will take care of scaling servers automatically. Because of this, AWS Lambda is adequate both for functions that are called a few times as well as thousands of times per second. We will only be charged when our code is running. In this tutorial we will use AWS Lambda to act over data stored inside S3.\n",
    "\n",
    "Once we know how to store data in the cloud using S3 and how to manipulate it using AWS Lambda, we could run a whole data science project in the cloud!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up an AWS Account\n",
    "In order to follow this tutorial, you will need an AWS account. Please follow this [link](https://aws.amazon.com/free) in case you don't already have one. Then sign in to the [console](https://signin.aws.amazon.com) to check that your account works properly.\n",
    "\n",
    "The console allows you to access all of the AWS services, including S3 and Lambda. It is a web interface for all of Amazon's cloud products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS S3\n",
    "Let's start with S3. We will use Boto 3, the Python API for AWS. Please note that most of what we will do here can be achieved by using the AWS console. However, this is no the goal here. We are trying to become familiar with the Boto 3 library.\n",
    "\n",
    "### Installing and Configuring Boto 3\n",
    "First run `pip install boto3` on your machine to install the latest version of Boto 3 and make sure the following Python import works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have succesfully installed Boto 3, we need to configure it with our AWS account. There are multiple ways to do this, they are all described in the [documentation](http://boto3.readthedocs.io/en/latest/guide/configuration.html) (feel free to consult this if you whish to connect to AWS in a different way from what we present).\n",
    "\n",
    "The recommended way to connect to AWS in Python is to use the [AWS CLI](https://aws.amazon.com/cli/). However, in this tutorial, we will simply provide our credentials as method parameters. This is condidered bad practice if our confidential information is hard coded. We do this as it is much more adapted to the jupyter notebook format of this tutorial.\n",
    "\n",
    "Before we can provide our credentials, we will need to generate them. Following best practices, we will create users with a restricted set of permissions to connect to Boto 3:\n",
    "* Go to the [following page](https://console.aws.amazon.com/iam/home?region=us-east-1#/users$new?step=details) to add a new user.\n",
    "* Provide a name and make sure to check the \"Programmatic access\" option.\n",
    "* Click on \"Next: Permissions\".\n",
    "* Select \"Attach existing policies directly\" and add \"AmazonS3FullAccess\", \"AWSLambdaFullAccess\" as well as \"AmazonSNSFullAccess\".\n",
    "* Click on \"Next: Review\" and then \"Create user\".\n",
    "\n",
    "The new user's credentials are now available to you. Please provide these values in the following variables if you wish to follow along. You can also provide your andrew id (or any personnal identifier) to personnalize any name that is required to be completely unique by Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID = ''\n",
    "AWS_SECRET_ACCESS_KEY = ''\n",
    "ANDREW_ID = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with AWS S3\n",
    "In order to interact with S3 using Boto 3 we can use two different objects: `client` and `resource`. `client` is used to call AWS's service API directly, it provides a low-level access.  `resource` is a more high-level representation of available actions in S3, it usually provides a more concise and elegant way to perform actions.\n",
    "Here we will see how to create both these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3', aws_access_key_id = AWS_ACCESS_KEY_ID, aws_secret_access_key = AWS_SECRET_ACCESS_KEY)\n",
    "s3_client = boto3.client('s3', aws_access_key_id = AWS_ACCESS_KEY_ID, aws_secret_access_key = AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the necessary objects to send calls to AWS S3, we will first create a bucket. A bucket is where we will store key-value pairs corresponding to files. You can think of it as a directory where our files will be stored. You can create several buckets if you wish. As much as possible, we will try to use both `client` and `resource` in order to compare them. The result should be identical whether we use one or the other.\n",
    "\n",
    "For this step and all the following ones, you can check on the [S3 page](https://s3.console.aws.amazon.com/s3) that everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Location': '/my-andrew-id.15388-bucket-client',\n",
       " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '0',\n",
       "   'date': 'Sat, 31 Mar 2018 22:24:25 GMT',\n",
       "   'location': '/my-andrew-id.15388-bucket-client',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': 'ayTrn50oyBU4CHWU5VSubLOq2p22Y7M7lWVpT9MzAuwAgZFVXne8Jy4ZT55pCOSQ88mOZ5INUtQ=',\n",
       "   'x-amz-request-id': 'E8A46D95CF4669A1'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HostId': 'ayTrn50oyBU4CHWU5VSubLOq2p22Y7M7lWVpT9MzAuwAgZFVXne8Jy4ZT55pCOSQ88mOZ5INUtQ=',\n",
       "  'RequestId': 'E8A46D95CF4669A1',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using resource:\n",
    "bucket_name_r = ANDREW_ID + '.15388-bucket-resource'\n",
    "s3.create_bucket(Bucket = bucket_name_r)\n",
    "\n",
    "#Using client:\n",
    "bucket_name_c = ANDREW_ID + '.15388-bucket-client'\n",
    "s3_client.create_bucket(Bucket = bucket_name_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that when creating a bucket, both `resource` and `client` use the same function. However, this is generally not the case as we will see when trying to list all the buckets created so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using resource:\n",
      "my-andrew-id.15388-bucket-client\n",
      "my-andrew-id.15388-bucket-resource\n",
      "\n",
      "\n",
      "Using client:\n",
      "my-andrew-id.15388-bucket-client\n",
      "my-andrew-id.15388-bucket-resource\n"
     ]
    }
   ],
   "source": [
    "# Using resource:\n",
    "print('Using resource:')\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "# Using client:\n",
    "response = s3_client.list_buckets()\n",
    "print('Using client:')\n",
    "for bucket in response['Buckets']:\n",
    "    print(bucket['Name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our buckets are empty for the moment. We can change this by uploading a file. Files in S3 are simply key-value pairs where the key is a file name and the value is the file content. We will see how to upload a file to the two buckets we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'aws_lambda_s3_tutorial.ipynb'\n",
    "\n",
    "# Using resource:\n",
    "file_content = open(file_name, 'rb')\n",
    "response_resource = s3.Object(bucket_name_r, file_name).put(Body = file_content)\n",
    "\n",
    "# Using client:\n",
    "s3_client.upload_file(file_name, bucket_name_c, file_name)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When uploading using `put`, we strored the response generated by that call. Most operations in Boto 3 return such a response, they can be used to fetch information about the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'BBDCC9BDCC27A265', 'HostId': 'Rr+WyMeKIUEmLCRAh96wFuio1unDAc6Ei3ZguziDnSkRKEHYqs1WccFrBrVsqp0EEOOxI6HeWBM=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'Rr+WyMeKIUEmLCRAh96wFuio1unDAc6Ei3ZguziDnSkRKEHYqs1WccFrBrVsqp0EEOOxI6HeWBM=', 'x-amz-request-id': 'BBDCC9BDCC27A265', 'date': 'Sat, 31 Mar 2018 22:25:40 GMT', 'etag': '\"61cff2c5c3a8f3505f1803f1792ddc11\"', 'content-length': '0', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'ETag': '\"61cff2c5c3a8f3505f1803f1792ddc11\"'}\n",
      "\n",
      "File successfully added on Sat, 31 Mar 2018 22:25:40 GMT.\n"
     ]
    }
   ],
   "source": [
    "# Most operations return a response that allows us to gather information\n",
    "print(str(response_resource) + '\\n')\n",
    "\n",
    "# Some fieds are more interesting than others\n",
    "if response_resource['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "    time = str(response_resource['ResponseMetadata']['HTTPHeaders']['date'])\n",
    "    print('File successfully added on ' + time + '.')\n",
    "else:\n",
    "    print('An error occured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can list the files that have been uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using resource:\n",
      "my-andrew-id.15388-bucket-client:\n",
      "  aws_lambda_s3_tutorial.ipynb\n",
      "my-andrew-id.15388-bucket-resource:\n",
      "  aws_lambda_s3_tutorial.ipynb\n",
      "\n",
      "\n",
      "Using client:\n",
      "my-andrew-id.15388-bucket-client:\n",
      "  aws_lambda_s3_tutorial.ipynb\n",
      "my-andrew-id.15388-bucket-resource:\n",
      "  aws_lambda_s3_tutorial.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Using resource:\n",
    "print('Using resource:')\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name + ':')\n",
    "    for key in bucket.objects.all():\n",
    "        print('  ' + key.key)\n",
    "        \n",
    "print('\\n')        \n",
    "        \n",
    "# Using client:\n",
    "print('Using client:')\n",
    "response = s3_client.list_buckets()\n",
    "for bucket in response['Buckets']:\n",
    "    bucket_name = bucket['Name']\n",
    "    print(bucket_name + ':')\n",
    "    for key in s3_client.list_objects_v2(Bucket = bucket_name)['Contents']:\n",
    "        print('  ' + key['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we notice in the previous cell that `client` is much slower than `resource` for listing all files. The notation for `resource` is also more concise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how to delete a bucket. Before deleting any bucket, we need to remove all of its keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using resource:\n",
    "# First we find our bucket\n",
    "bucket = None\n",
    "for b in s3.buckets.all():\n",
    "    if b.name == bucket_name_r:\n",
    "        bucket = b\n",
    "        \n",
    "# Now we delete all keys and the bucket\n",
    "if bucket != None:\n",
    "    for key in bucket.objects.all():\n",
    "        key.delete()\n",
    "    bucket.delete()\n",
    "\n",
    "# Using client:\n",
    "# We need to check that the bucket exists first\n",
    "bucket = None\n",
    "response = s3_client.list_buckets()\n",
    "for bucket in response['Buckets']:\n",
    "    if bucket['Name'] == bucket_name_c:\n",
    "        bucket = bucket['Name']\n",
    "        \n",
    "if bucket != None:        \n",
    "    for key in s3_client.list_objects(Bucket = bucket)['Contents']:\n",
    "        s3_client.delete_object(Bucket = bucket, Key = key['Key'])\n",
    "    s3_client.delete_bucket(Bucket = bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now familiar enough with the Boto 3 API to perform all the imporant operations related to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Lambda\n",
    "### Creating our function\n",
    "To create an AWS Lambda function, go to this [link](https://console.aws.amazon.com/lambda/home?region=us-east-1#/create). From here:\n",
    "* Give a name to your function (`15388-tutorial` for example).\n",
    "* Select Python 3.6 as the runtime.\n",
    "* Select \"Create a custom role\". This will take you to another page where you can provide a name and leave the rest as it is and click \"Allow\".\n",
    "* Now go back to the Lambda creation page and click on \"Create function\".\n",
    "\n",
    "You are now on the lambda management console. From here you can tweak different things, including the permissions of your function and the amount of memory and compute time available to it. Go to the \"Function code\" cell, from here you can edit your lambda function. We are given this template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_handler(event, context):\n",
    "    # TODO implement\n",
    "    return 'Hello from Lambda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `event` argument will contain a dictionary describing the event that caused the function to run. There are several ways to cause the function to run, one of the easier ones is AWS SNS which we will not cover in this tutorial. You can find more information about SNS [here](https://aws.amazon.com/sns/getting-started/).\n",
    "\n",
    "### Interacting with S3 from our lambda function\n",
    "Let's create two buckets like we have learned. We will use them to get familiar with using Boto 3 inside of AWS Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Bucket(name='15388-lambda-bucket2-my-andrew-id')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_bucket_1 = '15388-lambda-bucket1-' + ANDREW_ID\n",
    "lambda_bucket_2 = '15388-lambda-bucket2-' + ANDREW_ID\n",
    "\n",
    "s3.create_bucket(Bucket = lambda_bucket_1)\n",
    "s3.create_bucket(Bucket = lambda_bucket_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the \"Test\" button at the top-left of the Lambda management console. From here we will provide the following: \n",
    "`{\"bucket1\": \"\", \"bucket2\": \"\", \"key_id\": \"\", \"secret_key_id\": \"\"}` where we fill in the two fields with the names of the two newly created buckets and the rest with our credentials.\n",
    "\n",
    "Next time you press \"Test\", the function will be called with the provided input in `event`.\n",
    "\n",
    "Let's look at what a full example would look like (you can access the logs from calls to `print` by clicking on `logs` after each execution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Fetch the names from the event\n",
    "    bucket1_name = event['bucket1']\n",
    "    print('bucket1_name: ' + bucket1_name)\n",
    "    bucket2_name = event['bucket2']\n",
    "    print('bucket2_name: ' + bucket1_name)\n",
    "    \n",
    "    # We will use resource as it leads to cleaner code\n",
    "    s3 = boto3.resource('s3', aws_access_key_id = event['key_id'], aws_secret_access_key = event['secret_key_id'])\n",
    "    \n",
    "    # Let's acquire the objects\n",
    "    bucket1 = None\n",
    "    bucket2 = None\n",
    "    for bucket in s3.buckets.all():\n",
    "        if bucket.name == bucket1_name:\n",
    "            print('Found ' + bucket1_name + '!')\n",
    "            bucket1 = bucket\n",
    "        elif bucket.name == bucket2_name:\n",
    "            print('Found ' + bucket2_name + '!')\n",
    "            bucket2 = bucket\n",
    "    \n",
    "    # Check that they exist\n",
    "    if bucket1 == None or bucket2 == None:\n",
    "        return 'Could not find both buckets in S3.'\n",
    "            \n",
    "    # We can store the files in lists\n",
    "    file1_list = []\n",
    "    file2_list = []\n",
    "    for key in bucket1.objects.all():\n",
    "        file1_list.append(key.key)\n",
    "    for key in bucket2.objects.all():\n",
    "        file2_list.append(key.key)\n",
    "            \n",
    "    print(bucket1_name + ' contains ' + str(file1_list))\n",
    "    print(bucket2_name + ' contains ' + str(file2_list))\n",
    "    \n",
    "    # Here we download the files to the local server's temp directory\n",
    "    try:\n",
    "        for f in file1_list:\n",
    "            s3.Bucket(bucket1_name).download_file(f, '/tmp/' + f)\n",
    "        for f in file2_list:\n",
    "            s3.Bucket(bucket2_name).download_file(f, '/tmp/' + f)\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "    \n",
    "    # We check that the files have been properly downloaded\n",
    "    local_files = []\n",
    "    for f in os.listdir(\"/tmp/\"):\n",
    "        local_files.append(f)\n",
    "    \n",
    "    return 'The following files were downloaded ' + str(local_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to download locally the files, we can use everything we learned during the course to gather insight.\n",
    "\n",
    "# Further resources\n",
    "* [AWS S3 documentation](https://aws.amazon.com/documentation/s3/)\n",
    "* [AWS Lambda documentation](https://aws.amazon.com/lambda/getting-started/)\n",
    "* [AWS SNS documentation](https://aws.amazon.com/documentation/sns/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
