{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Text Classification to identify UGbots from the 2016 Ugandan Election\n",
    "We live in a time where `fake news` is becoming more and more prevalent, with users not really sure how much stock they should put in what they read online. It is important for social media users to be able to identify genuine information from biased information. This presents a unique opportunity to apply data science (i.e. Text Classification) to help with this identification. \n",
    "\n",
    "One good case for this was the 2016 Ugandan Presidential Election. It saw the increase of Social Media use in campaigns by politicians. From YouTube videos to Facebook Live sessions and trending hashtags and posts on Twitter. One interesting phenonmenon that rose from this election is the UGBot. UGBots are the term given to 'fake' accounts created in order to push the agenda of one candidate. These account are in no way automated, however borrow the 'bot' name due to the fact that their content during the election period was scripted, with several of them tweeting the exact same content at the same time. One challenge that came up was some of the accounts have since been suspended by Twitter. \n",
    "\n",
    "Twitter in particular saw a proliferation of these UGBot accounts, therefore this tutorial will focus on extracting Twitter data in order to identify these accounts. We shall be using the [Twitterscraper](https://github.com/taspinar/twitterscraper) to gather data from crucial points during the election period that saw a significant increase in Twitter traffic.  \n",
    "\n",
    "### Tutorial Content\n",
    "In this tutorial we shall cover the following\n",
    "- [Installations](#Installation)\n",
    "- [Data Pre-Processing: Collection, Conversion and Cleaning](#Data-Pre-processing:-Collection,-Conversion-and-Cleaning)\n",
    "- [Text Processing](#Text-Processing)\n",
    "- [Feature Extraction](#Feature-Extraction)\n",
    "- [Classification](#Classification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, you will need to set up your computer with the approporiate libraries. It is also recommended to use virtual environments. I used conda to set mine up on my machine. \n",
    "\n",
    "    $ conda create -new myenv python=3.6\n",
    "    \n",
    "    $ source activate myenv\n",
    "    \n",
    "Once the virtual environment is created and activated, you can proceed to install any environments that did not already come preinstalled with `conda`. To install Twitterscraper to get access to the tweets\n",
    "\n",
    "    $ sudo pip install twitterscraper\n",
    "\n",
    "Below are libraries that are needed to successfully complete this tutorial - Also included is how to install the libraries in case they were not included in your version of anaconda. \n",
    "- nltk: `conda install -c anaconda nltk`\n",
    "- sklearn: `conda install -c anaconda scikit-learn`\n",
    "- pandas: `conda install -c anaconda pandas`\n",
    "- numpy:  `conda install -c anaconda numpy`\n",
    "- scipy: `conda install -c anaconda scipy`\n",
    "\n",
    "Two other libraries that will be important in this tutorial are: `json`, `Counter` and `string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nl\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing: Collection, Conversion and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the libraries, we are now ready to download the data from Twitter and begin getting it ready for classification. \n",
    "\n",
    "Using the Twitterscraper, we shall download Twitter data from 10 accounts. 5 of the accounts will be known bot accounts and the other 5 accounts belonging to media personalities (i.e. Journalists). \n",
    "\n",
    "We shall be extracting tweets from the following 10 accounts: `@AnishaUwase, @SarahKagingo, @JonahByaru1, @CynthiaNyamai, @MtwahaN, @kacungira, @kasujja, @qataharraymond, @jkkarungi, @Snduhukire`\n",
    "\n",
    "We shall specifically be targeting tweets from `02-11-2016` to `02-20-2016` because this time period coincides with the 2nd ever presidential debate as well as the election day. \n",
    "\n",
    "To extract the data, we shall be using TwitterScraper in command line:\n",
    "\n",
    "    $ twitterscraper \"from:twitter-handle\" -l 1000 -bd 2016-02-11 -ed 2016-02-20 -o anisha.json\n",
    "\n",
    "*replace `twitter-handle` with the user's twitter handle\n",
    "\n",
    "_____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data, we are interested in the `user` and `text` columns - where text is the content of the tweet. \n",
    "\n",
    "The goal of this tutorial is to determine which accounts are the bot accounts, as our ground truth, we shall be using the user's twitter handle which is labelled `user`.\n",
    "\n",
    "- @AnishaUwase, @SarahKagingo, @JonahByaru1, @CynthiaNyamai, @MtwahaN: are considered the bot accounts\n",
    "- @kacungira, @kasujja, @qataharraymond, @jkkarungi, @Snduhukire: are considered to be users posting without influence or bias\n",
    "\n",
    "The data that is extracted is stored as json file, we shall read these files in, extract the columns that are relevant to our classification problem and then convert the result to a csv. \n",
    "\n",
    "Access to all the json (and csv) files used in this tutorial can be downloaded from [here](https://cmu.box.com/s/t52gfcczaswab429ndn1f1m95blv66mr). Ensure that these are saved in the same folder as your notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files=['ray_1','anisha_1','byaru_1','cynthia_1','kacungira_1','kasujja_1','karungi_1','nduhu_1','sarah_1','twaha_1']\n",
    "\n",
    "for file in all_files:\n",
    "    \n",
    "    data = json.load(open('{}.json'.format(file)))\n",
    "    row_data =[]\n",
    "    for each in data:\n",
    "        row = {}\n",
    "        row['user'] =each['user']\n",
    "        row['text'] = each['text']\n",
    "        row_data.append(row)\n",
    "    \n",
    "    file = open( \"{}.csv\".format(file), \"w\")\n",
    "    fileWriter = csv.writer(file , delimiter=\",\",quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    headers = ['user','text']\n",
    "    fileWriter.writerow(headers)\n",
    "\n",
    "    for rows in row_data:\n",
    "        output = [rows['user'], rows['text']]\n",
    "        fileWriter.writerow(output)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the csv conversion - we can then merge the user handles and tweets into one csv document. We use the RAND() function to randomize the order in which the data is displayed. \n",
    "\n",
    "The last step in getting our data ready will be to split into 3 chunks that we shall use in our classificaton, these are the `train-tweets`, `test-tweets` and `validation-tweets` - Access to these files can also be found [here](https://cmu.box.com/s/t52gfcczaswab429ndn1f1m95blv66mr)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we shall used the `train-tweets` data. Using the NLTK package that we previously imported, we shall clean the data that we have collected. \n",
    "\n",
    "So to process all the text, we are going to split the methods we use into two. While this may appear not to be necessary, it will reduce how complex it would be to try and fit everything into one function (You can also view this as an attempt to apply the laws of object oriented programming). \n",
    "\n",
    "The first function will be `tokenize_clean` and it will focus on tokenizing the a string of text and also perfomring a number of tasks.\n",
    "\n",
    "Here are the following tasks that we shall perform on our tweet content:\n",
    "1. Change all content to lower case\n",
    "2. Process all the punctuation, i.e \n",
    "    - Case 1: apostrophe where she's becomes she\n",
    "    - Case 2: apostrophe where don't becomes dont\n",
    "    - Case 3: hypen will cause a break in the work\n",
    "\n",
    "Note: It is usefule to check and see if your nltk download is working. To do so, I verified by downloading the stopwords. \n",
    "\n",
    "```python\n",
    "    >>> nl.download('stopwords')\n",
    "    >>> nl.download('wordnet')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_clean(text, lemmatizer=nl.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\"\n",
    "    This function will take a string of text and clean it according to the requirements set above.\n",
    "    We shall be cleaning out punctuation, i.e. apostrophe's and hyphens. \n",
    "    \n",
    "    Inputs: text (str) - the raw text of the tweet content\n",
    "    Outputs: list (str) -  tokenized version of the text\n",
    "    \n",
    "    \"\"\"\n",
    "    text = str(text.lower())\n",
    "    \n",
    "    if \"'s\" in text:\n",
    "        text = text.replace(\"'s\", \"\")\n",
    "    if \"'\" in text:\n",
    "        text = text.replace(\"'\", \"\")\n",
    "    \n",
    "    text_punc = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    clean_text = text.translate(text_punc)\n",
    "    tokens = nl.word_tokenize(clean_text)\n",
    "    final=[]\n",
    "    \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            word = lemmatizer.lemmatize(token)\n",
    "            final.append(word)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to test out function to see that it is working. It is a good idea to use a simple string of text and not the actual data yet.\n",
    "\n",
    "```python\n",
    "   >>> tokenize_clean(\"Africa is actually not a country. She's a huge land mass that can contain several other continents. Think of a country like Burkina-Faso, wait.. Do we use a dash or not?\")\n",
    "\n",
    "['africa', 'is', 'actually', 'not', 'a', 'country', 'she', 'a', 'huge', 'land', 'mass', 'that', 'can', 'contain', 'several', 'other', 'continent', 'think', 'of', 'a', 'country', 'like', 'burkina', 'faso', 'wait', 'do', 'we', 'use', 'a', 'dash', 'or', 'not']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete our text processing section, we shall implement one more feature that will bring it all together: this is the `process` function. This function will take in a dataframe as input and an instance of the lemmatize method. The output of the function will be a dataframe whose text column has been converted to a list of strings that have been cleaned by our `tokenize_clean` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data, lemmatizer=nl.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" This function will process all text in the dataframe using tokenize_clean() function.\n",
    "    Inputs\n",
    "        data: pd.DataFrame: dataframe containing a column 'text' loaded from the CSV file\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nl.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs\n",
    "        pandas dataframe: dataframe with altered text column - now a list of strings.\n",
    "    \"\"\"\n",
    "    for index, text in enumerate(data['text']):\n",
    "        data['text'].iloc[index] = tokenize_clean(text, lemmatizer)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"train-tweets.csv\", na_filter=False)\n",
    "all_tweets = process(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your final output should look like this:\n",
    "\n",
    "```python\n",
    "    >>> print(all_tweets.head())\n",
    "                 user                                               text\n",
    "    0    SarahKagingo  [president, museveni, i, refused, to, negotiat...\n",
    "    1    SarahKagingo  [uganda, olivia, byanyima, one, of, lioness, o...\n",
    "    2         MtwahaN  [in, zimbabwe, a, man, appeared, and, announce...\n",
    "    3  qataharraymond  [spartakussug, district, returning, officer, d...\n",
    "    4     Jonahbyaru1  [candidate, museveni, greets, other, candidate...\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tokenized tweets, we are ready to generate feature vectors that we shall use in our classification. To do this we shall be using Bag of Words TF-IDf (Term Frequency-Inverse Document Frequency). We define Bag of Words as a which takes as input a set of documents and outputs a table containing the frequency counts of each word in each document ([source](http://datameetsmedia.com/bag-of-words-tf-idf-explained/)) while TF-IDF is the product of the term frequency and the inverse document frequency ([source](http://datameetsmedia.com/bag-of-words-tf-idf-explained/)).\n",
    "\n",
    "When we think of Bag of Words TF IDF, we are looking at the words that are to be used in the corpus, however, we do not want to include terms that are used very frequently (e.g like the use of articles in sentences) or very rare words. The reason for this is that these two categories of words do not add or represent any information when looking at the similarity of text\n",
    "\n",
    "Again, like before, we shall be writing two functions that will help us reach our goals. The first function will be used to retreive all the rare words, we shall call this function `all_rare_words`. The logic of this method is to use the frequency with which a word appears in a text to determine whether it is rare or not. \n",
    "\n",
    "Our rare_words are then going to help us in the next method called `create_features`. In the create features we shall take this list of rare words and add to it stop words from the NLTK corpus, and then remove these words from our tweet content. We shall use SKLearn's TfidfVectorizer to create a vectorizer that we shall then transform to our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_rare_words(all_tweets):\n",
    "    \"\"\" from the word count of words in our corpus, determine which words are rare and return a list of them.\n",
    "    \n",
    "    Inputs:\n",
    "        all_tweets: pd.DataFrame: the output of process function\n",
    "    Outputs:\n",
    "        list(str): list of rare words, sorted alphabetically.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_tweets = all_tweets['text']\n",
    "    rare_word_counter = Counter()\n",
    "    \n",
    "    for tweets in all_tweets:\n",
    "        for word in tweets:\n",
    "            rare_word_counter[word] += 1\n",
    "    \n",
    "    rare_words_dict = { word:count for word, count in rare_word_counter.items() if count == 1 }\n",
    "    all_rare_words = sorted(list(rare_words_dict.keys()))\n",
    "    return all_rare_words\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "rare_words = retrieve_rare_words(all_tweets)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran the above method, I get a rare_word list of length: 1558\n",
    "``` python\n",
    "    >>> print(len(rare_words))\n",
    "    1558\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def create_features(all_tweets, rare_words):\n",
    "    \"\"\" creates the feature matrix using the processed tweet text\n",
    "    Inputs:\n",
    "        all_tweets: pd.DataFrame: tweets read from train/test csv file, containing the column 'text'\n",
    "        rare_words: list(str): one of the outputs of retrieve_rare_words function\n",
    "    Outputs:\n",
    "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "                                                we need this to tranform test tweets in the same way as train tweets\n",
    "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    stopwords = nl.corpus.stopwords.words(\"english\")\n",
    "    corpus = []\n",
    "    \n",
    "    for word in rare_words:\n",
    "        stopwords.append(word)\n",
    "    \n",
    "    for row in all_tweets['text']:\n",
    "        sentence = ' '.join(row)\n",
    "        corpus.append(sentence)\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords, max_features=655)\n",
    "    vect_corpus = vectorizer.fit_transform(corpus)\n",
    "    result_matrix = csr_matrix(vect_corpus)\n",
    "    \n",
    "    return vectorizer, result_matrix\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "(tfidf, X) = create_features(all_tweets, rare_words)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step that we are going to do is to label our data. We shall do this using the method `create_labels`. For the labels we shall be using 0 or 1. i.e.  0 for 'MtwahaN','JonahByaru1','CynthiaNyamai','AnishaUwase', 'SarahKagingo', and lastly 1 for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(all_tweets):\n",
    "    \"\"\" creates the class labels from user column\n",
    "    Inputs:\n",
    "        tweets: pd.DataFrame: tweets read from train file, containing the column 'user'\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary numpy array of class labels\n",
    "    \"\"\"\n",
    "    class_labels = []\n",
    "    \n",
    "    for name in all_tweets['user']:\n",
    "        if name == 'MtwahaN' or name == 'JonahByaru1'or name == 'CynthiaNyamai' or name == 'AnishaUwase'or name == 'SarahKagingo':\n",
    "            class_labels.append(int(0))\n",
    "        else:\n",
    "            class_labels.append(int(1))\n",
    "    \n",
    "    return np.asarray(class_labels)\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "y = create_labels(all_tweets)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you print out `y` it should look like this:\n",
    "\n",
    "``` python\n",
    "    >>> print(y)\n",
    "    [0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1\n",
    " 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
    " 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0\n",
    " 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1\n",
    " 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1\n",
    " 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0\n",
    " 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0\n",
    " 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0\n",
    " 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0\n",
    " 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1\n",
    " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1\n",
    " 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1\n",
    " 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1\n",
    " 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the parts ready, we shall be using the Support Vector Machine (SVM) classifier. A Support Vector Machine (SVM) is a supervised machine learning algorithm that can be employed for both classification and regression purposes ([source](http://blog.aylien.com/support-vector-machines-for-dummies-a-simple/)). SVM supports several kernel functions `linear`, `poly`, `rbf`,`sigmoid` - it is good practise try several of them to see which one best fits your date - however, for purposes of this tutorial, we shall be using the `linear` kernal function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def fit_classifier(X_train, y_train):\n",
    "    \"\"\" learns a classifier from the input features and labels using the linear kernal function\n",
    "    Inputs:\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features, output of create_features_and_labels()\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels, output of create_features_and_labels()\n",
    "    Outputs:\n",
    "        sklearn.svm.classes.SVC: classifier learnt from data\n",
    "    \"\"\"\n",
    "    X = X_train\n",
    "    y = y_train\n",
    "    \n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    return clf\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "classifier = fit_classifier(X, y)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have got our classifier ready, we can now evaluate it against the second batch of data that we set aside: the `validation-tweet` dataset. We shall measure using accuracy metric (although there are several other metrics that can be used to measure including precision, recall, etc). \n",
    "\n",
    "With accuracy we are looking at the fraction of how many of the tweets are correctly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare our validation data\n",
    "validation_tweets = pd.read_csv(\"validation-tweets.csv\", na_filter=False)\n",
    "validation_all_tweets = process(validation_tweets)\n",
    "validation_rare_words = retrieve_rare_words(validation_all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def create_validation_features(tfidf, all_tweets, rare_words):\n",
    "    \"\"\" creates the feature matrix using the processed tweet text\n",
    "    Inputs:\n",
    "        tweets: pd.DataFrame: tweets read from train/test csv file, containing the column 'text'\n",
    "        rare_words: list(str): one of the outputs of retrieve_rare_words function\n",
    "    Outputs:\n",
    "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "                                                we need this to tranform test tweets in the same way as train tweets\n",
    "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    stopwords = nl.corpus.stopwords.words(\"english\")\n",
    "    corpus = []\n",
    "    \n",
    "    for word in rare_words:\n",
    "        stopwords.append(word)\n",
    "    \n",
    "    for row in all_tweets['text']:\n",
    "        sentence = ' '.join(row)\n",
    "        corpus.append(sentence)\n",
    "    \n",
    "#     vectorizer = TfidfVectorizer(stop_words=stopwords, max_features=655)\n",
    "    vect_corpus = tfidf.transform(corpus)\n",
    "    result_matrix = csr_matrix(vect_corpus)\n",
    "    \n",
    "    return result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = create_validation_features(tfidf, validation_all_tweets, validation_rare_words)\n",
    "y_validation = create_labels(validation_all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def classifier_evaluation(classifier, X_validation, y_validation):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "    Inputs:\n",
    "        classifier: sklearn.svm.classes.SVC: classifer to evaluate\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels\n",
    "    Outputs:\n",
    "        double: accuracy of classifier on the validation data\n",
    "    \"\"\"\n",
    "    prediction = classifier.predict(X_validation)\n",
    "    accuracy = accuracy_score(y_validation, prediction)\n",
    "    return accuracy\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "accuracy = classifier_evaluation(classifier, X_validation, y_validation)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran the above method, I get the following accuracy\n",
    "\n",
    "``` python\n",
    "    >>> print(accuracy)\n",
    "    0.925851703407\n",
    "```\n",
    "_________\n",
    "\n",
    "Now we can use our model to classify unlablled tweets from our test-tweets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_unlabelled_data(tfidf, classifier, unlabeled_tweets):\n",
    "    \"\"\" predicts class labels for raw tweet text\n",
    "    Inputs:\n",
    "        tfidf: sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used on training data\n",
    "        classifier: sklearn.svm.classes.SVC: classifier learnt\n",
    "        unlabeled_tweets: pd.DataFrame: tweets read from tweets_test.csv\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary vector of class labels for unlabeled tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_tweets = process(unlabeled_tweets)\n",
    "    corpus = []\n",
    "        \n",
    "    for row in processed_tweets['text']:\n",
    "        sentence = ' '.join(row)\n",
    "        corpus.append(sentence)\n",
    "    \n",
    "    vect_corpus = tfidf.transform(corpus)\n",
    "    result_matrix = csr_matrix(vect_corpus)\n",
    "    y_pred = classifier.predict(result_matrix)\n",
    "    \n",
    "    return y_pred\n",
    "    \n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "classifier = fit_classifier(X, y)\n",
    "unlabelled_tweets = pd.read_csv(\"test-tweets.csv\", na_filter=False, encoding='latin-1')\n",
    "y_pred = classify_unlabelled_data(tfidf, classifier, unlabelled_tweets)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a model with which you can identify tweets that were posted without monetary influence or bias during the 2016 Ugandan Election. \n",
    "\n",
    "In an age where trending content quickly morphs into reality and influences decisions that people make - it is becoming increasingly important to identify whether content posted on social media is genuine or driven by hidden incentives such as monetary gain. \n",
    "\n",
    "In this tutorial we have covered: \n",
    "1. Installation of libraries that are important \n",
    "2. Data Pre-Processing: Collection, Conversion and Cleaning - How we collected the data as well as processed it.\n",
    "3. Text Processing: Cleaning up the text content so that it is ready to begin the process of feature extraction\n",
    "4. Feature Extraction: Here we looked at the identification of words that will not add value to our similarity calculations.\n",
    "5. Classification: Here we looked at using the SVM classifier with the linear kernal function to fit our data. We then evaluated our classifier against our validation data. After which we ran our model on our unlabelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
