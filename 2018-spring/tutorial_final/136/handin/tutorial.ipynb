{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Frequent Item set Mining is a data analysis method which takes a set of data and apply statistical methods to find interesting and previously-unknown patterns within this set of data. Patterns like which ones emerge frequently, which items are associated, and which items correlate with others. This topic always come with a Market Basket Dataset, which has a bunch of costumers' shopping items look like this:\n",
    "<img src=\"http://gdurl.com/kIhH\" width = \"300\" height = \"200\" alt=\"\" align=center/>\n",
    "\n",
    "One of the most common ways to represent these patterns is via association rules, a single example of which is given below:\n",
    "\n",
    "    milk => bread [support = 25%, confidence = 60%, lift = 1.3 ]\n",
    "\n",
    "`Support` is a measure of absolute frequency of milk and bread are purchased together.   \n",
    "`Confidence` is a measure of correlative frequency when those who purchased milk also purchased bread.   \n",
    "`Lift` lift indicates whether the two items are occuring together in the same orders simply by chance.\n",
    "* lift = 1 implies no relationship between A and B. \n",
    "* lift > 1 implies that there is a positive relationship between A and B.(occur together more often than random)\n",
    "* lift < 1 implies that there is a negative relationship between A and B.(occur together less often than random)\n",
    "\n",
    "\n",
    "The hidden patterns of history data can help us make some decisions. For example, we can optimize the placement of goods if we find frequent pair of goods bought together, or we can change product warehouse location to achieve cost savings and increase economic efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori Algorithm\n",
    "Apriori is the most well-known example of a frequent pattern mining algorithm.  \n",
    "Assume that we are running a grocery store that only have 4 kinds of goods: good 0, good 1, good 2, good 3. We can get all possible combinations set of goods that may be purchased together:\n",
    "<img src=\"http://gdurl.com/B_oY\" width = \"300\" height = \"200\" alt=\"\" align=center/>\n",
    "We find that there are 15 possible purchase sets, lines between sets of items indicate that two or more collections can be combined to form a larger set.  \n",
    "Apriori Algorithm uses Downward Closure Property:      \n",
    "* If a set of items is a frequent itemset, all its subsets are also frequent.    \n",
    "* If a set of items is infrequent, then all its supersets are also infrequent.   \n",
    "\n",
    "That is, if {0,1} is frequent, {0}, {1} must also be frequent. If {2} is not frequent, then {2,0} could not be frequent.\n",
    "Using this property, we can reduce the numer of sets need to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Apyori API\n",
    "Python package list has included a efficient implementation of apyori algorithm. We can use it directly by downloading the apyori.oy file form https://pypi.python.org/pypi/apyori/1.1.1 and run on a dataset.\n",
    "The following dataset we use is donated by Tom Brijs and contains the (anonymized) retail market basket data from an anonymous Belgian retail store.[2] Each row in the retail.csv file represents one transaction of a customer. We use frequent item set support threshold 0.3 and confidence threshold 0.6 and find only 2 related items. We will use this as ground truth to evaluate the correctness of our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RelationRecord(items=frozenset({'39', '48'}), support=0.33055057734624893, ordered_statistics=[OrderedStatistic(items_base=frozenset({'48'}), items_add=frozenset({'39'}), confidence=0.6916340334638661, lift=1.2032726128908016)])]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from apyori import apriori\n",
    "\n",
    "def readData( path ):\n",
    "    D = []\n",
    "    with open(path, 'r') as f:\n",
    "        dataset = csv.reader(f)\n",
    "        for row in dataset:\n",
    "            D.append(row)\n",
    "    return D\n",
    "\n",
    "D = readData('./retail.csv')\n",
    "results = list(apriori(D, min_support=0.3, min_confidence=0.6))\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create frequent set with single item\n",
    "Now we are going to implement apyori algorithm from scratch.   \n",
    "According to Wiki page, Apriori uses a \"bottom up\" approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found. So we start from the bottom where each candidate set has only one item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createC1(dataSet):\n",
    "    \"\"\" \n",
    "    Create candidate sets C1, each contain one item.\n",
    "    This function create a set contain each single item.\n",
    "    Args:\n",
    "        dataSet: dataset contains all transactions\n",
    "    Returns: \n",
    "        Set: a sorted list of candidate sets\n",
    "    \"\"\"\n",
    "    C1 = []\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction:\n",
    "            if [ item ] not in C1:\n",
    "                C1.append( [ item ] )\n",
    "    C1.sort()\n",
    "    return C1\n",
    "\n",
    "\n",
    "def scanD( D, Ck, minSupport ):\n",
    "    '''\n",
    "    This function sacn all transactions and compute how much support dataset D give to each candidate set in Ck \n",
    "    and filter the candidate sets with a min suppurt threshold\n",
    "    Args:\n",
    "        D: dataset contains all transactions\n",
    "        Ck: Candidate sets\n",
    "        minSuppot: frequent item support value threshold\n",
    "    Returns:\n",
    "        retList: list of frequent sets\n",
    "        supportData: dictionary of set: support value for each candidate set\n",
    "    '''      \n",
    "    ssCnt = {}\n",
    "    for tid in D:\n",
    "        # for each transaction\n",
    "        # make candidate set a frozen one sothat each candidate can be a key in the dictionary\n",
    "        C = map(frozenset, Ck )\n",
    "        for can in C:\n",
    "            # check if candidate set is subset of transaction\n",
    "            if can.issubset( tid ):\n",
    "                ssCnt[ can ] = ssCnt.get( can, 0) + 1\n",
    "                \n",
    "    numItems = len( D )\n",
    "    retList = []\n",
    "    supportData = {}\n",
    "    for key in ssCnt:\n",
    "        # support of each candidate set = number of all appearance / number of all transaction\n",
    "        support = ssCnt[ key ] / numItems\n",
    "        supportData[ key ] = support\n",
    "        # add to frequent set list\n",
    "        if support >= minSupport:\n",
    "            retList.insert( 0, key )\n",
    "    return retList, supportData\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all frequent set:  [frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})]\n",
      "all support value:  {frozenset({1}): 0.5, frozenset({3}): 0.75, frozenset({4}): 0.25, frozenset({2}): 0.75, frozenset({5}): 0.75}\n"
     ]
    }
   ],
   "source": [
    "# creat a small dataset for test\n",
    "myData = [ [ 1, 3, 4 ], [ 2, 3, 5 ], [ 1, 2, 3, 5 ], [ 2, 5 ] ]\n",
    "C1 = createC1(myData)\n",
    "\n",
    "L, supportData = scanD( myData, C1, 0.5 )\n",
    "   \n",
    "print (\"all frequent set: \", L)\n",
    "print (\"all support value: \", supportData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat frequent item set with k items\n",
    "The pseudo code for the algorithm is given below for a transaction database T, and a support threshold of epsilon. Usual set theoretic notation is employed, though note that T is a multiset. Ck is the candidate set for level k. At each step, the algorithm is assumed to generate the candidate sets from the large item sets of the preceding level, heeding the downward closure lemma.\n",
    "<img src=\"http://gdurl.com/5JtK\" width = \"500\" height = \"500\" alt=\"\" align=center/>\n",
    "( In many cases, we usually concern about only k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aprioriGen( Lk, k ):\n",
    "    '''\n",
    "    This function generate a new candiadte set by an exist frequent item set\n",
    "    Args:\n",
    "        Lk: List of frequent item sets\n",
    "        k: number of items in the new candidate sets\n",
    "    Returns:\n",
    "        retList: a list of all candidate sets, each set has k items\n",
    "    '''\n",
    "    retList = []\n",
    "    lenLk = len( Lk )\n",
    "    for i in range( lenLk ):\n",
    "        for j in range( i + 1, lenLk ):\n",
    "            L1 = list( Lk[ i ] )[ : k - 2 ];\n",
    "            L2 = list( Lk[ j ] )[ : k - 2 ];\n",
    "            L1.sort();L2.sort()\n",
    "            if L1 == L2:\n",
    "                retList.append( Lk[ i ] | Lk[ j ] ) \n",
    "    return retList\n",
    "\n",
    "def apriori( dataSet, minSupport = 0.5 ):\n",
    "    '''\n",
    "    This function compute support value for each frequent item set condidate\n",
    "    and resturn a list of frequent item sets\n",
    "    Args:\n",
    "        dataSet: dataset contains all transactions\n",
    "        minSupport: frequent item support value threshold\n",
    "    Returns:\n",
    "        L: a list of frequent item sets\n",
    "        suppData: dictionary of set: support value for each candidate set\n",
    "        \n",
    "    '''\n",
    "    C1 = createC1( dataSet )\n",
    "    L1, suppData = scanD( dataSet, C1, minSupport )\n",
    "    L = [ L1 ]\n",
    "    k = 2\n",
    "    \n",
    "    while ( len( L[ k - 2 ] ) > 0 ):\n",
    "        Ck = aprioriGen( L[ k - 2 ], k )\n",
    "        Lk, supK = scanD( dataSet, Ck, minSupport )\n",
    "        \n",
    "        suppData.update( supK )\n",
    "        L.append( Lk )\n",
    "        k += 1\n",
    "    \n",
    "    L = L[:-1]\n",
    "    return L, suppData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all frequent set:  [[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})], [frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5}), frozenset({1, 3})], [frozenset({2, 3, 5})]]\n",
      "all support value:  {frozenset({1}): 0.5, frozenset({3}): 0.75, frozenset({4}): 0.25, frozenset({2}): 0.75, frozenset({5}): 0.75, frozenset({1, 3}): 0.5, frozenset({2, 5}): 0.75, frozenset({3, 5}): 0.5, frozenset({2, 3}): 0.5, frozenset({1, 5}): 0.25, frozenset({1, 2}): 0.25, frozenset({2, 3, 5}): 0.5}\n"
     ]
    }
   ],
   "source": [
    "L, suppData = apriori( myData, 0.5 )\n",
    "print(\"all frequent set: \", L)\n",
    "print(\"all support value: \",suppData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining Association Rules from Frequent item set\n",
    "To find an association rule, we start with a frequent set. That is, an item or a set may derive another item. From example in figure 1, we can get that if there is a frequent item set {milk, bread}, then there may be a related rule \"milk --> bread\", which means that if someone buys milk, then he will statistically buy bread, but is not necessarily the reverse.   \n",
    "\\begin{equation}\n",
    "Confidence\\left( A\\rightarrow B\\right) =\\frac {Support\\left( A\\cup B\\right) }{Support\\left( A\\right) }\n",
    "\\end{equation}\n",
    "There is another property that reduces the number of rules for testing：\n",
    "\n",
    "* If a rule does not meet the minimum confidence requirements, then all subsets of the rule will not meet the minimum confidence requirements:\n",
    " \n",
    "we can explain this rule in the figure below:\n",
    "\n",
    "    The darker node in the graph represent the rule with low confidence.\n",
    "    if 012->3 is low confidence, then all rules which contain conclusion 3 are less reliable\n",
    "<img src=\"http://gdurl.com/VdnW\" width = \"500\" height = \"500\" alt=\"\" align=center/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calcConf( freqSet, H, supportData, brl, minConf=0.7 ):\n",
    "    '''\n",
    "    This function compute the confidence of each rule and return rules who have \n",
    "    larger confidence then threshold\n",
    "    Args:\n",
    "        freqSet: frozenset of frequent item set {1,2,3} \n",
    "        H: frozenset of single item {[1], [2], [3]}\n",
    "        supportData: dictionary of set: support value for each frequent item set\n",
    "        brl: tuples of association rules\n",
    "        minConf: minimum confidence threshold\n",
    "    '''\n",
    "    prunedH = []\n",
    "    conf1 = float('inf')\n",
    "    conf2 = float('inf')\n",
    "    if len(freqSet) == 2:\n",
    "        for conseq in H:\n",
    "            lift = supportData[ freqSet ] / (supportData[ freqSet - conseq ] * supportData[conseq ])\n",
    "            conf1 = supportData[ freqSet ] / supportData[ freqSet - conseq ]\n",
    "            if conf1 >= minConf:\n",
    "                print (freqSet - conseq, '-->', conseq, 'conf:', conf1, 'lift:', lift)\n",
    "                brl.append( ( freqSet - conseq, conseq, conf1, lift) )\n",
    "                prunedH.append( conseq )\n",
    "                return prunedH\n",
    "    for conseq in H:\n",
    "        lift = supportData[ freqSet ] / (supportData[ freqSet - conseq ] * supportData[conseq ])\n",
    "        conf1 = supportData[ freqSet ] / supportData[ freqSet - conseq ]\n",
    "        conf2 = supportData[ freqSet ] / supportData[ conseq ]\n",
    "        if conf1 >= minConf:\n",
    "            print (freqSet - conseq, '-->', conseq, 'conf:', conf1, 'lift:', lift)\n",
    "            brl.append( ( freqSet - conseq, conseq, conf1, lift ) )\n",
    "            if len(conseq) > 1:\n",
    "                prunedH.append( conseq )\n",
    "        if conf2 >= minConf:\n",
    "            print (conseq, '-->', freqSet - conseq, 'conf:', conf2, 'lift:', lift)\n",
    "            brl.append( ( conseq, freqSet - conseq, conf2, lift ) )\n",
    "            if len(freqSet - conseq) > 1:\n",
    "                prunedH.append( freqSet - conseq )\n",
    "    return prunedH\n",
    "\n",
    "def rulesFromConseq( freqSet, H, supportData, brl, minConf=0.7 ):\n",
    "    '''\n",
    "    This function take a frequent item set, compute confidence of each potencial rule recursively\n",
    "    until the size of frequent item is 2.\n",
    "    Args:\n",
    "        freqSet: frozenset of frequent item {1,2,3}\n",
    "        H: frozenset of single item {[1], [2], [3]}\n",
    "        supportData: dictionary of set: support value for each frequent item set\n",
    "        brl: tuples of association rules\n",
    "    \n",
    "    '''\n",
    "    m = len( H[ 0 ] )\n",
    "    print(H)\n",
    "    print(freqSet)\n",
    "    print(m)\n",
    "    if len( freqSet ) > m + 1:\n",
    "        Hmp1 = aprioriGen( H, m + 1 )\n",
    "        print(Hmp1)\n",
    "        Hmp1 = calcConf( freqSet, Hmp1, supportData, brl, minConf )\n",
    "        if len( Hmp1 ) > 1:\n",
    "            print(\"recursion\", Hmp1)\n",
    "            rulesFromConseq( freqSet, Hmp1, supportData, brl, minConf )\n",
    "\n",
    "def generateRules( L, supportData, minConf=0.7 ):\n",
    "    '''\n",
    "    This function generate association rule fron frequent item sets an support value\n",
    "    Args:\n",
    "        L: list of frequent item set\n",
    "        supportData: dictionary of all item set: support value\n",
    "        minConf: minimum confidence threshold\n",
    "    Return: \n",
    "        bigRuleList: list of association rules\n",
    "    '''\n",
    "    bigRuleList = []\n",
    "    for i in range( 1, len( L ) ):\n",
    "        for freqSet in L[ i ]:\n",
    "\n",
    "            H1 = [ frozenset( [ item ] ) for item in freqSet ]\n",
    "            if i > 1:\n",
    "                rulesFromConseq( freqSet, H1, supportData, bigRuleList, minConf )\n",
    "            else:\n",
    "                calcConf( freqSet, H1, supportData, bigRuleList, minConf )\n",
    "    return bigRuleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({3}) --> frozenset({2}) conf: 0.6666666666666666 lift: 0.8888888888888888\n",
      "frozenset({5}) --> frozenset({3}) conf: 0.6666666666666666 lift: 0.8888888888888888\n",
      "frozenset({5}) --> frozenset({2}) conf: 1.0 lift: 1.3333333333333333\n",
      "frozenset({3}) --> frozenset({1}) conf: 0.6666666666666666 lift: 1.3333333333333333\n",
      "[frozenset({2}), frozenset({3}), frozenset({5})]\n",
      "frozenset({2, 3, 5})\n",
      "1\n",
      "[frozenset({2, 3}), frozenset({2, 5}), frozenset({3, 5})]\n",
      "frozenset({5}) --> frozenset({2, 3}) conf: 0.6666666666666666 lift: 1.3333333333333333\n",
      "frozenset({2, 3}) --> frozenset({5}) conf: 1.0 lift: 1.3333333333333333\n",
      "frozenset({3}) --> frozenset({2, 5}) conf: 0.6666666666666666 lift: 0.8888888888888888\n",
      "frozenset({2, 5}) --> frozenset({3}) conf: 0.6666666666666666 lift: 0.8888888888888888\n",
      "frozenset({2}) --> frozenset({3, 5}) conf: 0.6666666666666666 lift: 1.3333333333333333\n",
      "frozenset({3, 5}) --> frozenset({2}) conf: 1.0 lift: 1.3333333333333333\n",
      "recursion [frozenset({2, 3}), frozenset({2, 5}), frozenset({3, 5})]\n",
      "[frozenset({2, 3}), frozenset({2, 5}), frozenset({3, 5})]\n",
      "frozenset({2, 3, 5})\n",
      "2\n",
      "rules:\n",
      " [(frozenset({3}), frozenset({2}), 0.6666666666666666, 0.8888888888888888), (frozenset({5}), frozenset({3}), 0.6666666666666666, 0.8888888888888888), (frozenset({5}), frozenset({2}), 1.0, 1.3333333333333333), (frozenset({3}), frozenset({1}), 0.6666666666666666, 1.3333333333333333), (frozenset({5}), frozenset({2, 3}), 0.6666666666666666, 1.3333333333333333), (frozenset({2, 3}), frozenset({5}), 1.0, 1.3333333333333333), (frozenset({3}), frozenset({2, 5}), 0.6666666666666666, 0.8888888888888888), (frozenset({2, 5}), frozenset({3}), 0.6666666666666666, 0.8888888888888888), (frozenset({2}), frozenset({3, 5}), 0.6666666666666666, 1.3333333333333333), (frozenset({3, 5}), frozenset({2}), 1.0, 1.3333333333333333)]\n"
     ]
    }
   ],
   "source": [
    "L, suppData = apriori( myData, 0.5 )\n",
    "rules = generateRules( L, suppData, minConf=0.65 )\n",
    "print ('rules:\\n', rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a large dataset\n",
    "Now is time to test our implementation on a large dataset. We get the same result as in the first part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequent item set finish\n",
      "[[frozenset({'48'}), frozenset({'39'})], [frozenset({'39', '48'})]]\n",
      "frozenset({'48'}) --> frozenset({'39'}) conf: 0.6916340334638661 lift: 1.2032726128908016\n",
      "rules:\n",
      " [(frozenset({'48'}), frozenset({'39'}), 0.6916340334638661, 1.2032726128908016)]\n"
     ]
    }
   ],
   "source": [
    "D = readData('./retail.csv')\n",
    "L, suppData = apriori( D, 0.3 )\n",
    "print (\"frequent item set finish\")\n",
    "print (L)\n",
    "rules = generateRules( L, suppData, minConf=0.6 )\n",
    "print ('rules:\\n', rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "The problem of this simple implementation is that it's not capable of handling large datasets. When I tried to apply this apriori on another dataset over 400MB, I run out of memory and the runtime is not acceptable. Paper [4] described an implementation of Quick Apriori using Trie data structure. \n",
    "<img src=\"http://gdurl.com/nG2l\" width = \"300\" height = \"300\" alt=\"\" align=center/>\n",
    "This figure presents a trie that stores the candidates {A,C,D}, {A,E,G}, {A,E,L}, {A,E,M}, {K,M,N}, every node stores the length of the longest directed path that starts from\n",
    "there, and a hash table is employed to store the edges of the node to accelerate searching.\n",
    "Significant advantages of this approach:\n",
    "* Candidate generation becomes easy and fast, can be generated from pairs of nodes that have the same parents \n",
    "* Association rules are produced much faster, since retrieving a support of an itemset is quicker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "1. Jiawei Han, Micheline Kamber and Jian Pei Data Mining: Concepts and Techniques, 3rd ed.\n",
    "2. Frequent Itemset Mining Dataset Repository http://fimi.ua.ac.be/data/\n",
    "3. Frequent Pattern Mining and the Apriori Algorithm: A Concise Technical Overview https://www.kdnuggets.com/2016/10/association-rule-learning-concise-technical-overview.html\n",
    "4. Bodon, Ferenc. \"A fast APRIORI implementation.\" FIMI. Vol. 3. 2003.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
