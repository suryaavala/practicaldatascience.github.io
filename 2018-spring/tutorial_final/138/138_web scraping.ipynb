{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](https://4qr7k2a2xza2vctux33bisalkw-wpengine.netdna-ssl.com/wp-content/uploads/2013/09/Scrapy_logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Web Scraping\n",
    "    \n",
    "The saying, \"Information is Power\" is especially true for a data scientist and a data scientist's work is worthless without the right data. The entire process of data science and analytics starts with obtaining the right data and involves interaction with it every step of the process.\n",
    "\n",
    "Data collection is one of  the most important aspects of Data Science, period. Statistical evaluation, Machine learning models, they all come after one has successfully collected and cleaned up the required data. Often times, this data hasn't already been collected and cleaned up and necessitates work on the part of the data scientist to collect the right data and clean it up. \n",
    "\n",
    "In today's digital age, this data is usually scattered all over the web, and unless someone has taken the pains to create a usable API for data collection, this data collection process requires web scraping. This is why, every data scientist should be familiar with at least the basics of the web scraping process allowing one to easily collect data from websites, a situation which inevitably arises quite often. \n",
    "\n",
    "Data collection from the internet usually involves three steps - \n",
    "1. Crawling - this refers to browsing the internet in a systematic and automated way.        An example of the power of large-scale web crawling is presented in Google, whose web crawlers have essentially indexed the  world wide web making it accessible at everyone's fingertips. \n",
    "2. Scraping - this technically refers to scraping i.e, extracting some part of the data from a particular domain or a website however colloquially,  it refers to the entire process of data collection (all three steps as a whole)\n",
    "3. Parsing - this refers to interpreting the data extracted from webpages or domains and converting it into usable data\n",
    "    \n",
    "This entire process leaves one with data which one can then analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy \n",
    "\n",
    "Now that we can better appreciate the importance of web scraping, we can better appreciate the need to understand a technology like Scrapy. \n",
    "\n",
    "An open source framework for web scraping, scrapy makes it painless to obtain data from all kinds of websites. A robust and well-documented back-end combined with scrapy's user friendliness and relatively easy learning curve make it one of the most useful web scraping frameworks for anyone looking to get into web scraping. \n",
    "\n",
    "Scrapy has been intended to serve as a complete data collection toolkit and allows one to complete all three steps of the data collection which is why it is the framework of choice for this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Structure\n",
    "\n",
    "Now that we have hopefully motivated the need to learn and understand the basics of a web scraping framework, we will begin our tutorial. This tutorial will guide the reader through the basics of Scrapy while introducing core concepts in Scrapy that translate to any web scraper in general. It will end with a reddit meme-scraper that scrapes and stores in a `.csv` file, the names and URLs of a user-inputted no. of pages of Reddit. The tutorial be structured as follows - \n",
    "\n",
    "1. [Installing Scrapy](#one)\n",
    "2. [Spiders](#two)\n",
    "    * Basic Scraping\n",
    "    * Following Links\n",
    "3. [Selectors (a Deeper Discussion)](#three)\n",
    "4. [Items](#four)\n",
    "5. [Basics of Reddit Scraper](#five)\n",
    "6. [Feed Exports](#six)\n",
    "7. [Spider Arguments](#seven)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"one\"> </a> \n",
    "## 1. Installing Scrapy\n",
    "Scrapy works with Python 2.7 and Python 3.4 and above. \n",
    "\n",
    "For Anaconda users, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one can use `pip` to install scrapy as well, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install Scrapy\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more informations on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "pip install Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Scrapy's official documentation, it is strongly recommended to install Scrapy in its own `virtualenv` due to the dependency issues it might create. This would require running a Jupypter notebook on a particular virtual environment but that is unnecessary for the purposes of this tutorial and will not be covered here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import scrapy.cmdline\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"two\"> </a> \n",
    "## 2. Spiders\n",
    "Spiders are custom classes which define how a particular group of sites are to be scraped (think complex web parser) including instructions on how the websites should be crawled and how data should be parsed.\n",
    "\n",
    "They must subclass `scrapy.Spider` and define the initial requests to make, optionally how to follow links in the pages, and how to parse the downloaded page content to extract data.\n",
    "\n",
    "In basic terms, spiders are like web crawlers which, when set loose on a set of start_urls, keep scraping data and following new  links according to a pre-defined set of rules. They essentially crawl the internet, hence the name.\n",
    "\n",
    "Here's the code for a very bare-bones spider,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySpider(scrapy.Spider):\n",
    "    name = 'your_url.com'\n",
    "    allowed_domains = ['my_url.com']\n",
    "    start_urls = [\n",
    "        'http://www.my_url.com/1.html',\n",
    "        'http://www.my_url.com/2.html',\n",
    "        'http://www.my_url.com/3.html',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        self.logger.info('Response recieved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few observations one should make here. \n",
    "`allowed_domains` and `start_urls` are self-explanatory and serve as a starting point for our spider. \n",
    "Note that the `parse` method, by default, is the registered callback for the start_urls and is called with the response for each of the urls in `start_urls`. Note that our parse function currently just returns a log message but parse functions can in general `yield` either dicts (data extracted from the webpage) or a `Request` object (making the spider follow the this `Request` object)\n",
    "\n",
    "Now, observe that this isn't very different from what you can do with just the standard `requests` package in python or any other language for that matter. (Send a few GET requests to a few webpages and store their responses)\n",
    "\n",
    "The real magic of a web scraper like scrapy comes into play when you actually make you spider crawl, i.e, you return multiple  items from your parse function (a combination of Requests and data), making your spider follow links according to your rules. \n",
    "\n",
    "For instance, if you want your spider to follow every link in the webpages in `star_urls` and return all `h1` headings in the page, you could modify your spider as follows, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySpider(scrapy.Spider):\n",
    "    name = 'your_url.com'\n",
    "    allowed_domains = ['my_url.com']\n",
    "    start_urls = [\n",
    "        'http://www.my_url.com/1.html',\n",
    "        'http://www.my_url.com/2.html',\n",
    "        'http://www.my_url.com/3.html',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for h3 in response.css('h3').extract():\n",
    "            yield {'h3::text'}\n",
    "\n",
    "        for url in response.css('a::attr(href)').extract():\n",
    "            yield scrapy.Request(url, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, scrapy supports CSS selectors using the syntax `response.css()` and the function `.extract()` is required to convert the output of response.css (a `SelectorList`) to a list (scrapy has a custom `Selector` class that provides the mechanism required to extract data from returned webpages).  A deeper discussion of Scrapy's Selectors is done below covering the specifics.\n",
    "\n",
    "Also, note that scrapy also provides the convenience function `.follow()` for the `response` object which can be used as follows, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySpider(scrapy.Spider):\n",
    "    name = 'your_url.com'\n",
    "    allowed_domains = ['my_url.com']\n",
    "    start_urls = [\n",
    "        'http://www.my_url.com/1.html',\n",
    "        'http://www.my_url.com/2.html',\n",
    "        'http://www.my_url.com/3.html',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for h3 in response.css('h3').extract():\n",
    "            yield {'h3::text'}\n",
    "\n",
    "        for url in response.css('a::attr(href)'):\n",
    "            yield response.follow(url, callback = self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this convenience function is that it handles relative urls automatically and also works with selectors in addition to string arguments eliminating the need to convert one's selector object into a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"three\"> </a> \n",
    "## 3. Selectors\n",
    "\n",
    "Let us now briefly discuss selectors before moving ahead since selectors form an integral part of the entire scraping process. \n",
    "\n",
    "As mentioned earlier, selectors are essentially the mechanism for extracting data from a received response. Scrapy supports \"selecting\" different parts of the HTML response using either XPath or CSS expressions. XPath is a language for selecting nodes in XML documents and outside the scope of this tutorial. We will instead focus on using widely popular CSS to achieve this.\n",
    "\n",
    "As we saw earlier, one can query a response using the `.css()` function which returns a `SelectorList` instance (a list of new selectors). Scrapy provides the `.extract()` adn `.extract_first()` functions to convert this `SelectorList` instance to a normal list of matches or the first match in the focument respectively. note that one added advantage benefit of `.extract_first()` compared to manually getting the first element of the `SelectorList` instance using `.extract_first()` compared to manually indexing the first element of the `SelectorList` is that `.extract_first()` returns  `None`if no match is found. \n",
    "\n",
    "Also, CSS selectors can select text or attribute nodes through CSS3 pseudo-elements (`.css('h3::text')` or `.css('a::attr(href)')`)\n",
    "\n",
    "Note that while collecting data, especially with a web crawler, it is important to make one's code resillient to the the wasteland of badly organized CSS and HTML that the internet is in order to ensure to maximum data collection.\n",
    "\n",
    "Now, while Selectors by themselves are quite a vast topic, we now have enough knowledge about them to scrape most well-created websites and since we are more focused on learning Scrapy as a whole, we encourgae the interested reader to look through Srapy's own (quite extensive) documentation on Selectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"four\"> </a> \n",
    "# 4. Items\n",
    "\n",
    "Now that we know the basics of traversing webpages, and selecting and parsing data, we will now talk about returning this data in a structured form. We can definitely use Python dictionaries for doing this right now but for bigger projects with multiple spiders (following a complicated set of rules), this can very easily lead to type inconsistencies or typos leading to various issues. To resolve this, scrapy provides a common data output format in the form of an `Item` class. They are simple data containers to provide a unified method of structuring data and give the user a dictionary-like API with convenient syntax to declare fields. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meme(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    link = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have no need of Items in our small-scale example, we will still use a `Meme` Item to illustrate it's utility which will help the reader during independent larger projects. \n",
    "\n",
    "Note: The `Field` objects are used to specify metadata for each field of the item. Since we will not be needing this for our purposes, we invite the interested reader to learn more about these through our follow-up reading. \n",
    "\n",
    "As mentioned earlier, scrapy provides a dictionary-like interface to use the `Item` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'link': 'www.xkcd.com', 'name': 'the ultimate'}\n",
      "the ultimate\n",
      "the ultimate 2\n",
      "dict_keys(['name', 'link'])\n",
      "ItemsView({'link': 'www.xkcd.com', 'name': 'the ultimate 2'})\n"
     ]
    }
   ],
   "source": [
    "meme = Meme(name=\"the ultimate\", link=\"www.xkcd.com\")\n",
    "print(meme)\n",
    "print(meme.get('name'))\n",
    "\n",
    "meme[\"name\"] = \"the ultimate 2\"\n",
    "print(meme.get('name'))\n",
    "\n",
    "print(meme.keys())\n",
    "print(meme.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that it is also easy to convert an `Item` into a normal python dictionary, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'the ultimate 2', 'link': 'www.xkcd.com'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(meme))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"five\"> </a> \n",
    "## 5. Basics of Reddit Scraper\n",
    "\n",
    "Believe it or not, we now have enough tools to create a pretty robust example, a Reddit meme scraper! The beauty behind Scrapy, in my opinion, is that it abstracts away different amounts of funtionality for different kinds of users and one can very easily peel away at its layers to go deeper and obtain more and more advanced features. \n",
    "\n",
    "While this tutorial is meant as an introduction to web scraping and Scrapy in general, it will leave the reader with enough tools to confidently dive into Scrapy's documentation and explore its several intricacies. \n",
    "\n",
    "We will now combine the concepts we have talked about earlier to create a basic Reddit meme scraper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first start off with a simple Spider that simply visits https://www.reddit.com/r/Jokes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spider.py\n",
    "import scrapy\n",
    "class RedditScraper(scrapy.Spider):\n",
    "    name = \"reddit_scraper\"\n",
    "    start_urls = ['https://www.reddit.com/r/memes/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy officially recommends using its CLI tool to run the spiders one writes so that is the route we'll be following. (Note that there is a way to run your spiders programmatically in a python script as well) \n",
    "\n",
    "Note that achieving this in a Jupyter notebook this will require us using the magic commands `%%bash` (runs cell as bash commands) and `%%writefile` (to  write out python code to a file which can then be run by our command-line level bash commands). On an actual development machine, as one would imagine, the workflow is more streamlined.  Also, the `2>&1` at the end of our bash commands can be safely ignored. (All it does, essentially, is make the output of our bash commands look decent)  \n",
    "\n",
    "Running a spider is as simple as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-30 22:19:17 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n",
      "2018-03-30 22:19:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.2.1, Platform Windows-10-10.0.16299-SP0\r\n",
      "2018-03-30 22:19:17 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\r\n",
      "2018-03-30 22:19:17 [scrapy.middleware] INFO: Enabled extensions:\r\n",
      "['scrapy.extensions.corestats.CoreStats',\r\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\r\n",
      " 'scrapy.extensions.logstats.LogStats']\r\n",
      "2018-03-30 22:19:17 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n",
      "2018-03-30 22:19:17 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n",
      "2018-03-30 22:19:17 [scrapy.middleware] INFO: Enabled item pipelines:\r\n",
      "[]\r\n",
      "2018-03-30 22:19:17 [scrapy.core.engine] INFO: Spider opened\r\n",
      "2018-03-30 22:19:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n",
      "2018-03-30 22:19:17 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n",
      "2018-03-30 22:19:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.reddit.com/r/memes/> (referer: None)\r\n",
      "2018-03-30 22:19:17 [scrapy.core.engine] INFO: Closing spider (finished)\r\n",
      "2018-03-30 22:19:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n",
      "{'downloader/request_bytes': 221,\r\n",
      " 'downloader/request_count': 1,\r\n",
      " 'downloader/request_method_count/GET': 1,\r\n",
      " 'downloader/response_bytes': 30809,\r\n",
      " 'downloader/response_count': 1,\r\n",
      " 'downloader/response_status_count/200': 1,\r\n",
      " 'finish_reason': 'finished',\r\n",
      " 'finish_time': datetime.datetime(2018, 3, 31, 2, 19, 17, 844920),\r\n",
      " 'log_count/DEBUG': 2,\r\n",
      " 'log_count/INFO': 7,\r\n",
      " 'response_received_count': 1,\r\n",
      " 'scheduler/dequeued': 1,\r\n",
      " 'scheduler/dequeued/memory': 1,\r\n",
      " 'scheduler/enqueued': 1,\r\n",
      " 'scheduler/enqueued/memory': 1,\r\n",
      " 'start_time': datetime.datetime(2018, 3, 31, 2, 19, 17, 457377)}\r\n",
      "2018-03-30 22:19:17 [scrapy.core.engine] INFO: Spider closed (finished)\r\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "venv/Scripts/scrapy.exe runspider spider.py  2>&1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amidst this jungle of INFO messages, one can see `Crawled (200) <GET https://www.reddit.com/r/memes/> (referer: None)` which signifies that we successfully visited the reddit meme page! \n",
    "\n",
    "We can now modify our `parse` function to actually extract the titles of the posts and their  links as well as follow the links to next pages. (Note that we have also added our `Meme` class from earlier to be written to `spider.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spider.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class Meme(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "\n",
    "class RedditScraper(scrapy.Spider):\n",
    "    no_pages = 2\n",
    "    name = \"reddit_scraper\"\n",
    "    start_urls = ['https://www.reddit.com/r/memes/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        next_link = response.css('span.next-button').css('a::attr(href)')[0]\n",
    "        if next_link and self.no_pages:\n",
    "            self.no_pages -= 1\n",
    "            yield response.follow(next_link, callback=self.parse)\n",
    "        \n",
    "        links = response.css('div.thing')\n",
    "        for link in links:\n",
    "            meme = Meme()\n",
    "            meme[\"link\"] = link.css('::attr(data-url)').extract_first()\n",
    "            meme[\"name\"] = link.css('a.title::text').extract_first()\n",
    "            yield meme\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-30 19:48:40 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n",
      "2018-03-30 19:48:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.2.1, Platform Windows-10-10.0.16299-SP0\r\n",
      "2018-03-30 19:48:40 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\r\n",
      "2018-03-30 19:48:40 [scrapy.middleware] INFO: Enabled extensions:\r\n",
      "['scrapy.extensions.corestats.CoreStats',\r\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\r\n",
      " 'scrapy.extensions.logstats.LogStats']\r\n",
      "2018-03-30 19:48:40 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n",
      "2018-03-30 19:48:40 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n",
      "2018-03-30 19:48:40 [scrapy.middleware] INFO: Enabled item pipelines:\r\n",
      "[]\r\n",
      "2018-03-30 19:48:40 [scrapy.core.engine] INFO: Spider opened\r\n",
      "2018-03-30 19:48:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n",
      "2018-03-30 19:48:40 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n",
      "2018-03-30 19:48:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.reddit.com/r/memes/> (referer: None)\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/96fd1neerwo01.jpg',\r\n",
      " 'name': \"When it's time to end it all.\"}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/vxkEdC7.jpg', 'name': 'Mmmhm...'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/pbqtnmyrqvo01.png',\r\n",
      " 'name': 'Press F To Pay Respects'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/srm1pqxkrxo01.jpg', 'name': \"Could've been worse.\"}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/gdxbQRN.jpg', 'name': 'Watch it happen'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/t8g1spx9mxo01.jpg', 'name': 'Hide your girlfriend'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/9m2i6rzsxwo01.jpg',\r\n",
      " 'name': 'I only kinda like Reddit'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/v27u2zy30xo01.jpg',\r\n",
      " 'name': \"considering their past history, maybe they shouldn't have invited \"\r\n",
      "         'her over'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/8a3pru4e4wo01.jpg', 'name': 'Fake blonde.'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/4s8bz3wb4uo01.jpg', 'name': 'Every Time...'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/trortbkd6wo01.jpg',\r\n",
      " 'name': '(Basically it smells like strippers and Scotch)'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/n9ebjux2mwo01.png',\r\n",
      " 'name': 'iPhone? Must be defective.'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/ymng5u49kxo01.jpg',\r\n",
      " 'name': 'When someone asks me how I’m doing'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/rkjglbroovo01.jpg', 'name': 'Too late'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/yvjd3n2szxo01.jpg', 'name': 'My favorite'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/mnpakfadywo01.jpg',\r\n",
      " 'name': 'when he’s not afraid to nail you right on the couch'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/t74fru1d2vo01.jpg', 'name': 'Need more souls!'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://cdn.discordapp.com/attachments/412373201063247873/429291521229979649/image-8.jpg',\r\n",
      " 'name': 'NOW it’s getting interesting!'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/y8eu4bk8eyo01.jpg', 'name': 'Happens all the time'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/jano7sdtnwo01.jpg', 'name': 'Smooth af'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/5jGY5xO.jpg', 'name': 'Yes daddy'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/kihfuavylyo01.jpg',\r\n",
      " 'name': 'I excuse you, not the bell'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/f9i2qxi6fvo01.png',\r\n",
      " 'name': \"I'm pretty sure that wasn't a rule.\"}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/xuhvkazeowo01.jpg', 'name': 'Android > iOS'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/pejmhfrszuo01.jpg', 'name': 'Inside Job.'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.reddit.com/r/memes/?count=25&after=t3_888tkg> (referer: https://www.reddit.com/r/memes/)\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/ci4kf7h00xo01.jpg', 'name': 'And disappear.'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/zkz8wqit7zo01.jpg', 'name': 'Rip Gru figurine'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/45oykye1xyo01.jpg',\r\n",
      " 'name': 'Who said that *look right*'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/l8h760yn1vo01.jpg', 'name': 'Sad reacxx only'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://imgur.com/BC864zz.jpeg',\r\n",
      " 'name': 'They are like real words spams'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/w4o4yw5z7zo01.jpg',\r\n",
      " 'name': 'This is how I play PUBG.'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/z47n42zh6xo01.jpg', 'name': 'Open up my eagle eye'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://imgur.com/ZkTXoCB', 'name': \"They're noble warriors\"}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/zjzo1qq1txo01.jpg', 'name': 'That was a close one'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/uPGueee.jpg', 'name': 'Fun fact'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/s6pkclvxtyo01.png', 'name': \"He can't decide\"}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/iotog5q71so01.jpg', 'name': 'CHAOS MODE'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/zz2r6wsh8to01.jpg', 'name': 'Lol wtf'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/sizk04rntyo01.jpg', 'name': 'Puns are life'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/2UUF6P2.jpg', 'name': 'This is my jam'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/f9wzjhb1qyo01.jpg',\r\n",
      " 'name': 'This spongebob episode looks lit tbh'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/r67aqlduvvo01.jpg', 'name': 'Try again tomorrow...'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/r6bl9eyeiwo01.jpg',\r\n",
      " 'name': 'Happened to a friend of mine.'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/xg948vyj3zo01.jpg',\r\n",
      " 'name': 'My friend Brad is of a superior specie.'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/4i45id7e3yo01.jpg',\r\n",
      " 'name': 'I have a bad camera mom'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/whA1v1L.jpg', 'name': 'Happy Passover!'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/71vipx3uzwo01.jpg', 'name': 'Successful so far.'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/18yzctmmmyo01.jpg',\r\n",
      " 'name': 'my whole life has been a lie'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/uj622txudxo01.png', 'name': 'Monkey Business'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_888tkg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/a3r7hrya9to01.jpg', 'name': 'Chicken.'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.reddit.com/r/memes/?count=50&after=t3_8872b9> (referer: https://www.reddit.com/r/memes/?count=25&after=t3_888tkg)\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/z8a3byyn6wo01.jpg',\r\n",
      " 'name': 'Flip floppin like a fish outta water'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://imgur.com/rTtgeeN.jpg',\r\n",
      " 'name': 'Got to keep that addiction under wraps!'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/uho72rn5sso01.jpg',\r\n",
      " 'name': 'How did this even happen'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/v2afby9mbyo01.jpg',\r\n",
      " 'name': \"Don't spend it all in one place\"}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://imgur.com/YE9wi1t.jpeg', 'name': 'The future is scary'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/j97d5angoxo01.jpg', 'name': 'I can start today.'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/fmso1btuqyo01.jpg', 'name': 'Please send help'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/1jumwj41bzo01.jpg', 'name': '#Relatable immiright.'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/siwge3212yo01.jpg', 'name': 'Stahp taking my broom'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://imgur.com/SEy5ebU.jpg', 'name': 'The truth behind this meme'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/rt7crhtq9zo01.jpg', 'name': 'Just my luck'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/beyxz5axyxo01.jpg',\r\n",
      " 'name': 'I almost got it for the entire summer vacation'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/tcbussxjpto01.jpg', 'name': 'I never overreact...'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://imgur.com/rrW3I4m.jpeg',\r\n",
      " 'name': 'The human brain at its finest'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/BGkPvIA.png', 'name': 'Classic'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/fte35p0ywxo01.jpg',\r\n",
      " 'name': 'Don’t get me started on NyQuil'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/ng7r71tspyo01.png', 'name': 'Relatable (1/2 OC)'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/1bupzou5tyo01.png', 'name': 'No Homo'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/31svwacptpo01.jpg', 'name': 'sick burn'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/d066tkgt6vo01.png',\r\n",
      " 'name': 'Anybody here who can relate to this?'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/zzhdvngbuyo01.jpg', 'name': 'What a great Dad!'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/1dz1w3cr8yo01.jpg',\r\n",
      " 'name': 'Better than Black Panther'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/vvl6gaa1nwo01.jpg', 'name': 'Pax romana'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/1akibdayfzo01.jpg',\r\n",
      " 'name': 'The times have changed'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_8872b9>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/6e6i0xn5eyo01.png', 'name': 'My days looks amazing'}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.engine] INFO: Closing spider (finished)\r\n",
      "2018-03-30 19:48:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n",
      "{'downloader/request_bytes': 1870,\r\n",
      " 'downloader/request_count': 3,\r\n",
      " 'downloader/request_method_count/GET': 3,\r\n",
      " 'downloader/response_bytes': 92397,\r\n",
      " 'downloader/response_count': 3,\r\n",
      " 'downloader/response_status_count/200': 3,\r\n",
      " 'finish_reason': 'finished',\r\n",
      " 'finish_time': datetime.datetime(2018, 3, 30, 23, 48, 41, 722451),\r\n",
      " 'item_scraped_count': 75,\r\n",
      " 'log_count/DEBUG': 79,\r\n",
      " 'log_count/INFO': 7,\r\n",
      " 'request_depth_max': 2,\r\n",
      " 'response_received_count': 3,\r\n",
      " 'scheduler/dequeued': 3,\r\n",
      " 'scheduler/dequeued/memory': 3,\r\n",
      " 'scheduler/enqueued': 3,\r\n",
      " 'scheduler/enqueued/memory': 3,\r\n",
      " 'start_time': datetime.datetime(2018, 3, 30, 23, 48, 40, 445928)}\r\n",
      "2018-03-30 19:48:41 [scrapy.core.engine] INFO: Spider closed (finished)\r\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "venv/Scripts/scrapy.exe runspider spider.py  2>&1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note: The `no_pages` parameter decides the number of pages to scrape. (It is simply a counter that decides how many times a `Request` with the next page's url is yielded) \n",
    "\n",
    "Our scraper works! Amidst the INFO messages, one can see that our program has crawled, scraped and outputted after parsing, in a structured format, the names and links of all the memes on the first two pages of the memes subreddit. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"six\"> </a> \n",
    "# 6. Feed Exports\n",
    "\n",
    "Now that our scraper is working, we actually need to store the parsed data in a usable format like .JSON, .CSV and such. Scrapy, once again, makes this extremely simple to do through Feed Exports. Feed Exports allow one to create a feed of the  scraped data, using a wide array of different serialization formats and storage options. Scrapy supports JSON, JSON Lines, CSV and XML out  of  the box but can be easily extended to support any other serialization format.\n",
    "\n",
    "Scrapy feed exports also support a variety of storage options in addition to one's local filesystem like FTP, S3, and Standard Output. For our purposes, we will be creating a feed for .JSON on our local filesystem but the reader should  note that it is extremely easy for one to switch these up and use whichever option one's project requires. \n",
    "\n",
    "All one needs to do to create a feed export on the local system is add a `-o` switch to one's CLI command and supply the name of the file one want to feed into. (note that scrapy feed exports append to a pre-existing file and does not overwrite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "venv/Scripts/scrapy.exe runspider spider.py -o results.json 2>&1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{\"link\": \"https://i.redd.it/srm1pqxkrxo01.jpg\", \"name\": \"Could've been worse.\"},\n",
      "{\"link\": \"https://i.imgur.com/vxkEdC7.jpg\", \"name\": \"Mmmhm...\"},\n",
      "{\"link\": \"https://i.redd.it/t8g1spx9mxo01.jpg\", \"name\": \"Hide your girlfriend\"},\n",
      "{\"link\": \"https://i.imgur.com/gdxbQRN.jpg\", \"name\": \"Watch it happen\"},\n",
      "{\"link\": \"https://i.redd.it/9m2i6rzsxwo01.jpg\", \"name\": \"I only kinda like Reddit\"},\n",
      "{\"link\": \"https://i.redd.it/zkz8wqit7zo01.jpg\", \"name\": \"Rip Gru figurine\"},\n",
      "{\"link\": \"https://i.redd.it/yvjd3n2szxo01.jpg\", \"name\": \"My favorite\"},\n",
      "{\"link\": \"https://i.redd.it/v27u2zy30xo01.jpg\", \"name\": \"considering their past history, maybe they shouldn't have invited her over\"},\n",
      "{\"link\": \"https://i.redd.it/y8eu4bk8eyo01.jpg\", \"name\": \"Happens all the time\"},\n",
      "{\"link\": \"https://i.redd.it/kihfuavylyo01.jpg\", \"name\": \"I excuse you, not the bell\"},\n",
      "{\"link\": \"https://i.redd.it/45oykye1xyo01.jpg\", \"name\": \"Who said that *look right*\"},\n",
      "{\"link\": \"https://i.redd.it/ymng5u49kxo01.jpg\", \"name\": \"When someone asks me how I\\u2019m doing\"},\n",
      "{\"link\": \"https://cdn.discordapp.com/attachments/412373201063247873/429291521229979649/image-8.jpg\", \"name\": \"NOW it\\u2019s getting interesting!\"},\n",
      "{\"link\": \"https://i.redd.it/n9ebjux2mwo01.png\", \"name\": \"iPhone? Must be defective.\"},\n",
      "{\"link\": \"https://i.redd.it/4s8bz3wb4uo01.jpg\", \"name\": \"Every Time...\"},\n",
      "{\"link\": \"https://i.redd.it/trortbkd6wo01.jpg\", \"name\": \"(Basically it smells like strippers and Scotch)\"},\n",
      "{\"link\": \"https://i.redd.it/w4o4yw5z7zo01.jpg\", \"name\": \"This is how I play PUBG.\"},\n",
      "{\"link\": \"https://i.redd.it/rkjglbroovo01.jpg\", \"name\": \"Too late\"},\n",
      "{\"link\": \"https://i.redd.it/t74fru1d2vo01.jpg\", \"name\": \"Need more souls!\"},\n",
      "{\"link\": \"https://i.redd.it/jano7sdtnwo01.jpg\", \"name\": \"Smooth af\"},\n",
      "{\"link\": \"https://i.imgur.com/5jGY5xO.jpg\", \"name\": \"Yes daddy\"},\n",
      "{\"link\": \"https://imgur.com/ZkTXoCB\", \"name\": \"They're noble warriors\"},\n",
      "{\"link\": \"https://i.redd.it/xuhvkazeowo01.jpg\", \"name\": \"Android > iOS\"},\n",
      "{\"link\": \"https://i.redd.it/f9i2qxi6fvo01.png\", \"name\": \"I'm pretty sure that wasn't a rule.\"},\n",
      "{\"link\": \"https://i.redd.it/s6pkclvxtyo01.png\", \"name\": \"He can't decide\"},\n",
      "{\"link\": \"https://i.redd.it/ci4kf7h00xo01.jpg\", \"name\": \"And disappear.\"},\n",
      "{\"link\": \"https://i.redd.it/sizk04rntyo01.jpg\", \"name\": \"Puns are life\"},\n",
      "{\"link\": \"https://i.redd.it/l8h760yn1vo01.jpg\", \"name\": \"Sad reacxx only\"},\n",
      "{\"link\": \"https://imgur.com/BC864zz.jpeg\", \"name\": \"They are like real words spams\"},\n",
      "{\"link\": \"https://i.redd.it/z47n42zh6xo01.jpg\", \"name\": \"Open up my eagle eye\"},\n",
      "{\"link\": \"https://i.redd.it/zjzo1qq1txo01.jpg\", \"name\": \"That was a close one\"},\n",
      "{\"link\": \"https://i.redd.it/f9wzjhb1qyo01.jpg\", \"name\": \"This spongebob episode looks lit tbh\"},\n",
      "{\"link\": \"https://i.redd.it/75gw5ktynzo01.jpg\", \"name\": \"Bird leaf \\ud83c\\udf43\"},\n",
      "{\"link\": \"https://i.imgur.com/whA1v1L.jpg\", \"name\": \"Happy Passover!\"},\n",
      "{\"link\": \"https://i.imgur.com/uPGueee.jpg\", \"name\": \"Fun fact\"},\n",
      "{\"link\": \"https://i.redd.it/xg948vyj3zo01.jpg\", \"name\": \"My friend Brad is of a superior specie.\"},\n",
      "{\"link\": \"https://i.redd.it/iotog5q71so01.jpg\", \"name\": \"CHAOS MODE\"},\n",
      "{\"link\": \"https://i.redd.it/zz2r6wsh8to01.jpg\", \"name\": \"Lol wtf\"},\n",
      "{\"link\": \"https://i.imgur.com/2UUF6P2.jpg\", \"name\": \"This is my jam\"},\n",
      "{\"link\": \"https://i.redd.it/4i45id7e3yo01.jpg\", \"name\": \"I have a bad camera mom\"},\n",
      "{\"link\": \"https://i.redd.it/rt7crhtq9zo01.jpg\", \"name\": \"Just my luck\"},\n",
      "{\"link\": \"https://i.redd.it/r6bl9eyeiwo01.jpg\", \"name\": \"Happened to a friend of mine.\"},\n",
      "{\"link\": \"https://i.redd.it/18yzctmmmyo01.jpg\", \"name\": \"my whole life has been a lie\"},\n",
      "{\"link\": \"https://i.redd.it/1jumwj41bzo01.jpg\", \"name\": \"#Relatable immiright.\"},\n",
      "{\"link\": \"https://i.redd.it/v2afby9mbyo01.jpg\", \"name\": \"Don't spend it all in one place\"},\n",
      "{\"link\": \"https://i.redd.it/fmso1btuqyo01.jpg\", \"name\": \"Please send help\"},\n",
      "{\"link\": \"https://i.redd.it/uj622txudxo01.png\", \"name\": \"Monkey Business\"},\n",
      "{\"link\": \"https://i.imgur.com/BGkPvIA.png\", \"name\": \"Classic\"},\n",
      "{\"link\": \"https://i.redd.it/z8a3byyn6wo01.jpg\", \"name\": \"Flip floppin like a fish outta water\"},\n",
      "{\"link\": \"https://i.redd.it/a3r7hrya9to01.jpg\", \"name\": \"Chicken.\"},\n",
      "{\"link\": \"https://imgur.com/rTtgeeN.jpg\", \"name\": \"Got to keep that addiction under wraps!\"},\n",
      "{\"link\": \"https://i.redd.it/uho72rn5sso01.jpg\", \"name\": \"How did this even happen\"},\n",
      "{\"link\": \"https://imgur.com/YE9wi1t.jpeg\", \"name\": \"The future is scary\"},\n",
      "{\"link\": \"https://i.redd.it/siwge3212yo01.jpg\", \"name\": \"Stahp taking my broom\"},\n",
      "{\"link\": \"https://i.redd.it/1akibdayfzo01.jpg\", \"name\": \"The times have changed\"},\n",
      "{\"link\": \"https://i.redd.it/j97d5angoxo01.jpg\", \"name\": \"I can start today.\"},\n",
      "{\"link\": \"https://i.redd.it/1bupzou5tyo01.png\", \"name\": \"No Homo\"},\n",
      "{\"link\": \"https://i.redd.it/95vk6vhpszo01.jpg\", \"name\": \"Independence\"},\n",
      "{\"link\": \"https://i.redd.it/beyxz5axyxo01.jpg\", \"name\": \"I almost got it for the entire summer vacation\"},\n",
      "{\"link\": \"https://i.redd.it/zzhdvngbuyo01.jpg\", \"name\": \"What a great Dad!\"},\n",
      "{\"link\": \"https://imgur.com/SEy5ebU.jpg\", \"name\": \"The truth behind this meme\"},\n",
      "{\"link\": \"https://i.redd.it/fte35p0ywxo01.jpg\", \"name\": \"Don\\u2019t get me started on NyQuil\"},\n",
      "{\"link\": \"https://i.redd.it/tcbussxjpto01.jpg\", \"name\": \"I never overreact...\"},\n",
      "{\"link\": \"https://i.redd.it/ng7r71tspyo01.png\", \"name\": \"Relatable (1/2 OC)\"},\n",
      "{\"link\": \"https://imgur.com/rrW3I4m.jpeg\", \"name\": \"The human brain at its finest\"},\n",
      "{\"link\": \"https://i.redd.it/6e6i0xn5eyo01.png\", \"name\": \"My days looks amazing\"},\n",
      "{\"link\": \"https://i.redd.it/twbeozsmvyo01.jpg\", \"name\": \"Ur meme a wet dream\"},\n",
      "{\"link\": \"https://i.redd.it/o5wh96fkbzo01.png\", \"name\": \"Every day.\"},\n",
      "{\"link\": \"https://i.redd.it/vvl6gaa1nwo01.jpg\", \"name\": \"Pax romana\"},\n",
      "{\"link\": \"https://i.redd.it/d066tkgt6vo01.png\", \"name\": \"Anybody here who can relate to this?\"},\n",
      "{\"link\": \"https://i.redd.it/31svwacptpo01.jpg\", \"name\": \"sick burn\"},\n",
      "{\"link\": \"https://i.redd.it/ay8iq3wnszo01.jpg\", \"name\": \"You sound like an owl.\"},\n",
      "{\"link\": \"https://i.redd.it/st23d6srxwo01.jpg\", \"name\": \"in a heated debate currently\"},\n",
      "{\"link\": \"https://imgur.com/7uL3DjO\", \"name\": \"Legally allowed\"},\n",
      "{\"link\": \"https://i.redd.it/wtv2h2j8mxo01.png\", \"name\": \"North Korea\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"results.json\", encoding=\"utf8\")\n",
    "print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, we have a cleanly formatted .JSON file with the links and names of all the memes on the first page of reddit. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seven\"> </a> \n",
    "## 7. Spider Arguments\n",
    "\n",
    "Wouldn't it be useful if we could actually specify the number of pages we want to scrape while running our spider? This issue of user-controlled variables in one's code arises frequently during the data scraping process since one usually needs to experiment with different sized data-sets and such. \n",
    "\n",
    "Scrapy makes this simple through spider arguments i.e, by using the `-a` tag while running our CLI command with a name=value pair for each argument. We will now, implement this in our existing meme scraper to make it possible to change the number of pages scraped with each CLI command to run our spider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spider.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class Meme(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "\n",
    "class RedditScraper(scrapy.Spider):\n",
    " \n",
    "    name = \"reddit_scraper\"\n",
    "    start_urls = ['https://www.reddit.com/r/memes/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        next_link = response.css('span.next-button').css('a::attr(href)')[0]\n",
    "        if next_link and self.no_pages:\n",
    "            self.no_pages = int(self.no_pages) - 1\n",
    "            yield response.follow(next_link, callback=self.parse)\n",
    "        \n",
    "        links = response.css('div.thing')\n",
    "        for link in links:\n",
    "            meme = Meme()\n",
    "            meme[\"link\"] = link.css('::attr(data-url)').extract_first()\n",
    "            meme[\"name\"] = link.css('a.title::text').extract_first()\n",
    "            yield meme\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `start_requests` method is simply the default starting function that scrapy provides. The arguments passed to our spider are accessible in our code through `self.name`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-30 21:07:34 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n",
      "2018-03-30 21:07:34 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.2.1, Platform Windows-10-10.0.16299-SP0\r\n",
      "2018-03-30 21:07:34 [scrapy.crawler] INFO: Overridden settings: {'FEED_FORMAT': 'json', 'FEED_URI': 'results2.json', 'SPIDER_LOADER_WARN_ONLY': True}\r\n",
      "2018-03-30 21:07:34 [scrapy.middleware] INFO: Enabled extensions:\r\n",
      "['scrapy.extensions.corestats.CoreStats',\r\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\r\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\r\n",
      " 'scrapy.extensions.logstats.LogStats']\r\n",
      "2018-03-30 21:07:34 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n",
      "2018-03-30 21:07:34 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n",
      "2018-03-30 21:07:34 [scrapy.middleware] INFO: Enabled item pipelines:\r\n",
      "[]\r\n",
      "2018-03-30 21:07:34 [scrapy.core.engine] INFO: Spider opened\r\n",
      "2018-03-30 21:07:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n",
      "2018-03-30 21:07:34 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n",
      "2018-03-30 21:07:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.reddit.com/r/memes/> (referer: None)\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/srm1pqxkrxo01.jpg', 'name': \"Could've been worse.\"}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/vxkEdC7.jpg', 'name': 'Mmmhm...'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/zkz8wqit7zo01.jpg', 'name': 'Rip Gru figurine'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/t8g1spx9mxo01.jpg', 'name': 'Hide your girlfriend'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/45oykye1xyo01.jpg',\r\n",
      " 'name': 'Who said that *look right*'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/gdxbQRN.jpg', 'name': 'Watch it happen'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/kihfuavylyo01.jpg',\r\n",
      " 'name': 'I excuse you, not the bell'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/9m2i6rzsxwo01.jpg',\r\n",
      " 'name': 'I only kinda like Reddit'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/yvjd3n2szxo01.jpg', 'name': 'My favorite'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/y8eu4bk8eyo01.jpg', 'name': 'Happens all the time'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/v27u2zy30xo01.jpg',\r\n",
      " 'name': \"considering their past history, maybe they shouldn't have invited \"\r\n",
      "         'her over'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/ymng5u49kxo01.jpg',\r\n",
      " 'name': 'When someone asks me how I’m doing'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/w4o4yw5z7zo01.jpg',\r\n",
      " 'name': 'This is how I play PUBG.'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://cdn.discordapp.com/attachments/412373201063247873/429291521229979649/image-8.jpg',\r\n",
      " 'name': 'NOW it’s getting interesting!'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/n9ebjux2mwo01.png',\r\n",
      " 'name': 'iPhone? Must be defective.'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/4s8bz3wb4uo01.jpg', 'name': 'Every Time...'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/trortbkd6wo01.jpg',\r\n",
      " 'name': '(Basically it smells like strippers and Scotch)'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/rkjglbroovo01.jpg', 'name': 'Too late'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/t74fru1d2vo01.jpg', 'name': 'Need more souls!'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/jano7sdtnwo01.jpg', 'name': 'Smooth af'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://imgur.com/ZkTXoCB', 'name': \"They're noble warriors\"}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/s6pkclvxtyo01.png', 'name': \"He can't decide\"}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/5jGY5xO.jpg', 'name': 'Yes daddy'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/xuhvkazeowo01.jpg', 'name': 'Android > iOS'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/f9i2qxi6fvo01.png',\r\n",
      " 'name': \"I'm pretty sure that wasn't a rule.\"}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.reddit.com/r/memes/?count=25&after=t3_8895vq> (referer: https://www.reddit.com/r/memes/)\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/75gw5ktynzo01.jpg', 'name': 'Bird leaf \\U0001f343'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/sizk04rntyo01.jpg', 'name': 'Puns are life'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/ci4kf7h00xo01.jpg', 'name': 'And disappear.'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/l8h760yn1vo01.jpg', 'name': 'Sad reacxx only'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/whA1v1L.jpg', 'name': 'Happy Passover!'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/f9wzjhb1qyo01.jpg',\r\n",
      " 'name': 'This spongebob episode looks lit tbh'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://imgur.com/BC864zz.jpeg',\r\n",
      " 'name': 'They are like real words spams'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/z47n42zh6xo01.jpg', 'name': 'Open up my eagle eye'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/zjzo1qq1txo01.jpg', 'name': 'That was a close one'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/xg948vyj3zo01.jpg',\r\n",
      " 'name': 'My friend Brad is of a superior specie.'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/uPGueee.jpg', 'name': 'Fun fact'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/iotog5q71so01.jpg', 'name': 'CHAOS MODE'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/zz2r6wsh8to01.jpg', 'name': 'Lol wtf'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/rt7crhtq9zo01.jpg', 'name': 'Just my luck'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/2UUF6P2.jpg', 'name': 'This is my jam'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/4i45id7e3yo01.jpg',\r\n",
      " 'name': 'I have a bad camera mom'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/95vk6vhpszo01.jpg', 'name': 'Independence'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/18yzctmmmyo01.jpg',\r\n",
      " 'name': 'my whole life has been a lie'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/BGkPvIA.png', 'name': 'Classic'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/v2afby9mbyo01.jpg',\r\n",
      " 'name': \"Don't spend it all in one place\"}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/r6bl9eyeiwo01.jpg',\r\n",
      " 'name': 'Happened to a friend of mine.'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/xvhz3tvnwzo01.jpg', 'name': 'Get on my level'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/fmso1btuqyo01.jpg', 'name': 'Please send help'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/1jumwj41bzo01.jpg', 'name': '#Relatable immiright.'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=25&after=t3_8895vq>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/ay8iq3wnszo01.jpg',\r\n",
      " 'name': 'You sound like an owl.'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.reddit.com/r/memes/?count=50&after=t3_88f09r> (referer: https://www.reddit.com/r/memes/?count=25&after=t3_8895vq)\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/uj622txudxo01.png', 'name': 'Monkey Business'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/siwge3212yo01.jpg', 'name': 'Stahp taking my broom'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/z8a3byyn6wo01.jpg',\r\n",
      " 'name': 'Flip floppin like a fish outta water'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/1akibdayfzo01.jpg',\r\n",
      " 'name': 'The times have changed'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/a3r7hrya9to01.jpg', 'name': 'Chicken.'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://imgur.com/rTtgeeN.jpg',\r\n",
      " 'name': 'Got to keep that addiction under wraps!'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/uho72rn5sso01.jpg',\r\n",
      " 'name': 'How did this even happen'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://imgur.com/YE9wi1t.jpeg', 'name': 'The future is scary'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/1bupzou5tyo01.png', 'name': 'No Homo'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/j97d5angoxo01.jpg', 'name': 'I can start today.'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/zzhdvngbuyo01.jpg', 'name': 'What a great Dad!'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/beyxz5axyxo01.jpg',\r\n",
      " 'name': 'I almost got it for the entire summer vacation'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/o5wh96fkbzo01.png', 'name': 'Every day.'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/ng7r71tspyo01.png', 'name': 'Relatable (1/2 OC)'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/fte35p0ywxo01.jpg',\r\n",
      " 'name': 'Don’t get me started on NyQuil'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/6e6i0xn5eyo01.png', 'name': 'My days looks amazing'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://imgur.com/SEy5ebU.jpg', 'name': 'The truth behind this meme'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/tcbussxjpto01.jpg', 'name': 'I never overreact...'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://imgur.com/rrW3I4m.jpeg',\r\n",
      " 'name': 'The human brain at its finest'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/twbeozsmvyo01.jpg', 'name': 'Ur meme a wet dream'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/gy2sk9i5ezo01.png',\r\n",
      " 'name': 'It is not that I feel lonely, it is just that I am lonely.'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/8o9i64z1jyo01.jpg', 'name': 'Poor Gru'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/gxhuk82woyo01.png', 'name': \"Groot's Plan\"}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/wtv2h2j8mxo01.png', 'name': 'North Korea'}\r\n",
      "2018-03-30 21:07:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=50&after=t3_88f09r>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/st23d6srxwo01.jpg',\r\n",
      " 'name': 'in a heated debate currently'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.reddit.com/r/memes/?count=75&after=t3_88auez> (referer: https://www.reddit.com/r/memes/?count=50&after=t3_88f09r)\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/d066tkgt6vo01.png',\r\n",
      " 'name': 'Anybody here who can relate to this?'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/vvl6gaa1nwo01.jpg', 'name': 'Pax romana'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/31svwacptpo01.jpg', 'name': 'sick burn'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/yx8020j7ezo01.jpg',\r\n",
      " 'name': 'Which one would you believe?'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://imgur.com/7uL3DjO', 'name': 'Legally allowed'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/t74d2o6fgzo01.jpg', 'name': 'I hev made mem'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/11pckib7mzo01.jpg', 'name': 'Silly Women.'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/j7pscqyalzo01.jpg',\r\n",
      " 'name': 'As Kevin Spacey walks into the bus.'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/472mf2b9qwo01.jpg',\r\n",
      " 'name': 'You get a repost, you get a repost, everyone gets a repost!'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/3i13oki8vto01.jpg',\r\n",
      " 'name': 'C’mon floor, just this one time..'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/076mr3pvszo01.jpg', 'name': 'Superior format'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/uxojp6hxbzo01.png', 'name': 'Fuck slingshots'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/g34dj96tfro01.jpg', 'name': 'Undercover'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/p7rhayl65yo01.jpg',\r\n",
      " 'name': 'Congrats on coming forward for the big face reveal Jack'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/unqjec2gmzo01.jpg', 'name': '-9 out of 11'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/hwbc4dfajyo01.jpg',\r\n",
      " 'name': 'How about something that’s actually funny?'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/oamjfe9tjuo01.jpg',\r\n",
      " 'name': 'Was almost over it too'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/w9qgc2q23yo01.jpg',\r\n",
      " 'name': 'When someone calls your mom gay'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/Up69RJY.jpg',\r\n",
      " 'name': 'When you get together with your friends on a Friday evening and two '\r\n",
      "         'of them announce promotions.....'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/dnghk4wqpyo01.png',\r\n",
      " 'name': \"It ain't safe for the black or the white girls\"}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/fzvst6vltwo01.jpg', 'name': 'She GoT assets.'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/mfxo2jj98xo01.jpg', 'name': 'Get lost jack'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'http://imgur.com/lybgixR', 'name': 'Good ol’ Country Cap'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/xm7r4fxeexo01.jpg',\r\n",
      " 'name': 'Anyone up for some PHYSICS MEMES ?'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=75&after=t3_88auez>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/rfiyvgkq1ro01.jpg',\r\n",
      " 'name': 'this is gonna look foolish if it flops'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.reddit.com/r/memes/?count=100&after=t3_883wrg> (referer: https://www.reddit.com/r/memes/?count=75&after=t3_88auez)\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/imrsfp3s1zo01.jpg', 'name': 'Are pigs green?'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/03gugbxm0zo01.jpg',\r\n",
      " 'name': 'After this picture was taken the girls were never found again. All '\r\n",
      "         \"they found was Barney's mask.\"}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/1mnyychmcyo01.jpg',\r\n",
      " 'name': 'But why would it hurt me like that'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/mgqg420ygwo01.jpg', 'name': 'Rules are rules'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://imgur.com/R7XtvVY', 'name': \"Guess I'll just be a bun\"}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/jDgVKdO.jpg',\r\n",
      " 'name': 'Internet- meet Concerned Dog'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/KTmlh49.jpg', 'name': 'Tap Tap Tap'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/tkjhi5xfryo01.jpg',\r\n",
      " 'name': 'Is this still relevant?'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/96yy2qsa1xo01.jpg',\r\n",
      " 'name': 'I drew a picture of myself...'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/vtucvw4vnzo01.jpg',\r\n",
      " 'name': 'Stop trying to deny it, Siri...'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/p5vi4zjunwo01.png',\r\n",
      " 'name': '[Meta]Me vs The guy she told me not to worry about'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/z50duafibxo01.jpg', 'name': 'yes, it exists'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/mozozfntwro01.jpg',\r\n",
      " 'name': 'Damn you Brad & your clouds of lies !!!'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/1mdqpvx7xyo01.jpg',\r\n",
      " 'name': 'For anyone doing maintainance on their car.'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/gcvfrku5xyo01.jpg', 'name': 'Facebook is supreme'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/ycoi8x90kzo01.jpg',\r\n",
      " 'name': 'Is this really how it works....'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/4x7y3kb0dzo01.jpg',\r\n",
      " 'name': \"If this baby isn't here in 15 minutes they are legally allowed to \"\r\n",
      "         'leave'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/qrjesi4owzo01.jpg', 'name': 'Anddddd action.'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/zkwust5bhxo01.jpg',\r\n",
      " 'name': 'Together or not at all'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/Ourek7U.jpg', 'name': 'We get to go, Planet.'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/t7fwcfxf0to01.jpg', 'name': 'Struggles'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/ouwmrdzv0yo01.png',\r\n",
      " 'name': 'Picture of a cancerous cell'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/qlqscyjtdto01.jpg',\r\n",
      " 'name': 'The biggest blockbuster EVER!'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/xlab1yox6yo01.png',\r\n",
      " 'name': 'Did somebody say excess oil?\\U0001f914\\U0001f914\\U0001f914\\U0001f914'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=100&after=t3_883wrg>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/zpl2vcbuazo01.png',\r\n",
      " 'name': '\"Are you ready Patrick?\"'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.reddit.com/r/memes/?count=125&after=t3_88edqj> (referer: https://www.reddit.com/r/memes/?count=100&after=t3_883wrg)\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/b446wo60vqo01.jpg', 'name': 'we did itttttttt'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/pi66lk3p4ro01.jpg', 'name': 'Not the only wood eh'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://imgur.com/PfhJHnP.jpeg', 'name': 'Very poisonous though!'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://imgur.com/Xl0JlG7', 'name': 'Happens every time'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/hql6k9vc2qo01.jpg',\r\n",
      " 'name': 'What’s this meme called?'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/scctuxluwzo01.jpg', 'name': 'Fer Realz'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/k4oey6vplyo01.jpg', 'name': 'Looking for a job?'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/h3h2nhgivso01.jpg',\r\n",
      " 'name': 'You’re under arrest for the feelings you hurt'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/o1cft7mqcso01.jpg',\r\n",
      " 'name': 'oh wow what a great hat!'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://imgur.com/CJJKyEX.jpeg',\r\n",
      " 'name': 'His head is brighter than my future'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/mj9bmqmn5ro01.jpg',\r\n",
      " 'name': 'Time heals all wounds but scars are a constant reminder...'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/p6689gmyzzo01.png',\r\n",
      " 'name': 'When someone turns on the lights in the middle of the day'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/jriz3eninso01.jpg',\r\n",
      " 'name': 'This happens too often'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/pcjw5f6srno01.jpg', 'name': 'Familiar?'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/fawhh92zxoo01.jpg', 'name': 'Heartbreaking indeed.'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/yssn7bd9jyo01.png', 'name': 'Boss Fight'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/krsbd1xqnvo01.png', 'name': 'I want that gold...'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/b0up1z1hqyo01.jpg', 'name': 'hmmm...'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/a30euuplczo01.jpg', 'name': 'No dumb chicks'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/k1mz1j9p6ro01.jpg', 'name': \"I'm fine\"}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/z3gO8aB.jpg', 'name': 'Maybe as an appy'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.imgur.com/JHBIlVn.jpg', 'name': 'So much fun'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/h22silhhjxo01.jpg',\r\n",
      " 'name': \"When she says she didn't hit you because she couldn't but then she \"\r\n",
      "         'says there is no escape'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'http://www.livememe.com/ht24mo1.jpg',\r\n",
      " 'name': 'Spring Holiday Expectations'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.reddit.com/r/memes/?count=125&after=t3_88edqj>\r",
      "\r\n",
      "{'link': 'https://i.redd.it/iv22h4r6pqo01.png', 'name': 'Amy Schummer is funny'}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.engine] INFO: Closing spider (finished)\r\n",
      "2018-03-30 21:07:36 [scrapy.extensions.feedexport] INFO: Stored json feed (150 items) in: results2.json\r\n",
      "2018-03-30 21:07:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n",
      "{'downloader/request_bytes': 4384,\r\n",
      " 'downloader/request_count': 6,\r\n",
      " 'downloader/request_method_count/GET': 6,\r\n",
      " 'downloader/response_bytes': 184817,\r\n",
      " 'downloader/response_count': 6,\r\n",
      " 'downloader/response_status_count/200': 6,\r\n",
      " 'finish_reason': 'finished',\r\n",
      " 'finish_time': datetime.datetime(2018, 3, 31, 1, 7, 36, 849866),\r\n",
      " 'item_scraped_count': 150,\r\n",
      " 'log_count/DEBUG': 157,\r\n",
      " 'log_count/INFO': 8,\r\n",
      " 'request_depth_max': 5,\r\n",
      " 'response_received_count': 6,\r\n",
      " 'scheduler/dequeued': 6,\r\n",
      " 'scheduler/dequeued/memory': 6,\r\n",
      " 'scheduler/enqueued': 6,\r\n",
      " 'scheduler/enqueued/memory': 6,\r\n",
      " 'start_time': datetime.datetime(2018, 3, 31, 1, 7, 34, 608824)}\r\n",
      "2018-03-30 21:07:36 [scrapy.core.engine] INFO: Spider closed (finished)\r\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "venv/Scripts/scrapy.exe runspider spider.py -o results2.json -a no_pages=5 2>&1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{\"link\": \"https://i.redd.it/srm1pqxkrxo01.jpg\", \"name\": \"Could've been worse.\"},\n",
      "{\"link\": \"https://i.imgur.com/vxkEdC7.jpg\", \"name\": \"Mmmhm...\"},\n",
      "{\"link\": \"https://i.redd.it/zkz8wqit7zo01.jpg\", \"name\": \"Rip Gru figurine\"},\n",
      "{\"link\": \"https://i.redd.it/t8g1spx9mxo01.jpg\", \"name\": \"Hide your girlfriend\"},\n",
      "{\"link\": \"https://i.redd.it/45oykye1xyo01.jpg\", \"name\": \"Who said that *look right*\"},\n",
      "{\"link\": \"https://i.imgur.com/gdxbQRN.jpg\", \"name\": \"Watch it happen\"},\n",
      "{\"link\": \"https://i.redd.it/kihfuavylyo01.jpg\", \"name\": \"I excuse you, not the bell\"},\n",
      "{\"link\": \"https://i.redd.it/9m2i6rzsxwo01.jpg\", \"name\": \"I only kinda like Reddit\"},\n",
      "{\"link\": \"https://i.redd.it/yvjd3n2szxo01.jpg\", \"name\": \"My favorite\"},\n",
      "{\"link\": \"https://i.redd.it/y8eu4bk8eyo01.jpg\", \"name\": \"Happens all the time\"},\n",
      "{\"link\": \"https://i.redd.it/v27u2zy30xo01.jpg\", \"name\": \"considering their past history, maybe they shouldn't have invited her over\"},\n",
      "{\"link\": \"https://i.redd.it/ymng5u49kxo01.jpg\", \"name\": \"When someone asks me how I\\u2019m doing\"},\n",
      "{\"link\": \"https://i.redd.it/w4o4yw5z7zo01.jpg\", \"name\": \"This is how I play PUBG.\"},\n",
      "{\"link\": \"https://cdn.discordapp.com/attachments/412373201063247873/429291521229979649/image-8.jpg\", \"name\": \"NOW it\\u2019s getting interesting!\"},\n",
      "{\"link\": \"https://i.redd.it/n9ebjux2mwo01.png\", \"name\": \"iPhone? Must be defective.\"},\n",
      "{\"link\": \"https://i.redd.it/4s8bz3wb4uo01.jpg\", \"name\": \"Every Time...\"},\n",
      "{\"link\": \"https://i.redd.it/trortbkd6wo01.jpg\", \"name\": \"(Basically it smells like strippers and Scotch)\"},\n",
      "{\"link\": \"https://i.redd.it/rkjglbroovo01.jpg\", \"name\": \"Too late\"},\n",
      "{\"link\": \"https://i.redd.it/t74fru1d2vo01.jpg\", \"name\": \"Need more souls!\"},\n",
      "{\"link\": \"https://i.redd.it/jano7sdtnwo01.jpg\", \"name\": \"Smooth af\"},\n",
      "{\"link\": \"https://imgur.com/ZkTXoCB\", \"name\": \"They're noble warriors\"},\n",
      "{\"link\": \"https://i.redd.it/s6pkclvxtyo01.png\", \"name\": \"He can't decide\"},\n",
      "{\"link\": \"https://i.imgur.com/5jGY5xO.jpg\", \"name\": \"Yes daddy\"},\n",
      "{\"link\": \"https://i.redd.it/xuhvkazeowo01.jpg\", \"name\": \"Android > iOS\"},\n",
      "{\"link\": \"https://i.redd.it/f9i2qxi6fvo01.png\", \"name\": \"I'm pretty sure that wasn't a rule.\"},\n",
      "{\"link\": \"https://i.redd.it/75gw5ktynzo01.jpg\", \"name\": \"Bird leaf \\ud83c\\udf43\"},\n",
      "{\"link\": \"https://i.redd.it/sizk04rntyo01.jpg\", \"name\": \"Puns are life\"},\n",
      "{\"link\": \"https://i.redd.it/ci4kf7h00xo01.jpg\", \"name\": \"And disappear.\"},\n",
      "{\"link\": \"https://i.redd.it/l8h760yn1vo01.jpg\", \"name\": \"Sad reacxx only\"},\n",
      "{\"link\": \"https://i.imgur.com/whA1v1L.jpg\", \"name\": \"Happy Passover!\"},\n",
      "{\"link\": \"https://i.redd.it/f9wzjhb1qyo01.jpg\", \"name\": \"This spongebob episode looks lit tbh\"},\n",
      "{\"link\": \"https://imgur.com/BC864zz.jpeg\", \"name\": \"They are like real words spams\"},\n",
      "{\"link\": \"https://i.redd.it/z47n42zh6xo01.jpg\", \"name\": \"Open up my eagle eye\"},\n",
      "{\"link\": \"https://i.redd.it/zjzo1qq1txo01.jpg\", \"name\": \"That was a close one\"},\n",
      "{\"link\": \"https://i.redd.it/xg948vyj3zo01.jpg\", \"name\": \"My friend Brad is of a superior specie.\"},\n",
      "{\"link\": \"https://i.imgur.com/uPGueee.jpg\", \"name\": \"Fun fact\"},\n",
      "{\"link\": \"https://i.redd.it/iotog5q71so01.jpg\", \"name\": \"CHAOS MODE\"},\n",
      "{\"link\": \"https://i.redd.it/zz2r6wsh8to01.jpg\", \"name\": \"Lol wtf\"},\n",
      "{\"link\": \"https://i.redd.it/rt7crhtq9zo01.jpg\", \"name\": \"Just my luck\"},\n",
      "{\"link\": \"https://i.imgur.com/2UUF6P2.jpg\", \"name\": \"This is my jam\"},\n",
      "{\"link\": \"https://i.redd.it/4i45id7e3yo01.jpg\", \"name\": \"I have a bad camera mom\"},\n",
      "{\"link\": \"https://i.redd.it/95vk6vhpszo01.jpg\", \"name\": \"Independence\"},\n",
      "{\"link\": \"https://i.redd.it/18yzctmmmyo01.jpg\", \"name\": \"my whole life has been a lie\"},\n",
      "{\"link\": \"https://i.imgur.com/BGkPvIA.png\", \"name\": \"Classic\"},\n",
      "{\"link\": \"https://i.redd.it/v2afby9mbyo01.jpg\", \"name\": \"Don't spend it all in one place\"},\n",
      "{\"link\": \"https://i.redd.it/r6bl9eyeiwo01.jpg\", \"name\": \"Happened to a friend of mine.\"},\n",
      "{\"link\": \"https://i.redd.it/xvhz3tvnwzo01.jpg\", \"name\": \"Get on my level\"},\n",
      "{\"link\": \"https://i.redd.it/fmso1btuqyo01.jpg\", \"name\": \"Please send help\"},\n",
      "{\"link\": \"https://i.redd.it/1jumwj41bzo01.jpg\", \"name\": \"#Relatable immiright.\"},\n",
      "{\"link\": \"https://i.redd.it/ay8iq3wnszo01.jpg\", \"name\": \"You sound like an owl.\"},\n",
      "{\"link\": \"https://i.redd.it/uj622txudxo01.png\", \"name\": \"Monkey Business\"},\n",
      "{\"link\": \"https://i.redd.it/siwge3212yo01.jpg\", \"name\": \"Stahp taking my broom\"},\n",
      "{\"link\": \"https://i.redd.it/z8a3byyn6wo01.jpg\", \"name\": \"Flip floppin like a fish outta water\"},\n",
      "{\"link\": \"https://i.redd.it/1akibdayfzo01.jpg\", \"name\": \"The times have changed\"},\n",
      "{\"link\": \"https://i.redd.it/a3r7hrya9to01.jpg\", \"name\": \"Chicken.\"},\n",
      "{\"link\": \"https://imgur.com/rTtgeeN.jpg\", \"name\": \"Got to keep that addiction under wraps!\"},\n",
      "{\"link\": \"https://i.redd.it/uho72rn5sso01.jpg\", \"name\": \"How did this even happen\"},\n",
      "{\"link\": \"https://imgur.com/YE9wi1t.jpeg\", \"name\": \"The future is scary\"},\n",
      "{\"link\": \"https://i.redd.it/1bupzou5tyo01.png\", \"name\": \"No Homo\"},\n",
      "{\"link\": \"https://i.redd.it/j97d5angoxo01.jpg\", \"name\": \"I can start today.\"},\n",
      "{\"link\": \"https://i.redd.it/zzhdvngbuyo01.jpg\", \"name\": \"What a great Dad!\"},\n",
      "{\"link\": \"https://i.redd.it/beyxz5axyxo01.jpg\", \"name\": \"I almost got it for the entire summer vacation\"},\n",
      "{\"link\": \"https://i.redd.it/o5wh96fkbzo01.png\", \"name\": \"Every day.\"},\n",
      "{\"link\": \"https://i.redd.it/ng7r71tspyo01.png\", \"name\": \"Relatable (1/2 OC)\"},\n",
      "{\"link\": \"https://i.redd.it/fte35p0ywxo01.jpg\", \"name\": \"Don\\u2019t get me started on NyQuil\"},\n",
      "{\"link\": \"https://i.redd.it/6e6i0xn5eyo01.png\", \"name\": \"My days looks amazing\"},\n",
      "{\"link\": \"https://imgur.com/SEy5ebU.jpg\", \"name\": \"The truth behind this meme\"},\n",
      "{\"link\": \"https://i.redd.it/tcbussxjpto01.jpg\", \"name\": \"I never overreact...\"},\n",
      "{\"link\": \"https://imgur.com/rrW3I4m.jpeg\", \"name\": \"The human brain at its finest\"},\n",
      "{\"link\": \"https://i.redd.it/twbeozsmvyo01.jpg\", \"name\": \"Ur meme a wet dream\"},\n",
      "{\"link\": \"https://i.redd.it/gy2sk9i5ezo01.png\", \"name\": \"It is not that I feel lonely, it is just that I am lonely.\"},\n",
      "{\"link\": \"https://i.redd.it/8o9i64z1jyo01.jpg\", \"name\": \"Poor Gru\"},\n",
      "{\"link\": \"https://i.redd.it/gxhuk82woyo01.png\", \"name\": \"Groot's Plan\"},\n",
      "{\"link\": \"https://i.redd.it/wtv2h2j8mxo01.png\", \"name\": \"North Korea\"},\n",
      "{\"link\": \"https://i.redd.it/st23d6srxwo01.jpg\", \"name\": \"in a heated debate currently\"},\n",
      "{\"link\": \"https://i.redd.it/d066tkgt6vo01.png\", \"name\": \"Anybody here who can relate to this?\"},\n",
      "{\"link\": \"https://i.redd.it/vvl6gaa1nwo01.jpg\", \"name\": \"Pax romana\"},\n",
      "{\"link\": \"https://i.redd.it/31svwacptpo01.jpg\", \"name\": \"sick burn\"},\n",
      "{\"link\": \"https://i.redd.it/yx8020j7ezo01.jpg\", \"name\": \"Which one would you believe?\"},\n",
      "{\"link\": \"https://imgur.com/7uL3DjO\", \"name\": \"Legally allowed\"},\n",
      "{\"link\": \"https://i.redd.it/t74d2o6fgzo01.jpg\", \"name\": \"I hev made mem\"},\n",
      "{\"link\": \"https://i.redd.it/11pckib7mzo01.jpg\", \"name\": \"Silly Women.\"},\n",
      "{\"link\": \"https://i.redd.it/j7pscqyalzo01.jpg\", \"name\": \"As Kevin Spacey walks into the bus.\"},\n",
      "{\"link\": \"https://i.redd.it/472mf2b9qwo01.jpg\", \"name\": \"You get a repost, you get a repost, everyone gets a repost!\"},\n",
      "{\"link\": \"https://i.redd.it/3i13oki8vto01.jpg\", \"name\": \"C\\u2019mon floor, just this one time..\"},\n",
      "{\"link\": \"https://i.redd.it/076mr3pvszo01.jpg\", \"name\": \"Superior format\"},\n",
      "{\"link\": \"https://i.redd.it/uxojp6hxbzo01.png\", \"name\": \"Fuck slingshots\"},\n",
      "{\"link\": \"https://i.redd.it/g34dj96tfro01.jpg\", \"name\": \"Undercover\"},\n",
      "{\"link\": \"https://i.redd.it/p7rhayl65yo01.jpg\", \"name\": \"Congrats on coming forward for the big face reveal Jack\"},\n",
      "{\"link\": \"https://i.redd.it/unqjec2gmzo01.jpg\", \"name\": \"-9 out of 11\"},\n",
      "{\"link\": \"https://i.redd.it/hwbc4dfajyo01.jpg\", \"name\": \"How about something that\\u2019s actually funny?\"},\n",
      "{\"link\": \"https://i.redd.it/oamjfe9tjuo01.jpg\", \"name\": \"Was almost over it too\"},\n",
      "{\"link\": \"https://i.redd.it/w9qgc2q23yo01.jpg\", \"name\": \"When someone calls your mom gay\"},\n",
      "{\"link\": \"https://i.imgur.com/Up69RJY.jpg\", \"name\": \"When you get together with your friends on a Friday evening and two of them announce promotions.....\"},\n",
      "{\"link\": \"https://i.redd.it/dnghk4wqpyo01.png\", \"name\": \"It ain't safe for the black or the white girls\"},\n",
      "{\"link\": \"https://i.redd.it/fzvst6vltwo01.jpg\", \"name\": \"She GoT assets.\"},\n",
      "{\"link\": \"https://i.redd.it/mfxo2jj98xo01.jpg\", \"name\": \"Get lost jack\"},\n",
      "{\"link\": \"http://imgur.com/lybgixR\", \"name\": \"Good ol\\u2019 Country Cap\"},\n",
      "{\"link\": \"https://i.redd.it/xm7r4fxeexo01.jpg\", \"name\": \"Anyone up for some PHYSICS MEMES ?\"},\n",
      "{\"link\": \"https://i.redd.it/rfiyvgkq1ro01.jpg\", \"name\": \"this is gonna look foolish if it flops\"},\n",
      "{\"link\": \"https://i.redd.it/imrsfp3s1zo01.jpg\", \"name\": \"Are pigs green?\"},\n",
      "{\"link\": \"https://i.redd.it/03gugbxm0zo01.jpg\", \"name\": \"After this picture was taken the girls were never found again. All they found was Barney's mask.\"},\n",
      "{\"link\": \"https://i.redd.it/1mnyychmcyo01.jpg\", \"name\": \"But why would it hurt me like that\"},\n",
      "{\"link\": \"https://i.redd.it/mgqg420ygwo01.jpg\", \"name\": \"Rules are rules\"},\n",
      "{\"link\": \"https://imgur.com/R7XtvVY\", \"name\": \"Guess I'll just be a bun\"},\n",
      "{\"link\": \"https://i.imgur.com/jDgVKdO.jpg\", \"name\": \"Internet- meet Concerned Dog\"},\n",
      "{\"link\": \"https://i.imgur.com/KTmlh49.jpg\", \"name\": \"Tap Tap Tap\"},\n",
      "{\"link\": \"https://i.redd.it/tkjhi5xfryo01.jpg\", \"name\": \"Is this still relevant?\"},\n",
      "{\"link\": \"https://i.redd.it/96yy2qsa1xo01.jpg\", \"name\": \"I drew a picture of myself...\"},\n",
      "{\"link\": \"https://i.redd.it/vtucvw4vnzo01.jpg\", \"name\": \"Stop trying to deny it, Siri...\"},\n",
      "{\"link\": \"https://i.redd.it/p5vi4zjunwo01.png\", \"name\": \"[Meta]Me vs The guy she told me not to worry about\"},\n",
      "{\"link\": \"https://i.redd.it/z50duafibxo01.jpg\", \"name\": \"yes, it exists\"},\n",
      "{\"link\": \"https://i.redd.it/mozozfntwro01.jpg\", \"name\": \"Damn you Brad & your clouds of lies !!!\"},\n",
      "{\"link\": \"https://i.redd.it/1mdqpvx7xyo01.jpg\", \"name\": \"For anyone doing maintainance on their car.\"},\n",
      "{\"link\": \"https://i.redd.it/gcvfrku5xyo01.jpg\", \"name\": \"Facebook is supreme\"},\n",
      "{\"link\": \"https://i.redd.it/ycoi8x90kzo01.jpg\", \"name\": \"Is this really how it works....\"},\n",
      "{\"link\": \"https://i.redd.it/4x7y3kb0dzo01.jpg\", \"name\": \"If this baby isn't here in 15 minutes they are legally allowed to leave\"},\n",
      "{\"link\": \"https://i.redd.it/qrjesi4owzo01.jpg\", \"name\": \"Anddddd action.\"},\n",
      "{\"link\": \"https://i.redd.it/zkwust5bhxo01.jpg\", \"name\": \"Together or not at all\"},\n",
      "{\"link\": \"https://i.imgur.com/Ourek7U.jpg\", \"name\": \"We get to go, Planet.\"},\n",
      "{\"link\": \"https://i.redd.it/t7fwcfxf0to01.jpg\", \"name\": \"Struggles\"},\n",
      "{\"link\": \"https://i.redd.it/ouwmrdzv0yo01.png\", \"name\": \"Picture of a cancerous cell\"},\n",
      "{\"link\": \"https://i.redd.it/qlqscyjtdto01.jpg\", \"name\": \"The biggest blockbuster EVER!\"},\n",
      "{\"link\": \"https://i.redd.it/xlab1yox6yo01.png\", \"name\": \"Did somebody say excess oil?\\ud83e\\udd14\\ud83e\\udd14\\ud83e\\udd14\\ud83e\\udd14\"},\n",
      "{\"link\": \"https://i.redd.it/zpl2vcbuazo01.png\", \"name\": \"\\\"Are you ready Patrick?\\\"\"},\n",
      "{\"link\": \"https://i.redd.it/b446wo60vqo01.jpg\", \"name\": \"we did itttttttt\"},\n",
      "{\"link\": \"https://i.redd.it/pi66lk3p4ro01.jpg\", \"name\": \"Not the only wood eh\"},\n",
      "{\"link\": \"https://imgur.com/PfhJHnP.jpeg\", \"name\": \"Very poisonous though!\"},\n",
      "{\"link\": \"https://imgur.com/Xl0JlG7\", \"name\": \"Happens every time\"},\n",
      "{\"link\": \"https://i.redd.it/hql6k9vc2qo01.jpg\", \"name\": \"What\\u2019s this meme called?\"},\n",
      "{\"link\": \"https://i.redd.it/scctuxluwzo01.jpg\", \"name\": \"Fer Realz\"},\n",
      "{\"link\": \"https://i.redd.it/k4oey6vplyo01.jpg\", \"name\": \"Looking for a job?\"},\n",
      "{\"link\": \"https://i.redd.it/h3h2nhgivso01.jpg\", \"name\": \"You\\u2019re under arrest for the feelings you hurt\"},\n",
      "{\"link\": \"https://i.redd.it/o1cft7mqcso01.jpg\", \"name\": \"oh wow what a great hat!\"},\n",
      "{\"link\": \"https://imgur.com/CJJKyEX.jpeg\", \"name\": \"His head is brighter than my future\"},\n",
      "{\"link\": \"https://i.redd.it/mj9bmqmn5ro01.jpg\", \"name\": \"Time heals all wounds but scars are a constant reminder...\"},\n",
      "{\"link\": \"https://i.redd.it/p6689gmyzzo01.png\", \"name\": \"When someone turns on the lights in the middle of the day\"},\n",
      "{\"link\": \"https://i.redd.it/jriz3eninso01.jpg\", \"name\": \"This happens too often\"},\n",
      "{\"link\": \"https://i.redd.it/pcjw5f6srno01.jpg\", \"name\": \"Familiar?\"},\n",
      "{\"link\": \"https://i.redd.it/fawhh92zxoo01.jpg\", \"name\": \"Heartbreaking indeed.\"},\n",
      "{\"link\": \"https://i.redd.it/yssn7bd9jyo01.png\", \"name\": \"Boss Fight\"},\n",
      "{\"link\": \"https://i.redd.it/krsbd1xqnvo01.png\", \"name\": \"I want that gold...\"},\n",
      "{\"link\": \"https://i.redd.it/b0up1z1hqyo01.jpg\", \"name\": \"hmmm...\"},\n",
      "{\"link\": \"https://i.redd.it/a30euuplczo01.jpg\", \"name\": \"No dumb chicks\"},\n",
      "{\"link\": \"https://i.redd.it/k1mz1j9p6ro01.jpg\", \"name\": \"I'm fine\"},\n",
      "{\"link\": \"https://i.imgur.com/z3gO8aB.jpg\", \"name\": \"Maybe as an appy\"},\n",
      "{\"link\": \"https://i.imgur.com/JHBIlVn.jpg\", \"name\": \"So much fun\"},\n",
      "{\"link\": \"https://i.redd.it/h22silhhjxo01.jpg\", \"name\": \"When she says she didn't hit you because she couldn't but then she says there is no escape\"},\n",
      "{\"link\": \"http://www.livememe.com/ht24mo1.jpg\", \"name\": \"Spring Holiday Expectations\"},\n",
      "{\"link\": \"https://i.redd.it/iv22h4r6pqo01.png\", \"name\": \"Amy Schummer is funny\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"results2.json\", encoding=\"utf8\")\n",
    "print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, we now have a working reddit meme scraper that cleanly outputs to a JSON file and allows us to scrape a variable number pages. It has all the basic features one might need for such a task and leaves the user with data that is ready to be analyzed in any sort of way. It is worth noting that all this functionality has been achieved with less than 30 lines of code in our final version of `spider.py` which just goes to show how easy it is for a data scientist to quickly scrape some interesting data from a blog/website/news-aggregator or anything of the sort and analyse it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and References\n",
    "\n",
    "As is clear, this tutorial just highlighted and guided the reader through the basic concepts and functionality of scrapy and web scraping in general.  We even referred the reader to look in one of our suggested readings to gain further insight on some topics. \n",
    "\n",
    "Here are some readings we suggest to understand this extremely powerful framework better - \n",
    "   * Offical Documentation for Scrapy (MOST IMPORTANT) https://doc.scrapy.org/en/latest/index.html#section-basics\n",
    "   * An extremely useful beginner tutorial on scrapy (similar to this one) https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3\n",
    "   * A github repo with various useful examples of scrapy usage https://github.com/feiskyer/scrapy-examples\n",
    "   * An advanced tutorial for Scrapy(make sure you're very familiar with the basics before reading this) http://sangaline.com/post/advanced-web-scraping-tutorial/\n",
    "   * A page with several nifty tricks and common practices https://doc.scrapy.org/en/latest/topics/practices.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter2_Python_2",
   "language": "python",
   "name": "jupyter2_python_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}