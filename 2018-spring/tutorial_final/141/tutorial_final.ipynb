{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TENSORFLOW TUTORIAL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "If you are a data science and machine learning enthusiast, you must have already stumbled upon neural networks and deep learning. They have now become an integral part of modelling and the analysis aspect of the data science pipeline. A lot of advancements are happening in this field and thus it is quite exciting to dive deep into this topic. But if you haven't started you journey yet, this tutorial is a perfect start point. The tutorial introduces TensorFlow, which is a machine learning library to build models in a distributed environment. After reading this tutorial, you will be able to create your own machine learning models (primarily neural networks since TensorFlow is more suited for deep learning applications) using TensorFlow to solve a real-world problem. The tutorial assumes that the reader has some prior knowledge about neural nets and programming in python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial content\n",
    "\n",
    "In this tutorial, we will discuss how to use TensorFlow for building models and get ourselves familiar with the TensorFlow environment, it's classes and methods.\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [TensorFlow](#TensorFlow)\n",
    "- [Installation](#Installing-the-libraries)\n",
    "- [API Components](#API-Components)\n",
    "- [Estimators](#Estimators)\n",
    "- [Tensors](#Tensors)\n",
    "- [Graphs](#Graphs)\n",
    "- [Sessions](#Sessions)\n",
    "- [Program Flow](#Program-Flow)\n",
    "- [Losses and Optimizers](#Losses-and-Optimizers)\n",
    "- [Neural Network Example](#Neural-Network-Example)\n",
    "- [Save and Restore](#Save-and-Restore)\n",
    "- [Limitations](#Limitations)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is Google's open source software library for doing efficient numerical computation through data flow graphs and is very well suited for deep learning applications. However, basic models like Linear Regression, Logistic Regression, Nearest Neighbours, etc. can also be implemented using TensorFlow.  TensorFlow is supported to run on both CPUs and GPU. This is so because often deep neural networks take a lot of training time, as we must train the model against massive datasets involving a lot of matrix multiplication to achieve a desired level of accuracy. This becomes a bottleneck in many scenarios and thus running them on GPUs is faster. \n",
    "\n",
    "[<img src=\"tf_image.png\">](http://quintagroup.com/cms/python/images/tensorflow-logo.png/image_view_fullscreen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now based on the architecture of the computer, one can either download the GPU or CPU version. For this tutorial, we will stick to the installation of a CPU version on a windows machine. TensorFlow supports Python 3.5.x and 3.6.x versions. If you’re machine doesn’t have one of them, please install it now. Now to install TensorFlow, start a terminal and type the following command:\n",
    "\n",
    "    $ pip3 install --upgrade tensorflow\n",
    "    \n",
    "If you’re planning to install via Anaconda, you must first create a virtual environment separate from where your Python Libraries are installed. This approach is unofficial but supported by the community. \n",
    "\n",
    "    $ conda create -n tensorflow pip python=3.5 \n",
    "    \n",
    "Next, we need to activate our virtual environment and install TensorFlow in that environment as follows:\n",
    "\n",
    "    $ activate tensorflow\n",
    "    \n",
    "    $ pip install --ignore-installed --upgrade tensorflow\n",
    "\n",
    "Now, to check if it has been installed successfully or not, run the below code snippet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'You are ready to use TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "ready = tf.constant('You are ready to use TensorFlow!')\n",
    "init_sess = tf.Session()\n",
    "print(init_sess.run(ready))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive deep into understanding how to use TensorFlow, let us first understand the TensorFlow environment and what built-in functionalities it provides us with. TensorFlow provides a lot of programming APIs ranging from high-level APIs like Estimators, Datasets to low level-level APIs like Python, C++, Java, Go. The high-level APIs are present to simplify and automate the processes. Estimators are a representative of the complete model and are aimed at encapsulating training, evaluation and prediction aspects of the model. Let us talk more about estimators.\n",
    "\n",
    "<img src=\"tf_api.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators as mentioned above are high-level APIs that makes our work easy. TensorFlow comes handy with various built-in estimators which are termed as ‘pre-made estimators’. They all belong to the tf.estimator.Estimator class in TensorFlow. A few of the examples of pre-made estimator include DNNClassifier, DNNRegressor, LinearClassifier, etc. Apart from using pre-made estimators, you can custom create your estimators by instantiating the tf.estimator.Estimator class. However, we will not discuss much about creating your own estimator, as this out of the scope for this tutorial.  These pre-made estimators do everything for us and we do not have to bother about the creating the computational graphs and sessions for training our model. The workflow of any TensorFlow program will be discussed in subsequent section. \n",
    "\n",
    "It is good that we have readily available Estimators but knowing the Core features of the TensorFlow library is important in developing and understanding the art of building complex machine learning applications. As mentioned earlier, a TensorFlow program at the backend builds a computational graph to run the program. This graph contains series of operations to be performed on the graphical nodes. The main idea of implementing a low-level API is build the computational graph right from scratch. Now, each node of the graph can comprise of one or many tensors. Let us understand what tensor are in TensorFlow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything in TensorFlow is processed as Tensors. They are the basic building blocks of the computation graphs. These tensors store data in any number of dimensions and are analogous to n-dimensional NumPy arrays.  Tensors are basically high dimensional vectors or matrices, which forms the basis for any machine learning algorithm. A tensor object has two properties: a data type which can be int32, foat32, etc. and shape of that tensor. Tensors are of various types and let us see the three basic ones which will help us in building our models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants are like final fields in java. They are immutable, which once declared cannot be changed. They can be nodes without any inputs and outputting a value that they store internally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Constant 1: Tensor(\"const1:0\", shape=(), dtype=float32) \n",
      "\n",
      "My Constant 2: Tensor(\"const2:0\", shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Examples to define a constant:\n",
    "constant1 = tf.constant(2.0, name = 'const1')\n",
    "constant2 = tf.constant([1., 2., 3., 4.], name='const2')\n",
    "\n",
    "print(\"My Constant 1:\", constant1, '\\n')\n",
    "print(\"My Constant 2:\", constant2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders are the type of tensors that will store data from external sources. Let’ say that we don not know before hand what data we are expecting to store in the variables, but at a later stage we want the tensors to take care of it. It just like a promise we make that we will provide you with data, but do not know at this stage. Thus, we just define the structure or rather a place to hold the data and the type of data since we do not provide any initialization for the placeholder’s declaration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder 1: Tensor(\"phold_one:0\", shape=(?, 1), dtype=float32) \n",
      "\n",
      "Placeholder 2: Tensor(\"phold_two:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Examples to define placeholders:\n",
    "placeholder1 = tf.placeholder(tf.float32, [None, 1], name='phold_one')\n",
    "placeholder2 = tf.placeholder(\"float\", name = 'phold_two')\n",
    "\n",
    "print(\"Placeholder 1:\", placeholder1, '\\n')\n",
    "# the shape has a question mark since we do want to be cagey about how much data can be stored in our tensor.\n",
    "# all we know is that, it is going to be a 1-dimensional vector or rather tensor\n",
    "print(\"Placeholder 2:\", placeholder2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables are the only mutable tensors whose values can be changed during the duration of a graph in TensorFlow. In machine learning applications, in particular neural networks, we generally want to initialise variables for weights, output of hidden layers and biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable 1:  <tf.Variable 'my_variable_one:0' shape=(1, 2, 3) dtype=int32_ref> \n",
      "\n",
      "Variable 2: <tf.Variable 'my_variable_two:0' shape=() dtype=float32_ref> \n",
      "\n",
      "Variable 3: <tf.Variable 'my_variable_three:0' shape=(2,) dtype=int32_ref>\n"
     ]
    }
   ],
   "source": [
    "#Examples to define variables:\n",
    "\n",
    "# different ways of creating and initializing variables\n",
    "\n",
    "variable1 = tf.get_variable(\"my_variable_one\", [1, 2, 3], dtype=tf.int32, initializer=tf.zeros_initializer)\n",
    "variable2 = tf.Variable(2.0, name = \"my_variable_two\")\n",
    "variable3 = tf.get_variable(\"my_variable_three\", dtype=tf.int32, initializer=tf.constant([10, 20]))\n",
    "\n",
    "print(\"Variable 1: \",variable1, '\\n')\n",
    "print(\"Variable 2:\", variable2, '\\n')\n",
    "print(\"Variable 3:\", variable3)\n",
    "\n",
    "# the first creates a variable named 'my_variable_one'  as athree-dimensional tesnor having shape [1,2,3]\n",
    "# the second statement is similar to constant and placeholder initialization\n",
    "# in the third statement, we do not specify the shape, as the shape of the initializer will be used \n",
    "# and we do not have to explicitly specify it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create collections of variables, as many separate sessions or part of the TensorFlow code will create variables differently and we would want to have a common way of accessing it. By default, when we create variables, they are placed in these two collections: tf.GraphKeys.GLOBAL_VARIABLES and tf.GraphKeys.TRAINABLE_VARIABLES. But if you do not the variables to be trainable, we can set the 'trainable' parameter as 'false'. See the commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local variable:  <tf.Variable 'my_local_variable_one:0' shape=() dtype=float32_ref> \n",
      "\n",
      "Alternatively\n",
      "<tf.Variable 'my_local_variable_two:0' shape=(1, 2, 3, 4) dtype=float32_ref> \n",
      "\n",
      "This is my collection: [<tf.Variable 'my_local_variable_one:0' shape=() dtype=float32_ref>, <tf.Variable 'my_local_variable_two:0' shape=(1, 2, 3, 4) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "#collections\n",
    "\n",
    "local1 = tf.get_variable(\"my_local_variable_one\", shape=(), collections =[tf.GraphKeys.LOCAL_VARIABLES])\n",
    "print(\"Local variable: \", local1, '\\n')\n",
    "\n",
    "#alternatively\n",
    "local2 = tf.get_variable(\"my_local_variable_two\", [1,2,3,4], trainable = False)\n",
    "print(\"Alternatively\")\n",
    "print(local2, '\\n')\n",
    "\n",
    "#You can add these variables to previously created collections and retrieve them using the following commands:\n",
    "tf.add_to_collection(\"new_collection\", local1)\n",
    "tf.add_to_collection(\"new_collection\", local2)\n",
    "\n",
    "#retrieve\n",
    "my_collections = tf.get_collection(\"new_collection\")\n",
    "print(\"This is my collection:\" , my_collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When working with low-level APIs you need to explicitly initialize them before you can use them. \n",
    "# They can be done as follows: \n",
    "\n",
    "#initialize variables\n",
    "\n",
    "init1 = tf.global_variables_initializer()\n",
    "\n",
    "#alternative, we can also write:\n",
    "# init = tf.initialize_all_variables()\n",
    "# However, writing it this way is not recommended\n",
    "\n",
    "#initialize individual variables use 'variable_name.initializer'\n",
    "\n",
    "get_uninitialized = tf.report_uninitialized_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this tutorial we’ve talked about how TensorFlow is all about dataflow graph. We define our graph and create a session which runs parts of the TensorFlow graph on different machines in a distributed fashion. Dataflow graphs are a common paradigm for parallel computing. We represent the nodes as the computational units and the edges as the representing the flow of data. For example, an add operation would correspond to a node with two incoming edges representing the two elements to be added and one outgoing edge representing the output.\n",
    "\n",
    "<img src=\"tensors_flowing-3.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training a model in TensorFlow, the graph should run in a session. Each session has its physical resources on the program in a session is supposed to run. We first create and initialize the tf.Session object using with operation and then run the run for training the model. Let us create some simple operations to be run in a session to understand sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable output is 7.0\n"
     ]
    }
   ],
   "source": [
    "#create variables\n",
    "x = tf.Variable(2.0, name='x')\n",
    "y = tf.Variable(3.0, name='y')\n",
    "\n",
    "#create operations\n",
    "x_square = tf.square(x, name='x_square')\n",
    "y_square = tf.square(y, name='y_square')\n",
    "mult = tf.multiply(x,y, name='multiply')\n",
    "add = tf.add(x_square, y_square, name='add')\n",
    "subt = tf.subtract(add, mult, name='subt')\n",
    "\n",
    "#setup initialization of variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    # compute the output of the graph\n",
    "    out = sess.run(subt)\n",
    "    # Note:\n",
    "    # thr run method also takes feeds as input \n",
    "    # feeds is a dictionary which substitutes tensor objects with values during the execution of the program\n",
    "    print(\"Variable output is {}\".format(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we’ve discussed each of the terminologies in TensorFlow individually. Let us now put everything together and understand the complete flow of the program. The following is the workflow of a TensorFlow program:\n",
    "1.\tWe first build the computational graph defining the operations\n",
    "2.\tWe create, initialize and compile all the variables defined\n",
    "3.\tWe create a session wherein our computational graph can be run\n",
    "4.\tModel training happens in the session during the execution of the compiled graph\n",
    "5.\tOnce we’re done with training, we close the session and free all the allocated resources\n",
    "Let us see the entire flow putting all the pieces together and build a simple linear regression model in TensorFlow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0100 cost= 0.129281968 W= 0.24567033 b= 0.6915515\n",
      "Epoch: 0200 cost= 0.128382444 W= 0.24023163 b= 0.732614\n",
      "Epoch: 0300 cost= 0.128214493 W= 0.23905449 b= 0.7415014\n",
      "Epoch: 0400 cost= 0.128179386 W= 0.23879972 b= 0.743425\n",
      "Epoch: 0500 cost= 0.128171891 W= 0.23874472 b= 0.74384016\n",
      "Epoch: 0600 cost= 0.128170177 W= 0.23873268 b= 0.7439313\n",
      "Epoch: 0700 cost= 0.128169864 W= 0.23873024 b= 0.74394965\n",
      "Epoch: 0800 cost= 0.128169701 W= 0.238729 b= 0.74395907\n",
      "Epoch: 0900 cost= 0.128169701 W= 0.238729 b= 0.74395907\n",
      "Epoch: 1000 cost= 0.128169701 W= 0.238729 b= 0.74395907\n",
      "Optimization Finished!\n",
      "Training cost= 0.1281697 W= 0.238729 b= 0.74395907 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFxEJoP8PMZLfd1oYACUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x202a2e44128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.125\n",
    "training_epochs = 1000\n",
    "display_step = 100\n",
    "\n",
    "# Training Data\n",
    "X_train = np.asarray([3.15,4.5,5.25,6.91,6.12,4.35,9.375,6.295,7.68,2.527,\n",
    "                         7.128,10.821,5.287,7.863,5.712,9.356,3.412])\n",
    "Y_train = np.asarray([1.75,2.89,2.25,3.765,1.789,1.685,3.698,2.363,2.425,1.289,\n",
    "                         2.225,3.296,1.685,2.92,2.496,2.9678,1.291])\n",
    "sample_size = X_train.shape[0]\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(np.random.randn(), name=\"weight\")\n",
    "b = tf.Variable(np.random.randn(), name=\"bias\")\n",
    "\n",
    "\n",
    "# Construct a linear model\n",
    "model_prediction = tf.add(tf.multiply(X, W), b)\n",
    "\n",
    "# Mean squared error\n",
    "cost = tf.reduce_sum(tf.square(model_prediction-Y))/(2*sample_size)\n",
    "# Gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (x, y) in zip(X_train, Y_train):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={X: X_train, Y:Y_train})\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "\n",
    "    print (\"Optimization Finished!\")\n",
    "    training_cost = sess.run(cost, feed_dict={X: X_train, Y: Y_train})\n",
    "    print (\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
    "    #Graphic display\n",
    "    plt.plot(X_train, Y_train,'ro', label='Original data')\n",
    "    plt.plot(X_train, sess.run(W) * X_train + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve seen how the line fits our pre-generated random data. But, however, we’ve encountered a few terms which we haven’t talked about like cost, optimizer, etc. Let us discuss these terminologies. (Please feel free to skip the below section if you’re familiar with the terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses and Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function is the incorrectness of the model or how off our prediction is from its true value. Various loss functions have been defined depending upon the use case in hand. In TensorFlow, we have classes implementing these loss functions and by default they are added to the GraphKeys.LOSSES collection. One such example of loss is cross entropy loss. \n",
    "\n",
    "Mathematically:\n",
    "<img src = \"cr.PNG\" >\n",
    "\n",
    "Where: m is the number of training examples\n",
    "\n",
    "\tN is the number of class labels\n",
    "    \n",
    "\tY is the output for the jth node of the ith trainng example\n",
    "    \n",
    "\tY_ is the predicted output of the jth node for the ith training example.\n",
    "\n",
    "The underlying implementation of this in TensorFlow is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note:\n",
    "# Define your y_pred and y before using this command\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_pred)\n",
    "                         + (1 - y) * tf.log(1 - y_pred), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify our optimizer, which helps us to reduce this loss. Depending upon the use case we choose our optimizer and a learning rate at which we want the model to learn and correct its mistakes. The Linear Model example used gradient descent, which uses backpropagation to converge to an optimal solution. TensorFlow has a library containing major training optimizers used in neural networks, which can be found [here](https://www.tensorflow.org/api_guides/python/train) and for losses found [here](https://www.tensorflow.org/api_docs/python/tf/losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that neural nets have a wide range of applications ranging from speech recognition, image classification, text generation, etc. TensorFlow library is specialized in developing these neural nets efficiently and with ease. We have a lot of readily available classes and built-in functions for training out model. Neural nets represent a function which we want to learn as a network of computational units which might contain millions of trained parameters. The steps involved in training a neural net are:\n",
    "1.\tFirstly, we choose a loss function which we want to optimize. This can be expressed as sum of squared errors, as maximum conditional likelihood, as maximum a posterior estimate, etc.\n",
    "2.\tSecondly, we design our network architecture which comprises of network units like ReLU, sigmoid, convolutions, tanh, etc. Then we determine the width of our layers and whether they are partially or fully connected\n",
    "3.\tLastly, we decide our training algorithm deriving the gradient formula and choosing an appropriate gradient descent method and also decide upon the stopping condition. \n",
    "We keep doing this iteratively, experimenting with the architecture and fine tuning the hyperparameters until we get the best results. Now let us replicate this in TensorFlow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# we plan to use the standard MNIST dataset for our demonstration purposes\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "\n",
    "# deciding on the number of hidden units\n",
    "total_nodes_hl1 = 500\n",
    "total_nodes_hl2 = 500\n",
    "\n",
    "# number of pre-determined classes\n",
    "total_classes = 10\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# defining placeholders\n",
    "x = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# designing the neural net architecture\n",
    "def neural_network_model(data):\n",
    "    hidden_layer_1 = {'weights':tf.Variable(tf.random_normal([784, total_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([total_nodes_hl1]))}\n",
    "\n",
    "    hidden_layer_2 = {'weights':tf.Variable(tf.random_normal([total_nodes_hl1, total_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([total_nodes_hl2]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([total_nodes_hl2, total_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([total_classes])),}\n",
    "\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_layer_1['weights']), hidden_layer_1['biases'])\n",
    "    layer_1 = tf.nn.sigmoid(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(layer_1,hidden_layer_2['weights']), hidden_layer_2['biases'])\n",
    "    layer_2 = tf.nn.relu(l2)\n",
    "\n",
    "    output = tf.matmul(layer_2,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_nn = neural_network_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training the neural network\n",
    "def train_neural_network(x, prediction, hm_epochs = 1):\n",
    "\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Restore model weights from previously saved model\n",
    "        # UNCOMMENT BELOW LINE IF YOU WANT TO LOAD A SAVED MODEL\n",
    "        load_path = saver.restore(sess, model_path)\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            avg_cost = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                # UNCOMMENT THIS FOR TRAINING RNN\n",
    "#                 epoch_x = epoch_x.reshape((batch_size,n_units,unit_size))\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                avg_cost += c\n",
    "\n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',avg_cost)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "        \n",
    "        #UNCOMMENT BELOW IF YOU WANT TO SAVE THE MODEL\n",
    "        \n",
    "        # Save model weights to disk\n",
    "        save_path = saver.save(sess, model_path)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "# UNCOMMENT BELOW LINE WHILE RUNNING RNN\n",
    "#         print('Accuracy:',accuracy.eval({x:mnist.test.images.reshape((-1, n_units, unit_size)), y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed out of 3 loss: 15320.214398622513\n",
      "Epoch 2 completed out of 3 loss: 3979.937641978264\n",
      "Epoch 3 completed out of 3 loss: 2370.055524110794\n",
      "Accuracy: 0.9127\n"
     ]
    }
   ],
   "source": [
    "# start training the model and get the accuracy of the model\n",
    "train_neural_network(x, prediction_nn,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Similarly we can design sequencial neural nets as follows:\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib import rnn\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "\n",
    "hm_epochs = 1\n",
    "total_classes = 10\n",
    "batch_size = 128\n",
    "unit_size = 28\n",
    "n_units = 28\n",
    "rnn_size = 128\n",
    "\n",
    "\n",
    "x = tf.placeholder('float', [None, n_units,unit_size])\n",
    "y = tf.placeholder('float',[None, total_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining your RNN\n",
    "def recurrent_neural_network(x):\n",
    "    layer = {'weights':tf.Variable(tf.random_normal([rnn_size,total_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal([total_classes]))}\n",
    "\n",
    "    x = tf.transpose(x, [1,0,2])\n",
    "    x = tf.reshape(x, [-1, unit_size])\n",
    "    x = tf.split(x, n_units, 0)\n",
    "\n",
    "    lstm_cell = rnn.BasicLSTMCell(rnn_size,state_is_tuple=True)\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    output = tf.matmul(outputs[-1],layer['weights']) + layer['biases']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed out of 7 loss: 194.0784340724349\n",
      "Epoch 2 completed out of 7 loss: 56.50654832087457\n",
      "Epoch 3 completed out of 7 loss: 39.62241821363568\n",
      "Epoch 4 completed out of 7 loss: 29.30630011856556\n",
      "Epoch 5 completed out of 7 loss: 24.340168490074575\n",
      "Epoch 6 completed out of 7 loss: 20.931029825937003\n",
      "Epoch 7 completed out of 7 loss: 18.009069211315364\n",
      "Accuracy: 0.9813\n"
     ]
    }
   ],
   "source": [
    "# Model output\n",
    "prediction_rnn = recurrent_neural_network(x)\n",
    "\n",
    "# start training your RNN\n",
    "train_neural_network(x, prediction_rnn,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all know that training the model takes up a lot of resources and time. Now, we would want to overcome this issue, by saving our model and being able to restore it in the future for further training. Thus, in TensorFlow, the tf.train.Saver class provides methods to save and restore models. It adds save and restore operations to the graph for all or a list of parameters that have been passed. Let’s see how this can be done in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the path where you want to store the model\n",
    "model_path = \"/tmp/model.ckpt\"\n",
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the 1st run of training the model...\n",
      "Epoch 1 completed out of 1 loss: 13694.817237138748\n",
      "Accuracy: 0.8716\n",
      "Model saved in file: /tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting the 1st run of training the model...\")\n",
    "train_neural_network(x, prediction_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the 2nd run of training the model...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Epoch 1 completed out of 10 loss: 16679.21664237976\n",
      "Epoch 2 completed out of 10 loss: 4177.576410770416\n",
      "Epoch 3 completed out of 10 loss: 2494.067486070795\n",
      "Epoch 4 completed out of 10 loss: 1605.5410353541374\n",
      "Epoch 5 completed out of 10 loss: 1073.2675522095396\n",
      "Epoch 6 completed out of 10 loss: 737.434275756443\n",
      "Epoch 7 completed out of 10 loss: 487.58215398768147\n",
      "Epoch 8 completed out of 10 loss: 345.54974446637124\n",
      "Epoch 9 completed out of 10 loss: 250.09764612251487\n",
      "Epoch 10 completed out of 10 loss: 162.28058818298797\n",
      "Accuracy: 0.9357\n",
      "Model saved in file: /tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Running a new session\n",
    "print(\"Starting the 2nd run of training the model...\")\n",
    "\n",
    "# print(\"Model restored from file: %s\" % save_path)\n",
    "train_neural_network(x, prediction_nn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, we seen what all we can do using TensorFlow, but however, there are a few caveats to it as mentioned below:\n",
    "1.\tIt is still in its nascent stages and significant development are being made to the library.\n",
    "2.\tIt is very much dependent upon the underlying hardware used to train the model.\n",
    "3.\tIt is best suitable for deep learning applications and can complicate things for simple applications or models wherein scikit-learn might suffice our purpose\n",
    "4.\tIt only has APIs for a few languages excluding R, Ruby, etc.\n",
    "5.\tIt is at a much lower level compared to Keras which is more abstract and built on top of TensorFlow, which makes model training more intuitive. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have seen in this tutorial, how TensorFlow is a machine learning library specialized for deep learning application. It builds these models through computational graphs and we can design our own neural network architecture for solving any real-life problem. It allows distributed processing while training the model which is not provided by other machine learning libraries out there. TensorFlow has been around for approximately 4 years along with other libraries like Theano, Torch, etc. and has its own community being built. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "This tutorial was just a high-level introduction of how we can use TensorFlow. It was kept simple keeping in mind the readers are completely unfamiliar with TensorFlow. I have used the following links to convey my idead and the readers can also refer them to get much deeper understanding.\n",
    "Links:\n",
    "1. TensorFlow Official Documentation: https://www.tensorflow.org/\n",
    "2. Linear Model: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/linear_regression.py\n",
    "3. Introduction to TensorFlow: https://www.analyticsvidhya.com/blog/2016/10/an-introduction-to-implementing-neural-networks-using-tensorflow/\n",
    "4. Model building: https://pythonprogramming.net/rnn-tensorflow-python-machine-learning-tutorial/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
