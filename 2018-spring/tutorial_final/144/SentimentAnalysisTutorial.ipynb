{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the first step you take when you plan to go to a restaurant you have no idea about? You open Yelp! Yelp ratings and reviews help you understand how good a restaurant is. People are so pationate about food that they leave bing long paragrpahs of reviews these days, more so when they don't like the place. Go to yelp page of any restaurant and you can find 100's of reviews. But who has the time to go through all of those review ??? And the 5-point rating system can be very ambigious, a 4 for me can be a 2 for you! So what's the solution ? Woudn't it be great if Yelp added a simple smiley showing happy, neutral or sad face next to review. This will give a  clear picture of whether the review is positive or negative. Since Yelp, has not done it yet, lets learn how we can do it using the art of data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in any data scince task is to gather the data! In order to perform sentiment analysis of Yelp review, we will first need some labelled data, i.e. data which contains the reviews and the related sentiment for those reviews. This can be done in two ways - tkae it of the internet or scrape it from Yelp directly. We will do both! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training and testing our model, I will be using the a dataset containing 10,000 review present in the link - https://www.dropbox.com/s/wc6rzl1a2os721d/yelp.csv?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For running our model to understand the sentiments around a particualr restaurant, I will scrape the Yelp page of that restaurant. This is discused in detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from csv file\n",
    "df_yelp_raw_data = pd.read_csv('Yelp_Reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample view of the data collected is given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp_raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing / cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost always, the data colelcted in not ready-to-eat. We have to pre-process the data to amke it fit for our modelling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10,000 row dataset we collected contains a lot of intervation about each review. For the pupose of sentiment analysis, we are only interested in the review text and ratings. A sample view of the our relevant data is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  My wife took me here on my birthday for breakf...      5\n",
       "1  I have no idea why some people give bad review...      5\n",
       "2  love the gyro plate. Rice is so good and I als...      4\n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5\n",
       "4  General Manager Scott Petello is a good egg!!!...      5"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop extra columns\n",
    "df_yelp_review = df_yelp_raw_data[['text' , 'stars']]\n",
    "df_yelp_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating a model which can classify a review as positive or negative, we first need our training data to have the same classsification. This is called labelling the data. \n",
    "\n",
    "In this tutorial, I will perform this task in 2 different ways and will compare the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing Approach 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first approach, I will consider all reviews with rating 1 or 2 as a negative reivew and will label it with a '0' and all reviews with rating 3, 4 or 5 as a positive review and will label it with '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data labels considering rating = 1 or 2 as negative and rest positive.\n",
    "df_yelp_review['stars'] = df_yelp_review['stars'].apply(lambda x : 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our definition, let's check how many review are positive and negative in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6863\n",
       "0    3137\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp_review.stars.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values above indicate that 6863 reviews are considered as positive review and 3137 reviews are considered as negative review in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have our data labelled, the next step is to clean the text in our data. Oue text contians symbols, puntuations, new line characters, etc. These are not relevant for predicting the sentiment of a review, so I will remove them from the data.\n",
    "\n",
    "Since cleaning the text is a common activity and has to be done every time we colelct data, so I will make a function for this. The fucntion will use replace fucntion and regex to remove unwanted text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctutions, new line tag and covert to lower case\n",
    "def cleanText(df):\n",
    "    df['text'] = df['text'].str.replace('[^\\w\\s]','')\n",
    "    df['text'] = df['text'].str.replace('\\n',' ')\n",
    "    df['text'] = df['text'].apply(lambda x : x.lower())\n",
    "    return df\n",
    "    \n",
    "# df_yelp_review['text'] = df_yelp_review['text'].str.replace('[^\\w\\s]','')\n",
    "# df_yelp_review['text'] = df_yelp_review['text'].str.replace('\\n',' ')\n",
    "# df_yelp_review['text'] = df_yelp_review['text'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Call the fuction to clean text in dataset\n",
    "df_yelp_review = cleanText(df_yelp_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important part of clening a text data is lemmatization and stopword removal. For lemmatization, we will using NLTK's lemmatize function. \n",
    "Stopwords are common words like 'a', 'an', etc. NLTK library has a set of standard stopwords defined, we will use them for removing the stopwords from our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, lammatize and remove stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.stem.wordnet as wordnet\n",
    "\n",
    "stopwrds = set(stopwords.words(\"english\"))\n",
    "\n",
    "def lemmatiseAndRemoveStopwords(text, lemmatizer=wordnet.WordNetLemmatizer()):\n",
    "    token = nltk.word_tokenize(text)\n",
    "    lametized_token = [lemmatizer.lemmatize(t) for t in token]\n",
    "    for word in lametized_token:\n",
    "        if word in stopwrds:\n",
    "            lametized_token.remove(word)\n",
    "    text = \" \".join(lametized_token)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since lemmatization and stopword removal has to be done for all texts, I defined a fucntion for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_yelp_review['text'] = df_yelp_review['text'].apply(lambda x : lemmatiseAndRemoveStopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test data creation\n",
    "\n",
    "Now that we have clean data ready, we will split it into two sets. One set containing 80% of the data will be used for training the model. Remaining 20% of the data will be used for testing the model. Since the focus of this tutorial is Sensitivity analysis, I am simply using random 80% of the sample for training and remaining 20% for testing. This approach has some drwabacks and a better way of model fitting is cross validation in which you split your data into k folds, fit your model on k-1 folds and test on the kth fold allowing each fold to act as test once. For more information on Cross validation, refer to the link - https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into 80-20. \n",
    "#We will use 80% of the data for training and 20% for testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df_yelp_review, test_size=0.2)\n",
    "\n",
    "#Reset the index\n",
    "train_data.reset_index()\n",
    "test_data.reset_index()\n",
    "\n",
    "#Convert to a list of strings\n",
    "train_data_list = train_data.text.tolist()\n",
    "test_data_list = test_data.text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering : Using bag of words technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most obvious and common problem in Sentiment Analysis is that the input data is plain text. We cannot just feed raw text in machine learning models. To work with learning models, we have to convert our text data into some sort of mathematical representaion. Bag of words is simple technique to do that. It creates a vocablury from all docuemtns and then counts the number of times each word appears in a document. To create feautures using the bag of words model, I will use the Vectorizer provided in Scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#  Create features from trainign data using Count Vectorizer \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None) \n",
    "\n",
    "train_features = vectorizer.fit_transform(train_data_list)\n",
    "train_features = train_features.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 29326)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check how many features are created\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the bag of words model created 28901 features from our training data. We can also check the vocabulory created using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check the vocablury of words created\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words only takens in the counts. To improve upon that, we can also create features using TFIDF which uses the inverse document frequency to contain the effect of highly repeated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TFIDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None) \n",
    "\n",
    "train_tfidf = tfidf.fit_transform(train_data_list)\n",
    "train_tfidf = train_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "Using the features created, we will now train a model. There are many machine learning models that can be used for classification. In this tutorial, we will use Random Forest and Multinomial Naive Bayes for model fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random forest is a decision tree based ensemble learning model generally used for classification. It works on the idea of decision tree but avoids overfitting by selecting split features from a random set. To learn more about Random Forest , you can have a look the link - https://en.wikipedia.org/wiki/Random_forest. We will use the Random Forest classifier provided by scikit- learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Random Forest classifier from scikit learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "rf = RandomForestClassifier(n_estimators = 100) \n",
    "# Fit the forest using bag of words features \n",
    "rf = rf.fit( train_features, train_data[\"stars\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test our model. To do that, we will first create bag of words feature for the test data and than predict the sentiment using our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create bag of words for the test data\n",
    "test_features = vectorizer.transform(test_data_list)\n",
    "test_features = test_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "rf_pred = rf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 259,  346],\n",
       "       [  44, 1351]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix for the prediction\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = test_data.stars.tolist()\n",
    "y_pred = rf_pred\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80500000000000005"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model fit using abg of words features gives an accuracy of around 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fit our forest model using the TFIDF feature created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using tfidf\n",
    "rf_tfidf = RandomForestClassifier(n_estimators = 100)\n",
    "rf_tfidf = rf_tfidf.fit(train_tfidf, train_data[\"stars\"])\n",
    "\n",
    "test_tfidf = tfidf.transform(test_data_list)\n",
    "test_tfidf = test_tfidf.toarray()\n",
    "\n",
    "result_tfidf = rf_tfidf.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79200000000000004"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix with TFIDF features\n",
    "confusion_matrix(y_true, result_tfidf)\n",
    "accuracy_score(y_true, result_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random forest model fit using TFIDF feature also gives an accuracy of around 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "The multinomial naive bayes model is a version of naive bayes that performs better on text documents. We will use our bag of words features to fit and predict using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_features, train_data[\"stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nb.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83150000000000002"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = test_data.stars.tolist()\n",
    "y_pred = preds\n",
    "confusion_matrix(y_true, y_pred)\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial naive bayes is giving an accuracy of aroud 83%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing Approach 2\n",
    "In the previous approach, we were considering all reviews with rating 1 and 2 as negative and rest as positive. Using this approach, the models we created gave accuracy in the lower 80's. Now we take a more conservative appraoch and will consider only extreme reviews.\n",
    "\n",
    "In this appraoch, we will consider only review with 1 rating as negaitve and reviews with 5 rating as positive. We will ignore all other reviews while training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3337\n",
       "0     749\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_data = pd.read_csv('Yelp_Reviews.csv')\n",
    "data_extreme = df_raw_data[(df_raw_data['stars'] == 1) | (df_raw_data['stars'] == 5)]\n",
    "data_extreme['stars'] = data_extreme['stars'].apply(lambda x : 1 if x == 5 else 0)\n",
    "data_extreme.stars.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this approch, we have 3337 review as positive and 749 as negative. Now we will clean the data as before, split the data into test and train and create bag of words features for modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning \n",
    "data_extreme = cleanText(data_extreme)\n",
    "data_extreme['text'] = data_extreme['text'].apply(lambda x : lemmatiseAndRemoveStopwords(x))\n",
    "\n",
    "#Split the data into 80-20. \n",
    "#We will use 80% of the data for training and 30% for testing\n",
    "train_data_ext, test_data_ext = train_test_split(data_extreme, test_size=0.2)\n",
    "\n",
    "#Reset the index\n",
    "train_data_ext.reset_index()\n",
    "test_data_ext.reset_index()\n",
    "\n",
    "# Convert to list\n",
    "train_ext_list = train_data_ext.text.tolist()\n",
    "test_ext_list = test_data_ext.text.tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create bag of word vector\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None) \n",
    "\n",
    "train_features_ext = vectorizer.fit_transform(train_ext_list)\n",
    "#Convert to numpy array\n",
    "train_features_ext = train_features_ext.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest for 2nd approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random forest classifier\n",
    "rf_ext = RandomForestClassifier(n_estimators = 100) \n",
    "rf_ext = rf_ext.fit( train_features_ext, train_data_ext[\"stars\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words vector for test data\n",
    "test_features_ext = vectorizer.transform(test_ext_list)\n",
    "test_features_ext = test_features_ext.toarray()\n",
    "\n",
    "# Predict using above model\n",
    "result_ext = rf_ext.predict(test_features_ext)\n",
    "y_true = test_data_ext.stars.tolist()\n",
    "y_pred = result_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 48, 117],\n",
       "       [  2, 651]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix for random forest using 2nd approach\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8545232273838631"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy for random forest fit using 2nd approach\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model fit using this approach gives an accuracy of around 85%. This shows the importance of data preprocessing and feature creation in data science. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes for 2nd approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the multinomial naive bayes model\n",
    "nb_ext = MultinomialNB()\n",
    "nb_ext = nb_ext.fit(train_features_ext, train_data_ext[\"stars\"])\n",
    "\n",
    "#Predict of test data using this model\n",
    "preds_ext = nb_ext.predict(test_features_ext)\n",
    "y_true = test_data_ext.stars.tolist()\n",
    "y_pred = preds_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[108,  57],\n",
       "       [ 12, 641]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix for naive bayes using 2nd approach\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91564792176039123"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy for naive bayes using 2nd approach\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial naive bayes model fit using 2nd approach gives an accuracy of around 92%. This is a significant improvement but this is only based on training and test data. The real performance of any model is judged by how is works on completely unseen data. This is what we will check in the next half of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving public sentiment for a restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratings of a restaurant is good overall representaion of public opinion. But, rating can be done only on 1-5 scale. People write a lot more in reviews and can be a better judge on public sentiment. We will now check that using our model. \n",
    "\n",
    "To check public opinion for a restaurant using sentoment analysis, we first need the reviews for that restaurant. As mentioned initially in this tutorial, scrapping is a technique using which you can collect information from Yelp page of any restaurant. Using the code below, we hit the Yelp page of cafe in San Francisco called The Temporarium Coffee And Tea, scrape all reviews and put them in dataframe. Then, we use our model to predict whether people are talking positively or negatively about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def retrieve_html(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    return(response.status_code, response.text)\n",
    "\n",
    "def parse_page(html):\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser') \n",
    "    review_list = []\n",
    "\n",
    "    for review in soup.find_all(\"div\", class_=\"review review--with-sidebar\"):\n",
    "        rt = review.find(\"div\", class_=\"rating-large\")\n",
    "        rating = rt['title']\n",
    "        text = review.find(\"p\")\n",
    "        \n",
    "        review_dict = {'stars':float(rating[:3]),'text': text.text.strip()}\n",
    "        review_list  = review_list + [review_dict]\n",
    "    \n",
    "    url = None\n",
    "    current_page = soup.find(\"span\", class_=\"pagination-links_anchor\")\n",
    "    if current_page is not None:\n",
    "        current_page = int(current_page.text.strip())\n",
    "    for page in soup.find_all(\"a\", class_=\"available-number pagination-links_anchor\"):\n",
    "        if page is not None:            \n",
    "            page_num = int(page.text.strip())            \n",
    "            if page_num == current_page+1:\n",
    "                url = page['href']   \n",
    "    return (review_list, url)\n",
    "\n",
    "def extract_reviews(url):\n",
    "    \n",
    "    review_list = []\n",
    "    while url != None:\n",
    "        yelp_html = retrieve_html(url)\n",
    "        r_list , url = parse_page(yelp_html[1])\n",
    "        review_list += r_list\n",
    "    return review_list\n",
    "    pass\n",
    "\n",
    "data = extract_reviews('https://www.yelp.com/biz/the-temporarium-coffee-and-tea-san-francisco')\n",
    "df = pd.DataFrame()\n",
    "df = df.append(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all the reviews for this restaurant, we will follow the same cleaning and preprossing we did during our model fitting. We then create bag of words vector for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data cleaning \n",
    "df = cleanText(df)\n",
    "\n",
    "#Clean this data\n",
    "# df['text'] = df['text'].str.replace('[^\\w\\s]','')\n",
    "# df['text'] = df['text'].str.replace('\\n',' ')\n",
    "# df['text'] = df['text'].apply(lambda x : x.lower())\n",
    "df['text'] = df['text'].apply(lambda x : lemmatiseAndRemoveStopwords(x))\n",
    "#Convert to list\n",
    "validation_list = df.text.tolist()\n",
    "# Create bag of words features\n",
    "features = vectorizer.transform(validation_list)\n",
    "features = features.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing public sentiment using the multinomial naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.701298701298697"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_predict_nb = nb_ext.predict(features)\n",
    "validation_predict_nb.sum()\n",
    "percent_positive = (validation_predict_nb.sum() / len(validation_predict_nb))*100\n",
    "percent_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model suggests that almost 99% of the reviews for this restaurant are positive. This matches with the overall '5' rating of this restaurant on Yelp. But the interesthing thing to see is the model found some negative review for this famous restaurrant. Now, catching those reviews among 100's of review direclty on Yelp would be difficult. This is one way how sentiment analysis can be use. \n",
    "\n",
    "Let's check which review was predicted as negative by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I had an odd interaction with the guy working there. He was busy talking to a customer/friend and didn't realize I wanted something. Anyway, the ice coffee was okay but overpriced and probably unsanitary (guy pulled it from a shelf and spilled half of it on the ground). I can't say that this was a comfortable coffee shop experience.\""
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[validation_predict_nb.tolist().index(0)]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we saw in this tutorial is a simple example of how and what can be done using sentiment analysis. Analyzing text is very interesting field and if you are interested in going deep into it, you can check deep learning models. Overall, sentiment analysis can be very useful in understanding overall sentiment of 1000's of reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://medium.com/tensorist/classifying-yelp-reviews-using-nltk-and-scikit-learn-c58e71e962d9\n",
    "2. https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
