{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The web crawler, which can also be called internet bot or web spider, is an application program that scan through the internet programmatically and fetch the data for further use. After almost thirty years of development, the web spider can be categorized into four major types, include general purpose web crawler, focused web crawler, incremental web crawler and deep web crawler. Besides other types of web crawler, we will focus on implementing the general purpose web crawler in this tutorial. More specifically, we will make use of the Scrapy library in python to implement a basic web crawler to grab some information about food ingredients through [BBC Food Channel](https://www.bbc.co.uk/food/ingredients), format the data as JSON and display them in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Content\n",
    "\n",
    "In this tutorial, we will focus on how to write a basic web crawler using [Scrapy](https://scrapy.org/) library. After we extract the data, we will simply use [panda](https://pandas.pydata.org/) libraries to display them.\n",
    "\n",
    "We will be covering the following topics in this tutorial. However, since using Scrapy library is mostly involved in using terminal, the results won't be generated by the notebook directly. In this case, the results will be screenshoted and inserted as pictures in this tutorial.\n",
    "\n",
    "- [Install the Scrapy library](#Install the Scrapy library)\n",
    "- [Create a starter project](#Create a starter project)\n",
    "- [Implement a basic web crawler](#Implement a basic crawler)\n",
    "- [Use Item to Hold Data](#Use Item to Hold Data)\n",
    "- [Set up Item Pipeline to process and store the data](#Set Up Item Pipeline)\n",
    "- [Display Data with Pandas](#Load Data Into Pandas)\n",
    "\n",
    "After you followed these topics , you should gain a basic understanding of using web crawler with Scrapy Library. Moreover, you can find more functionalities of scrapy in [the official document](https://docs.scrapy.org/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the Scrapy library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the scrapy library is fast and easy. Based on the official document of Scrapy, you can install the Scrapy in the following two ways:\n",
    "First, use the python package manager of python3, `pip`:\n",
    "\n",
    "     $ pip3 install scrapy\n",
    "\n",
    "Second, if you haven't install the [Anaconda](https://www.anaconda.com/download/#macos), you can click the link and follow the installation guides on the webpage. If you have already installed it, you can use anaconda to install scrapy:\n",
    "\n",
    "     $ conda install -c conda-forge scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a starter project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using command line tools provided by Scrapy Framework, it is really easy to create a web crawler project and start implementing our own crawler. First, navigate to the directory where you want to store the project and then type in the following command into the terminal(Since we are using notebook, you can also use the terminal inside the notebook very easily, simply go to the directory page of the notebook, click the New button on the upper right corner of page and select terminal, which will start a new terminal in the notebook):\n",
    "\n",
    "    $ scrapy startproject tutorial\n",
    "\n",
    "This command will create a folder named tutorial, which contains the following directory of the project(the comments on the left side denote the main functionality of that file):\n",
    "\n",
    "tutorial/\n",
    "    scrapy.cfg  # the configuration file for deployment\n",
    "    \n",
    "    tutorial/\n",
    "        __init__.py\n",
    "        \n",
    "        items.py  # define your items in this file and set up item pipeline\n",
    "        \n",
    "        middlewares.py  # project middleware file\n",
    "        \n",
    "        pipelines.py  # project pipelines file\n",
    "        \n",
    "        settings.py  # project settings file\n",
    "        \n",
    "        spiders/  # the directory where the user places customized spider\n",
    "            __init__.py \n",
    "            \n",
    "In this tutorial, we will implement our crawler and put it into the spiders directory. Then, we will need to modify our crawler and items.py, pipelines.py and settings.py. <b>Therefore, the codes will be shown in the cell but running them in the notebook won't generate any output directly. Instead, we suggest you use terminal to run the command and examine the output. Additionaly, we will also include sample output as pictures in this tutorial.</b> So, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Basic Web Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement the web crawler, we should have a clear view of which data we need to extract and how should we format them. Since we are extracting information about food ingredient from the index pages and each detail page. From the index page, assume that we want to extract the following three data field:\n",
    "\n",
    "1. name of this food ingredient\n",
    "2. url of the page containing the detail of this food ingredient\n",
    "3. url of this food ingredient image\n",
    "\n",
    "From each detail page, we will just extract the following two data field just to keep things simple:\n",
    "\n",
    "1. name of this food ingredient\n",
    "2. description of this food ingredient\n",
    "\n",
    "In this part, we will implement a basic crawler in this tutorial project. To create a web crawler in the Scrapy framework, you have to subclass [scrapy.Spider](https://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider), define the initial requests to send and hwo to parse the response. Typically, there are three things you need to keep eyes on when implementing this class:\n",
    "\n",
    "1. name: this is an attribute inside the your spider class, it identifies your spider/crawler in this project. Thus, you need to keep in mind that you cannot assign the name to two different spiders in one project.\n",
    "2. start_request function: the initial requests are normally defined in this function. You should return an iterable of Request objects which the crawler will begin to crawl. Subsequent requests will be generated from these initial requests.\n",
    "3. parse function: this function is mainly used for handling the downloaded response for each of the requests. The response parameter is an instance of [TextResponse](https://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.TextResponse) object that contains the response content and some other helpful methods for processing the response.\n",
    "\n",
    "The following code is a basic food ingredient spider that fetches the web content from BBC food channel about food ingredients which all ingreidents' names starting with 'a'. <b>Note that we are currently just outputing the fecthed data into the terminal, self.log() function is the log function inside Scrapy Framework that will enable us to print debug information in the terminal. In the next few sections, we will actually modify the code step by step to achieve ultimate goal of spider.</b> As for the project structure, you should create a new python file called ingredient_spider.py and place it under the spiders directory.\n",
    "\n",
    "<b>Please don't run this cell because this is just a prototype version of the web crawler, instead you could run the same code using terminal!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import re\n",
    "\n",
    "class IngredientsSpider(scrapy.Spider):\n",
    "    # name: identifies the Spider. It must be unique within a project, \n",
    "    #       that is, you can’t set the same name for different Spiders.\n",
    "    name = \"ingredients\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        \"\"\"\n",
    "        start_requests(): must return an iterable of Requests (you can return a \n",
    "            list of requests or write a generator function) which the Spider will begin to crawl from. \n",
    "            Subsequent requests will be generated successively from these initial requests.\n",
    "        (you can just define a start_urls class attribute with a list of URLs. )\n",
    "        \"\"\"\n",
    "        urls = [\n",
    "            'http://www.bbc.co.uk/food/ingredients/by/letter/a',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse_ingredient)\n",
    "\n",
    "    def parse_ingredient(self, response):\n",
    "        \"\"\"\n",
    "        parse(): a method that will be called to handle the response downloaded for each of the requests made. \n",
    "            The response parameter is an instance of TextResponse that holds the page content and has further helpful methods to handle it.\n",
    "        \"\"\"\n",
    "        # we can use regular expression to extract the attribute value\n",
    "        alt_attr = re.compile('alt=\\\"([\\w\\W]+?)\\\"')\n",
    "        href_attr = re.compile('href=\\\"([\\w\\W]+?)\\\"')\n",
    "        src_attr = re.compile('src=\\\"([\\w\\W]+?)\\\"')\n",
    "        list_results = response.xpath('//ol[@class=\"resources foods grid-view\"]/li/a[not(@class)]').extract()\n",
    "        # error handling: if there is no such element in the response content, stop the spider\n",
    "        if (len(list_results) == 0):\n",
    "            return\n",
    "        for temp in list_results:\n",
    "            name = re.search(alt_attr, temp).group(1)\n",
    "            url = re.search(href_attr, temp).group(1)\n",
    "            src = re.search(src_attr, temp).group(1)\n",
    "            self.log('Ingredient Name: ' + name)\n",
    "            self.log('URL link to Details: ' + url)\n",
    "            self.log('Ingredient Image Link: ' + src)\n",
    "            # continue to crawl the web page that contains the details of this ingredient\n",
    "            next_url = response.urljoin(url)\n",
    "            # use yield to send the requests to the downloader\n",
    "            yield scrapy.Request(next_url, callback=self.parse_ingredient_detail)\n",
    "\n",
    "    def parse_ingredient_detail(self, response):\n",
    "        \"\"\"\n",
    "        this parse function is used to parse the response content from detail page of one food ingredient\n",
    "        \"\"\"\n",
    "        # get the description of the ingredient\n",
    "        name = response.xpath('//div[@id=\"summary\"]/h1[@class]/text()').extract()\n",
    "        # error handling: set the variable as empty string if no such element is found\n",
    "        if (len(name) != 0):\n",
    "            name = name[0].rstrip()\n",
    "        else:\n",
    "            name = ''\n",
    "        desp = response.xpath('//div[@id=\"summary\"]/div/p/text()').extract()\n",
    "        # error handling: set the variable as empty string if no such element is found\n",
    "        if (len(desp) != 0):\n",
    "            desp = desp[0]\n",
    "        else:\n",
    "            desp = ''\n",
    "        self.log('Ingredient Name: ' + name)\n",
    "        self.log('Ingredient Description: ' + desp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few things that you need to pay attention regarding the bussiness logic behind the code shown above. They are listed as the following:\n",
    "\n",
    "1. the creation of scrapy.Request object: notice that there are two important parameters that are passed in during the construction of this object, a url and a call back function. The url parameter specifies the endpoint of this request while the call-back function is the function that will be called when response is downloaded. In most case, this function is mainly for parsing the response content and other initial request for subsequent scrapying.\n",
    "\n",
    "2. We locate and extract html element using xpath expression. [xpath](https://www.w3.org/TR/xpath/) is a language for addressing the part of an XML document. It is actually a very powerful tool for extracting data from XML-like file, such as HTML.\n",
    "   \n",
    "3. The three regular expressions used in the code are used to extract the value of the attribute, including 'alt', 'href', 'src'. \n",
    "\n",
    "After implementing the code shown above in the project and you can run the following command in the terminal to start up your spider:\n",
    "    \n",
    "    $ scrapy crawl ingredients\n",
    "    \n",
    "The output of the program is very long and we will only show the part where the debug information is displayed as we instruct it in the spider:\n",
    "\n",
    "![Figure 1](sample_output1.png)\n",
    "The picture shown above denotes that the ingredient name, link to detail page and link to image are all displayed in the terminal, which also means the data is scraped successfully from the index page.\n",
    "\n",
    "![Figure 2](sample_output2.png)\n",
    "The picture shown above contains similar information. The ingredient name and description are also shown in the terminal, which means that the data is scraped successfully from each ingredient detail page.\n",
    "\n",
    "Obviously, although the data is scraped successfully from out target website, simply displaying the data in the terminal is of no use to us. Thus, in the next section, we will use the Item provided by the Scarpy to wrap the data and then set up the Item Pipeline to write the data into JSON format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Item to Hold Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will learn to use the Item class provided by the Scrapy Framework to wrap the extracted data, which can be further used in Item Pipeline. In Scarpy Framework, Item classes are just containers for collecting the extracted data. They provide python dictionary-like API for developer to use. Thus, setting, getting and changing the value of an Item class is very easy and convenient. First, we should define an ingredient Item class inside the <b>items.py</b> in the project as the following: \n",
    "\n",
    "<b>You can run the following cell in notebook or copy the code into items.py and start the spider using terminal.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class IngredientItem(scrapy.Item):\n",
    "    # define this class for collecting data about ingredient name, url and image link.\n",
    "    name = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    img_src = scrapy.Field()\n",
    "    \n",
    "class IngredientDetailItem(scrapy.Item):\n",
    "    # define this class for collecting data about ingredient name and description about it.\n",
    "    name = scrapy.Field()\n",
    "    description = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we have defined two Item classes for holding two different forms of data that we want to extract from the websites. Also, inside each class, you can simply define the field of the class by using [scrapy.Field](https://docs.scrapy.org/en/latest/topics/items.html) object. This object can hold any data type and you can also pass a serializer function for each field. More often, we will just create the Item class using its constructor and directly pass the field value into this function, like the following way:\n",
    "\n",
    "    ingredient = IngredientItem(name=name, url=url, img_src=src)\n",
    "    ingredient_detail = IngredientDetailItem(name=name, description=desp)\n",
    "    \n",
    "The above code snippets will be included in our finalized version of spider, to replace the log function that we previously have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Item Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this section, you will learn to set up item pipeline to process the items that contained the data we extracted from our spider. Specifically, item pipelines are tyically used to clean or validate the extracted data, checking for duplicates or store the data in the database. For the purpose of demonstration, we will only show you how to set up item pipeline to write two types of items into two JSONLine files. The following code shows how should the pipeline be set up to achieve this functionality: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "    # the method will be called when the spider is opened\n",
    "    def open_spider(self, spider):\n",
    "        self.file_ingredient = open('ingredient.jl', 'w')\n",
    "        self.file_ingredient_detail = open('ingredient_detail.jl', 'w')\n",
    "\n",
    "    # this method will be called when the spider is closed\n",
    "    def close_spider(self, spider):\n",
    "        self.file_ingredient.close()\n",
    "        self.file_ingredient_detail.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # if it is an Ingredient, write it into the 'ingredient.jl' file\n",
    "        if (isinstance(item, IngredientItem)):\n",
    "            line = json.dumps(dict(item)) + '\\n'\n",
    "            self.file_ingredient.write(line)\n",
    "        # if it is an ingredient detail item, write it into the 'ingredient_detail.jl' file\n",
    "        elif (isinstance(item, IngredientDetailItem)):\n",
    "            line = json.dumps(dict(item)) + '\\n'\n",
    "            self.file_ingredient_detail.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few things that need to be explained for the code shown above:\n",
    "\n",
    "1. The three functions will be called in a specific order. The first function will be called is the <b>open_spider</b> when spider is opened, the second one is <b>process_item</b> and third one is <b>close_spider</b> when spider is closed.\n",
    "\n",
    "2. After knowing the order of function calls, it is easy to understand that the open and close file operations should be conducted after the spider is opened and closed. The logic of how data should be processed needs to be put into the process_item function.\n",
    "\n",
    "3. In the process_item function, since all the items will go through the same pipeline. We will use isinstance function to check whether the item is IngreidentItem or IngredientDetailItem and write them into different files as JSON object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up item and item pipeline, there is one thing to add in order to enable using pipeline for our spider. We have to add custom settings to specify we want to use pipeline for our spider. Note that this is not necessary if you are running you spider in the terminal, this is only needed for running Scrapy in the notebook. You can type in the following command in the terminal to start you spider:\n",
    "\n",
    "    $ scrapy crawl ingredients\n",
    "    \n",
    "<b>Or you can just run the following two cells to start our spiders.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "\n",
    "class IngredientsSpider(scrapy.Spider):\n",
    "    name = \"ingredients\"\n",
    "    # settings for start the pipeline and change the log level\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
    "    }\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://www.bbc.co.uk/food/ingredients/by/letter/a',\n",
    "        ]\n",
    "    \n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse_ingredient)\n",
    "\n",
    "    def parse_ingredient(self, response):\n",
    "        \"\"\"\n",
    "        parse(): a method that will be called to handle the response downloaded for each of the requests made. \n",
    "            The response parameter is an instance of TextResponse that holds the page content and has further helpful methods to handle it.\n",
    "        \"\"\"\n",
    "        # we can use regular expression to extract the attribute value\n",
    "        alt_attr = re.compile('alt=\\\"([\\w\\W]+?)\\\"')\n",
    "        href_attr = re.compile('href=\\\"([\\w\\W]+?)\\\"')\n",
    "        src_attr = re.compile('src=\\\"([\\w\\W]+?)\\\"')\n",
    "        list_results = response.xpath('//ol[@class=\"resources foods grid-view\"]/li/a[not(@class)]').extract()\n",
    "        # error handling: if there is no such element in the response content, stop the spider\n",
    "        if (len(list_results) == 0):\n",
    "            return\n",
    "        for temp in list_results:\n",
    "            name = re.search(alt_attr, temp).group(1)\n",
    "            url = re.search(href_attr, temp).group(1)\n",
    "            src = re.search(src_attr, temp).group(1)\n",
    "            # delete the log function and add Item loader\n",
    "            # self.log('Ingredient Name: ' + name)\n",
    "            # self.log('URL link to Details: ' + url)\n",
    "            # self.log('Ingredient Image Link: ' + src)\n",
    "            ingredient = IngredientItem(name=name, url=url, img_src=src)\n",
    "            yield ingredient\n",
    "            # continue to crawl the web page that contains the details of this ingredient\n",
    "            next_url = response.urljoin(url)\n",
    "            # use yield to send the requests to the downloader\n",
    "            yield scrapy.Request(next_url, callback=self.parse_ingredient_detail)\n",
    "\n",
    "    def parse_ingredient_detail(self, response):\n",
    "        \"\"\"\n",
    "        this parse function is used to parse the response content from detail page of one food ingredient\n",
    "        \"\"\"\n",
    "        # get the description of the ingredient\n",
    "        name = response.xpath('//div[@id=\"summary\"]/h1[@class]/text()').extract()\n",
    "        # error handling: set the variable as empty string if no such element is found\n",
    "        if (len(name) != 0):\n",
    "            name = name[0].rstrip()\n",
    "        else:\n",
    "            name = ''\n",
    "        desp = response.xpath('//div[@id=\"summary\"]/div/p/text()').extract()\n",
    "        # error handling: set the variable as empty string if no such element is found\n",
    "        if (len(desp) != 0):\n",
    "            desp = desp[0]\n",
    "        else:\n",
    "            desp = ''\n",
    "        ingredient_detail = IngredientDetailItem(name=name, description=desp)\n",
    "        yield ingredient_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-27 15:54:08 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\n",
      "2018-03-27 15:54:08 [scrapy.utils.log] INFO: Versions: lxml 4.1.0.0, libxml2 2.9.4, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.3 |Anaconda, Inc.| (default, Oct  6 2017, 12:04:38) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 17.2.0 (OpenSSL 1.0.2l  25 May 2017), cryptography 2.0.3, Platform Darwin-17.4.0-x86_64-i386-64bit\n",
      "2018-03-27 15:54:08 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 30, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(IngredientsSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Into Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this step in our tutorial is just to verify that those two files have been created successfully and extracted data has been written into those two files. If implemented correctly, you should see formated output if you run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_src</th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>acidulated water</td>\n",
       "      <td>/food/acidulated_water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>ackee</td>\n",
       "      <td>/food/ackee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>acorn squash</td>\n",
       "      <td>/food/acorn_squash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>aduki beans</td>\n",
       "      <td>/food/aduki_beans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>Advocaat</td>\n",
       "      <td>/food/egg_liqueur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>agar-agar</td>\n",
       "      <td>/food/agar-agar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>ale</td>\n",
       "      <td>/food/ale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>Aleppo pepper</td>\n",
       "      <td>/food/aleppo_pepper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>alfalfa sprouts</td>\n",
       "      <td>/food/alfalfa_sprouts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>allspice</td>\n",
       "      <td>/food/allspice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>almond</td>\n",
       "      <td>/food/almond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>almond essence</td>\n",
       "      <td>/food/almond_essence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>almond extract</td>\n",
       "      <td>/food/almond_extract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>Almond milk</td>\n",
       "      <td>/food/almond_milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>amaranth</td>\n",
       "      <td>/food/amaranth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>amaretti</td>\n",
       "      <td>/food/amaretti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>anchovies</td>\n",
       "      <td>/food/anchovy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>anchovy essence</td>\n",
       "      <td>/food/anchovy_essence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>angelica</td>\n",
       "      <td>/food/angelica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>angostura bitters</td>\n",
       "      <td>/food/bitters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>anise</td>\n",
       "      <td>/food/anise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>apple</td>\n",
       "      <td>/food/apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>apple chutney</td>\n",
       "      <td>/food/apple_chutney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>apple juice</td>\n",
       "      <td>/food/apple_juice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>apple sauce</td>\n",
       "      <td>/food/apple_sauce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>apricot</td>\n",
       "      <td>/food/apricot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>apricot jam</td>\n",
       "      <td>/food/apricot_jam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>arborio rice</td>\n",
       "      <td>/food/arborio_rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>arbroath smokie</td>\n",
       "      <td>/food/arbroath_smokie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>argan oil</td>\n",
       "      <td>/food/argan_oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>arrowroot</td>\n",
       "      <td>/food/arrowroot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>artichoke</td>\n",
       "      <td>/food/artichoke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>asafoetida</td>\n",
       "      <td>/food/asafoetida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>asparagus</td>\n",
       "      <td>/food/asparagus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>aubergine</td>\n",
       "      <td>/food/aubergine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://ichef.bbc.co.uk/food/ic/food_16x9_111/...</td>\n",
       "      <td>avocado</td>\n",
       "      <td>/food/avocado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              img_src               name  \\\n",
       "0   https://ichef.bbc.co.uk/food/ic/food_16x9_111/...   acidulated water   \n",
       "1   https://ichef.bbc.co.uk/food/ic/food_16x9_111/...              ackee   \n",
       "2   https://ichef.bbc.co.uk/food/ic/food_16x9_111/...       acorn squash   \n",
       "3   https://ichef.bbc.co.uk/food/ic/food_16x9_111/...        aduki beans   \n",
       "4   https://ichef.bbc.co.uk/food/ic/food_16x9_111/...           Advocaat   \n",
       "5   https://ichef.bbc.co.uk/food/ic/food_16x9_111/...          agar-agar   \n",
       "6   https://ichef.bbc.co.uk/food/ic/food_16x9_111/...                ale   \n",
       "7   https://ichef.bbc.co.uk/food/ic/food_16x9_111/...      Aleppo pepper   \n",
       "8   https://ichef.bbc.co.uk/food/ic/food_16x9_111/...    alfalfa sprouts   \n",
       "9   https://ichef.bbc.co.uk/food/ic/food_16x9_111/...           allspice   \n",
       "10  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...             almond   \n",
       "11  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...     almond essence   \n",
       "12  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...     almond extract   \n",
       "13  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...        Almond milk   \n",
       "14  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...           amaranth   \n",
       "15  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...           amaretti   \n",
       "16  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...          anchovies   \n",
       "17  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...    anchovy essence   \n",
       "18  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...           angelica   \n",
       "19  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...  angostura bitters   \n",
       "20  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...              anise   \n",
       "21  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...              apple   \n",
       "22  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...      apple chutney   \n",
       "23  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...        apple juice   \n",
       "24  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...        apple sauce   \n",
       "25  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...            apricot   \n",
       "26  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...        apricot jam   \n",
       "27  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...       arborio rice   \n",
       "28  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...    arbroath smokie   \n",
       "29  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...          argan oil   \n",
       "30  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...          arrowroot   \n",
       "31  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...          artichoke   \n",
       "32  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...         asafoetida   \n",
       "33  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...          asparagus   \n",
       "34  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...          aubergine   \n",
       "35  https://ichef.bbc.co.uk/food/ic/food_16x9_111/...            avocado   \n",
       "\n",
       "                       url  \n",
       "0   /food/acidulated_water  \n",
       "1              /food/ackee  \n",
       "2       /food/acorn_squash  \n",
       "3        /food/aduki_beans  \n",
       "4        /food/egg_liqueur  \n",
       "5          /food/agar-agar  \n",
       "6                /food/ale  \n",
       "7      /food/aleppo_pepper  \n",
       "8    /food/alfalfa_sprouts  \n",
       "9           /food/allspice  \n",
       "10            /food/almond  \n",
       "11    /food/almond_essence  \n",
       "12    /food/almond_extract  \n",
       "13       /food/almond_milk  \n",
       "14          /food/amaranth  \n",
       "15          /food/amaretti  \n",
       "16           /food/anchovy  \n",
       "17   /food/anchovy_essence  \n",
       "18          /food/angelica  \n",
       "19           /food/bitters  \n",
       "20             /food/anise  \n",
       "21             /food/apple  \n",
       "22     /food/apple_chutney  \n",
       "23       /food/apple_juice  \n",
       "24       /food/apple_sauce  \n",
       "25           /food/apricot  \n",
       "26       /food/apricot_jam  \n",
       "27      /food/arborio_rice  \n",
       "28   /food/arbroath_smokie  \n",
       "29         /food/argan_oil  \n",
       "30         /food/arrowroot  \n",
       "31         /food/artichoke  \n",
       "32        /food/asafoetida  \n",
       "33         /food/asparagus  \n",
       "34         /food/aubergine  \n",
       "35           /food/avocado  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Serve them as an after-dinner treat with sweet...</td>\n",
       "      <td>Amaretti recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Also known as Chinese spinach or callaloo in C...</td>\n",
       "      <td>Amaranth recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lactose and cholesterol-free, almond milk is a...</td>\n",
       "      <td>Almond milk recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Alfalfa sprouts recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>Almond essence recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Our almond recipes make the most of this versa...</td>\n",
       "      <td>Almond recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aleppo pepper, also known as pul biber, Halaby...</td>\n",
       "      <td>Aleppo pepper recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Almond extract is distilled from the essential...</td>\n",
       "      <td>Almond extract recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>An aromatic spice that looks like a large, smo...</td>\n",
       "      <td>Allspice recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A large family of beers with a great deal of s...</td>\n",
       "      <td>Ale recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A vegetarian alternative to gelatine, agar-aga...</td>\n",
       "      <td>Agar-agar recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Small, shiny, red-brown beans with a little wh...</td>\n",
       "      <td>Aduki beans recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A sweet, rich, spirit-based liqueur made with ...</td>\n",
       "      <td>Advocaat recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Water that has been made slightly acidic by th...</td>\n",
       "      <td>Acidulated water recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Shaped like an acorn but larger, this winter s...</td>\n",
       "      <td>Acorn squash recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>Ackee recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>An extremely pungent spice extracted from a pl...</td>\n",
       "      <td>Asafoetida recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sometimes also called an avocado pear, the avo...</td>\n",
       "      <td>Avocado recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Although the season is very short, British asp...</td>\n",
       "      <td>Asparagus recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Everyone knows that aubergines are delicious o...</td>\n",
       "      <td>Aubergine recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>‘Smokies’ are whole wood-smoked haddock with t...</td>\n",
       "      <td>Arbroath smokie recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Confusingly, three different, unrelated plants...</td>\n",
       "      <td>Artichoke recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>A starch extract of the root of a tropical pla...</td>\n",
       "      <td>Arrowroot recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Believed to be one of the rarest oils in the w...</td>\n",
       "      <td>Argan oil recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Arborio is the classic risotto rice from the n...</td>\n",
       "      <td>Arborio rice recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td>Apricot jam recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The classic accompaniment to roast pork, but a...</td>\n",
       "      <td>Apple sauce recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fresh apricots are loose-stoned fruit that ran...</td>\n",
       "      <td>Apricot recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td>Apple juice recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>Anise recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Apple chutney is the perfect partner for chees...</td>\n",
       "      <td>Apple chutney recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Make the most of the British apple season with...</td>\n",
       "      <td>Apple recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td></td>\n",
       "      <td>Angostura bitters recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Angelica may be familiar as the acid-green cry...</td>\n",
       "      <td>Angelica recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Fresh anchovies are an oily fish that look and...</td>\n",
       "      <td>Anchovies recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>A natural juice concentrate from anchovies, th...</td>\n",
       "      <td>Anchovy essence recipes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          description  \\\n",
       "0   Serve them as an after-dinner treat with sweet...   \n",
       "1   Also known as Chinese spinach or callaloo in C...   \n",
       "2   Lactose and cholesterol-free, almond milk is a...   \n",
       "3                                                       \n",
       "4                                                       \n",
       "5   Our almond recipes make the most of this versa...   \n",
       "6   Aleppo pepper, also known as pul biber, Halaby...   \n",
       "7   Almond extract is distilled from the essential...   \n",
       "8   An aromatic spice that looks like a large, smo...   \n",
       "9   A large family of beers with a great deal of s...   \n",
       "10  A vegetarian alternative to gelatine, agar-aga...   \n",
       "11  Small, shiny, red-brown beans with a little wh...   \n",
       "12  A sweet, rich, spirit-based liqueur made with ...   \n",
       "13  Water that has been made slightly acidic by th...   \n",
       "14  Shaped like an acorn but larger, this winter s...   \n",
       "15                                                      \n",
       "16  An extremely pungent spice extracted from a pl...   \n",
       "17  Sometimes also called an avocado pear, the avo...   \n",
       "18  Although the season is very short, British asp...   \n",
       "19  Everyone knows that aubergines are delicious o...   \n",
       "20  ‘Smokies’ are whole wood-smoked haddock with t...   \n",
       "21  Confusingly, three different, unrelated plants...   \n",
       "22  A starch extract of the root of a tropical pla...   \n",
       "23  Believed to be one of the rarest oils in the w...   \n",
       "24  Arborio is the classic risotto rice from the n...   \n",
       "25                                                      \n",
       "26  The classic accompaniment to roast pork, but a...   \n",
       "27  Fresh apricots are loose-stoned fruit that ran...   \n",
       "28                                                      \n",
       "29                                                      \n",
       "30  Apple chutney is the perfect partner for chees...   \n",
       "31  Make the most of the British apple season with...   \n",
       "32                                                      \n",
       "33  Angelica may be familiar as the acid-green cry...   \n",
       "34  Fresh anchovies are an oily fish that look and...   \n",
       "35  A natural juice concentrate from anchovies, th...   \n",
       "\n",
       "                         name  \n",
       "0            Amaretti recipes  \n",
       "1            Amaranth recipes  \n",
       "2         Almond milk recipes  \n",
       "3     Alfalfa sprouts recipes  \n",
       "4      Almond essence recipes  \n",
       "5              Almond recipes  \n",
       "6       Aleppo pepper recipes  \n",
       "7      Almond extract recipes  \n",
       "8            Allspice recipes  \n",
       "9                 Ale recipes  \n",
       "10          Agar-agar recipes  \n",
       "11        Aduki beans recipes  \n",
       "12           Advocaat recipes  \n",
       "13   Acidulated water recipes  \n",
       "14       Acorn squash recipes  \n",
       "15              Ackee recipes  \n",
       "16         Asafoetida recipes  \n",
       "17            Avocado recipes  \n",
       "18          Asparagus recipes  \n",
       "19          Aubergine recipes  \n",
       "20    Arbroath smokie recipes  \n",
       "21          Artichoke recipes  \n",
       "22          Arrowroot recipes  \n",
       "23          Argan oil recipes  \n",
       "24       Arborio rice recipes  \n",
       "25        Apricot jam recipes  \n",
       "26        Apple sauce recipes  \n",
       "27            Apricot recipes  \n",
       "28        Apple juice recipes  \n",
       "29              Anise recipes  \n",
       "30      Apple chutney recipes  \n",
       "31              Apple recipes  \n",
       "32  Angostura bitters recipes  \n",
       "33           Angelica recipes  \n",
       "34          Anchovies recipes  \n",
       "35    Anchovy essence recipes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the data into the using panda dataframe\n",
    "import pandas as pd\n",
    "ingredient_df = pd.read_json('ingredient.jl', lines=True)\n",
    "display(ingredient_df)\n",
    "\n",
    "ingredient_detail_df = pd.read_json('ingredient_detail.jl', lines=True)\n",
    "display(ingredient_detail_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
