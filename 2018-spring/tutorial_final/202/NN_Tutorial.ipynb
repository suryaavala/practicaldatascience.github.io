{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PyTorch Neural Network For Wine Quality Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a computational model created from the basis of biologicial neural networks. Artificial Neural Networks (ANN) are able to somewhat mimic the procedures that our biological neural networks go through when recieving, processing, and transmitting information from neuron to neuron. A *neuron* in our ANN will be a part of a layer: input, hidden, or output, as shown in the diagram below. They will hold a weights that get updated using gradient descent. The neuron will recieve information, multiply the information by the weights it holds, and then transmit that information to the next layer. The end-goal of an ANN is to learn weights  that minimize error on our training data. Neural networks are highly useful for complex problems such as image classification, speech recognition, and natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ANN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer takes in the information that is fed in by the user. Every neuron in the input layer represents an independent variable that influences the output of our neural network. The information from our input layer will be transferred into the hidden layer. The hidden layer is consisted of neurons that have activation functions applied to it, and holds *hidden* information that is useful to getting our output value. The hidden layer is in between the input and output layer, and its job is to process the information that is fed in by the input layer. This layer is responsible for extracting the important and required features from our input data in order to get the correct output value. Most problems in machine learning can be solved with just two hidden layers.The output layer of the ANN collects all the information held by the hidden layer and transmits the information out of the model. \n",
    "\n",
    "Before we move on further, if you don't have experience with the feedforward neural network, you can read more about the algorithm [here](https://en.wikipedia.org/wiki/Backpropagation).\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- Introduction to PyTorch\n",
    "- Data Loading & Pre-Processing\n",
    "- Building the DataLoader and Two-Layer ANN\n",
    "- Testing & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a python package that provides two high-level features:\n",
    "\n",
    "1. Tensor computation (like numpy) with strong GPU acceleration\n",
    "2. Deep Neural Networks built on a tape-based autodiff system\n",
    "\n",
    "You can install the package by going to the PyTorch website, and selecting your operating system. You can install it using anaconda on Mac using:\n",
    "\n",
    "`$ conda install pytorch torchvision -c pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are data objects in Pytorch. We will be storing our data for the neural network model as Tensors. [Here](http://pytorch.org/docs/master/torch.html#tensors) you can find different inititalization functions for Tensors. They're similar to NumPy matrices, and they also carry a type. For example, we can initialize a Tensor from standard normal, a Tensor of byte zeros, and a Tensor of integer ones with 4 rows and 5 columns and also determine the size of the Tensors (similar to .shape in numpy) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.6388 -0.7407  0.7655  0.3467  0.0453\n",
      "-0.3150  0.5700  1.1143  0.7824 -0.6093\n",
      "-0.4862 -1.2893  0.5329  0.0096 -0.9164\n",
      " 0.5025 -0.9909  0.3173  0.3256 -0.7056\n",
      "[torch.FloatTensor of size 4x5]\n",
      " \n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.ByteTensor of size 4x5]\n",
      " \n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      "[torch.IntTensor of size 4x5]\n",
      "\n",
      "torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# initialize torch from standard normal\n",
    "x = torch.randn(4, 5)\n",
    "# initialize torch of zeros\n",
    "y = torch.zeros(4, 5).byte()\n",
    "# initialize torch of zeros\n",
    "z = torch.ones(4, 5).int()\n",
    "\n",
    "print(x, y, z)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can slice through tensors as you can with numpy arrays. You also have the option to create tensors from numpy arrays, and switch back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.9817\n",
      " 0.9750\n",
      " 1.2354\n",
      "[torch.DoubleTensor of size 3]\n",
      "\n",
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "z = z[:, :3]\n",
    "\n",
    "# convert numpy array to torch\n",
    "a = np.random.randn(3)\n",
    "print(torch.from_numpy(a))\n",
    "\n",
    "# convert tensor to numpy array\n",
    "print(z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can do matrix operations just as we do in numpy. Note that adding an underscore after a function name allows for in-place calculations. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.6388 -0.7407  0.7655  0.3467  0.0453\n",
      "-0.3150  0.5700  1.1143  0.7824 -0.6093\n",
      "-0.4862 -1.2893  0.5329  0.0096 -0.9164\n",
      " 0.5025 -0.9909  0.3173  0.3256 -0.7056\n",
      "[torch.FloatTensor of size 4x5]\n",
      "\n",
      "\n",
      " 0.3612  0.2593  1.7655  1.3467  1.0453\n",
      " 0.6850  1.5700  2.1143  1.7824  0.3907\n",
      " 0.5138 -0.2893  1.5329  1.0096  0.0836\n",
      " 1.5025  0.0091  1.3173  1.3256  0.2944\n",
      "[torch.FloatTensor of size 4x5]\n",
      "\n",
      "\n",
      "-0.1972 -1.5291  1.4242  0.3151\n",
      "-0.1207 -0.1091  0.0299 -0.6320\n",
      " 1.6606  1.6022  0.3049 -0.6754\n",
      " 0.9234 -0.5473  0.7598  0.2179\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "\n",
      " 0.2149 -0.6179 -1.3002  0.1482\n",
      "-0.0807 -0.0109 -0.0124  0.7098\n",
      " 0.6654 -0.1049  0.3331  0.0191\n",
      "-0.8010 -0.2615  0.6902 -0.0177\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = torch.ones(4, 5)\n",
    "x.add(y)\n",
    "print(x)\n",
    "x.add_(y)\n",
    "print(x)\n",
    "\n",
    "a = torch.randn(4, 4)\n",
    "b = torch.randn(4, 4)\n",
    "a.mul(b)\n",
    "print(a) # a didn't change\n",
    "a.mul_(b) # a changes\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation (Autograd)\n",
    "\n",
    "Our neural network model learns optimal weights by doing backpropagation. Backpropagation updates the weights based on a loss function, usually through gradient descent. In order to decrease the loss, we need to calculate the derivatives and know the gradient. \n",
    "\n",
    "Autograd is a method that is built into PyTorch, and it computes derivatives without obtaining the closed-form solutions. It backpropagates gradients automatically, without us having to manually calculate them. In order to use `Autograd`, we will need to wrap a `Variable` object to wrapper around a tensor object. When using the `Variable` object, we have the option to turn on and off the preference to calculate gradients. For response variables, we won't need gradients, since we will simply be comparing the outputs, but we will need them for when we want to update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.8868\n",
      " 1.3589\n",
      "-0.0781\n",
      " 2.0799\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.randn(4), requires_grad=True)\n",
    "# FloatTensor (4,4)\n",
    "print(x.data)\n",
    "# Gradient (starts off with nothing since we didn't backpropagate yet)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will compute the gradient of $y = \\sum_{i=0}^4 (X_i*10)$. We first need to calculate the gradient with respect to y by backwards push, and then we can see our x will be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 42.4742\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 42.4742\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 10\n",
      " 10\n",
      " 10\n",
      " 10\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = x.mul(10).sum()\n",
    "print(y)\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Pre-Processing\n",
    "\n",
    "To load our data, we will be using [pandas](https://pandas.pydata.org/) to load in and pre-process our data, and [sklearn](http://scikit-learn.org/stable/model_selection.html#model-selection)'s `train_test_split` to split our training and testing dataset. We will be using a [wine quality dataset](http://archive.ics.uci.edu/ml/datasets/Wine+Quality) from the UCI Machine Learning Repository to classify the quality of our wine as high or low quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "red_df = pd.read_csv(\"winequality-red.csv\", sep = \";\")\n",
    "white_df = pd.read_csv(\"winequality-white.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the two datasets: winequality-red.csv and winequality-white.csv to your directory. We will load the datasets (note the csv file is separated with a semicolon) into two pandas dataframes. We will see the following columns:\n",
    "\n",
    "1. fixed acidity \n",
    "2. volatile acidity \n",
    "3. citric acid \n",
    "4. residual sugar \n",
    "5. chlorides \n",
    "6. free sulfur dioxide \n",
    "7. total sulfur dioxide \n",
    "8. density \n",
    "9. pH \n",
    "10. sulphates \n",
    "11. alcohol \n",
    "13. type\n",
    "12. quality (score between 0 and 10)\n",
    "\n",
    "There are 6497 complete cases with 13 attributes each in the red and white wine datasets combined.\n",
    "\n",
    "Because we want to use the type of wine (red or white) as one of our predictor variables, we will need to add in an additional column `type`, and mark red wines with 1, white wines with -1. We also need to change the variable `quality` to a categorical variable, where we mark wines with rating over 6 as high quality and 6 and below as low quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "red_df['type'] = 1\n",
    "white_df['type'] = -1\n",
    "\n",
    "df = pd.concat([red_df, white_df])\n",
    "df = df[['type']+list(df)[0:12]]\n",
    "\n",
    "df['quality'] = df['quality'].apply(lambda x: 1 if x > 6 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing our data into the format we want, we will normalize our input. Normalizing our inputs so that they are scaled similarly will help our model to converge faster. In our case, we will will normalize it so that our data ranges from -1 to 1. We will do so by scaling all the continuous columns to range from 0 to 1, multiplying the new values by 2, and subtracting one from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# columns: list of continuous variables\n",
    "columns = list(df)\n",
    "del columns[-1]\n",
    "del columns[0]\n",
    "\n",
    "# normalizing continuous columns\n",
    "df[columns]=(df[columns]-df[columns].min())/(df[columns].max()-df[columns].min())\n",
    "df[columns] = df[columns]*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 13)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will split our training and testing data. We will pick 80% of our data randomly for training and 20% of it for training, and then convert our dataframes to numpy matrices. Using the `train_test_split` function from `sklearn`, we can do it with one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split train & test\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "train, test = train.as_matrix(), test.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the DataLoader and Two-Layer ANN\n",
    "\n",
    "Because our training data is fairly large (it has 5197 rows), it would be time consuming to manually run through every row of data and compute the loss, compute the gradient, and update the weights. We are alternatively going to divide the entire dataset into small *batches*, and go through each batch at once to compute the loss, gradient, and then update the weights. PyTorch allows us to create iterable objects called DataLoaders that create batches to feed into our model. Once we have our custom DataLoader, we can simply iterate through the batches to compute the loss, gradient, and update the weights.\n",
    "\n",
    "First, we will need to import the `Dataset` and `DataLoader` from `torch.utils.data`. Then we are going to make a class called WineDataset by extending the `Dataset` class. To do so, we need to store the data as x_data and y_data, write a `getitem(i)` function and `len()` function for our class. \n",
    "\n",
    "After making two instances of `WineDataset` using train and test data, we will feed the train and test Datasets into DataLoader, with batch size of 80. We will also turn on shuffling, and set the `num_workers` to the number of epochs, or the number of times we want to go through our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(1013)\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wine Dataset: contains 12 predictor variables and 1 response variable\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, winedata):\n",
    "        self.x_data = torch.from_numpy(winedata[:, 0:-1]).float()\n",
    "        self.y_data = torch.from_numpy(winedata[:, -1]).float()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]\n",
    "\n",
    "# Create train and test datasets\n",
    "train_data = WineDataset(train)\n",
    "test_data = WineDataset(test)\n",
    "\n",
    "# Create a custom dataloader with unique batch size\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=80, shuffle=True, num_workers =100)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=80, shuffle=True, num_workers=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will build the two-layer feedforward neural network model. This model will have an input layer, two hidden layers, and an output layer. The activation function we will use is the sigmoid function. Before we train our model, we have to define a unique class that suits our purpose. In our two-layered ANN, we will have 12 input nodes (since we have 12 predictor variables), two hidden layers with 10 nodes and 5 nodes, and an output layer with one node (since our task is to simply classify the wine as high or low quality (0 or 1). \n",
    "\n",
    "\\begin{equation*}\n",
    "\\textrm{Sigmoid}(x)= \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation*}\n",
    "\n",
    "There are a lot of design choices that we can make with a neural network model. we can choose the number of layers, the number of nodes in a hidden layer, and the activation function that we want to use. The more layers we use, the more complex our model becomes, and we will most likely be overfitting to our data. Using more hidden neurons allows us to extract more features, whereas using less hidden neurons can only keep the key features. Though it all depends on the amount of data you have, you should usually use less neurons than the layer before (so hidden layer 1 has less neurons than the input layer, and so on). \n",
    "\n",
    "We create the class and instantiate three linear layers for the two hidden layers and output. The layers will connect each other, and therefore we will need to set the inputs to layer 1 as the number of input neurons and the outputs as number of neurons in hidden layer 1. We will also define our activation function as the sigmoid function. Then, we are required to write the `forward()` function, where we have to \"connect\" our network using the layers and activation function defined in the init function. This function is in charge of the forward pass, calculating output values for a given input X.\n",
    "\n",
    "*Note that we can use different activation functions, such as ReLu, LogSigmoid, and SoftMax, and different layers such as Conv2d, which are provided in torch.nn*\n",
    "\n",
    "We will instantiate a `TwoLayerANN`, and call it `wine_model`. To see our network's parameters and the weights at each layer, we can call `list(wine_model.parameters())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerANN(nn.Module):\n",
    "    def __init__(self, inputs, hidden1, hidden2, output):\n",
    "        super(TwoLayerANN,self).__init__()\n",
    "        self.layer1 = nn.Linear(inputs, hidden1)\n",
    "        self.layer2 = nn.Linear(hidden1, hidden2)\n",
    "        self.layer3 = nn.Linear(hidden2, output)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Feed input layer into hidden layer 1 & apply activation\n",
    "        hidden1_in = self.layer1(X)\n",
    "        hidden1_out = self.activation(hidden1_in)\n",
    "        \n",
    "        # Feed hidden layer 1 into hidden layer 2 & apply activation\n",
    "        hidden2_in = self.layer2(hidden1_out)\n",
    "        hidden2_out = self.activation(hidden2_in)\n",
    "        \n",
    "        # Feed hidden layer 2 into output layer & apply activation\n",
    "        output_in = self.layer3(hidden2_out)\n",
    "        output_out = self.activation(output_in)\n",
    "        return output_out\n",
    "    \n",
    "wine_model = TwoLayerANN(12, 10, 5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "Now that we have created the model, we will move on to training the weights. We did not have to implement a backwards function in which we calculate the loss, gradient, and update the weights using the delta rule because the `autograd` package has it build in.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "We will need a loss function that calculates how well our model performs. There are several options for the loss function in `torch.nn`, such as log-likelihood, cross-entropy loss, and negative log-likelihood loss. In our case, we are going to use the [Mean Squared Error loss function](http://pytorch.org/docs/master/nn.html#torch.nn.MSELoss), which calculates the mean squared error between our predicted and observed y values.\n",
    "\n",
    "\n",
    "### Optimization Algorithm\n",
    "\n",
    "Once we have a loss function, we will also need an optimization algorithm. There are also several options for the loss function in `torch.optim`, but we are going to stick to stochastic gradient descent. This means that we are going to update the weights for every batch, unlike gradient descent, where we update the weights for every epoch or full round of forward propagation. For the optimization algorithm, we will have to set the parameters to optimize as our model's parameters (`wine_model.parameters()`) and set the learning rate as 0.1. This is the step size that our algorithm takes to gradually improve our model. Here is the stochastic gradient descent algorithm, where $\\theta$ are the parameters we want to optimize over, and $\\delta$ is the learning rate. \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta = \\theta - \\delta \\times \\nabla_\\delta MSE(\\theta; X_i, y_i)\n",
    "\\end{equation*}\n",
    "\n",
    "### Training Algorithm\n",
    "\n",
    "Once we have our loss function and optimization algorithm, we can set our model to training mode, and run through 100 epochs of training. We will use our `train_loader` that we created earlier, and iterate through the batches to update the weights. As we iterate through the batches and epochs, we will get inputs (predictor variable data) and outputs (quality), and we will wrap them in a Variable class. Then we will feed the model with our input variables to get an output, and calculate the loss using our MSE loss function. To update the weights, we will follow these steps:\n",
    "\n",
    "1. Clear the existing gradients in order to make sure we are not accumulating existing gradients\n",
    "2. Do a backward pass of network to calculate the sum of gradients\n",
    "3. Update the weights using the learning rate defined in our optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn = torch.nn.MSELoss(size_average=False) # MSE Loss Function\n",
    "optimizer = torch.optim.SGD(wine_model.parameters(), lr=0.1) # optimization algorithm\n",
    "\n",
    "wine_model.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (inputs, outputs) in enumerate(train_loader):\n",
    "        # Wrap data in Variable class\n",
    "        inputs, outputs = Variable(inputs.float()), Variable(outputs.float(), requires_grad=False)\n",
    "        \n",
    "        # Feed the model with inputs to get predictions\n",
    "        y_pred = wine_model(inputs)\n",
    "        \n",
    "        # Calculate loss using MSE\n",
    "        loss = lossfn(y_pred, outputs)\n",
    "        \n",
    "        # Clear existing gradients, backward pass, update weights\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model\n",
    "\n",
    "Now that we have trained our model, we can go ahead and see the accuracy level of the model. We will do so by setting the model to evaluation mode, since we won't be updating any of the weights. Then we will get our test data matrix, and split our inputs and outputs. Now all we have to do is gather the predicted values from our test data, and calculate the accuracy. \n",
    "\n",
    "We classify our predicted values as high quality if the output is greater than 0.5, and low quality if the output is lower than or equal to 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816923076923077\n"
     ]
    }
   ],
   "source": [
    "wine_model.eval()\n",
    "\n",
    "test_x = Variable(torch.from_numpy(test[:, 0:-1]).float())\n",
    "test_y = torch.from_numpy(test[:, [-1]]).int()\n",
    "\n",
    "correct = 0\n",
    "j = 0\n",
    "\n",
    "out = wine_model(test_x)\n",
    "pred = torch.IntTensor(1300, 1)\n",
    "\n",
    "for i in out.data:\n",
    "    output = i > 0.5\n",
    "    pred[j] = output.int()\n",
    "    j += 1\n",
    "\n",
    "\n",
    "print(torch.sum(test_y == pred)/1300)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [PyTorch](http://pytorch.org/)\n",
    "- [PyTorch Tutorials](https://github.com/pytorch/tutorials)\n",
    "- [UCI Machine Learning Repo](https://archive.ics.uci.edu/ml/datasets/Wine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
