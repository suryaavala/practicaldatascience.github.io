{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This tutorial will introduce you to a powerful neural network-RNN in solving variable-length data. Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. Specifically, we consider the problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "The architecture of sequence prediction with LSTM is shown in the picture below. \n",
    "![LSTM in sequence prediction](https://cdn-images-1.medium.com/max/525/1*epcf2SBjRHBynBNFf-CpQA.png) The model can predict next word by maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence, thus dependency between words and language rules can be learned by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the libraries\n",
    "In this tutorial, we will use [PyTorch](http://pytorch.org) to build and train the LSTM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and preprocessing\n",
    "The whole logic of preprocessing the real data is first we make everyting lowercase, and trim most punctuation. Then, we need to generate the vocabulary of words in an pre-defined order and represent each text by indexing the vocabulary to be a numerical vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "vocab = {}\n",
    "\n",
    "def generate_vocab():\n",
    "    # Read the file and split into lines\n",
    "    lines = open('xxx.txt').read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    lines = [normalize_string(s) for l in lines]\n",
    "    \n",
    "    words = [l.split(' ') for l in lines]\n",
    "    \n",
    "    cnt = 0\n",
    "    for w in words:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = cnt\n",
    "            cnt += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the data from [Word-level WikiText-2](https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip), which is a preprocessed dataset. The vocabulary file contains an array of strings. Each string is a word in the vocabulary. There are 33,278 vocabulary items. The train and validation file contain an array of articles. Each article is an array of integers, corresponding to words in the vocabulary. There are 579 articles in the training set. For example, the first article in the training set contains 3803 integers. The first 6 integers of the first article are [1420 13859 3714 7036 1420 1417]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch](http://pytorch.org) provides many tools to make data loading easy and hopefully, to make the code more readable. Dataset is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n",
    "\n",
    "*   \\__len\\__ so that len(dataset) returns the size of the dataset.\n",
    "*  \\__getitem\\__ to support the indexing such that dataset[i] can be used to get ith sample\n",
    "\n",
    "We can create a dataset class for the dataset to read train and validation data. We will read the .npy in the method \\__init\\__ but leave the reading of images to \\__getitem\\__. There are two kinds of frequently used data type: Tensor and Variable. Tensors are similar to NumPyâ€™s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing. Variable is a very thin wrapper around a Tensor. You can access the raw tensor through the .data attribute, and after computing the backward pass, a gradient w.r.t. this variable is accumulated into .grad attribute. So we need to first convert our numpy array to tensor and then convert tensor to Variable to support autograd within Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(numpy_array):\n",
    "    # Numpy array -> Tensor\n",
    "    return torch.from_numpy(numpy_array).long()\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, datatype):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.dataType = datatype\n",
    "        self.dataX, self.labels = load_data(datatype)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataX.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        utterance = self.dataX[index]\n",
    "        if self.dataType == 'test':\n",
    "            label = []\n",
    "        try:\n",
    "            label = self.labels[index]\n",
    "        except:\n",
    "            print(index, self.dataX.shape[0], len(self.labels))\n",
    "            print(self.labels)\n",
    "        return utterance, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a new class called myDataLoader which simulates the function of DataLoader in Pytorch since we will not use the random mechanism of sampling one batch of the default one. However, if we want to maintain the sampling logic but do something else, we can override the method of collate_fn in DataLoader to customize our needs. For example, through overriding collate_fn, we can manipulate the data to extract features or adjust the shape or type of one batch data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class myDataLoader(object):\n",
    "    def __init__(self, datatype, batch_size=1):\n",
    "        dataset = None\n",
    "        if datatype == 'train':\n",
    "            dataset = np.load('wiki.train.npy')\n",
    "        if datatype == 'valid':\n",
    "            dataset = np.load('wiki.valid.npy')\n",
    "            \n",
    "        # shuffle texts\n",
    "        random_data = np.concatenate(\n",
    "            dataset[np.random.permutation(dataset.shape[0])], axis=0)\n",
    "        size = batch_size * (len(random_data) // batch_size)\n",
    "        true_data = random_data[:size + 1]  # discard extra data\n",
    "        self.inputs = true_data[:-1].reshape((batch_size, -1)).T\n",
    "        self.targets = true_data[1:].reshape((batch_size, -1)).T\n",
    "\n",
    "    def __iter__(self):\n",
    "        index = 0\n",
    "        while index < self.inputs.shape[0]:\n",
    "            x, y = np.random.normal(70, 5), np.random.normal(35, 5)\n",
    "            seq_len = int(\n",
    "                np.random.choice([x, y], size=1, p=[0.95, 0.05]))\n",
    "            yield to_tensor(\n",
    "                self.inputs[index:index + seq_len]), to_tensor(\n",
    "                self.targets[index:index + seq_len])\n",
    "            index += seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process of processing the data is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Read .npy file as a whole, randomly generate a permutation of the dataset.\n",
    "*   split data into input data and target data. Every pair of input and target word are consecutive in the original dataset.\n",
    "*   Tanspose input and target dataset to set batch_size to be the second axis.\n",
    "*   In \\__iter\\__ method, we randomly choose the value of batch_size to be $0.95 * N(70,5)+0.05 * N(35,5)$ according to the setting in the paper [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model\n",
    "\n",
    "In [PyTorch](http://pytorch.org), we can inherit the class nn.Module to define the structure of our network and override the forward method in the LSTM class to facilitate the forward propagation. Additionaly, we define the method to initialize the weights and bias of embedding layer and linear layer, and the first hidden state for each layer within a LSTM cell. This method of implementation is very flexible so if you want to define another totally different neural network, you only need to change how the input is passed along and how different layers are connected together according to official document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also print the structure of the model very conveniently by just calling an instantiated LSTM object and printing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (drop): Dropout(p=0.2)\n",
      "  (embedding): Embedding(1000, 400)\n",
      "  (rnns): LSTM(400, 1150, num_layers=3, dropout=0.3)\n",
      "  (projection): Linear(in_features=1150, out_features=1000)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, word_cnt=1000, dropout=0.2, embedding_dim=400,\n",
    "                 hidden_dim=1150, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(num_embeddings=word_cnt,\n",
    "                                      embedding_dim=embedding_dim)\n",
    "        self.rnns = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers, dropout=0.3)\n",
    "        self.projection = nn.Linear(in_features=hidden_dim,\n",
    "                                    out_features=word_cnt)\n",
    "        self.init_weights()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.projection.bias.data.fill_(0)\n",
    "        self.projection.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedding = self.drop(self.embedding(input))\n",
    "        output, hidden = self.rnns(embedding, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.projection(\n",
    "            output.view(output.size(0) * output.size(1),\n",
    "                        output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1),\n",
    "                            decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        return (Variable(weight.new(self.num_layers, batch_size,\n",
    "                                    self.hidden_dim).zero_()),\n",
    "                Variable(weight.new(self.num_layers, batch_size,\n",
    "                                    self.hidden_dim).zero_()))\n",
    "\n",
    "print(LSTM())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "After preprocessing the data and designing the model, we can start to train the LSTM model. For convenience, we separate the train logic and validation logic into two different methods. Later, we will call them in the main function and compare the loss of train data and validation data to make sure that our training will not overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The routine for training is to iterate through all the train data--call the model to train one batch, calculate the loss and backpropagate to update the weights. \n",
    "Here, I have to point out the repackage_hidden method whose purpose is to take out the tensor containing the hidden state through h.data and wrap it in a new Variable, which has no history in it. Since autograd mechanism of Pytorch will replay every variable and differentiate each operation within a graph, if you want BPTT not to backpropagate to the hidden state after finishing a sentence, then you need to get rid of the reference and free up the memory for next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    lstm_model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    hidden = lstm_model.init_hidden(batch_size)\n",
    "    train_data = myDataLoader('train', batch_size)\n",
    "\n",
    "    for i, (input, targets) in enumerate(train_data):\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        lstm_model.zero_grad()\n",
    "        output, hidden = lstm_model(to_variable(input), hidden)\n",
    "        loss = criterion(output.view(-1, word_cnt),\n",
    "                         to_variable(targets).view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(lstm_model.parameters(), clip)\n",
    "        optim.step()  # Update the network\n",
    "        total_loss += loss.data\n",
    "        print_log(i, total_loss, start_time)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for evaluation part, one thing I need to point out is that we need to set the model to be evaluation mode to skip Dropout or BatchNorm during validation and test time. Reasons for us to do this is:\n",
    "*   Dropout makes neurons output 'wrong' values on purpose.\n",
    "*   Avoid inconsistency because we disable neurons randomly and the network will have different outputs every activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    lstm_model.eval()\n",
    "    total_loss = 0\n",
    "    valid_data = myDataLoader('valid', batch_size)\n",
    "    hidden = lstm_model.init_hidden(batch_size)\n",
    "    cnt = 0\n",
    "    for i, (input, targets) in enumerate(valid_data):\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        \n",
    "        output, hidden = lstm_model(to_variable(input), hidden)\n",
    "        output_flat = output.view(-1, word_cnt)\n",
    "        total_loss += criterion(output_flat, to_variable(targets).view(-1)).data\n",
    "        cnt += 1\n",
    "\n",
    "    return total_loss[0] / cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can try different regularization methods to tune the hyperparameters of the model in order to obtain a reasonably good model without overfitting.\n",
    "There are some common regularization methods for this task. You can do some experiments to either combine them all or just use a subset of them. \n",
    "*   Apply locked dropout between LSTM layers\n",
    "*   Apply embedding dropout\n",
    "*   Apply weight decay\n",
    "*   Tie the weights of the embedding and the output layer\n",
    "*   Activity regularization\n",
    "*   Temporal activity regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not familiar with tuning the parameters, I list some major methods to tune the hyperparameters here. However, this topic is beyond the scope of this tutorial so I will not implement different methods talked below. I focus on the \n",
    "*   Manual Search\n",
    "*   Grid Search [Grid Search Hyperparameters in Python with Keras](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)\n",
    "*   Random Search\n",
    "*   Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these helper functions in place (it looks like extra work, but it makes it easier to run multiple experiments) we can actually initialize a network and start training. While you train the model, feel free to interrupt the kernel and adjust the learning rate then run it again later with loading the model file kept previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "embedding_dim = 400\n",
    "hidden_dim = 200\n",
    "num_layers = 3\n",
    "clip = 0.25\n",
    "lr = 0.0001\n",
    "dropout = 0.2\n",
    "epochs = 40\n",
    "log_interval = 20\n",
    "word_cnt = 33278\n",
    "\n",
    "lstm_model = LSTM(word_cnt, dropout, embedding_dim, hidden_dim, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(lstm_model.parameters(), lr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    lstm_model = lstm_model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "def to_variable(tensor):\n",
    "    # Tensor -> Variable (on GPU if possible)\n",
    "    if torch.cuda.is_available():\n",
    "        # Tensor -> GPU Tensor\n",
    "        tensor = tensor.cuda()\n",
    "    return torch.autograd.Variable(tensor)\n",
    "\n",
    "\n",
    "def main():\n",
    "    best_val_loss = None\n",
    "    epoch = 0\n",
    "    global lr\n",
    "    try:\n",
    "        while epoch < epochs:\n",
    "            epoch_start_time = time.time()\n",
    "            train(epoch)\n",
    "            val_loss = evaluate()\n",
    "            print('-' * 89)\n",
    "            print(\n",
    "                '| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(\n",
    "                    epoch, (time.time() - epoch_start_time), val_loss,\n",
    "                    math.exp(val_loss)))\n",
    "            print('-' * 89)\n",
    "\n",
    "            if not best_val_loss or val_loss < best_val_loss:\n",
    "                torch.save(lstm_model.state_dict(), 'model.pt')\n",
    "                best_val_loss = val_loss\n",
    "            else:\n",
    "                lr /= 4.0\n",
    "            epoch += 1\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        torch.save(lstm_model.state_dict(), 'model.pt')\n",
    "        print('-' * 89)\n",
    "        print('Exiting from training early')\n",
    "        \n",
    "def print_log(i, total_loss, start_time):\n",
    "    if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss[0] / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print()\n",
    "            print('| epoch {:3d} | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, lr, elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "This function takes as input a batch of sequences, shaped [batch size, sequence length].\n",
    "This function will load your trained model and perform a forward pass.\n",
    "These input sequences are drawn from the unseen test data. We will calculate the negative log likelihood to have a sense of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "hidden_dim = 200\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "word_cnt = 33278\n",
    "\n",
    "def log_softmax(x, axis):\n",
    "    ret = x - np.max(x, axis=axis, keepdims=True)\n",
    "    lsm = np.log(np.sum(np.exp(ret), axis=axis, keepdims=True))\n",
    "    return ret - lsm\n",
    "\n",
    "def prediction(inp):\n",
    "    # Load the best saved model.\n",
    "    lstm = LSTM(word_cnt, dropout, embedding_dim, hidden_dim,\n",
    "                num_layers)\n",
    "\n",
    "    model_path = 'model.pt'\n",
    "\n",
    "    # Load dictionary into your model\n",
    "    m = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    lstm.load_state_dict(m)\n",
    "    lstm.eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        lstm.cuda()\n",
    "\n",
    "    hidden = lstm.init_hidden(inp.shape[0])\n",
    "    input = to_variable(to_tensor(inp)).transpose(0, 1)\n",
    "    output, hidden = lstm(input, hidden)\n",
    "\n",
    "    return output[-1, :, :].data.cpu().numpy()\n",
    "\n",
    "def test_prediction():\n",
    "    fixture = np.load('prediction.npz')\n",
    "    inp = fixture['inp']\n",
    "    targ = fixture['out']\n",
    "    out = prediction(inp.copy())\n",
    "    vocab = np.load('vocab.npy')\n",
    "    out = log_softmax(out, 1)\n",
    "    nlls = out[np.arange(out.shape[0]), targ]\n",
    "    nll = -np.mean(nlls)\n",
    "    print(\"Your mean NLL for predicting a single word: {}\".format(nll))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your mean NLL for predicting a single word: 4.747096061706543\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    test_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary and References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial highlighted just a few elements of how to generate new text based on variable-length sequence of words in Python.  Much more detail about the libraries and questions on LSTM and sequence prediction are available from the following links.\n",
    "\n",
    "1. Pytorch: http://pytorch.org\n",
    "2. Regularizing and Optimizing LSTM Language Models: https://arxiv.org/pdf/1708.02182.pdf\n",
    "3. Grid Search Hyperparameters in Python with Keras: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "4. Word-level language modeling RNN: https://github.com/pytorch/examples/tree/master/word_language_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
