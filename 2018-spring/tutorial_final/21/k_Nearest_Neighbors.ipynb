{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial will introduce you to the k-nearest neighbors algorithm (k-NN), a non-parametric method which in pattern recognition is used for classification and regression. A \"non parametric method\" means that it does not make any assumptions on the underlying data distribution & the model structure is determined from the data itself. In real world, most of the data does not obey the typical theoretical assumptions. Hence one of the most popular choice for a classification study when there is little or no prior knowledge about the distributuion of data.\n",
    "Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point. It can be used to assign weights to the contribution of the neighbors so that the nearer neighbors contribute more to the average than the more distant ones. The neighbors are taken from a set of objects for which the class or object property value is known. This can be considered as a training set for the algorithm. A basic visual representation would look like this:\n",
    "___\n",
    "\n",
    "[<img src=\"https://cdn-images-1.medium.com/max/1000/0*Sk18h9op6uK9EpT8.\">](https://cdn-images-1.medium.com/max/1000/0*Sk18h9op6uK9EpT8.)\n",
    "___\n",
    "\n",
    "The test sample (green circle) should be classified either to the first class of blue squares or to the second class of red triangles. If k = 3 (solid line circle) it is assigned to the second class because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the first class (3 squares vs. 2 triangles inside the outer circle). In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small).\n",
    "\n",
    "\n",
    "## Tutorial Content\n",
    "\n",
    "In this tutorial, we will be focussing on the classification aspect with the help of a dataset. The test problem that we will be using is iris classification.  We'll be using data collected from the UCI Machine Learning repository: https://archive.ics.uci.edu/ml/datasets/iris. The iris data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2. The problem is comprised of 150 observations of iris flowers from three different species. There are 4 measurements of given flowers: sepal length, sepal width, petal length and petal width, all in the same unit of centimeters. The predicted attribute is the species, which is one of setosa, versicolor or virginica.\n",
    "\n",
    "It is a standard dataset where we can split the data into training and test datasets and use the results to evaluate our algorithm implementation. Good classification accuracy on this problem is above 90%, typically 96% or better. We will cover the following topics in this tutorial:\n",
    "\n",
    "- [Getting started](#Getting-started)\n",
    "- [Handling the Data](#Handling-the-Data)\n",
    "- [Calculating the Similarity](#Calculating-the-Similarity)\n",
    "- [Finding the Neighbors](#Finding-the-Neighbors)\n",
    "- [Generating a Response](#Generating-a-Response)\n",
    "- [Checking the Accuracy](#Checking-the-Accuracy)\n",
    "- [Tieing it all together](#Tieing-it-all-together)\n",
    "\n",
    "## Getting started\n",
    "The k-nearest neighbors algorithm is based around the simple idea of predicting unknown values by matching them with the most similar known values. Let's say that we have 3 different types of cars. We know the name of the car, its Horsepower, whether or not it is Fuel Efficient, and whether or not it's Fast. \n",
    "\n",
    "| Car | Horsepower | Fuel efficient | Fast |\n",
    "|-----|------------|----------------|------|\n",
    "| Honda | 180 | True | False |\n",
    "| Ferrari | 500 | False | True |\n",
    "| Audi | 200 | True | True |\n",
    "\n",
    "Let's say that we now have another car, but we don't know how fast it is:\n",
    "\n",
    "| Car | Horsepower | Fuel efficient | Fast |\n",
    "|-----|------------|----------------|------|\n",
    "| BMW | 400 | True | Unknown |\n",
    "\n",
    "We want to figure out if the car is fast or not. In order to predict if it is with k nearest neighbors, we first find the most similar known car. In this case, we would compare the horsepower and fuel efficient values to find the most similar car, which is Ferrari. Since Ferrari is fast, we would predict that the BMW is also fast. This is an example of 1-nearest neighbors.\n",
    "\n",
    "- If we performed a 2-nearest neighbors, we would end up with 2 True values (for Ferrari and Audi), which would average out to True. Hence Ferrari and Audi are the two most similar cars, giving us a k of 2.\n",
    "- If we did 3-nearest neighbors, we would end up with 2 True values and a False value, which would average out to True.\n",
    "\n",
    "The number of neighbors we use for k-nearest neighbors (k) can be any value less than the number of rows in our dataset. In practice, looking at only a few neighbors makes the algorithm perform better, because the less similar the neighbors are to our data, the worse the prediction will be.\n",
    "\n",
    "The steps in the following diagram provides a high-level overview of the tasks that we'll need to accomplish in our code. \n",
    "\n",
    "[<img src=\"https://cambridgecoding.files.wordpress.com/2016/01/knn2.jpg\">](https://cambridgecoding.files.wordpress.com/2016/01/knn2.jpg)\n",
    "\n",
    "- Here the input consists of the k closest training examples in the feature space. The output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors. If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "\n",
    "## Handling the Data\n",
    "\n",
    "- The first step is to download the iris.data from https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data. Save the file in CSV format in the same directory alongside your code to be run on local machine. \n",
    "- Next we will load our data file. As the data is in CSV format without a header line, we can directly open the file with the open function and read the data lines using the csv.reader function in the [csv module](https://docs.python.org/3.1/library/csv.html). In order to use the csv reader feature, we first needs to import csv module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open function\n",
    "with open('iris.data.csv') as data:\n",
    "    # Returns a reader object line.\n",
    "    lines = csv.reader(data)\n",
    "    # Can use the reader object to iterate over the lines in the data\n",
    "#     for line in lines:\n",
    "#         print (' '.join(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we will split our data into a training dataset that kNN can use to make predictions about the classifications of a new sampling point and into a test dataset that we can use to evaluate how accurately our model is predicting the new sample points.\n",
    "\n",
    "- We will first convert the flower measures that were loaded as strings into floating numbers so that we can perform various arithmetic operations.\n",
    "\n",
    "- Next what we need to do is to split our dataset randomly into training datasets and test datasets by importing the [random module](https://docs.python.org/2/library/random.html) in a specific, pre determined ratio. Since 67/33 is a standard ratio used, we will go ahead with the same values. You can pick up some other value if you want. According to a [study](https://scialert.net/fulltext/?doi=jas.2014.171.176), the training set = 95% has the highest accuracy set.\n",
    "\n",
    "- By bringing it all together, we can define a function called loadData that loads a CSV with the provided name of the file and splits it randomly into training datasets and test datasets using the defined split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def loadData(file, split, trainSet=[], testSet=[]):\n",
    "    with open(file) as data:\n",
    "        # Reader object\n",
    "        lines = csv.reader(data)\n",
    "        # Converting the reader object into a list of list of flower measures\n",
    "        dataset = list(lines)\n",
    "        for a in range(len(dataset) - 1):\n",
    "            for b in range(4):\n",
    "                # Converting the data type of flower measures from string to float\n",
    "                dataset[a][b] = float(dataset[a][b])\n",
    "            # Random.random return the next random floating point number in the range [0.0, 1.0).\n",
    "            if random.random() < split:\n",
    "                trainSet.append(dataset[a])\n",
    "            else:\n",
    "                testSet.append(dataset[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function out with our iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainining Data: 101\n",
      "Test Data: 49\n"
     ]
    }
   ],
   "source": [
    "trainSet=[]\n",
    "testSet=[]\n",
    "loadData('iris.data.csv', 0.67, trainSet, testSet)\n",
    "print ('Trainining Data: ' + repr(len(trainSet)))\n",
    "print ('Test Data: ' + repr(len(testSet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculating the Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now in order to make predictions we need to calculate how similar two given data instances are. This step is crucial as we need to locate the k most similar data instances in the training dataset for a given member of the test dataset in order to make a prediction.\n",
    "\n",
    "- Before we can predict using KNN, we need to find some way to figure out which data rows are \"closest\" to the row we're trying to predict on. A simple way to do this is to use Euclidean distance.\n",
    "\n",
    "- Since all the flower measurements have the same units and are float, we can directly use the [Euclidian Distance Measure](https://en.wikipedia.org/wiki/Euclidean_distance). We can calculate the distance by using math module & performing operations like **math.sqrt(pow((value1 - value2), 2))**. However, in python we can use the [numpy.linalg.norm function](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.norm.html) to calculate the euclidian distance directly.\n",
    "\n",
    "- Also since there are 4 float attributes and one string, we need to control the fields to include in the euclidian distance calculation as we only require the first 4 values. One approach is to limit euclidian distance to a fixed length, ignoring the last dimension.\n",
    "\n",
    "- Hence by putting this all together, we can define a Euclidian Distance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def eucDistance(value1, value2, length):\n",
    "    distance = 0\n",
    "    for a in range(length):\n",
    "        distance += np.linalg.norm(value1[a] - value2[a])\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check our function with some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 8.0\n"
     ]
    }
   ],
   "source": [
    "data1 = [3, 1, 2, 'a']\n",
    "data2 = [7, 4, 3, 'b']\n",
    "distance = eucDistance(data1, data2, 3)\n",
    "print ('Distance: ' + repr(distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now since we have a similarity measure, we can use it to collect the k most similar instances for a given unseen instance.\n",
    "- The process involves calculating the distance of all instances and selecting a subset with the smallest distance values\n",
    "- We will define a function that returns the k most similar neighbors from the training set for a given test instance using the already defined Euclidian Distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding k similar neighbors from train set for a given test instance\n",
    "def findNeighbors(trainSet, testInstance, k):\n",
    "    distances = []\n",
    "    # Reducing the length by 1 to exclude the 5th string parameter  \n",
    "    length = len(testInstance) - 1\n",
    "    for a in range(len(trainSet)):\n",
    "        # Calculating the distance between all instances\n",
    "        dist = eucDistance(testInstance, trainSet[a], length)\n",
    "        distances.append((trainSet[a], dist))\n",
    "    # Sorting the list of distances in ascending order    \n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    neighbors = []\n",
    "    for a in range(k):\n",
    "        # Returning the k training set instances with the shortest distances\n",
    "        neighbors.append(distances[a][0])\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check whether the function is returning the correct training set with minimum distance on some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors =  [[0, 3, 6, 'class_y'], [1, 4, 7, 'class_x']]\n"
     ]
    }
   ],
   "source": [
    "trainSet = [[1, 4, 7, 'class_x'], [2, 5, 8, 'class_y'],[3, 6, 9,'class_z'],[0, 3, 6,'class_y']]\n",
    "testInstance = [3, 2, 1, 'class_z']\n",
    "k = 2\n",
    "neighbors = findNeighbors(trainSet, testInstance, k)\n",
    "print('Neighbors = ',neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In KNN there is no training phase, for each new data it calculates Euclidean distance and compares with the nearest K neighbors. Class with maximum number of data points in nearest K neighbors list is chosen as the class of new data point.\n",
    "- So now after locating the most similar neighbors for a particular test instance, we need to devise a prediction or a predicted response based on those neighbors. \n",
    "- One way of doing this is by allowing each neighbor to vote for their class attributes. By following such measure, we can take the one with the majority of votes as the prediction.\n",
    "- Hence by using the nearest neighbors we just identified, we can get a prediction for the class of the test instance by majority voting which involves simply tallying up which class comes up most often among the nearest neighbors.\n",
    "- Below provides a function for getting the majority voted response from a number of neighbors and according to our used dataset, it assumes the class is the last attribute for each neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPrediction(neighbors):\n",
    "    votes = {}\n",
    "    # There can be multiple neighbors based on value of k\n",
    "    for a in range(len(neighbors)):\n",
    "        # Getting the last index which is the string class of flowers\n",
    "        response = neighbors[a][-1]\n",
    "        if response in votes:\n",
    "            # If the response is same from the previously checked neighbors\n",
    "            votes[response] += 1\n",
    "        else:\n",
    "            votes[response] = 1\n",
    "    # Sorting the responses in a descending order based on their values\n",
    "    sortedVotes = sorted(votes.items(), key=lambda x : x[1], reverse=True)\n",
    "    # Returning the class which came up most often among the nearest neighbors\n",
    "    return sortedVotes[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's check this function with some sample data. This approach returns only one response even in the case of a draw. However, you can handle that accordingly by either returning no response or selecting an unbiased random response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class which comes up most often as the nearest neighbor is \"c\"\n"
     ]
    }
   ],
   "source": [
    "neighbors = [[1,1,1,'c'], [2,2,2,'a'], [3,3,3,'b'], [4,4,4,'c'], [5,5,5,'a'], [6,6,6,'c']]\n",
    "response = getPrediction(neighbors)\n",
    "print('The class which comes up most often as the nearest neighbor is \"'+response+'\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have all the pieces and have implemented our KNN algorithm. However, an important concern is how to evaluate the accuracy of our predictions? How do we find out that our algorithm is giving acceptable solutions in terms of predictions accuracy?\n",
    "- In this case, accuracy is the ratio of number of data points correctly classified to total number of data points. An easy way to calculate the accuracy of our model is to calculate a ratio of total correct predictions out of all the predictions made, the Classification Accuracy.\n",
    "- So below is the calculate Accuracy function that sums the total current predictions and returns the accuracy as a percentage of correct classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for a in range(len(testSet)):\n",
    "        if testSet[a][-1] == predictions[a]:\n",
    "            correct += 1\n",
    "    return (correct / float(len(testSet))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function with a sample test dataset and some sample predictions. On the actual data set, accuracy of atleast 90% is what we seek in order to classify our predictions as successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 50.0 %\n"
     ]
    }
   ],
   "source": [
    "testSet = [[1,1,1,'c'], [2,2,2,'a'], [3,3,3,'b'], [4,4,4,'c'], [5,5,5,'a'], [6,6,6,'c']]\n",
    "predictions = ['c', 'c', 'c', 'c', 'c', 'c']\n",
    "accuracy = checkAccuracy(testSet, predictions)\n",
    "print('Accuracy is',accuracy,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tieing it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now have all the elements that we needed to test our algorithm. We can now tie them all together with a main function to check the accuracy of our predictions on our loaded dataset. Below is the complete integration of all the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 99\n",
      "Test set: 51\n",
      "\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-setosa'*--*-Actual Value is = 'Iris-setosa'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-versicolor'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-versicolor'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "*Predicted Value is = 'Iris-virginica'*--*-Actual Value is = 'Iris-virginica'*\n",
      "\n",
      "Accuracy = '96.08'%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def loadData(file, split, trainSet=[], testSet=[]):\n",
    "    with open(file) as data:\n",
    "        # Reader object\n",
    "        lines = csv.reader(data)\n",
    "        # Converting the reader object into a list of list of flower measures\n",
    "        dataset = list(lines)\n",
    "        for a in range(len(dataset) - 1):\n",
    "            for b in range(4):\n",
    "                # Converting the data type of flower measures from string to float\n",
    "                dataset[a][b] = float(dataset[a][b])\n",
    "            # Random.random return the next random floating point number in the range [0.0, 1.0).\n",
    "            if random.random() < split:\n",
    "                trainSet.append(dataset[a])\n",
    "            else:\n",
    "                testSet.append(dataset[a])\n",
    "\n",
    "def eucDistance(value1, value2, length):\n",
    "    distance = 0\n",
    "    for a in range(length):\n",
    "        distance += np.linalg.norm(value1[a] - value2[a])\n",
    "    return distance\n",
    "\n",
    "def findNeighbors(trainSet, testInstance, k):\n",
    "    distances = []\n",
    "    # Reducing the length by 1 to exclude the 5th string parameter  \n",
    "    length = len(testInstance) - 1\n",
    "    for a in range(len(trainSet)):\n",
    "        # Calculating the distance between all instances\n",
    "        dist = eucDistance(testInstance, trainSet[a], length)\n",
    "        distances.append((trainSet[a], dist))\n",
    "    # Sorting the list of distances in ascending order    \n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    neighbors = []\n",
    "    for a in range(k):\n",
    "        # Returning the k training set instances with the shortest distances\n",
    "        neighbors.append(distances[a][0])\n",
    "    return neighbors\n",
    "\n",
    "def getPrediction(neighbors):\n",
    "    votes = {}\n",
    "    # There can be multiple neighbors based on value of k\n",
    "    for a in range(len(neighbors)):\n",
    "        # Getting the last index which is the string class of flowers\n",
    "        response = neighbors[a][-1]\n",
    "        if response in votes:\n",
    "            # If the response is same from the previously checked neighbors\n",
    "            votes[response] += 1\n",
    "        else:\n",
    "            votes[response] = 1\n",
    "    # Sorting the responses in a descending order based on their values\n",
    "    sortedVotes = sorted(votes.items(), key=lambda x : x[1], reverse=True)\n",
    "    # Returning the class which came up most often among the nearest neighbors\n",
    "    return sortedVotes[0][0]\n",
    "\n",
    "def checkAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for a in range(len(testSet)):\n",
    "        if testSet[a][-1] == predictions[a]:\n",
    "            correct += 1\n",
    "    return (correct / float(len(testSet))) * 100.0\n",
    "\n",
    "def kNN():\n",
    "    trainSet = []\n",
    "    testSet = []\n",
    "    # can use split = 0.95 for the best accuracy\n",
    "    split = 0.67\n",
    "    # Loading the Dataset\n",
    "    loadData('iris.data.csv', split, trainSet, testSet)\n",
    "    print('Train set: ' + repr(len(trainSet)))\n",
    "    print('Test set: ' + repr(len(testSet)) + '\\n')\n",
    "    # Generating predictions \n",
    "    predictions = []\n",
    "    k = 4\n",
    "    for a in range(len(testSet)):\n",
    "        neighbors = findNeighbors(trainSet, testSet[a], k)\n",
    "        result = getPrediction(neighbors)\n",
    "        predictions.append(result)\n",
    "        print('*Predicted Value is = ' + repr(result) + '*--*-Actual Value is = ' + repr(testSet[a][-1]) + '*')\n",
    "    accuracy = checkAccuracy(testSet, predictions)\n",
    "    # Formatting the value of accuracy to upto 2 decimal places\n",
    "    accuracy = \"{0:.2f}\".format(accuracy)\n",
    "    print('\\nAccuracy = ' + repr(accuracy) + '%')\n",
    "    \n",
    "kNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary and references\n",
    "\n",
    "This tutorial highlighted how kNN algorithm works on a certain dataset & can be used for classifciation purpose. Further details about how to extend the idea and to get a better understanidng are available from the following links.\n",
    "\n",
    "- Further understanding of algorithm: https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "- Nearest Neighbors: http://scikit-learn.org/stable/modules/neighbors.html\n",
    "- knn Regression: http://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html\n",
    "- Iris flower dataset: https://en.wikipedia.org/wiki/Iris_flower_data_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
