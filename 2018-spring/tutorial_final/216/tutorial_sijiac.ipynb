{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Spark?\n",
    "\n",
    "Data science always comes with big data. To handle big data, you need paralization. However, paralization is a hard problem. Spark is designed for data science and the abstractions of Spark make it easier. With Spark framework, you can totally focus on data processing and model building. Because of its Simplicity, Performance and flexibility, Spark is pretty popular among almost all Big data developers and Data scientists.\n",
    "\n",
    "![spark-concepts](http://chuantu.biz/t6/269/1522566870x-1566683256.png)\n",
    "\n",
    "## Tutorial content\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Spark and MapReduce](#Spark-and-MapReduce)\n",
    "- [Launch Apache Spark on AWS](#Launch-Apache-Spark-on-AWS)\n",
    "- [Spark Programming Basics](#Spark-Programming-Basics)\n",
    "- [Advanced Apache Spark](#Advanced-Apache-Spark)\n",
    "- [Optimization Suggestions](#Optimization-Suggestions)\n",
    "- [Example: Calculate Pi](#Example:-Calculate-Pi)\n",
    "- [Example: Logistic Regression](#Example:-Logistic-Regression)\n",
    "\n",
    "## Spark and MapReduce\n",
    "\n",
    "Apache Hadoop's MapReduce is a programming model for processing large amounts of data. The model contains two components, map function and reduce function. Users can use map function to process data into intermediate **key/value** pair and then use reduce function to merge all values with the same intermediate key. MapReduce relies on the distributed file system (HDFS) of Hadoop. At the begining of the job, MapReduce read the input data from HDFS. At the end of the job, MapReduce stores the result to HDFS. MapReduce has become very popular over the years, because many real tasks can be expressed easily through MapReduce. However, many tasks now (such as maching learning tasks) involve many iterative jobs. Each iterative job can be expressed as one MapReduce job. Each job must reload from HDFS and load to HDFS, which causes a performance bottleneck.\n",
    "\n",
    "Spark is designed to solve this bottleneck. Spark tries to keep data in memory and also supports fault-tolerant, which could accelerate iterative computation over 10x. To do this, Spark introduces a new data abstraction called **Resilient Distributed Datasets** (RDDs). RDDs are cached in memory across various computational stages and are reused among multiple MapReduce jobs. Fault tolerance of RDDs is achieved by using **lineage** information, that is keeping all operations which were performed to get current state of RDDs. If a partition of an RDD is missed, the RDD can use lineage information to recover that partition from other RDDs.\n",
    "\n",
    "![compare](http://chuantu.biz/t6/269/1522566882x-1566683256.png)\n",
    "\n",
    "Spark is a brilliant programming model, now I'd like to show you how to write Apache Spark programs in Python, which can be deployed on real clusters and solve real tasks. **Amazaing!!**\n",
    "\n",
    "## Launch Apache Spark on AWS\n",
    "\n",
    "To start, you need to launch your own Apache Spark cluster on AWS.\n",
    "\n",
    "I highly recommend a command-line tool for launching Spark clusters on AWS [Flintrock](https://github.com/nchammas/flintrock#configurable-cli-defaults). In addition, it's strongly suggested to read the README.md of this tool.\n",
    "\n",
    "Remember to set up the environment variables AWS\\_SECRET\\_ACCESS_KEY and AWS\\_ACCESS\\_KEY\\_ID before using the tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarification\n",
    "\n",
    "**I ran all programs in aws cluser launched by Flintrock and pasted the output to notebook cell.**\n",
    "\n",
    "**So codes here can't be executed in jupyter notebook directly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Programming Basics\n",
    "\n",
    "Let's see a Spark WordCount program in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "APP_NAME = \"Spark WordCount Application\"\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "sc = SparkContext(conf=conf)\n",
    "textRDD = sc.textFile(\"s3a://tutorial-688/README.md\")\n",
    "counts = (textRDD.flatMap(lambda line: line.split(' '))\n",
    "                .map(lambda term: (term, 1))\n",
    "                .reduceByKey(lambda a, b: a + b))\n",
    "counts.saveAsTextFile(\"s3a://tutorial-688/output/wordcount_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use README.md of flintrock as the input file, here is part of the output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(u'', 344)\n",
    "(u'APIs,', 1)\n",
    "(u'all', 4)\n",
    "(u'code', 1)\n",
    "(u'managed', 1)\n",
    "(u'forget', 1)\n",
    "(u'limited', 1)\n",
    "(u'tradeoff.', 1)\n",
    "(u'Juju](http://www.ubuntu.com/cloud/tools/juju).', 1)\n",
    "(u'existing', 1)\n",
    "(u'single-purpose', 1)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: <https://s3.amazonaws.com/tutorial-688/README.md>\n",
    "\n",
    "Output: <https://s3.amazonaws.com/tutorial-688/output/wordcount_output/part-00000>\n",
    "\n",
    "To run previous code, you should launch your cluster first, then log in your master node by using:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "flintrock login <your_cluster_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flintrock uses **Amazon S3** as input and output. To run above programe, you need to create a bucket in S3 first. Use <font color=INDIANRED>s3a://</font> as prefix (e.g. <font color=INDIANRED>s3a://bucket/path/to/file</font>) to read and write data.\n",
    "\n",
    "Make sure export **access\\_key** and **access\\_key\\_id** in environment of master node:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "export AWS_ACCESS_KEY_ID=<your_access_key_id>\n",
    "export AWS_SECRET_ACCESS_KEY=<your_secret_access_key>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create <font color=INDIANRED>spark_wordcount.py</font> contains previous WordCount program.\n",
    "\n",
    "Launch WordCount task in Spark cluster,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit spark_wordcount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After task is completed, result is stored at <font color=INDIANRED>s3a://tutorial-688/output/wordcount_output</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Line by Line** explaination how it works:\n",
    "\n",
    "1. Use <font color=INDIANRED>SparkConf()</font> to create an <font color=INDIANRED>conf</font>, <font color=INDIANRED>setAppName()</font> gives its AppName. You can also set other attributes, see [pyspark API](https://spark.apache.org/docs/latest/api/python/pyspark.html) for details.\n",
    "2. Use <font color=INDIANRED>conf</font> to create a <font color=INDIANRED>SparkContext</font>. A <font color=INDIANRED>SparkContext</font> represents a Spark cluster connection. Users can use it to create RDD and shared variables (introduced later) like broadcast variables on that cluster.\n",
    "3. After we set up <font color=INDIANRED>SparkContext</font> as <font color=INDIANRED>sc</font>, we use <font color=INDIANRED>textFile()</font> to read data from HDFS and create an RDD of strings, named <font color=INDIANRED>textRDD</font>.\n",
    "4. Next, split each line by using lambda function. <font color=INDIANRED>flatMap()</font> applies the lambda function to all elements of RDD and flattening all results. So far, we get a RDD of terms.\n",
    "5. To convert terms into (key, value) pairs, we use <font color=INDIANRED>map()</font> on RDD. Here previous <font color=INDIANRED>flatMap()</font> and current <font color=INDIANRED>map()</font> together work as the mapper in Hadoop MapReduce.\n",
    "6. <font color=INDIANRED>reduceByKey()</font> works as reducer in Hadoop MapReduce. It adds up all values with same intermediate key, therefore the final value is the frequence of each word in README.md.\n",
    "7. <font color=INDIANRED>saveAsTextFile()</font> save the RDD to <font color=INDIANRED>s3a://tutorial-688/output/wordcount_output</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Apache Spark\n",
    "\n",
    "In this section, I'll introduce some ideas and conceps of Apache Spark. All of these are pretty important when you design and implement a spark program and helpful when you try to optimize your spark programs.\n",
    "\n",
    "### Actions and Transformations\n",
    "\n",
    "Transformations derive new RDDs from current RDDs. Transformations are lazy operations, which aren't actually executed unless they meet an action. Results of actions aren't RDDs and most of them are stored in memeory, such as reduce(), count(), collect() and so on. The lazy policy of transformations can reduce the overhead of computation and provide Spark opportunities to compress and optimize programs. Using transformations builds RDD lineages, which can be mapped into a Directed Acyclic Graph for RDDs. With this graph, lost RDDs can recover from its parents. It's the heart of fault tolerance in Spark.\n",
    "\n",
    "Advice: Use as less as possible actions when designing your Spark programs.\n",
    "\n",
    "### Wide and Narrow Dependencies\n",
    "\n",
    "* Narrow dependencies (as NDs): Each child RDD partition only need one parent RDD partition. In above WordCount example, <font color=INDIANRED>flatMap()</font> and <font color=INDIANRED>Map()</font> are NDs transformations. Because each RDD is independent during transformations, NDs transformations can be executed within previous work nodes, therefore NDs are pretty fast.\n",
    "\n",
    "\t```\n",
    "\t(child RDD partition) <---- (parent RDD partition)\n",
    "\t```\n",
    "\n",
    "* Wide dependencies (as WDs): Each child RDD partition may need mutiple parent RDD partitions. WDs transformations always accompany with **shuffle** (same in MapReduce). Shuffle reorganizes all partitions. Therefore, shuffle is very expensive, for it involves transferring over network and [straggler](http://pages.cs.wisc.edu/~dkhan/sparkstraglers.pdf) problems. In above WordCount programe, <font color=INDIANRED>reduceByKey()</font> is a WD transformation, shuffle operation make sure all partition pairs with same key stored in same worker node.\n",
    "\n",
    "\t```\n",
    "\t                      <---- (parent RDD partition 0)\n",
    "\t(child RDD partition) <---- (parent RDD partition 1)\n",
    "\t                      <---- (parent RDD partition 2)\n",
    "\t```\n",
    "\t\n",
    "![dependencies](http://chuantu.biz/t6/269/1522566825x-1404812823.png)\n",
    "<center>Dependencies in above WordCount program</center>\n",
    "                        \n",
    "Advice: Avoid to use narrow dependencies as much as possible.\n",
    "\n",
    "### Apache Spark Moniter UI\n",
    "\n",
    "Each action forms a **job**. NDs transformations between WDs form **stage**. Each partition in one stage is called a **task**. Each job involves multiple stages.\n",
    "\n",
    "Here we can get an equation: NumberOfTasks = NumberOfPartitions * NumberOfStages\n",
    "\n",
    "![stage](http://chuantu.biz/t6/269/1522566895x-1566683256.png)\n",
    "\n",
    "During the execution of the program, you can visit <font color=INDIANRED>http://[master-node-public-dns]:4040</font> for Monitor UI.\n",
    "\n",
    "To see passed tasks, visit <font color=INDIANRED>http://[master-node-public-dns]:18080</font>.\n",
    "\n",
    "### Partition\n",
    "\n",
    "By default, large files in HDFS are divided into partitions with size of 64MB. In above WordCount program, <font color=INDIANRED>textFile(file_name)</font> will create an RDD with one partition if the file size is smaller than 64MB. Therefore, there is only one task for each stage. In Spark, each task is assigned to one core of one slave machine. It means only one core of one slave machine works during the whole execution period. Thus, we pay much attention to set partition number when designing our Spark program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the minimum partition number\n",
    "\n",
    "textRDD = sc.textFile(\"/input/README.md\", minPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Suggestions\n",
    "\n",
    "### 1. Don't forget to use broadcast variables.\n",
    "\n",
    "Spark has two types of shared variables, **broadcast variables and accumulators**. Due to lazy policy in Spark, accumulators are not reliable and may get some unexpected results, so I don't recommand to use accumulators. Accompanied with lambda function in Python, **broadcast variables** are very convenient. Here is an example of using broadcast variables to count stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_set = set(stop_words)\n",
    "bc_set = sc.broadcast(stop_words_set)\n",
    "\n",
    "textRDD = sc.textFile(\"/input/README.md\")\n",
    "counts = (textRDD.flatMap(lambda line: line.split(' '))\n",
    "                .map(lambda term: 1 if term in bc_set.value else 0)\n",
    "                .reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Resonable partitions.\n",
    "\n",
    "Usually **TotalCores * 4** as the partition number, where **TotalCores = number of slaves * number of cores for each slave**. I highly recommand you to partition the RDD when creating it. You can either assign the partition number in <font color=INDIANRED>parallelize()</font> and <font color=INDIANRED>textFile()</font> or use <font color=INDIANRED>RDD.repartition()</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cache RDDs.\n",
    "\n",
    "Avoid frequent use results can avoid repeating computation. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textRDD= sc.textFile(FILE_NAME)\n",
    "sourceRDD = textRDD.flatMap(...)\n",
    "sourceRDD.cache()\n",
    "# sourceRDD.persist(pyspark.storagelevel.StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "RDDa = sourceRDD.map(...)\n",
    "RDDb = sourceRDD.map(...)\n",
    "\n",
    "RDDa.count()\n",
    "RDDb.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With <font color=INDIANRED>sourceRDD.cache()</font> RDDb can use **sourceRDD** without executing <font color=INDIANRED>textRDD.flatMap(...)</font>, for action <font color=INDIANRED>RDDa.count()</font> has gotten the result.\n",
    "\n",
    "When the object is very large (can't be stored in Memeory), I recommand to use <font color=INDIANRED>RDD.persist(MEMORY_AND_DISK)</font> to replace <font color=INDIANRED>RDD.cache()</font>.\n",
    "\n",
    "Additionally, when the cached object is going to be updated, remember to use <font color=INDIANRED>RDD.unpersist()</font> first, then update it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedRDD.unpersist()\n",
    "cachedRDD = cachedRDD.map(...)\n",
    "cachedRDD.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others\n",
    "\n",
    "1. Keep locality of large size RDDs if possible.\n",
    "2. Don't break original data structure if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Calculate Pi\n",
    "\n",
    "In this example, we use [Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration) to caculate Pi. The idea is very simple, we cast beans into a 2x2 squre and use probability of beans in cycle to estimate the area of the cycle.\n",
    "\n",
    "With Spark, all slave nodes can work simultaneously and master node can add all results together.\n",
    "\n",
    "![Monte Carlo](http://chuantu.biz/t6/269/1522566910x-1566683256.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from random import uniform\n",
    "\n",
    "APP_NAME = \"Spark Pi\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "    total_cores = int(sys.argv[1])\n",
    "    \n",
    "    # total number of beans\n",
    "    total_n = 1000000 * total_cores\n",
    "    task_list = [1000000] * total_cores\n",
    "\n",
    "\n",
    "    def monte_calor(n):\n",
    "        '''casting beans in square\n",
    "\n",
    "        :param n: number of beans\n",
    "        :return: [0, 1, 0, 1, ...], 0 means not in cycle, 1 means in cycle\n",
    "        '''\n",
    "        ret = []\n",
    "        for i in range(n):\n",
    "            x = uniform(-1, 1)\n",
    "            y = uniform(-1, 1)\n",
    "\n",
    "            # <= 1 means in cycle\n",
    "            ret.append(1 if x ** 2 + y ** 2 <= 1 else 0)\n",
    "        return ret\n",
    "\n",
    "    # count is number of beans in cycle\n",
    "    count = (sc.parallelize(task_list, total_cores)\n",
    "             .flatMap(monte_calor)\n",
    "             .reduce(lambda a, b: a + b))\n",
    "\n",
    "    pi = 4.0 * count / total_n\n",
    "\n",
    "    print(\"Spark Pi = {:.7f}\".format(pi))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Spark Pi = 3.1410320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit pi.py 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Logistic Regression\n",
    "\n",
    "In this example, we try to use Spark to do logistic regression.\n",
    "\n",
    "Remember to install numpy in your cluster before executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flintrock run-command test-cluster 'sudo yum install -y gcc'\n",
    "flintrock run-command test-cluster 'pip install --user numpy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the process and test the correctness of the LR program directly, I generate a binary classification dataset, which follows linear distribution.\n",
    "\n",
    "Dataset Link: <https://s3.amazonaws.com/tutorial-688/final_data>\n",
    "\n",
    "![dataset](http://chuantu.biz/t6/269/1522566850x-1566683256.png)\n",
    "<center> Virtulization of the Dataset </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "\n",
    "APP_NAME = \"Spark LR\"\n",
    "INPUT_FILE = \"s3a://tutorial-688/data_small.csv\"\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''sigmoid\n",
    "\n",
    "    :param x: x = X.dot(w)\n",
    "    :return: probability of y = 1\n",
    "    '''\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "def gd_partition(samples, weights):\n",
    "    '''get weight updates\n",
    "\n",
    "    :param samples: list, training samples\n",
    "    :param weights: np.array(), weights for current iteration, shape: (1 x num_features)\n",
    "    :return: np.array(), updates of weights, shape: (1 x num_features)\n",
    "    '''\n",
    "    weight_updates = np.zeros(num_features)\n",
    "\n",
    "    for sample in samples:\n",
    "        label = float(sample[0])\n",
    "        value = sample[1:]\n",
    "        value = np.array(map(lambda x: float(x), value))\n",
    "\n",
    "        pred = sigmoid(value.dot(weights))\n",
    "        diff = label - pred\n",
    "\n",
    "        weight_updates += alpha * (diff * value + L * weights)\n",
    "\n",
    "    return weight_updates\n",
    "\n",
    "\n",
    "def pred_partition(samples, weights):\n",
    "    '''predict the label of current samples\n",
    "\n",
    "    :param samples: list, training samples\n",
    "    :param weights: np.array(), shape: (1 x num_features)\n",
    "    :return: list, [0, 1, 0, 1, ...] 0 means wrong, 1 means correct\n",
    "    '''\n",
    "    ret = []\n",
    "    for sample in samples:\n",
    "        label = float(sample[0])\n",
    "        value = sample[1:]\n",
    "        value = np.array(map(lambda x: float(x), value))\n",
    "\n",
    "        pred = sigmoid(value.dot(weights))\n",
    "        pred_label = 1 if pred > 0.5 else 0\n",
    "        ret.append((1, 1 if pred_label == label else 0))\n",
    "    return ret\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    total_cores = int(sys.argv[1])\n",
    "    max_iters = int(sys.argv[2])\n",
    "    num_features = int(sys.argv[3])\n",
    "\n",
    "    # according to optimization 2\n",
    "    partition_number = total_cores * 4\n",
    "\n",
    "    w_initial_value = 0.001\n",
    "    alpha = 0.01\n",
    "    alpha_decay = 0.95\n",
    "    # L2 regularization parameter\n",
    "    L = 0.01\n",
    "\n",
    "    sc = SparkContext(appName=APP_NAME)\n",
    "\n",
    "    t0 = time.time()\n",
    "    samples_rdd = sc.textFile(INPUT_FILE, partition_number).map(lambda x: x.split(','), preservesPartitioning=True)\n",
    "    samples_rdd.cache()\n",
    "\n",
    "    w = np.ones(num_features) * w_initial_value\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        print(\"Now is {}th iteration, complete {:.2f}\".format(i+1, (i+1)/max_iters))\n",
    "\n",
    "        # apply updates to current weights\n",
    "        w += samples_rdd.mapPartitions(lambda x: gd_partition(x, w)).reduce(lambda a, b: a + b)\n",
    "        alpha *= alpha_decay\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Training complete, time cost is {:.2f}m.\".format((t1-t0)/60))\n",
    "\n",
    "    # mapPartitions() can convert each partition of the source RDD into multiple elements of the result\n",
    "    # mapPartitions() is called once for each Partition\n",
    "    res = samples_rdd.mapPartitions(lambda x: pred_partition(x, w)).reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "    t2 = time.time()\n",
    "    print(\"Test complete, time cost is {:.2f}m.\".format((t2-t1)/60))\n",
    "    print(\"Accuracy: {:.2f}\".format(ret[1]/ret[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run it for 100 iterations. Results are as following, as you can see accuracy can achieve 98%."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training complete, time cost is 122.73s.\n",
    "Test complete, time cost is 1.03s.\n",
    "('Weights = ', array([-7.25812393,  3.20584195]))\n",
    "Accuracy: 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous code, I use <font color=INDIANRED>mapPartitions()</font> instead of <font color=INDIANRED>map()</font>. The reason is <font color=INDIANRED>mapPartitions()</font> is faster than <font color=INDIANRED>map()</font>, for it only execute one operation for each partition. In addition, we add <font color=INDIANRED>preservesPartitioning=True</font> when we create the initial RDD, this is corresponding to 1st of Others Topic of Optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this code, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit spark_lr.py 4 100 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and references\n",
    "\n",
    "This tutorial is a basic introduction of spark programming, including the motivation for data scienctists to learn Spark, basic Spark python api and ideas of optimization. However, some concepts may be hard to understand for beginners. The best learning route is run to some baisc examples first, like **wordcount** and **calculate pi**. You may meet some trouble when understanding the **Logistic Regression** example, at that time, reivewing above advanced and optimization sections would be a good choice.\n",
    "\n",
    "1. [Spark original paper](https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf)\n",
    "2. [MapReduce original paper](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf)\n",
    "3. [RDD programming guide (APIs)](http://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "4. [Spakr quick start](http://spark.apache.org/docs/latest/quick-start.html)\n",
    "5. [Narrow and wide dependencies](https://github.com/rohgar/scala-spark-4/wiki/Wide-vs-Narrow-Dependencies)\n",
    "6. [Apache Spark examples](https://spark.apache.org/examples.html)\n",
    "7. [Spark Moniter UI](https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
