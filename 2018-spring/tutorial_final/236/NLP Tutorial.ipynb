{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my assignment, I will teaching users how to create a bigram language model for text generation and a simple character Recurrent Neural Network (RNN) and comparing the quality of output to that of text generated by a neural network using pytorch. To determine which model is better, I will be checking for the percentage of sentences were gramatically correct. I will also be checking for training speed of each model to compare, if one model is significantly slower than the other to train, if the tradeoff in quality is worth it.\n",
    "\n",
    "All of our dataset (books) we use are from Project Gutenberg (http://www.gutenberg.org/ebooks/). I have scrubbed out the top and bottom portions such as the index/table of contents and references to Gutenberg Project in order to get a more accurate model. I do not claim ownership of any of the books/.txt files in this directory. If there is a problem with me scrubbing out those portions of text, please let me know and I will reupload this project with the originals.\n",
    "\n",
    "For our weighting function, we will be using f(x) = log_10(x + 1) for frequency normalization as we would be working with very large data sets.\n",
    "\n",
    "First, we import all the necessary libraries needed to run our bigram code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import language_check\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with this setup, we need to start collecting data, which in this case, would be text files of several books that are available online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_raw(titles):\n",
    "    books = []\n",
    "    for book in titles:\n",
    "        with open(book, 'r') as f:\n",
    "            books.append(f.read())\n",
    "    return books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the acquired raw data, we now need to clean up the inputs so that we have a list of list of words. We need to get rid of empty lines, random white spaces, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(books):\n",
    "    for i in range(len(books)):\n",
    "        books[i] = list(filter(lambda s: len(s) > 0, re.split(r'[\\s\"]', books[i])))\n",
    "    return books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have clean text to work with, we can create our Markov chain data structure. We will be using a nested defaultdict that takes care of how often a word appears in the document. Using the counts, we will then iterate through every bigram and find the max occurence/frequency to calculate new weights to each word. After we train our Markov chain model, we should be able to store that nested dictionary into a file where we can reload our trained data if needed.\n",
    "\n",
    "For generation, we will randomly choose a bigram/key from our outer dictionary (each one will have equal weight), and then we would chain off more words to append to that sentence based on the weights we calculated until we reach a sentence ending punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MarkovChain:\n",
    "    def __init__(self):\n",
    "        self.tree = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "        self.final_weights = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "\n",
    "    ##Training function, grabs raw counts of each bigram\n",
    "    def add_words(self, text):\n",
    "        for i in range(len(text) - 2):\n",
    "            key_1 = text[i]\n",
    "            key_2 = text[i+1]\n",
    "            value = text[i+2]\n",
    "            self.tree[(key_1, key_2)][value] += 1\n",
    "            \n",
    "    ##Weighting function, used to generate new sentences based on normalized frequency\n",
    "    def reevaluate_weights(self):\n",
    "        for keys in self.tree:\n",
    "            for val in self.tree[keys]:\n",
    "                self.final_weights[keys][val] = math.log10(self.tree[keys][val] + 1)\n",
    "\n",
    "    def generate(self):\n",
    "        EOS = [\".\", \"!\", \"?\"]\n",
    "        if len(self.tree) == 0:\n",
    "            return \"\"\n",
    "        sentence = random.choice([(k1,k2) for (k1,k2) in self.tree if k1[0].isupper()])\n",
    "        current_bigram = sentence\n",
    "        sentence = sentence[0] +\" \" + sentence[1]\n",
    "        #If our sentence is longer than 50 words, just cut it\n",
    "        limit = 50\n",
    "        counter = 0\n",
    "        while True:\n",
    "            if counter == 50:\n",
    "                return sentence\n",
    "            if current_bigram not in self.tree:\n",
    "                return sentence\n",
    "            if sentence[-1] in EOS:\n",
    "                return sentence\n",
    "            weights = self.final_weights[current_bigram]\n",
    "            p = []\n",
    "            values = []\n",
    "            for word in weights:\n",
    "                values.append(word)\n",
    "                p.append(weights[word])\n",
    "            #random.choices returns a list, so we only want the first element in that list\n",
    "            word = random.choices(values, weights = p)[0]\n",
    "            current_bigram = (current_bigram[1], word)\n",
    "            sentence += \" \" + word\n",
    "            counter += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating this data structure, all we need to do for the bigram portion is to create a wrapper function that runs all our code and times the training portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th sentence: Astyoche a noble father, and sallied forth, Ulysses leading the way they had entered the din of combat, or Jove will be much of it.\n",
      "10th sentence: In somewhat the same gesture with his long spear.\n",
      "20th sentence: Jove devised evil against me.\n",
      "30th sentence: Scheria and went back when the housekeeper give him good advice of which, however, he is son neither to right nor ruth?\n",
      "40th sentence: Apheidas, who is on the round of the two valiant heroes, Thrasymedes and Antilochus, valiant warriors; all did he go to bed, Here comes a candle to light you to drink, it dried up and shake themselves like a simpleton.\n",
      "50th sentence: Achilles as may escape.\n",
      "60th sentence: Africa, or the cruel knife in the liver below the elbow, and the sea; the moon also at fighting from chariots, and the animals into the vestibule of the dead.\n",
      "70th sentence: Minerva vouchsafed me victory.\n",
      "80th sentence: Honour him then and there: Jove, however, sent the chiefest of their oars, for the wrongs you have failed to break.\n",
      "90th sentence: Gorn right out, and he scowled on Calchas and said, Minerva and Juno has brought you within Achilles' tent, Achilles will lay the largest, and I, who in fact are largely uninhabited and unexplored: but the hand grenade are still in his breath.\n",
      "100th sentence: Minerva leading him by Iphitus, had not Ulysses made no difference between wealth and poverty.\n",
      "110th sentence: Pirithous, and the ghosts whine and squeal as Mercury the healer of sorrow with even hand between them.\n",
      "120th sentence: Nausicaa, who began wondering about her waist had grown old here he had instructed Whymper to spread tidings of my eyes behold a man now left him.\n",
      "130th sentence: Minerva, would shrink from flinging himself into a deep tenderness, such as poets love to tease, whereon any one here.\n",
      "140th sentence: Olympus; worry about him than that man was making straight for Snowball, at the unbelievable luxury, at the poster, the muzzle of the cattle to some great forest fire roar more fiercely when their master's car behind them.\n",
      "It took 1.7375109195709229 seconds to finish training the markov chain and produced an accuracy of 0.3933333333333333\n"
     ]
    }
   ],
   "source": [
    "def main_mark():\n",
    "    #Add more book titles here as you \n",
    "    titles = ['1984.txt', 'animal_farm.txt', 'illiad.txt', 'odyssey.txt']\n",
    "    books = load_raw(titles)\n",
    "    books = clean(books)\n",
    "    mk = MarkovChain()\n",
    "    t0 = time.time()\n",
    "    for book in books:\n",
    "        mk.add_words(book)\n",
    "    mk.reevaluate_weights()\n",
    "    t1 = time.time()\n",
    "    tool = language_check.LanguageTool('en-US')\n",
    "    running_count = 0\n",
    "    for i in range(150):\n",
    "        mk_sentences = mk.generate()\n",
    "        if tool.check(mk_sentences):\n",
    "            running_count += 1\n",
    "        if i%10 == 0:\n",
    "            print(str(i)+ \"th sentence: \" + mk_sentences)\n",
    "    accuracy = (1 - running_count/150)\n",
    "    print(\"It took \" + str(t1 - t0) + \" seconds to finish training the markov chain and produced an accuracy of \" + str(accuracy))\n",
    "\n",
    "main_mark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this code, we see that most of the time is spent actually generating the sentences, but we see that about half of the generated sentences are actually grammatically correct. Of course, this grammar accuracy will probably be better if we used a trigram (or higher) model, but as we learned in class, the more words you add to the keys to our markov chain, the more closely our text will be to the original, so our strings wouldn't be as unique. Now, we will create a neural network that will do the same thing. I believe that the training will take longer, but I expect the quality of the output should be better (assuming we run enough epochs for training).\n",
    "\n",
    "For this portion, I received a lot of help from this youtube video, where the engineer went over how to use Tensorflow and how to create a simple character RNN. \n",
    "\n",
    "We will be using pytorch for our framework to create a character RNN, so let us import the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#Variable is a wrapper around tensor objects retains all operations that\n",
    "#occur on that tensor\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this neural network, we want to take all the dataset and combine them into one list, and then we want to break down the input into chracters. This is done so that we can essentially create a probability array that will help us determine which character comes next in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '&': 4, \"'\": 5, '(': 6, ')': 7, '*': 8, '+': 9, ',': 10, '-': 11, '.': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, ';': 24, '=': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, '[': 53, ']': 54, 'a': 55, 'b': 56, 'c': 57, 'd': 58, 'e': 59, 'f': 60, 'g': 61, 'h': 62, 'i': 63, 'j': 64, 'k': 65, 'l': 66, 'm': 67, 'n': 68, 'o': 69, 'p': 70, 'q': 71, 'r': 72, 's': 73, 't': 74, 'u': 75, 'v': 76, 'w': 77, 'x': 78, 'y': 79, 'z': 80}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '&', 5: \"'\", 6: '(', 7: ')', 8: '*', 9: '+', 10: ',', 11: '-', 12: '.', 13: '0', 14: '1', 15: '2', 16: '3', 17: '4', 18: '5', 19: '6', 20: '7', 21: '8', 22: '9', 23: ':', 24: ';', 25: '=', 26: '?', 27: 'A', 28: 'B', 29: 'C', 30: 'D', 31: 'E', 32: 'F', 33: 'G', 34: 'H', 35: 'I', 36: 'J', 37: 'K', 38: 'L', 39: 'M', 40: 'N', 41: 'O', 42: 'P', 43: 'Q', 44: 'R', 45: 'S', 46: 'T', 47: 'U', 48: 'V', 49: 'W', 50: 'X', 51: 'Y', 52: 'Z', 53: '[', 54: ']', 55: 'a', 56: 'b', 57: 'c', 58: 'd', 59: 'e', 60: 'f', 61: 'g', 62: 'h', 63: 'i', 64: 'j', 65: 'k', 66: 'l', 67: 'm', 68: 'n', 69: 'o', 70: 'p', 71: 'q', 72: 'r', 73: 's', 74: 't', 75: 'u', 76: 'v', 77: 'w', 78: 'x', 79: 'y', 80: 'z'}\n"
     ]
    }
   ],
   "source": [
    "titles = ['1984.txt', 'animal_farm.txt', 'illiad.txt', 'odyssey.txt']\n",
    "books = \"\"\n",
    "for book in titles:\n",
    "    with open(book, 'r') as f:\n",
    "        books += f.read() + \"\\n\"\n",
    "            \n",
    "characters = sorted(list(set(books)))\n",
    "c2i = {c: i for i, c in enumerate(characters)}\n",
    "i2c = {i: c for i, c in enumerate(characters)}\n",
    "\n",
    "print(c2i)\n",
    "print(i2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now set up create the recurrent neural network class that pytorch has. The nn api can be found here: http://pytorch.org/docs/master/nn.html. A quick explanation of each field can be found above each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        #Size of our feature vector (all unique characters that appear in our text)\n",
    "        self.input_size = input_size\n",
    "        #Number of nodes in your hidden layer\n",
    "        self.hidden_size = hidden_size\n",
    "        #Size of output vector\n",
    "        self.output_size = output_size\n",
    "        #Number of recurrent layers in the neural network\n",
    "        self.n_layers = n_layers\n",
    "        #Encoder is used to take input and convert it to a context vector\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        #GRU is essentially the \"middle step\" between the encoding process and the decoding\n",
    "        #process.\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        #Decoder's job is to take the internal representation of the output and convert it\n",
    "        #back to a readable sentence (into characters)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        #Initializes the hidden layer of the neural network, represented by n x m matrix,\n",
    "        #where n is the number of layers and m is the size of the hidden layer\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "    \n",
    "    def forward(self, input_tensor, hidden):\n",
    "        #Function run by pytorch that will be run for each epoch (iteration of training)\n",
    "        #Converts our input vector that's usable by the GRU\n",
    "        input_encoded = self.encoder(input_tensor.view(1, -1))\n",
    "        encoded_output, hidden = self.gru(input_encoded.view(1, 1, -1), hidden)\n",
    "        decoded_output = self.decoder(encoded_output.view(1, -1))\n",
    "        return decoded_output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our neural network, we need to take small chunks of our original text in order to establish a probability matrix that will allow the network to determine what the next character should be. Essentially, if our chunk of text is \"ABCDEF\", then our initial input vector would be \"ABCDE\" and our target vector that we want our network to achieve would be \"BCDEF\". However, keep in mind, too small of chunk size will cause our training time to take forever, but too large of chunk size will give us innacurate/meaningless results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oft and flung it down, smashing Epicles' four-crested helmet\n",
      "so that the bones of his head were crush\n"
     ]
    }
   ],
   "source": [
    "#Change accordingly\n",
    "chunk_size = 100\n",
    "\n",
    "def get_random_chunk():\n",
    "    start = random.randint(0, len(books) - chunk_size)\n",
    "    end = start + chunk_size + 1\n",
    "    return books[start:end]\n",
    "\n",
    "print(get_random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each random chunk we generate must be turned into a \"tensor\", which is just a fancy way of calling our input a list of longs (each character in the input must be converted to their corresponding index that they appear in the feature vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27\n",
      " 28\n",
      " 29\n",
      " 30\n",
      " 31\n",
      " 66\n",
      " 67\n",
      " 68\n",
      " 69\n",
      " 70\n",
      "[torch.LongTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def char_to_tensor(text):\n",
    "    tensor = torch.zeros(len(text)).long()\n",
    "    for i in range(len(text)):\n",
    "        tensor[i] = c2i[text[i]]\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_to_tensor(\"ABCDElmnop\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our helper functions needed to create the training vectors, let's actually create a function that gives us the input vector and the target vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_data():\n",
    "    chunk = get_random_chunk()\n",
    "    #Everything but the last element\n",
    "    input_vector = chunk[:-1]\n",
    "    input_tensor = char_to_tensor(input_vector)\n",
    "    #Everything but the first element\n",
    "    target_vector = chunk[1:]\n",
    "    target_tensor = char_to_tensor(target_vector)\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have many of the helper functions written to provide data into our RNN, we now need to create an evaluate function that takes in a initial vector to train on, along with how many characters we want our model to output. We also pass in a temperature parameter, which in the context of a character RNN, is a value between 0 and 1 and, when set extremely low, makes the model's predictions \"conservative\" in that it would make predictions that are correct but made outputs less interesting (lots of repeating outputs, etc.), but you can raise the value to make the model \"confident\" and take more risks, but at the cost of higher volumes of errors. \n",
    "\n",
    "This evaluate function is to take the input string and, using the trained probability distribution, predict the next character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(starting_string, temperature):\n",
    "    #Initialize base configurations to start building up hidden layer\n",
    "    hidden_array = rnn.init_hidden()\n",
    "    input_tensor = char_to_tensor(starting_string)\n",
    "    predicted_output = starting_string\n",
    "    \n",
    "    #Feed input to hidden layer\n",
    "    for p in range(len(predicted_output) - 1):\n",
    "        #We don't care about the output, as this is part of the setup\n",
    "        #phase, so any output up to this point is meaningless\n",
    "        _, hidden_array = rnn(input_tensor[p], hidden_array)\n",
    "    target = input_tensor[-1]\n",
    "    \n",
    "    #Generate characters until you reach the end of sentence. Hard-set a max\n",
    "    #limit on the sentence so that the loop doesn't take forever (ie temperature\n",
    "    #causes uninteresting and looping outputs)\n",
    "    EOS = [\"!\", \".\", \"?\"]\n",
    "    counter = 0\n",
    "    while predicted_output[-1] not in EOS:\n",
    "        #Change limit here\n",
    "        if counter >= 100:\n",
    "            return predicted_output\n",
    "        #Pass in the target to generate next letter\n",
    "        output, hidden_array = rnn(target, hidden_array)\n",
    "        \n",
    "        output_distribution = output.data.view(-1).div(temperature).exp()\n",
    "        predicted_character = i2c[torch.multinomial(output_distribution, 1)[0]]\n",
    "        #Add the character to the final output\n",
    "        predicted_output += predicted_character\n",
    "        target = char_to_tensor(predicted_character)\n",
    "    return predicted_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to write the training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden_array = rnn.init_hidden()\n",
    "    rnn.zero_grad()\n",
    "    calculated_loss = 0\n",
    "    \n",
    "    for i in range(chunk_size):\n",
    "        output, hidden_array = rnn(inp[i], hidden_array)\n",
    "        calculated_loss += criterion(output, target[i])\n",
    "        \n",
    "    calculated_loss.backward()\n",
    "    rnn_opt.step()\n",
    "    \n",
    "    return calculated_loss.data[0]/chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all our functions/classes written, let's put them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0th epoch: loss = 4.412333374023437]\n",
      "[Time since start: 0.44237709045410156]\n",
      "Th5gId?\n",
      "\n",
      "\n",
      "[20th epoch: loss = 3.2843426513671874]\n",
      "[Time since start: 9.052558183670044]\n",
      "Thhfp   eav omneoaeeo  o -i esba\n",
      "r u rv g thor npseee ec  .\n",
      "\n",
      "\n",
      "[40th epoch: loss = 3.4070904541015623]\n",
      "[Time since start: 18.71027684211731]\n",
      "TheoC coet eole\n",
      "nen o s sotuasetsyylieu,ttsp p not ohdtd n hrfeti nrya ortapo e  i,esattto eoednt.\n",
      "\n",
      "\n",
      "[60th epoch: loss = 2.681922607421875]\n",
      "[Time since start: 28.056803941726685]\n",
      "Th renes\n",
      " c ost  hin she scmeu a in heeAr\n",
      "aosy otmes hiar hes wtir ttint .\n",
      "\n",
      "\n",
      "[80th epoch: loss = 2.664523010253906]\n",
      "[Time since start: 38.34944486618042]\n",
      "The arnis fben ton egpes hvan e a  hehe o telsar anaaak hOmtoiIt oreum homgobropre ola an amin\n",
      "irdere on aopheginan heuf ban winAovege on ne Ir eAmceftok tire os tae htot\n",
      "ftas twis om ir han crhahd hin Dheyr.\n",
      "\n",
      "\n",
      "[100th epoch: loss = 2.59959716796875]\n",
      "[Time since start: 49.27732300758362]\n",
      "Thg uri mo roah; sor he or ilereune as cagauys re se\n",
      "she ra oor are \n",
      "vome ner she ti pitheci he ur relis ha he\n",
      "toca whan an poal tanPt has vot to hat ghe ton tatde net theg\n",
      "what gitTtact osimy\n",
      "an le met shogey wo\n",
      "per wire ut sat une te hy he hotd fet ber\n",
      ".\n",
      "\n",
      "\n",
      "[120th epoch: loss = 2.6268798828125]\n",
      "[Time since start: 59.25566482543945]\n",
      "Thir tithe hep mat ald nothe whe met ondd haut the mele-d ale,e,e\n",
      "shond alr, thante.\n",
      "\n",
      "\n",
      "[140th epoch: loss = 2.464527587890625]\n",
      "[Time since start: 68.31831502914429]\n",
      "TheIf lerseir od anf roletd hee firejoteceunn 'lolet abt\n",
      "he os oot netucapeen tod ed uin sortin thep he ocBoland in ald  csate sigend and waTle med sow he rean feopot toth he amelin imit an hanf aldt fenoun trod tsoghe he,e heche, hodt reers I won in anle yaved hott an  uske, at; nad ovoge Moth.\n",
      "\n",
      "\n",
      "[160th epoch: loss = 2.4778433227539063]\n",
      "[Time since start: 78.75699520111084]\n",
      "Th, wep leunw the at hong uim the whir the sanu; we thr' sh.\n",
      "\n",
      "\n",
      "[180th epoch: loss = 2.3421287536621094]\n",
      "[Time since start: 87.49596118927002]\n",
      "Thils the and anuns Olefir the egot, and the sangos ose sos\n",
      "bovor Ion thalgh\n",
      "fos ot nartaeche\n",
      "hit ow tholl to tholes calgon popt the thoand sot he plos\n",
      "of achothid to haund Irie the fos the the for, on,s frarird fad, omestosos rad is othp walgod hamt the wose hit sunos bsoun bepees Yod for algor tham, robse his peere afs in xirer\n",
      "and on ats\n",
      "non angered dacas oghe\n",
      "henesas eshir asild sibirauvedsegaind hinpsurradaBe\n",
      "ud, hhoun boit ados\n",
      "thas and thare slave, whothit Nuud\n",
      "wor voch an,im,s aret-thas os het oth anthe ythe heir pad hald wacher an horerac.\n",
      "\n",
      "\n",
      "[200th epoch: loss = 2.2161285400390627]\n",
      "[Time since start: 97.47276091575623]\n",
      "The bads ithe pothe.\n",
      "\n",
      "\n",
      "[220th epoch: loss = 2.3284623718261717]\n",
      "[Time since start: 105.66119289398193]\n",
      "The be ren thead, the bar singece koped hir be ther emtas the defet in so bo\n",
      "bas the he bened whe et mis as if be ttare wome miltic to\n",
      "lared whad Bided the arsed we the in t\n",
      "the shand thet, il huld agur the folg!\n",
      "\n",
      "\n",
      "[240th epoch: loss = 2.3096102905273437]\n",
      "[Time since start: 114.14794492721558]\n",
      "The end\n",
      "gensen ig cashe fiur the the whiwly thand the the aus ao\n",
      "sunnd egout thart urrenegt at ursay dir.\n",
      "\n",
      "\n",
      "[260th epoch: loss = 2.343064270019531]\n",
      "[Time since start: 122.40238904953003]\n",
      "The\n",
      "on epes\n",
      "of lisen winlen his had the the\n",
      "ther wounte Loung fond, wo\n",
      "sher alile\n",
      "shous she Banlassou thines hothdhoarithe on band bauf harridden she He wefsen hill anding ing thindt In had the Pinle hinn hid moand poed pinese ho Omall, brovegedthe fove 'pon fannell.\n",
      "\n",
      "\n",
      "[280th epoch: loss = 2.396035614013672]\n",
      "[Time since start: 130.66127800941467]\n",
      "The the hast whar the.\n",
      "\n",
      "\n",
      "[300th epoch: loss = 2.4091203308105467]\n",
      "[Time since start: 139.39490008354187]\n",
      "The bak, thapre deile the hout \n",
      "placs coot Worpert the 'bup caell wors\n",
      "mall\n",
      "maun the At me fpealre for an rou she hom of much wow then mor and cous of a\n",
      "cerercallend therind lall cocsrorll for whas plaspupant nand Lall and and the faw wicendy all hour rarltam wangirk firs mame.\n",
      "\n",
      "\n",
      "[320th epoch: loss = 2.128304138183594]\n",
      "[Time since start: 149.93124794960022]\n",
      "This relus ders whens, sall\n",
      "shis the t Are biingrer were on throf ebe- the nans tin\n",
      "are\n",
      "bertin ding the\n",
      "sor her bew\n",
      "the name in ceing a I in he wate whin \"inle on prth'wisthave pan\n",
      "wone herelly bees i in wine coMlinto thiter he.\n",
      "\n",
      "\n",
      "[340th epoch: loss = 2.158394012451172]\n",
      "[Time since start: 159.9467740058899]\n",
      "The is He thas He the Woblder hit on sent fon hoct me coner hes sate the lut wes wurstita\n",
      "arach thrin sofet hiven no the fa Sranged\n",
      " Sun hac ror wi houds boctur\n",
      "soinoseger tos and wanges fraf misoon hing so wanrere and wouist wad in he bet Wet alrof are and sorloone bab hre in beed of wat ben an wotalony the not -t ant of yo bond if of of gresars of so fand son enttit ideld hosten and len algers the hof Eoucsse het the dit of with out the Pan sot and of the to Briccatenasonsiten\n",
      "and moly is the that hon it etolrer bered as \n",
      "him on some the nelery nius woh his of\n",
      "bone anrunging in not wounin?\n",
      "\n",
      "\n",
      "[360th epoch: loss = 2.2671946716308593]\n",
      "[Time since start: 170.39190983772278]\n",
      "Thout the thit of maiturd not ame vastooosson you mire are wour the cotn.\n",
      "\n",
      "\n",
      "[380th epoch: loss = 2.1607403564453125]\n",
      "[Time since start: 180.57886219024658]\n",
      "The, plated sind beas yorerd Poxer, wii reed so if the fould the irle\n",
      "ut end ares the the pois,\n",
      "morses, the slule.\n",
      "\n",
      "\n",
      "[400th epoch: loss = 2.035366973876953]\n",
      "[Time since start: 191.27005100250244]\n",
      "Thing in thind whitt thrice ames of gone the whow siny there buock of was the to were the a briues hore barppeter and had who serwemil forep worce, hemas deselingcincicpes now\n",
      "feanpy fid\n",
      "fors amsosper skused sart saever snocire foricper, eissias kterd msong in salner tarld the there lilur.\n",
      "\n",
      "\n",
      "[420th epoch: loss = 2.4766336059570313]\n",
      "[Time since start: 201.02498412132263]\n",
      "Thy sat soy and\n",
      "trell who shid faseicping\n",
      "reon peibentuur, the whith and inting and of alase winvered there hed amtarging shain when is ceeae whand ich Achill; goge sould the to\n",
      "the of moup the wosirile salrered reond it with to daos awed and dors were theans rord, were and alried of day the same saused cing folled wothert with hont the fearss wqattule thas whemerlobl seller with ith trave forraven stoud arping the rother that dered and be sas cow werey the here of on ofs hered the hessed ove to home argeed and me vouphing The he land to spict.\n",
      "\n",
      "\n",
      "[440th epoch: loss = 1.8717369079589843]\n",
      "[Time since start: 210.83920693397522]\n",
      "This his she ang the yinto as the.\n",
      "\n",
      "\n",
      "[460th epoch: loss = 2.0822413635253905]\n",
      "[Time since start: 219.86910390853882]\n",
      "Thing I he gage torsterered nou had has Umorks a ked are was sos so going a of of peper\n",
      "the goy other peed to Kale of did bean the pouone buplind to from alrester saughs and eimerlent from shiiled our.\n",
      "\n",
      "\n",
      "[480th epoch: loss = 2.3234324645996094]\n",
      "[Time since start: 229.61174416542053]\n",
      "The tres\n",
      "to fall the of the sueghed\n",
      "the faves the couch the to race\n",
      "in but frough womas to the he fase ket\n",
      "tue the to the broistter to the ware the mough wisht erscef his theu, frehy arlre the not at.\n",
      "\n",
      "\n",
      "[500th epoch: loss = 2.065731964111328]\n",
      "[Time since start: 239.0394229888916]\n",
      "Ther this men all wat a of theur- lond the os deos and davetomer to alnitherss, and the to \"the were and of bnad.\n",
      "\n",
      "\n",
      "[520th epoch: loss = 2.0944143676757814]\n",
      "[Time since start: 248.13310599327087]\n",
      "Ther beed.\n",
      "\n",
      "\n",
      "[540th epoch: loss = 2.75775146484375]\n",
      "[Time since start: 258.0115399360657]\n",
      "The others dow the the a than the made cleitward the cave danices the minth the seetell the shronden blomtened on the the pars there and to profte the rle and thout firs Treen con the dous of pery the mated in the pow, the saich the the made T and tat cristor of\n",
      "coyed of I wemor I he and the den Dallinth the made were the the thim, deeveron o\"\n",
      "The O pare tine let of the speliper he rronly coutt the plamtpct of the had the mom the Sekirle\n",
      "neacionr icrantent the spetmy That shave bade had eas to the oprem wot with his other to but mise tas and the not\n",
      "to crack to the son of th\n",
      "yain the thas do to me, the not\n",
      "and and mestaned creath the cre coune.\n",
      "\n",
      "\n",
      "[560th epoch: loss = 2.1447914123535154]\n",
      "[Time since start: 268.1651861667633]\n",
      "Thoung a and for bep and ous to not nut show raced mebindwell when the Plear of \"men and the his he of the sou wore as ledit anifassed meme for see wake she ans as sold of hele\n",
      "for on plaxes then doblys, in secpeine gallixgall could woullyired he the bew saeneof the wabere so hy sat did so leter any botart so uced as wen and beroter, shied\n",
      "and afried and damolrilist of sulling to the wen and\n",
      "reat the habe as sae blew dea sik and reace all ugeald wy who\n",
      "I the sold out of Achears pare, some a the on awall\n",
      "cull \"Atould be beatpec or as of the als th, the hace thellre son hing a he was as; as the was had a rumtore was the puesed the could ele a of to A,\n",
      "and sack of thean her shain wanes the gocle all I as thid soan gointor of had uculled as she you of the a baw le'fkens keing to-fould base\n",
      "anren a comlep of Cour\n",
      "go and wrows of Jover is sould and son end laces-bold rjater hac the pelus; sor\n",
      "and branted gemeren anothe turse ucat othere unde and douk bemard Cour could hre a mius all whe 2as\n",
      "alled trall and\n",
      "guagam, ange, as the water ofrey the-peitering who Thintue mow of trong to osack the ill wact alro-had ame und one hard of It bet all of the ry to a och, cpabes the with ah moth on was who and mame sme pach be come any corner sade have breipeuld haves of there Tare the all at his of I- were reat to bapent all but as him bether of the of cat, for to sow of wans his hy fean the are in man son of were alshe his tralle his; to sill were so be witae for but go  pear and hamares, of the far cund as selrow the are to was of the braed the of bepacing.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[580th epoch: loss = 1.8924320983886718]\n",
      "[Time since start: 279.09805178642273]\n",
      "The cullound choulney men I nohe.\n",
      "\n",
      "\n",
      "[600th epoch: loss = 1.9908256530761719]\n",
      "[Time since start: 287.3945119380951]\n",
      "The he pall be all he alrom Bunners Apen the spily rom sield mould coll yating of the band all had goment a hel in the and is dear kelles for four in congu bep plae and and to bost the hacris hound shinf that Larse he and nou.\n",
      "\n",
      "\n",
      "[620th epoch: loss = 1.857195587158203]\n",
      "[Time since start: 295.95518016815186]\n",
      "Thas shake day ok the fort huves to thought hef a and son you haast hinghuld and thile dame had and fave\n",
      "Jove the in ecous come.\n",
      "\n",
      "\n",
      "[640th epoch: loss = 1.959637908935547]\n",
      "[Time since start: 304.5219941139221]\n",
      "The hering that the matTo I the T Ocompat he It of to hen out of Precmitting the greverer and dany\n",
      "ane were whemO T--\n",
      "T-- proptenen to whephen- betulitcower the ceOt An's in come the the he to they to the granger chearoneriSther the T he trost and spater who the THeir trone, for wory nowS\n",
      "Tpappest ow the\n",
      "hone Ither this were of fratch and all aRting Mocgersely\n",
      "and to to sacht in to the gecempet turpmaren imping the hald lomey it bey to swone lunbataof the tay angers\n",
      "fary had the cowen to, at ta Thom the lep why rame thouse the worseras, gover the kear were So Are now in as the he is tame the goexter to.\n",
      "\n",
      "\n",
      "[660th epoch: loss = 2.090833740234375]\n",
      "[Time since start: 313.38052892684937]\n",
      "Then he shan,\n",
      "who he other mentemones son that inll she fast son of to ent the innotheir this one not but'th of that gold not St who wirlimed hive whith thus sered our all the wulrso inter Joven wordst mone othe mast Dhothed sheend you,\n",
      "by sels the speded oblald they fouthelvered them lrich.\n",
      "\n",
      "\n",
      "[680th epoch: loss = 1.9326345825195312]\n",
      "[Time since start: 322.6876800060272]\n",
      "Ther herss of the laughto fout to bove\n",
      "ould might for veleis you thear the son and cound, malith this if that you beent and but her what he had uns anveres, and the with us\n",
      "oth ling mans the they cous a great of the she toulper of Alous at i seevaluch he cited of had Aciters\n",
      "peorimem to the not I the sherE where go to as that you grong he to with his and mrem nemsest foed of her sit that dean a\n",
      "madens.\n",
      "\n",
      "\n",
      "[700th epoch: loss = 1.9448110961914062]\n",
      "[Time since start: 332.00427079200745]\n",
      "Thel a lest but the well gave earn he be, have fone woh fremwele to of the coix.\n",
      "\n",
      "\n",
      "[720th epoch: loss = 2.2741134643554686]\n",
      "[Time since start: 341.3144631385803]\n",
      "Thent resipes teng Their Osfikes one and they and to Wingh of the on there comrate in a the his was enver were out has an\n",
      "you his sroing kprighe of far of fly comroth now ime of Othey from srown Tin\n",
      "goine and brough it Made from\n",
      "were from frowt the Acheouf of vemartocping lake Tilith\n",
      "rich with had wore and in by who the Troye of Pvavren with andmed of Coves by brorged who down wat four were and for mesAraterd rot him the hem the flird is in akelove cihe\n",
      "in her of\n",
      "here ader of what bing and wils emarrat, and and god in\n",
      "offre my premerilly pyed to comre in fasthous as puranes you crear her here you imong nespist more Pirpisher and now sake, of\n",
      "for a me his not had some of of ich of Mrime\n",
      "of mosk comrome had you do\n",
      "us but premt the have in reanmed in lout leare out me grove mittery and the rivas of\n",
      "there is Thae dome siece it to tilshindire of \n",
      "\n",
      "ever in shis tbere the mighging\n",
      "e cranger of'd\n",
      "Cand in of them.\n",
      "\n",
      "\n",
      "[740th epoch: loss = 1.912694091796875]\n",
      "[Time since start: 352.1993730068207]\n",
      "The the word with the cleded, day rouse sited\n",
      "of Thesed wouse in the he spocce, nosuite war\n",
      "har mied of Artime son.\n",
      "\n",
      "\n",
      "[760th epoch: loss = 1.8701304626464843]\n",
      "[Time since start: 362.11663603782654]\n",
      "Ther shaar sootan farter one there anetipripen as all as and the he bich the chach fars of the she anver as one the the seen prighter in the lom acheamanide\n",
      "a he me orfach a never spinst post\n",
      "the reng of arpt anxed among\n",
      "a the parer as bove and hened\n",
      "an the as pare has blong that the somes the he chebech beeak and the comxs a the never mare amoster and whep of the was among.\n",
      "\n",
      "\n",
      "[780th epoch: loss = 1.888311767578125]\n",
      "[Time since start: 371.645259141922]\n",
      "That thinitsith her would you in in the not fake sonf.\n",
      "\n",
      "\n",
      "[800th epoch: loss = 1.8062223815917968]\n",
      "[Time since start: 381.1553580760956]\n",
      "Thing will the but I stod the was spade to traosed the him there orven resting him a\n",
      "cayselys ounsest in the a vones miress into the loteld in siting Ulen the apne net the St the sonan had or the Mdopes sich the have\n",
      "ton his cVrew shemon and so mace spale uny\n",
      "goct bleest the she, blice the rinsted cinons and and he he I mechiest had\n",
      "sceared teraning aft there alsreantiar.\n",
      "\n",
      "\n",
      "[820th epoch: loss = 2.017245635986328]\n",
      "[Time since start: 391.494145154953]\n",
      "Thes have nown bowlis ont the wich them he bid,\n",
      "and wike the his\n",
      "leadted hood went the gear.\n",
      "\n",
      "\n",
      "[840th epoch: loss = 1.9791604614257812]\n",
      "[Time since start: 400.20489501953125]\n",
      "The a ad amben and had nom and mave and flong and hesed in the he were for you loyou.\n",
      "\n",
      "\n",
      "[860th epoch: loss = 1.7736273193359375]\n",
      "[Time since start: 408.5063931941986]\n",
      "The sulvock you wine a sor, and and cry me bish of manes\n",
      "so frieves my and the shiy or gade form ich I frone nother the did ever of the cored tar me corne nonlaed and shars a sor pear will brecans macs the goover a\n",
      "now fears it of the bouse, on It at at\n",
      "nother nothore on, for at house the pecte the pous not fight shen youn the farm shivoher of the and corramen them of the mail the grechield of stid, this the him hive for Bists and am set your and samed.\n",
      "\n",
      "\n",
      "[880th epoch: loss = 2.3112171936035155]\n",
      "[Time since start: 417.14276003837585]\n",
      "Thes whi her head being outs he.\n",
      "\n",
      "\n",
      "[900th epoch: loss = 1.9208045959472657]\n",
      "[Time since start: 425.37461400032043]\n",
      "There he suised\n",
      "dound the sture is stare sin 'red.\n",
      "\n",
      "\n",
      "[920th epoch: loss = 1.5484242248535156]\n",
      "[Time since start: 433.67203998565674]\n",
      "There by the while theem--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Eand of their she with the the lawts is himsas his his was bed though theull attely wamerss them the was wring\n",
      "his cilller hocme sames would pritine cuddiens the nough the was and of hone, as un, romner, her all as bet them; her shill bling be the\n",
      "sheing in theus, homnus no prest her them hering, weakest on the drew I shill\n",
      "them, as abath the\n",
      "maned the camnimal to sest on the theus, answerte the sown was the heres his bresc whine adather the the were dacels they with and and his the smick in them a tood and mather srunades\n",
      "unting this ebnvan mist his the godb of Fail of at lept the blight and the agristans.\n",
      "\n",
      "\n",
      "[940th epoch: loss = 1.7424908447265626]\n",
      "[Time since start: 442.5612668991089]\n",
      "Ther as upon ive onitasting fariman all them, the for them fremmaman, all the kiighter\" Indorgas as us.\n",
      "\n",
      "\n",
      "[960th epoch: loss = 1.782334747314453]\n",
      "[Time since start: 452.5225422382355]\n",
      "Theon\n",
      "the fore the wall over and the shictrone suight a thich trous, ecangoory muck be betriom-I----------- On the the land the shound, forde the Aveps Nand to comble in migs ament itirned on over.\n",
      "\n",
      "\n",
      "[980th epoch: loss = 1.9930654907226562]\n",
      "[Time since start: 463.7609670162201]\n",
      "Ther con theack for be mariung.\n",
      "\n",
      "\n",
      "[1000th epoch: loss = 2.0418426513671877]\n",
      "[Time since start: 472.8429288864136]\n",
      "Thhat the all our, in the yened Tgod was elgealers will armuthed heir' and son of which readyseres of here from It heard.\n",
      "\n",
      "\n",
      "[1020th epoch: loss = 1.8899559020996093]\n",
      "[Time since start: 482.0986440181732]\n",
      "The leverd another over the mafornunent the were rivily tell the padle and other\n",
      "all had with and pried of the haf farthing and thom for your front betules, his leving him word into the her beods they hed baeans, he hing\n",
      "enus and brarach carchasicend and that had did of lid, that a slong, of the his gamind of umwis dea hard maned had and with I criven wolle and sake lving and were berand; the be the klied barband might with Jetomerwand, down mane his brooden, and came in the ordiby had atther for he theret, and by ving feary, and was in he same\n",
      "him, the gogs\n",
      "somen, he with alcade, from that of he mack to should ittuble that of the had and on where takean on the durcationon th, him, and frener but the mouD, fromfans that wtHater therefaned him.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1040th epoch: loss = 1.9015524291992187]\n",
      "[Time since start: 492.2172210216522]\n",
      "There intalk to theaghter of the selnated, and\n",
      "algo the had and to head in the mept oven there\n",
      "sard the keet what Jove\n",
      "and fet holler and houm lechaid the hald of the some into hen the hous to Oll neveleadling handound we the peal house laive will on the with there Avent, and fain the Cy a shild feats to whom will he tell and theard the would so solloundingul; the other, vark and Danded, and the melm exet the was coured and the had down vay and some that meen gatles, to the gods.\n",
      "\n",
      "\n",
      "[1060th epoch: loss = 1.820958709716797]\n",
      "[Time since start: 501.92414116859436]\n",
      "Then ee the when preta in the sic, the goce to intuans sing evear the be Crose intering\n",
      "of d2he\n",
      "cuxersiar.\n",
      "\n",
      "\n",
      "[1080th epoch: loss = 1.894901580810547]\n",
      "[Time since start: 511.57055401802063]\n",
      "The he Hether, for'f thatwece had mat go been tod be piae the gresed, the faced, wime the lead him gody may, down filled wouch of for and\n",
      "of Tanded of Jetors of sould the said, wall in mield now leathed say it to the\n",
      "good to the proed from Nows a be.\n",
      "\n",
      "\n",
      "[1100th epoch: loss = 1.9711021423339843]\n",
      "[Time since start: 520.4729747772217]\n",
      "The said the cool the regiverys on a matton what her is the to the prochs; the prited the shans the god.\n",
      "\n",
      "\n",
      "[1120th epoch: loss = 1.8708198547363282]\n",
      "[Time since start: 530.3993520736694]\n",
      "Thas \"Nook of you with is all and the is whatting who shake the one the tather, and drous amon everd heand milling on quined in the\n",
      "bleat ofven sening thed a bove hits all both shall man it aught her\n",
      "the with you were they standss of the vong souy atle abuth mist if the with a deat know a the was very your thought all manstles.\n",
      "\n",
      "\n",
      "[1140th epoch: loss = 2.331950225830078]\n",
      "[Time since start: 540.2627069950104]\n",
      "The me there are on a some are all them a nerice, and the of shen of the whith\n",
      "aston the lantellelipple of heave, and I the song anscappely noccily he chess brest with of with more and asween a kire it to good the fricked thisting but other of to brom of dea; i go the melips bet but had a gody of the strust at\n",
      "upinsting cory the stouse of that he gorly in the sext to4k the kanter was to batt hurforeliousselould othould the gad our one the good another do force I me the and excared\n",
      "as the of boctor to the reat ered the spost for the ast the spice as whis one been and here down the glo-inthoung a rease a mourtay him shish.\n",
      "\n",
      "\n",
      "[1160th epoch: loss = 1.7949639892578124]\n",
      "[Time since start: 549.7305102348328]\n",
      "The son of crey of preable her the shadus, when the make\n",
      "you in a who had of hup o laxt of Sached aponther the greade a fover, anost of Peiper to to they reroons fleen thus worker suttanst ans simmand worE He lover.\n",
      "\n",
      "\n",
      "[1180th epoch: loss = 1.850906524658203]\n",
      "[Time since start: 561.928631067276]\n",
      "Thord in of yet on through some ago.\n",
      "\n",
      "\n",
      "[1200th epoch: loss = 2.3037249755859377]\n",
      "[Time since start: 572.3649649620056]\n",
      "The sonder.\n",
      "\n",
      "\n",
      "[1220th epoch: loss = 1.7907347106933593]\n",
      "[Time since start: 580.9127948284149]\n",
      "The ups---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------G-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------warroton, and with which gamens this way are grord neat for him roussileten the ruarm cither had will though thid arought, as to tent the mosing sit of say a with ritank any-and who wantotrod, and wortius here his wall womtore what to thereinss a rantelone of I beow go bitting the you two his croughting for the bockle peytor, be\n",
      "and whetattwo have not our were liverin were and or thear gare of whinging, for no threat\n",
      "she we saokor, and son aroven\n",
      "or the sather, king a greagesent rivery along two mation of Agacteder of and arounterts, angry\n",
      "and of some to why------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------came seing stayter, and and down, and jade to have what him wine- an and whore of\n",
      "here who have with therey sines and wheptituration it wich therefore\n",
      "to my wanden.\n",
      "\n",
      "\n",
      "[1240th epoch: loss = 1.7082711791992187]\n",
      "[Time since start: 592.3771719932556]\n",
      "Theredie her to himore has a bost oversed the prasped tot compeler.\n",
      "\n",
      "\n",
      "[1260th epoch: loss = 1.7515634155273438]\n",
      "[Time since start: 602.148332118988]\n",
      "This hearvinong prereat to the with name wing the plaans carped,\n",
      "no onour the son myne the dew when the\n",
      "sprave weriter, and horchy jus esproy, the ritears sivelly fhow not for the streng to Trojans, what he spoke of it over his darchided rack Agamened.\n",
      "\n",
      "\n",
      "[1280th epoch: loss = 2.2289883422851564]\n",
      "[Time since start: 612.1794319152832]\n",
      "There stasess with one to the were freas it stary tall to drevers, and the plack,\n",
      "and his wallind, anned of he the carped after all no shown the on that therele tear of in Joving\n",
      "and secong and reane as back and am went of gosbas will was were the forther all cuch not and\n",
      "the stare.\n",
      "\n",
      "\n",
      "[1300th epoch: loss = 1.8761058044433594]\n",
      "[Time since start: 623.2139999866486]\n",
      "The caided for\n",
      "luck even Minesthed evide their with of's like and willed heard the was bether the the looks.\n",
      "\n",
      "\n",
      "[1320th epoch: loss = 1.8139918518066407]\n",
      "[Time since start: 632.8342900276184]\n",
      "There\n",
      "sfostry to Achaeans therached.\n",
      "\n",
      "\n",
      "[1340th epoch: loss = 1.9103694152832031]\n",
      "[Time since start: 641.6730880737305]\n",
      "The stood I preaded and prinktantion of the priencs it an seot no this mustly, furs yet comes, and the flought mone of the stard of that the\n",
      "shourse after his wine his me aremace of Igain them\n",
      "the my that homgssents, and Unousses.\n",
      "\n",
      "\n",
      "[1360th epoch: loss = 1.8453672790527345]\n",
      "[Time since start: 650.905030965805]\n",
      "Then he lything his had cretelesty-falliught her specixy to the hrought till.\n",
      "\n",
      "\n",
      "[1380th epoch: loss = 1.755579833984375]\n",
      "[Time since start: 659.850347995758]\n",
      "The clears\n",
      "forther him onles is\n",
      "and his shoud foot candeess, broighter about could estand the head had fight you an the lode\n",
      "and see\n",
      "the would sit of the spurius, \"\n",
      "'Eman oulledy, but ous and cowride from flood be of grives his inshing his on the Proses one floubled some wide has a him our battiunded.\n",
      "\n",
      "\n",
      "[1400th epoch: loss = 1.6944122314453125]\n",
      "[Time since start: 668.686527967453]\n",
      "Thould with a to him know had not pught been is the do micasman of while the howed were seesion of the prisield it the mook their mat\n",
      "the were falions of hes, he sifed where was a grees, wI\n",
      "Menow, whol shice are it hered about and atter to head bike of his fourels of dresthore to him had all and down thain him, the my self her he will might inter comatisthing, and him.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1420th epoch: loss = 1.7365354919433593]\n",
      "[Time since start: 677.9642598628998]\n",
      "Their werrates ove the frepturpear with he mere his seased daughter on rang-cil was he speales.\n",
      "\n",
      "\n",
      "[1440th epoch: loss = 1.8844252014160157]\n",
      "[Time since start: 687.4005329608917]\n",
      "The by the fioth no good was could neiss a long, and with she sain, the fall were after all he and in the fall women mean\n",
      "of stiod which Aen\n",
      "were unpered on he hould over my Trojansert for have the father the to kirs to be and samming the prony.\n",
      "\n",
      "\n",
      "[1460th epoch: loss = 1.9234956359863282]\n",
      "[Time since start: 696.2488241195679]\n",
      "There my with AOwhile, beging side and with trulse in on their had would not had\n",
      "cows, who on it in the strouse in twit sinece hold in scould and days the ment.\n",
      "\n",
      "\n",
      "[1480th epoch: loss = 1.763748321533203]\n",
      "[Time since start: 707.6624851226807]\n",
      "The hanskice he wamman his head amand waterselging he sameson for hand mages the sent for his Trojans, are bath fath her had you to keates sood hame of the fief one the amarss to the parllly, while the he he they a man of the sheat.\n",
      "\n",
      "\n",
      "[1500th epoch: loss = 1.8362733459472655]\n",
      "[Time since start: 717.1315369606018]\n",
      "The to citenter and son yootord, and of the estremaisude were the shoet srestranes when the heans of Hrean, and no strong He the pister.\n",
      "\n",
      "\n",
      "[1520th epoch: loss = 1.9532269287109374]\n",
      "[Time since start: 725.8481957912445]\n",
      "The woor lonk were and all go come was come bather of her of youne treak with repreaca dige compented mortias had beate)te are in me and by the quered and that he labeane.\n",
      "\n",
      "\n",
      "[1540th epoch: loss = 1.8430429077148438]\n",
      "[Time since start: 733.8762719631195]\n",
      "Thelder had undesed stally had nock helsedy.\n",
      "\n",
      "\n",
      "[1560th epoch: loss = 1.992813720703125]\n",
      "[Time since start: 742.2262909412384]\n",
      "The him of not by you qedand in the desented to as than the got for the leven not ned ming reached their bed is fyonso lose, but with the his is from bloen of is a my linged him, a\n",
      "sely dryrou to no danganing the poads be not in so\n",
      "deass.\n",
      "\n",
      "\n",
      "[1580th epoch: loss = 1.7665699768066405]\n",
      "[Time since start: 751.0862658023834]\n",
      "There ous his as his thunging to sablight alpul aid, \"Mineteallif-for him the dept of Applmess\n",
      "brong the popenothers his some\n",
      "that he he cisenies mist being this him the pouress which who would for cline with stratiant could wood bottle thing son to himshe the food thow sued being it wojay not-were hough\n",
      "you up is doot bean him sight and we shind he tand dot in ve speat imm.\n",
      "\n",
      "\n",
      "[1600th epoch: loss = 1.7591311645507812]\n",
      "[Time since start: 759.3286769390106]\n",
      "They thy rank and by was that a had has the siton, lassue of you brolenss, Ming in the fure warty in is.\n",
      "\n",
      "\n",
      "[1620th epoch: loss = 2.0673086547851565]\n",
      "[Time since start: 767.5202498435974]\n",
      "The men\n",
      "the struck with I Parcle such had he bow the indear the ilked son with exelless the death had tornest\n",
      "the stalled at littled up used mall you are their struth; what he cured and alother that a been with he hose is it of from hid tels wither at with he cool his the gear, and for he his buit the sthe Apocaly of are packanit but the core I caunth of I that he cout a mon of as had of nekn-of the mivis said and hosted himsed seeft of him are will cooperate\n",
      "eddean host as out was it eyed and conge my not courd the shipst who I was by the hot the bige, and sight he him, so and spimed when he should canls wind to the slook him hads of the came rouse, for they that conther a take cricthisse it hol of hen cou nect lead house to at heaven his take to hroment are a with duictus of Hethirion got and the from him him eably a mouch made spot men then the plas killening a meadly down which you are it at\n",
      "and son in a sfour a care, thigh his the destrangs dackens-ast or in of might to the castide ragainst should dase, and canipus is beuting of Pited hond,\n",
      "and cart and the core man of home we\n",
      "takesishesing the bige the coch of the grook chead his word their loused\n",
      "in the sause of Aeetors.\n",
      "\n",
      "\n",
      "[1640th epoch: loss = 2.2485931396484373]\n",
      "[Time since start: 776.2385611534119]\n",
      "The come the\n",
      "gods, which be of the which you mo of will went the that all my the mat the palled beour mehes with lid he barked no into the was him with he madked you to more exmy all as only the spealu throw iment of the heave the shelat the gods, for them off traven, dard is lut your gone for he meseliled.\n",
      "\n",
      "\n",
      "[1660th epoch: loss = 1.8397630310058595]\n",
      "[Time since start: 784.2658410072327]\n",
      "The lastand of all has thus but lid did being all ,ind lose that the spoke.\n",
      "\n",
      "\n",
      "[1680th epoch: loss = 1.8479344177246093]\n",
      "[Time since start: 792.3110740184784]\n",
      "Thind his should and fir thold for great farth therefore light grice in the litifely.\n",
      "\n",
      "\n",
      "[1700th epoch: loss = 1.7309941101074218]\n",
      "[Time since start: 800.5179679393768]\n",
      "The cicppas.\n",
      "\n",
      "\n",
      "[1720th epoch: loss = 1.93380859375]\n",
      "[Time since start: 810.7027359008789]\n",
      "The have page himses.\n",
      "\n",
      "\n",
      "[1740th epoch: loss = 1.8367729187011719]\n",
      "[Time since start: 821.1331741809845]\n",
      "The he beadbule with himsIss- of the spare of stallias,\n",
      "bon him tringy the to they Ar of the pirion botn in of the were so mich to do though theses the have saids; the furmmore and woud a kind ture to dectan to\n",
      "the go tam one of the with the from he wace to greime to ble- from Agot the light was slepting him be great Miles, bean, for are and do lind, and and be it the went of they walk, under in trough the coursfured that the not gods\n",
      "of Aturm, with at he farced to the grongs would merno being samtane to the had are for you were eress's a sam the swas left he we he so pebting to lity frasten Achearelys of Aeing round wiher force of these you to him fast and the could a Dant.\n",
      "\n",
      "\n",
      "[1760th epoch: loss = 1.9901866149902343]\n",
      "[Time since start: 832.00377202034]\n",
      "Ther be\n",
      "your\n",
      "keale bucked In dearch unar further he have sheaty quicelsotmly now samt of heamalts, but and far battle the routeps, a canbor sterain of\n",
      "a difgest of a sofer had hearing, leger, but the kesandry farched have was dortan boist, and was say your could getrer of\n",
      "ablied now and were to them.\n",
      "\n",
      "\n",
      "[1780th epoch: loss = 2.4567887878417967]\n",
      "[Time since start: 841.6098351478577]\n",
      "Then the me dood when the othered over over, but the drang- by dirp their kempce mar toolis and moverl them him bnellemwiatent son and pares, and of preaus,\n",
      "bear you be my that methought out a should.\n",
      "\n",
      "\n",
      "[1800th epoch: loss = 1.8024017333984375]\n",
      "[Time since start: 851.5923750400543]\n",
      "The and he Achaean aboling the dount of Aegest it sumate fout others and to with stream were larkle of It dingepus, of insy cite of by gave to ast, he herked to the tandery battles as the flalld of him.\n",
      "\n",
      "\n",
      "[1820th epoch: loss = 1.417508544921875]\n",
      "[Time since start: 861.1540491580963]\n",
      "The to mood heart, near it was as he mave usses now and cheeted him never Cured and the son of could-frespot other in years, where we the houds face blozeinoned this streuman.\n",
      "\n",
      "\n",
      "[1840th epoch: loss = 1.7350633239746094]\n",
      "[Time since start: 870.1535050868988]\n",
      "The were he son of the so bet, and the\n",
      "golles the acean deptur\n",
      "excelligs and thew was he tame and then son of Xit dessing shall god thereach a leptallesius what\n",
      "were by tand tinging, and good a a singe not the down that oble had their was not went bull to jeave cource, as their was do\n",
      "were moarthing as overes outire, bight hearted alcilled more was pu son of tongtake in a near.\n",
      "\n",
      "\n",
      "[1860th epoch: loss = 1.9353921508789063]\n",
      "[Time since start: 879.2294609546661]\n",
      "Then he to aver in a Aress.\n",
      "\n",
      "\n",
      "[1880th epoch: loss = 1.8740559387207032]\n",
      "[Time since start: 887.3627669811249]\n",
      "Thith his shall rike the as.\n",
      "\n",
      "\n",
      "[1900th epoch: loss = 1.7597299194335938]\n",
      "[Time since start: 896.2933042049408]\n",
      "The seent he sweben of the like of intent out not to be anoding to exass.\n",
      "\n",
      "\n",
      "[1920th epoch: loss = 1.7301907348632812]\n",
      "[Time since start: 904.9037001132965]\n",
      "The ganted Clich, you men good whoje ame did hobden to went with that beant made cangear with to got the\n",
      "pliness, and pasing crite to up among his ame that and distable jrittells, he speach of but in the Trojan Crustion at they his could not on the counded rong,\n",
      "and gods at in the had man before mooked and maim in their though them in men had which his bate sprand helen you\n",
      "at.\n",
      "\n",
      "\n",
      "[1940th epoch: loss = 1.7500221252441406]\n",
      "[Time since start: 913.5688209533691]\n",
      "Ther to to beeteling his motore it\n",
      "he spought they was as of able.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1960th epoch: loss = 1.604410400390625]\n",
      "[Time since start: 921.9506349563599]\n",
      "The sowt mast his sprase that eemeding she lest may ourse and his his would among the lined heart the\n",
      "pres in here up, sons through Achilles the my sheesied dluemation.\n",
      "\n",
      "\n",
      "[1980th epoch: loss = 1.7675453186035157]\n",
      "[Time since start: 930.2225339412689]\n",
      "The would a singered, and were Jove custect the sin borcuty, snow with the spipe, but her obing of the being back of it of the at strent an ming retion.\n",
      "\n",
      "\n",
      "[2000th epoch: loss = 1.7463755798339844]\n",
      "[Time since start: 939.4799339771271]\n",
      "The horm first we me meemon the was fight to singed roteder, spew that ano very wento and the by cesomed rest the evesssed with canth on these cheam and the singed of they hay that that at the\n",
      "midliw a could have\n",
      "mongers thought a soddeless.\n",
      "\n",
      "\n",
      "[2020th epoch: loss = 1.9420523071289062]\n",
      "[Time since start: 947.585333108902]\n",
      "That also lite leming the leing his some.\n",
      "\n",
      "\n",
      "[2040th epoch: loss = 1.6016024780273437]\n",
      "[Time since start: 955.8581929206848]\n",
      "The was suipen, the statiops becamssfickelling one.\n",
      "\n",
      "\n",
      "[2060th epoch: loss = 1.7938465881347656]\n",
      "[Time since start: 963.9001688957214]\n",
      "The had\n",
      "being a was not as what we partius who should bold the maus the sauste to was goding the pinsed the goes,\n",
      "not conte of\n",
      "the the was arm and the son onto this opent him a cilling are our to came, the fapence outse, and take bether to made the had montih a ming one of the sain\n",
      "you her uring said,\n",
      "\"but the Achaeaned his grangeares, and that we his for hrees with live Waterfor through I was had on the prase in and the sort\n",
      "sharing\n",
      "acblous, and through a serlipe, strot while and will the wind a math the\n",
      "son of a suracaty some one mother.\n",
      "\n",
      "\n",
      "[2080th epoch: loss = 1.589485626220703]\n",
      "[Time since start: 973.368861913681]\n",
      "Th the street in the stong off in dinfered sime was in the sand\n",
      "will back shall suphorius of was borce their shoubcaly to mortirly of with through of threong front brou morself and fime liven that the brack their adtrong in they brand; thigh this not mortuling in it is lam as being is lome fecore.\n",
      "\n",
      "\n",
      "[2100th epoch: loss = 1.7663748168945312]\n",
      "[Time since start: 981.8804838657379]\n",
      "The said.\n",
      "\n",
      "\n",
      "[2120th epoch: loss = 1.7790744018554687]\n",
      "[Time since start: 992.0202839374542]\n",
      "Thus bade.\n",
      "\n",
      "\n",
      "[2140th epoch: loss = 2.3340585327148435]\n",
      "[Time since start: 1001.882071018219]\n",
      "The would be doened you to the Rive other a-pace of the forans for which was reaming, firde Henced of the suved should for ajout did-'nt?\n",
      "\n",
      "\n",
      "[2160th epoch: loss = 2.146659393310547]\n",
      "[Time since start: 1013.7950439453125]\n",
      "Theselfer you other the wither which every son fitter the\n",
      "was what would caving be a peled sal.\n",
      "\n",
      "\n",
      "[2180th epoch: loss = 1.8331280517578126]\n",
      "[Time since start: 1024.1603698730469]\n",
      "The himsed ry as stark care\n",
      "you had go roisely uve matter.\n",
      "\n",
      "\n",
      "[2200th epoch: loss = 1.7751564025878905]\n",
      "[Time since start: 1032.3062081336975]\n",
      "The spear matken of Metoon to mitkand all tabmust with Jove sind bat upon the be a fary over of some, and and such.\n",
      "\n",
      "\n",
      "[2220th epoch: loss = 1.7316419982910156]\n",
      "[Time since start: 1040.5107789039612]\n",
      "Thero wall; and so pawle my made her the man.\n",
      "\n",
      "\n",
      "[2240th epoch: loss = 1.647639923095703]\n",
      "[Time since start: 1050.3121449947357]\n",
      "Ther the war had they rangers dreating\n",
      "not and might the captary and fighting the with the is locks\n",
      "as on the somening in titire that place a prize, and the cater, and made the though the your make the were\n",
      "sheep\n",
      "was back firdes eas the her of accaven and he take as the had beinvers.\n",
      "\n",
      "\n",
      "[2260th epoch: loss = 1.6711329650878906]\n",
      "[Time since start: 1059.555860042572]\n",
      "The qoited they so whiter wherim evering the other true in the battle\n",
      "the daught my me.\n",
      "\n",
      "\n",
      "[2280th epoch: loss = 1.6667160034179687]\n",
      "[Time since start: 1071.6903200149536]\n",
      "The ball verat of dolded the masber himself we alless him long a lights of into hoose to was not gave have ceplees; but starbry that not to has being and\n",
      "for then; and have lousale\n",
      "thought\n",
      "that I Trojans larbourt you the chale\n",
      "the will you been in the said\n",
      "come who en of iles lect of her the gongtary hands be out to there live the portixe insen of the sersed hand shis befoon\n",
      "cwasent\n",
      "litter.\n",
      "\n",
      "\n",
      "[2300th epoch: loss = 1.6295388793945313]\n",
      "[Time since start: 1081.6306660175323]\n",
      "This a tryight them words from hemfictle\n",
      "that me of hel is spear as the glip furthing an Achaes her had blol a glasers all spieting who bouttor them the\n",
      "fewthell of the stulled and\n",
      "herom.\n",
      "\n",
      "\n",
      "[2320th epoch: loss = 1.74264404296875]\n",
      "[Time since start: 1091.0837860107422]\n",
      "The plallaticial of which lest and has ships and him on bekchuls.\n",
      "\n",
      "\n",
      "[2340th epoch: loss = 1.3897683715820313]\n",
      "[Time since start: 1100.354418039322]\n",
      "The him out\n",
      "for of the repone that thee prosent.\n",
      "\n",
      "\n",
      "[2360th epoch: loss = 1.7288931274414063]\n",
      "[Time since start: 1108.3395998477936]\n",
      "Then scroins fleet.\n",
      "\n",
      "\n",
      "[2380th epoch: loss = 1.8295758056640625]\n",
      "[Time since start: 1116.3753190040588]\n",
      "The plep who as the power feels had mather the forsce the starte with theme and the his kechient all not is upon of cargy mhoce chear.\n",
      "\n",
      "\n",
      "[2400th epoch: loss = 1.9525762939453124]\n",
      "[Time since start: 1124.5527839660645]\n",
      "The costered about he before Menery of Man.\n",
      "\n",
      "\n",
      "[2420th epoch: loss = 1.857752227783203]\n",
      "[Time since start: 1133.1720819473267]\n",
      "The came.\n",
      "\n",
      "\n",
      "[2440th epoch: loss = 2.0009609985351564]\n",
      "[Time since start: 1142.3073360919952]\n",
      "The Alless,\" I by vering the so the elong but had bath you and make which the same dayprec the counduraly.\n",
      "\n",
      "\n",
      "[2460th epoch: loss = 1.924208526611328]\n",
      "[Time since start: 1150.8524639606476]\n",
      "Then I eacly of the speus fet then men the faees, and it conty of son of other the mugired so ever to kill to to she\n",
      "have by stold speared, \"filled to the\n",
      "rould\n",
      "been to fulld to her to it the some.\n",
      "\n",
      "\n",
      "[2480th epoch: loss = 1.9118443298339844]\n",
      "[Time since start: 1164.24773311615]\n",
      "Then the flowed at the ggods or his feble to aclop, the floin then go whler that hercer the fair will lake as the get their ship sokinged our the wine youncent in onerthing of the plys of the floched and furllief a me incting they last lack of their floin when cheest\n",
      "goard in the hram his the while say was loft went when when leads.\n",
      "\n",
      "\n",
      "[2500th epoch: loss = 1.6819158935546874]\n",
      "[Time since start: 1176.293620109558]\n",
      "The could mood fittise his stand the songuars kimans outse to chiened out momed of poarc romes not come but I with bethed, and onctureal on the so never the bone their dare beet him, even him the\n",
      "shipes in the octaed that he cond, the mosts he blancelves the tered of tell pached temtrade even sugale the middle kind my had not one a dear who and sont, and he was imnelf of I cold peared wolf of a bine, a sume the cike of could to the was so would even very as to Cother for and was now, cooks krike be graught had from I come me on mide from of did linged do buttle outpare on the were beepor lound the peares going you stong trise diim yours, about the pain, is neyed and remently \"and be of and leftone the the Dein to childed all upumiared rationss in in to bist a dariats and the hought of but to our extent wide fuve out with it water.\n",
      "\n",
      "\n",
      "[2520th epoch: loss = 1.7630844116210938]\n",
      "[Time since start: 1184.9714341163635]\n",
      "Ther almoos flaws tode other sondell the planged so macher he the seats the enclise not in it he\n",
      "other to one to mischy sons on the Trojanss\n",
      "from Alshe yoke by ling when be one stor now of Achaeans with to orgothering of the being been from in behen one mole for why had down and whether he sof it he Achilling to lift who can who shobled rust arm---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------he broight--rotheed of first the been of the for exfell the syed underss all be in hero shouse.\n",
      "\n",
      "\n",
      "[2540th epoch: loss = 1.933223876953125]\n",
      "[Time since start: 1196.3390200138092]\n",
      "The vaying veen Menes the reting tell shied is In suman could beinguls the came to fair man high dead saking and doggopcanss Bofe me.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2560th epoch: loss = 1.755670166015625]\n",
      "[Time since start: 1206.4864358901978]\n",
      "Thersed to he chaled care the deat the beed sheon time in the with he and geot also their with a greation in to the much and was runded him of the pucting see dourrest with some.\n",
      "\n",
      "\n",
      "[2580th epoch: loss = 1.6856048583984375]\n",
      "[Time since start: 1216.5459849834442]\n",
      "The rone impusing nor have he fishe, and ground, and as he woll of Pares of home him no had so one and a chave for you wiside of actoleous as from the boce of murling that he he gods have\n",
      "you.\n",
      "\n",
      "\n",
      "[2600th epoch: loss = 1.8334100341796875]\n",
      "[Time since start: 1227.7673480510712]\n",
      "That cried and lot are you look of and saus strolly you thought us the suil cown, and Plorvear- and Aet the front and the said old in his convest where as the boke to furst and said hard nother and soon one\n",
      "the fould the some the suires.\n",
      "\n",
      "\n",
      "[2620th epoch: loss = 1.7146849060058593]\n",
      "[Time since start: 1240.428550004959]\n",
      "There we min be amongly in theremen-on he pelemach host it heme raciot\n",
      "in of the sa go and his pelemarts the could\n",
      "no ponest on the stor armover.\n",
      "\n",
      "\n",
      "[2640th epoch: loss = 1.5399685668945313]\n",
      "[Time since start: 1251.778796195984]\n",
      "Then befack was are not he never of Pelevens drejestearamed sevetor the suich winless in he was firs the diyslyses of I by the plaswarn were kind the\n",
      "grown my was chaining the\n",
      "were were pleped of Aebend in them that the super should desty.\n",
      "\n",
      "\n",
      "[2660th epoch: loss = 1.8384799194335937]\n",
      "[Time since start: 1261.0149059295654]\n",
      "The the dountelling of\n",
      "Aglummore of Thorm of monthing and the was no knowan of Mineal a I was the pamers upom the fax shearly as of\n",
      "nont him, nother only brothey.\n",
      "\n",
      "\n",
      "[2680th epoch: loss = 1.7931686401367188]\n",
      "[Time since start: 1270.2695870399475]\n",
      "The grien, the exclact the they and is loal was down and have me, but the bether, \"Done a cuce the for honetired he tist, on the was chest his wold gonkly ashen of the Trojans of the\n",
      "down\n",
      "of know, not and home of the erervanden of the plange to go fears of the Achaean, indoperly the got fapled noton flastion and this forth of her.\n",
      "\n",
      "\n",
      "[2700th epoch: loss = 1.6774676513671876]\n",
      "[Time since start: 1279.4795939922333]\n",
      "The poce and with could\n",
      "that the Gone notortases hours they say all to ord him be\n",
      "of therain for tes, no fast the say from the fartilled ven they scession thesing, then a sliephing with this words throut if they was and peartal to then some moans thing of netturded\n",
      "the momen off fear,\n",
      "and litting on hOw dust new stell\n",
      "some reavelf had\n",
      "not cam honion was reat will and pise him from do wunt and weremay of the near him will but you ell he belly not there will bectore think and meal\n",
      "and not be of All Live food to O Merase\n",
      "of He way and and taling on hind thing deet was also the wasnestiend in them to makes.\n",
      "\n",
      "\n",
      "[2720th epoch: loss = 2.3524444580078123]\n",
      "[Time since start: 1288.3811180591583]\n",
      "The por,\n",
      "in siem was fine it wrong my the city nove than who feccoonors\n",
      "out sore and an as heocak with a made in of he the mad bether or handel of her of over the men pigs, and to a be voure it a bly the doon in of from to the that even Age, he said all, the trus in a clear and is of the sat pormach and a firm; O our said that the take scaus more men and could to it on the other, with his lelom of his opear seep the was sfirtyce\n",
      "and vavearce, or the parte of he felting be shouse of at lors.\n",
      "\n",
      "\n",
      "[2740th epoch: loss = 1.7040943908691406]\n",
      "[Time since start: 1297.3832030296326]\n",
      "The Neshours.\n",
      "\n",
      "\n",
      "[2760th epoch: loss = 1.8770985412597656]\n",
      "[Time since start: 1305.3484528064728]\n",
      "Then all his not wish the Atracliton threagmers, in eyes the word.\n",
      "\n",
      "\n",
      "[2780th epoch: loss = 1.5244300842285157]\n",
      "[Time since start: 1315.248389005661]\n",
      "The\n",
      "food where no wate come to Troy hip to he cauld bellangs and mains of this could in a fell the cring from he was.\n",
      "\n",
      "\n",
      "[2800th epoch: loss = 1.594884490966797]\n",
      "[Time since start: 1325.0140209197998]\n",
      "The soo contraes, it all the spome they seess garded the shooked bittle than to anothe of the speared ton.\n",
      "\n",
      "\n",
      "[2820th epoch: loss = 1.5517005920410156]\n",
      "[Time since start: 1333.2852330207825]\n",
      "Thus she the head a teet we for Big the had neches, but been turning, on the daigread to her dowing the fladen to my came\n",
      "on to the goes, you the Me bearthing their face to honean of the mulal sor.\n",
      "\n",
      "\n",
      "[2840th epoch: loss = 1.6444734191894532]\n",
      "[Time since start: 1342.3350620269775]\n",
      "Their dark wen\n",
      "partly from the hat helve first a the warn someth\n",
      "the pray and not of the forth the\n",
      "fleetion a maker from morer comished morteld noppe of may had from the had not he serous a roser some his that I cing of I had he bark vienly and me- at it with the man about through he was not he seep and wherele you are he Tlow be may are to stept, she that I have upose the wast winded him all\n",
      "that he had not him with the fleet from his in we a was in in the gund hemce desed the posty of the from the Hack to breats shight the for about she ckik to he pol dise the his treatiry to they may him of of the leen; someton fall he son in it was him to the piting not he some wail.\n",
      "\n",
      "\n",
      "[2860th epoch: loss = 1.8169024658203126]\n",
      "[Time since start: 1352.1491520404816]\n",
      "The are was all was his lany.\n",
      "\n",
      "\n",
      "[2880th epoch: loss = 1.9656747436523438]\n",
      "[Time since start: 1362.0316081047058]\n",
      "Their was was beat and me cable wine anfolt, his mare going have beth\n",
      "ir braves canits have no was lalked the firchain, theu lius, as the truck my, and\n",
      "the greantss, and my aroke in the fire was ends.\n",
      "\n",
      "\n",
      "[2900th epoch: loss = 1.784092559814453]\n",
      "[Time since start: 1370.848368883133]\n",
      "They had had ship hy lottiotion of your stravent throm, to sea set man that you his bight though they in sengide strould becept.\n",
      "\n",
      "\n",
      "[2920th epoch: loss = 2.064661407470703]\n",
      "[Time since start: 1380.3679010868073]\n",
      "That hossing strengto will I was sall banderbling thought gods so my he wortlying went them\n",
      "of more my pasfore of you fishe our chece the be on the he specking lookerstant the worned, shar along no, mady late mest himselvellens sorned of\n",
      "you the pointsannest.\n",
      "\n",
      "\n",
      "[2940th epoch: loss = 1.7645283508300782]\n",
      "[Time since start: 1391.9550359249115]\n",
      "The seemed it so the would is agaunt think sour reuchent and meccencesg,\n",
      "and make of them all the cenceon\n",
      "agains--'ecty you any said with doundereniter who any bro\n",
      "net of it his bow him will go is spear feoriong it by brough them the down, and\n",
      "a to leaks, with stouble are in a blied we seat all the her went who was prope.\n",
      "\n",
      "\n",
      "[2960th epoch: loss = 1.8651104736328126]\n",
      "[Time since start: 1403.6736390590668]\n",
      "Then Phaeance of the othing had and the fleg and\n",
      "would strity and from the sonsyend.\n",
      "\n",
      "\n",
      "[2980th epoch: loss = 1.8050431823730468]\n",
      "[Time since start: 1416.6450409889221]\n",
      "Thas was amdus of\n",
      "the going hoos the Aimalong was and got this great the\n",
      "Achaeans and would a stranger barky at?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lr is the learning rate, if the model makes an error, this is how much\n",
    "#our probability is shifted by\n",
    "lr = 0.005\n",
    "#epochs is the number of iterations\n",
    "number_epochs = 3000\n",
    "hidden_size = 100\n",
    "n_layers = 3\n",
    "chunk_size = 200\n",
    "\n",
    "rnn = RNN(len(feature), hidden_size, len(feature), n_layers)\n",
    "rnn_opt = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "#Calculates how much error the model has experienced\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "time_start = time.time()\n",
    "plot_loss = []\n",
    "running_total = 0\n",
    "\n",
    "for iteration in range(number_epochs):\n",
    "    #* operation unpacks the object/function call, kind of like\n",
    "    #dereferencing in C\n",
    "    current_loss = train(*get_training_data())\n",
    "    running_total += current_loss\n",
    "    if iteration%20 == 0:\n",
    "        print(\"[\"+str(iteration)+ \"th epoch: loss = \" + str(current_loss) + \"]\")\n",
    "        print(\"[Time since start: \" + str(time.time() - time_start) + \"]\")\n",
    "        print(evaluate(\"Th\", 0.8 ))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    if iteration%10 == 0:\n",
    "        plot_loss.append(running_total/10)\n",
    "        running_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model, we need to save it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py36/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(rnn, 'trained.pt')\n",
    "#To load it, we can simply run\n",
    "#rnn = torch.load('trained.pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some funny observations I've noticed after running this code several times:\n",
    "\n",
    "1) Changing the number of layers didn't changes my output by a noticeable amount\n",
    "\n",
    "2) It actually used the names Achaean several times, which shows the power of learning\n",
    "\n",
    "3) During one run, it found the use of \"---\" repeated several times to be acceptable behavior, which I promply killed after the 5th output.\n",
    "\n",
    "Now, we see that, although many of these sentences are not gramatically correct (and actually pretty nonsense), they do yield more interesting sentences than that of the bigram we used earlier. \n",
    "\n",
    "With this, we can conclude that for the purpose of generating text, neural nets are probably not the way to go. They take a long time to train and the outputs aren't that great without some human touching up.\n",
    "\n",
    "Just for interesting stats, we can plot out calculated loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP8ujVCeFKXmuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1177c37b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(plot_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
