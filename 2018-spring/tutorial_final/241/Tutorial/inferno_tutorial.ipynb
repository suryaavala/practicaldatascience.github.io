{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will introduce you to a popular deep learning package in Python - PyTorch. Pytorch provides tensor-like(like numpy) operations with GPU support and functions for building and optimizing neural networks. Additionally, this tutorial leverages the convenient functions of inferno library that is built around of Pytorch. Inferno is a relatively new library that was released in 2017. It is still in progress of development but because it makes using PyTorch a lot simpler, this tutorial recommends using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "\n",
    "In this tutorial, we will show how to train a simple convolutional neural network and make predictions on the categories of food images. \n",
    "\n",
    "Since the professor will cover some basics of neural networks in future lectures, this tutorial won't go deep into explaining the concept. In machine learning, neural networks are formed based on layers of connected units or nodes. Each unit calculates a weighted sum of outputs from the previous layer and apply a non-linear function to output. Convolutional neural networks is a class of deep NNs that is commonly used in image recognition because of its shift-invariant and space-invariant properties.(Two images with a slight shift difference should still be recognized as the same class by the model.)\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Installing the libraries](#Installing-the-libraries)\n",
    "- [Preprocessing the Data](#Preprocessing-the-Data)\n",
    "- [Building the Model](#Building-the-Model)\n",
    "- [Training the Model](#Training-the-Model)\n",
    "- [Putting it Together](#Putting-it-Together)\n",
    "- [Testing the Model](#Testing-the-Model)\n",
    "- [Visualization](#Visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, you'll need to install the various libraries that we will use.  \n",
    "\n",
    "First, install PyTorch according to your python version and CUDA version for GPU. The corresponding command can be found here: http://pytorch.org/ \n",
    "Suppose you want to install on a Linux machine without GPU support using `pip`:\n",
    "\n",
    "    $ pip3 install http://download.pytorch.org/whl/cpu/torch-0.3.1-cp36-cp36m-linux_x86_64.whl \n",
    "    \n",
    "    $ pip3 install torchvision\n",
    "    \n",
    "Then install inferno folloing this page: https://pytorch-inferno.readthedocs.io/en/latest/installation.html\n",
    "It seems like conda doesn't support inferno. If none of the provided commands in the documentation work, use the following:\n",
    "    \n",
    "    $ pip install inferno-pytorch\n",
    "    \n",
    "Note that you have to install PyTorch before installing inferno!! Also, inferno only works with python3.X, not python2.X.\n",
    "(Installing packages can be a hassle, please double check before continuing)\n",
    "\n",
    "For visualization later, we need to install tensorflow. Please check https://www.tensorflow.org/install for the correct installation command for your machine. Suppose you had a linux machine with GPU support,\n",
    "    \n",
    "    $ pip3 install --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from inferno.extensions.layers.reshape import Flatten\n",
    "from inferno.trainers.basic import Trainer\n",
    "from inferno.trainers.callbacks.logging.tensorboard import TensorboardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've installed and loaded the libraries, let's take a look at our dataset.\n",
    "\n",
    "The dataset originally comes from a Kaggle competition to label color food images in 101 categories from apple pies to waffles: https://www.kaggle.com/kmader/food41\n",
    "\n",
    "The training data `food_c101_n10099_r64x64x3.h5` contains three fields: ['category', 'category_names', 'images']. The `images` field contains 10099 images of size 64x64x3 in HDF5 format. The category field is a 10099x101 boolean array that has value true at the correct category for each training sample. The `category_names` field matches `0,1,2...` to the actual category names `'apple_pie', 'baby_back_ribs', 'baklava'`. In order to use it in Python, we want to transform the original HDF5 format into numpy. \n",
    "\n",
    "The testing data `food_test_c101_n1000_r64x64x3.h5` has the same format as training. It contains 1000 testing samples.\n",
    "\n",
    "\n",
    "#Train data download: https://www.kaggle.com/kmader/food41/downloads/food_c101_n10099_r64x64x3.h5/4\n",
    "\n",
    "#Test data download: https://www.kaggle.com/kmader/food41/downloads/food_test_c101_n1000_r64x64x3.h5/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['category', 'category_names', 'images']\n",
      "train data size:\n",
      "(10099, 64, 64, 3)\n",
      "test data size:\n",
      "(1000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "trainf = h5py.File('food_c101_n10099_r64x64x3.h5', 'r+')\n",
    "testf = h5py.File('food_test_c101_n1000_r64x64x3.h5', 'r+')\n",
    "print(list(trainf.keys()))  # display fields: ['category', 'category_names', 'images']\n",
    "\n",
    "### Train Data\n",
    "train_images = trainf['images'].value\n",
    "category_names = trainf['category']\n",
    "# Change type to unicode for category names\n",
    "category_names = category_names.astype(np.unicode_)\n",
    "train_labels = trainf['category'].value\n",
    "# Transform category from one-hot boolean array to label\n",
    "_, train_labels = np.where(train_labels == True)\n",
    "print(\"train data size:\")\n",
    "print(train_images.shape)\n",
    "\n",
    "### Same for Test Data\n",
    "test_images = testf['images'].value\n",
    "test_labels = testf['category'].value\n",
    "# Transform category from one-hot boolean array to label\n",
    "_, test_labels = np.where(test_labels == True)\n",
    "print(\"test data size:\")\n",
    "print(test_images.shape)\n",
    "\n",
    "### Save to numpy file for future use\n",
    "np.save('food_train', train_images)\n",
    "np.save('food_train_labels', train_labels)\n",
    "np.save('food_test', test_images)\n",
    "np.save('food_test_labels', test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides many built-in neural network modules to choose for different types of layers. For example, it has `Conv1d` and `Conv2d` for convolutional, `Maxpool1d` or `Avgpool1d` for pooling, `Linear` for linear function, and `ReLU` or `Sigmoid` for non-linearity. Look here for more: http://pytorch.org/docs/master/nn.html\n",
    "\n",
    "Here we will be constructing a sequence of Conv1d and Maxpool1d layers, and a final linear layer in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(3, 192, 3, 1, 1),  # in_channel, out_channel, kernel, stride, padding\n",
    "        nn.MaxPool1d(2, 2),  # kernel, stride (2 shrinks the size to half)\n",
    "        nn.Conv1d(192, 192, 3, 1, 1),\n",
    "        nn.MaxPool1d(2, 2),\n",
    "        Flatten(),\n",
    "        nn.Linear(in_features=1024*192, out_features=101),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using inferno library, the training process is automated by the trainer once you have the data and set all the parameters correctly. \n",
    "\n",
    "First, let's prepare the dataset. The inferno Trainer takes a DataLoader which requires a Dataset class. It is common to subclass the abstract Dataset class so that you can initialize the data specifically in `__init__()`. One of the functions required to implement is `__getitem__(i)`. It takes an index parameter to specify which data sample in wanted. For example, if i is 1, then you may return data[1]. Notice that the returned data must be a torch tensor type and the corresponding label needs to be returned as well. The `__len__` function indicates how many samples there are in the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset yields features and labels\n",
    "    \"\"\"\n",
    "    def __init__(self, name, test=False):\n",
    "        super(FoodDataset, self).__init__()\n",
    "        self.name = name\n",
    "        self.data = np.load('food_{}.npy'.format(name))\n",
    "        # Flatten data\n",
    "        size = self.data.shape[0]\n",
    "        self.data = np.reshape(self.data, (size, -1 , 3))\n",
    "        self.data = self.data.transpose(0, 2, 1)\n",
    "        if test: # dummy array\n",
    "            self.labels = [np.zeros((d[1].shape[0],), dtype=np.int32) for d in self.data]\n",
    "        else:\n",
    "            self.labels = np.load('food_{}_labels.npy'.format(name))  \n",
    "        self.len = len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):  # return feature and label\n",
    "        print(self.data[item], self.labels[item])\n",
    "        return torch.from_numpy(self.data[item]).float(), self.labels[item]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a FoodDataset class, in the training process, we initialize a DataLoader class and specify the batch size. Then bind it to the trainer. The trainer will perform forward pass and backpropagation as many times as specified. Below are some important settings:\n",
    "* build_criterion: the loss function used, here we use 'CrossEntropyLoss' built in PyTorch\n",
    "* build_metric: used for visualization later\n",
    "* build_optimizer: optimization function, here we use 'Adam' built in PyTorch. There are also other methods such as 'SGD', 'Adagrad'...\n",
    "* save_every: how often to save our trained model\n",
    "* save_to_directory: directory to save our trained model\n",
    "* set_max_num_epochs: number of times to run in terms of 'epochs' or 'iterations'\n",
    "* build_logger: used for visualization\n",
    "\n",
    "Notice that some parameters use keywords from PyTorch. Inferno simplified the usage of functions in PyTorch. For example, in PyTorch you specify the optimizer and perform backprop explictly, where in inferno everything is already handled.\n",
    "\n",
    "PyTorch:\n",
    "    \n",
    "    $ optimizer = optim.Adam([var1, var2], lr = 0.0001)\n",
    "      for i in range(epochs):\n",
    "          ....\n",
    "          optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model = model_fn()\n",
    "    kwargs = {'num_workers': args['num_workers'], 'pin_memory': True} if args['cuda'] else {}\n",
    "    train_loader = DataLoader(\n",
    "        FoodDataset('train'), shuffle=True, batch_size=args['batch_size'], **kwargs)\n",
    "\n",
    "    # Build trainer\n",
    "    trainer = Trainer(model) \\\n",
    "        .build_criterion('CrossEntropyLoss') \\\n",
    "        .build_metric('CategoricalError') \\\n",
    "        .build_optimizer('Adam') \\\n",
    "        .save_every((1, 'epochs')) \\\n",
    "        .save_to_directory(args['save_directory']) \\\n",
    "        .set_max_num_epochs(20) \\\n",
    "        .build_logger(TensorboardLogger(log_scalars_every=(1, 'iteration'),\n",
    "                                        log_images_every='never'),\n",
    "                      log_directory=args['save_directory'])\n",
    "\n",
    "    # Bind loaders\n",
    "    trainer \\\n",
    "        .bind_loader('train', train_loader)\n",
    "\n",
    "    if args['cuda']:\n",
    "        trainer.cuda()  # Move data to GPU if available\n",
    "\n",
    "    # Go!\n",
    "    trainer.fit()  # Start training!!\n",
    "    trainer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training!\n",
    "\n",
    "*If you only have a CPU this could be really slow....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['batch_size'] = 64\n",
    "args['save_directory'] = 'output-log'\n",
    "args['cuda'] = torch.cuda.is_available()\n",
    "args['num_workers'] = 4 \n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be something like this indicating your progress....\n",
    "\n",
    "[+][2018-04-02 02:14:03.100711] Training iteration 0 (batch 1 of epoch 4).  \n",
    "[+][2018-04-02 02:14:03.234169] Training iteration 1 (batch 2 of epoch 4).  \n",
    "[+][2018-04-02 02:14:03.372486] Training iteration 2 (batch 3 of epoch 4).  \n",
    "[+][2018-04-02 02:14:03.516519] Training iteration 3 (batch 4 of epoch 4).  \n",
    "[+][2018-04-02 02:14:03.665878] Training iteration 4 (batch 5 of epoch 4).  \n",
    "[+][2018-04-02 02:14:03.817522] Training iteration 5 (batch 6 of epoch 4).  \n",
    "[+][2018-04-02 02:14:03.965428] Training iteration 6 (batch 7 of epoch 4).  \n",
    "[+][2018-04-02 02:14:04.112429] Training iteration 7 (batch 8 of epoch 4).  \n",
    "[+][2018-04-02 02:14:04.258638] Training iteration 8 (batch 9 of epoch 4).  \n",
    "[+][2018-04-02 02:14:04.404254] Training iteration 9 (batch 10 of epoch 4).  \n",
    "[+][2018-04-02 02:14:04.550624] Training iteration 10 (batch 11 of epoch 4).  \n",
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to see how well our model is trained! To test, we repeat similar steps. First, we initialize a FoodDataset with test=True, this way the labels would not be used. Then we loop through the DataLoader and feed each sample into the model. The output of a single sample is a 101x1 array where the index of the max value is the predicted category.\n",
    "\n",
    "Remember to set shuffle=False in dataloader because we want to preserve the order to compare to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Model setup, similar to training\n",
    "model = Trainer().load('output-log').model\n",
    "model.eval()  # Very important!!\n",
    "kwargs = {'num_workers': args['num_workers'], 'pin_memory': True} if args['cuda'] else {}\n",
    "test_loader = DataLoader(\n",
    "    FoodDataset('test', test=True), shuffle=False,\n",
    "    batch_size=1, **kwargs)\n",
    "\n",
    "# Make predictions\n",
    "result = []\n",
    "for sample,_ in test_loader:\n",
    "    if args['cuda']:\n",
    "        model.cuda()\n",
    "        sample = sample.cuda()\n",
    "    output = model(Variable(sample))\n",
    "    pred = torch.max(output, 1)[1]\n",
    "    result.append(pred.data[0])\n",
    "\n",
    "# Calculate accuracy\n",
    "label = np.load('food_test_labels.npy')\n",
    "count = 0\n",
    "total = 0\n",
    "for i, pred in enumerate(result):\n",
    "    total = total + 1\n",
    "    if result[i] == label[i]:\n",
    "        count += 1\n",
    "print(count/total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there isn't much output during the training. It would be a waste of time if the only way to verify our model is by testing in the end. Luckily, inferno can be used with tensorboard to log information and track the current loss.\n",
    "\n",
    "Start running tensorboard in a bash shell\n",
    "\n",
    "    $ tensorboard --logdir <directory>\n",
    "\n",
    "The directory should be the same as `save_to_directory` in Trainer. In our case, this would be `args['save_directory'] = output-log` .\n",
    "\n",
    "Then, open up your browser and go to port 6006 on the localhost. And you will see wonderful metrics demonstrating the training loss and error.\n",
    "\n",
    "    http://localhost:6006\n",
    "    \n",
    "<img src=\"tensorboard.png\">\n",
    "\n",
    "From visualization, we can speculate the reason behind the low testing accuracy. It is probable that the model has overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and references\n",
    "\n",
    "This tutorial gives an example of how to use the inferno package to train deep neural networks in Python. There is a lot more you can do with neural networks! Futhermore, it usualy requires a lot of tuning and optimization to acheive the best results \n",
    "For more complicated usage, you can visit the documentation pages below.\n",
    "\n",
    "1. inferno documentation: https://pytorch-inferno.readthedocs.io/en/latest/\n",
    "2. Pytorch documentation: http://pytorch.org/docs/0.3.1/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
