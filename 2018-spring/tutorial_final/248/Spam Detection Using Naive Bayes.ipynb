{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### INTRODUCTION\n",
    "This tutorial will talk about detection of spam using Multinomial Naive Bayes Algorithm. \n",
    "Naive bayes is a simple technique for the construction of classfiers. The fundamental rule that Naive Bayes algorithm uses\n",
    "is 'Bayes Theorem'. \n",
    "\\begin{equation}\n",
    "p(Y \\mid X) = \\frac{p(X \\mid Y)p(Y)}{p(X)}\n",
    "\\end{equation}\n",
    "where X, Y are the events and p() is the probability.\n",
    "\n",
    "Now, we will break the oroginal Bayes Theorem in the context of the text classification problem we are trying to solve \"Spam Detection\".\n",
    "Our hypothesis $H$ is something like \"given text is spam\" and teh evidence $E$ is the text of the email.\n",
    "We are trying to find the probabiltiy that our email is spam given the text in email.\n",
    "\\begin{equation}\n",
    "p(H \\mid E) = \\frac{p(E \\mid H)p(H)}{p(E)}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Assumption\n",
    "We are assuming that each word is independent of all other words. Hence, we can re-write the above equation as:\n",
    "\\begin{equation}\n",
    "p(Spam \\mid w_1,...,w_n) = \\frac{p(w_1 \\mid Spam).p(w_2 \\mid Spam)..p(w_n \\mid Spam).p(Spam)}{p(w_1,..,w_n)}\n",
    "\\end{equation}\n",
    "Now, we can interpret each term $p(w_1 \\mid Spam)$ is the probability of finding a word $w_1$ in the email.\n",
    "\n",
    "This is the Naive Bayes formulation which return a probabilty that an email message is spam given the words in that email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "Data set used for this tutorial is the Enron email dataset, which will be used as the training data.\n",
    "Enron Dataset can be downloaded from the google drive location- \"https://drive.google.com/file/d/1KMqathKI_U-mDahA0m2C7XkHBippdGQA/view?usp=sharing\" (use Google Chrome if unable to download from IE)\n",
    "\n",
    "Before starting, download the data. Put folders 'spam' and 'ham' in a top-level folder 'enronAll'. And put this enronAll folder in the same directory as your source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "PATH = 'enronAll'\n",
    "target_names = ['ham', 'spam']\n",
    "\n",
    "log_class_priors = {}\n",
    "word_counts = {}\n",
    "vocab = set()\n",
    "word_counts['spam'] = {}\n",
    "word_counts['ham'] = {}\n",
    "\n",
    "def load_data(PATH):\n",
    "    \n",
    "    text = []\n",
    "    spam_flag = []\n",
    "    \n",
    "    spams = os.listdir(os.path.join(PATH, 'spam'))\n",
    "    for spam in spams:\n",
    "        with open(os.path.join(PATH, 'spam', spam), encoding=\"latin-1\") as file:\n",
    "            text.append(file.read())\n",
    "            spam_flag.append(1)\n",
    "\n",
    "    not_spams = os.listdir(os.path.join(PATH, 'ham'))\n",
    "    for not_spam in not_spams:\n",
    "        with open(os.path.join(PATH, 'ham', not_spam), encoding=\"latin-1\") as file:\n",
    "            text.append(file.read())\n",
    "            spam_flag.append(0)\n",
    "            \n",
    "    text_df = pd.Series((t for t in text))\n",
    "    spam_flag_df = pd.Series((d for d in spam_flag))\n",
    "    \n",
    "    return text_df, spam_flag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Subject: fw : this is the solution i mentioned...\n",
      "1    Subject: adv : space saving computer to replac...\n",
      "2    Subject: advs\\ngreetings ,\\ni am benedicta lin...\n",
      "3    Subject: fw : account over due wfxu ppmfztdtet...\n",
      "4    Subject: whats new in summer ? bawled\\ncarolyn...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "text_df, spam_flag_df = load_data(PATH)\n",
    "print(text_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce two data frames:\n",
    "1. text_df: here element represents the text of an email\n",
    "2. spam_flag_df: a simple binary data frame where 1 meaning 'spam' and 0 meaning 'ham' (not spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the dataset\n",
    "Here we will clean the dataset by removing the special characters and symbols from the text and then tokenizing the string text into words. After text_df is cleaned and tokenized, a new dataframe is created by combining the text dataframe and spam flag data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(text_df):\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for ind, val in text_df.iteritems():\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        \n",
    "        text = val.translate(translator)\n",
    "        text = text.lower()\n",
    "        text = re.split(\"\\W+\", text)\n",
    "        \n",
    "        result.append(text) \n",
    "        \n",
    "    clean_text_df = pd.Series((t for t in result))\n",
    "        \n",
    "    return clean_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0  1\n",
      "0  [subject, fw, this, is, the, solution, i, ment...  1\n",
      "1  [subject, adv, space, saving, computer, to, re...  1\n",
      "2  [subject, advs, greetings, i, am, benedicta, l...  1\n",
      "3  [subject, fw, account, over, due, wfxu, ppmfzt...  1\n",
      "4  [subject, whats, new, in, summer, bawled, caro...  1\n"
     ]
    }
   ],
   "source": [
    "result = clean_data(text_df)\n",
    "combined_df = pd.concat([result, spam_flag_df], axis=1)\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will be used to count upto how many times each word is appearing in the given lsit of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_counts(words):\n",
    "    \n",
    "    word_counts = {}\n",
    "    for word in words:\n",
    "        word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead, lets first try to understand the algorithm. For training purposes, we actually require three things; the log class priors, i.e. it represents the probability whether or not a given message is spam or ham(not spam); a vocabulary of words and frequency of words i.e. the number of words appearing separately in spam and ham messages.\n",
    "\n",
    "1. Computing the log class probabilities. First counting that how many messages are there in ham and spam separately and then dividing it by the total number messages in spam/ham and then taking the log of that value.\n",
    "2. For each word, if the word is not present,update it individually to the respective vocabularies of spam/ham. If alreasy present then update the number of counts for that word. Also, add the word to the global vocabulary.\n",
    "\n",
    "Example: Lets assume that we have a spam message. First, we will count upto how many each word is appearing in the spam message and add that count to the vocabulary of the spam. \n",
    "\n",
    "We are also tracking the frequency of each word, when it appears in either spam or ham message. Suppose the word \"science\" is appearing in both spam and ham messages. So, on the basis of the frequency count we can assume that the likelihood of this word appearing in spam is more than in ham.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_data(combined_df):\n",
    "    \n",
    "    n = len(combined_df)\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    spam_sum = 0\n",
    "    ham_sum = 0\n",
    "    \n",
    "    for ind, val in combined_df.iterrows():\n",
    "        X.append(val[0])\n",
    "        Y.append(val[1])\n",
    "    \n",
    "    for label in Y:\n",
    "        if label == 1:\n",
    "            spam_sum += 1\n",
    "        else:\n",
    "            ham_sum += 1\n",
    "            \n",
    "            \n",
    "    global log_class_priors\n",
    "    log_class_priors['spam'] = math.log(spam_sum / n)\n",
    "    log_class_priors['ham'] = math.log(ham_sum / n)\n",
    "  \n",
    "    global word_counts\n",
    "    word_counts['spam'] = {}\n",
    "    word_counts['ham'] = {}\n",
    "       \n",
    "    global vocab\n",
    "    \n",
    "    for x, y in zip(X, Y):\n",
    "        c = 'spam' if y == 1 else 'ham'\n",
    "        \n",
    "        \n",
    "        counts = get_word_counts(x)\n",
    "        for word, count in counts.items():\n",
    "            if word not in vocab:\n",
    "                vocab.add(word)\n",
    "            if c not in word_counts:\n",
    "                continue\n",
    "            if word not in word_counts[c]:\n",
    "                word_counts[c][word] = 0.0\n",
    "                \n",
    "            word_counts[c][word] += count\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = fit_data(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted the required data from the training data, we can move ahead with the Naive Bayes classification.\n",
    "\n",
    "1. Given a document, we need to iterate each of the words and compute $\\log p(w_i|\\text{Spam})$ and sum them all up, and we also compute $\\log p(w_i|\\text{Ham})$ and sum them all up.\n",
    "2. Then we add the log class priors and check to see which score is bigger for that document. Whichever is larger, that is the predicted label!\n",
    "3. To compute $\\log p(w_i|\\text{Spam})$, the numerator is how many times we’ve seen $w_i$ in a “spam” message divided by the total count of all words in every “spam” message.\n",
    "\n",
    "#### Laplace Smoothing\n",
    "One thing that we need to take care of is that, if we encounter a word that is present in the spam vocabulary and not is ham vocabulary and vice-versa, Then $p(w_i|\\text{Ham})$ will be 0, hence the log of 0, will return undefined. To overcome this, we use Laplace Smoothing, i.e. we simply add 1 to the numerator, and add the size of the vocubulary to the denominator to balance it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_data(X):\n",
    "    result = []\n",
    "    \n",
    "    for x in X:\n",
    "        counts = get_word_counts(x)\n",
    "        spam_score = 0\n",
    "        ham_score = 0\n",
    "        \n",
    "        for word, _ in counts.items():\n",
    "            if word not in vocab: continue\n",
    "            # add Laplace smoothing\n",
    "            log_w_given_spam = math.log( (word_counts['spam'].get(word, 0.0) + 1) / (sum(word_counts['spam'].values()) + len(vocab)) )\n",
    "            log_w_given_ham = math.log( (word_counts['ham'].get(word, 0.0) + 1) / (sum(word_counts['ham'].values()) + len(vocab)) )\n",
    "\n",
    "            spam_score += log_w_given_spam\n",
    "            ham_score += log_w_given_ham\n",
    "\n",
    "        spam_score += log_class_priors['spam']\n",
    "        ham_score += log_class_priors['ham']\n",
    "\n",
    "        if spam_score > ham_score:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of correctly identifying a mail as spam or not = 54%\n"
     ]
    }
   ],
   "source": [
    "pred = predict_data(random.sample(X, 100))\n",
    "true = random.sample(Y, 100)\n",
    "\n",
    "accuracy = sum(1 for i in range(len(pred)) if pred[i] == true[i]) / float(len(pred))\n",
    "\n",
    "print(\"Accuracy of correctly identifying a mail as spam or not = {0:.0f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes using sklearn library\n",
    "\n",
    "In this part we will use sklearn package, and use the prdefined methods to calculate the accuracy of the model, when compared to the previous model.\n",
    "    \n",
    "a. Extraction of features:- we will convert the database into numerical feature vectors, by using the 'bag of words' model. We will segment each file into words and keep a count each word appears in the document. Using CountVectorizer.fit_transform() method, we are learning the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "new_combined_df = pd.concat([text_df, spam_flag_df], axis=1)\n",
    "train_df, test_df = train_test_split(new_combined_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_transform(train_df):\n",
    "    \n",
    "    count_vect = CountVectorizer()\n",
    "    counts = count_vect.fit_transform(train_df[0])\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = fit_transform(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Reducing the common words:- When counting the number of words in each document, more weightage will be given to longer documents. To overcome this we use Term_Frequency, i.e. count of word/total words in each document. Further we use TFIDF to represent the documents via a (weighted) bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(train_counts):\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidf = tfidf_transformer.fit_transform(train_counts)\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26961, 142761)\n"
     ]
    }
   ],
   "source": [
    "train_tfidf = tfidf(train_counts)\n",
    "print(train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Building a Naive Bayes Classfier:\n",
    "1. Building a Multinomial Naive Bayes classifier and train it on the training data.\n",
    "2. Building a pipeline- The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters\n",
    "3. Performance of NB classifier - Test the performance of the Multinomial Naive Bayes classifier against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NBclassifier(train_df, train_tfidf):\n",
    "    \n",
    "    clf = MultinomialNB().fit(train_tfidf, train_df[1])\n",
    "\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "    \n",
    "    text_clf = text_clf.fit(train_df[0], train_df[1])\n",
    "    \n",
    "    predicted = text_clf.predict(test_df[0])\n",
    "\n",
    "    return(np.mean(predicted == test_df[1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of correctly identifying a mail as spam or not = 99%\n"
     ]
    }
   ],
   "source": [
    "accuracy = NBclassifier(train_df, train_tfidf)\n",
    "print(\"Accuracy of correctly identifying a mail as spam or not = {0:.0f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Pandas: https://pandas.pydata.org/\n",
    "2. String: https://docs.python.org/2/library/string.html\n",
    "3. Random: https://docs.python.org/2/library/random.html\n",
    "4. Sklearn: http://scikit-learn.org/stable/documentation.html\n",
    "5. http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "6. https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "7. http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "8. https://pythonmachinelearning.pro/text-classification-tutorial-with-naive-bayes/\n",
    "8. http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
