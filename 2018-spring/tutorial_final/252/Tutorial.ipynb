{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for sequential data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This tutorial will give you an introduction to Long Short Term Memory networks (LSTMs), which is a powerful and widely-used model to recognize patterns in sequences of data, such as text, handwriting, speech, or numerical times series data from sensors, stock markets etc.\n",
    "The LSTMs, as a variation of recurrent neural ntworks (RNNs), was introduced by [Hochreiter and Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf). In case some of the students haven't learned artificial neural networks, I'll start from a brief introduction to the conventional feedforward neural networks. Then the recurrent neural network and its limitation is introduced, which provides the motivation for LSTMs. After that we will have a look into the structure of the LSTMs. Finally, the applictaion of the LSTM is illustrated by a text genarator with the python deep learning package Keras.\n",
    "\n",
    "### Table of contents\n",
    "- [Backgroung knowledge](#Background-knowledge)\n",
    "    * [Feedforward neural networks](#Feedforward-neural-networks )\n",
    "    * [Recurrent neural networks](#Recurrent-neural-networks)\n",
    "- [Motivation for LSTMs](#Motivation-for-LSTMs)\n",
    "- [Architecture of LSTMs](#Architecture-of-LSTMs)\n",
    "- [Application - text generator with LSTMs](#Text-generator-with-LSTMs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background knowledge\n",
    "- ### Feedforward neural networks\n",
    "<br/> For those who know little about artificial neural networks, here's a brief introduction to the conventional feedforward neural networks. The feedforward neural network is a computational model to approximate a  possibly complicated mapping $y=f(x;\\theta)$ from some input $x$ to the output $y$, where $\\theta$ is the parameters to learn. it's typically comprised of many computational units, often called neurons or nodes, connected in chain. Usually a node will calculate the products of the weights $w$ and the input $x$, and pass them through an activation function $a$ to introduce non-linearity into the outputs. The architecture of a simple feedforward network is showed in the diagram below.\n",
    "<img src=\"https://cdn-images-1.medium.com/max/479/1*QVIyc5HnGDWTNX3m-nIm9w.png\" style=\"width:400px;\"/>  \n",
    "The model is called __feedforward__ because the structure is a directed acylic graph where the information flows from the input $x$ through the intermediate hidden layers, to the output $y$, with no feedback connections to itself. The outputs of the last layer are ideally an approximation to the corresponding label for each training example, but the outputs of the intermediate layers are not specified, so they are called __hidden layers__.  \n",
    "For detailed more description of feedforward neural networks, including the training algorithm, activation functions and so on, the [chapter 6 of Goodfellow, Bengio, Courville (2016)](http://www.deeplearningbook.org/contents/mlp.html) is a great material to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Recurrent neural networks  \n",
    "<br/> Feedforward neural networks have no notion of order in time, and all training examples are considered independent. This becomes a shortcoming when handling sequential data. For instance, if you want to predict the stock price, it depends not only on the factors that influence the price for today, such as the negative news of the company, but also on the price of the previous day. Recurrent neural networks (RNNs) handle the influence from previous time by introducing 'memory' into the network, that is adding a feedback loop connected to their previous predictions. So in RNNs, the present input and the information passed from the past together determine the output. A chunk of RNN is like:  \n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/05231650/rnn-neuron.png\" style=\"width: 150px\"/>  \n",
    "It can be unfolded as copies of the same neural network connected in the sequence of time. The animation below illustrate the mechanism of RNNs, where each column represents a feedforward network at that point in time. The size of the unfolded graph depends on the sequence length. \n",
    "<img src=\"https://i.imgur.com/kpZBDfV.gif\" style=\"width: 500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the unfolded recurrence, the value of the hidden units after $t$ steps can be represented as:\n",
    "$$h^{(t)}=g^{(t)}(x^{(t)}, x^{(t-1)},...x^{(1)})=f(h^{(t-1)}, x^{(t)}, \\theta) \\tag {1}$$\n",
    "Here we take advantage of the idea of sharing the parameters across different part of the network, so it's possible to learn a single transition function $f$ at each time step (rather than a function $g^{(t)}$ for all time steps). Thus the model can be generalized to examples of different length, and can be trained with much fewer training examples.  \n",
    "It should be noted that except for the architecture depicted in the animation above (recurrent connections from the output of previous time step to the hidden units of this time step), RNNs can also have recurrent connections only between hidden units at different time steps. Also, it's possible to only have a single output at the end of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When neural networks become very deep, a problem rises when doing optimization: vanishing or exploding gradients. This is particularly true for RNNs, since they involve the composition of the __same__ function multiple times. If we ignore the activation function, the recurrence of RNNs is approximately matrix multiplication. When the recurrence spans for $t$ time steps, the state of the network can be simplified as $$h^{(t)}=(W^t)^Th^{(0)}$$\n",
    "Suppose $W$ has eigendecomposition $W=Vdiag(\\lambda)V^{-1}$, then $W^t=Vdiag(\\lambda)^tV^{-1}$. The eigenvalues $\\lambda$ increase to the power of t and they will easily explode or vanish depending on their magnitude. In actual application, exploding gradients are less often to occur and can be relatively easy to solve by, for example, trucation or squashing, so vanishing gradients are kottier.  \n",
    "For RNNs, the long-term dependencies will make the gradient-based optimization extremely difficult for sequence of only length 10 or 20 ([Bengio et al. (1994)](http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf)). For instance, if we want to predict the next word appearing in the sentence \"*The moon moves round the \\_\\_*\", it can effectively solved by RNNs. But consider this example: \"*He moved to France 20 years ago with his parents. Now he can speak fluent \\_\\_*\". The information we need to predict the word is far away, so plain RNNs cannot learn the connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs is one of the most successful approaches to solve the challange of long-term dependencies. It's proven effective in many applications such as speech recognition, machine translation and parsing. The core idea of LSTMs is to introduceg self-loops, called gated cells, to contain information outside the normal flow of RNNs, thus maintaining more constant errors through propagation. The gates in the cell filter the signals they receive and decide which information to read in , to remove or to output from the cell. Similar to hidden units in ordinary neural networks, the gated cells learn their own set of weights through gradient descent to control the flow of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of LSTMs\n",
    "A normal structure of LSTMs are depicte in the diagram below.  \n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" style=\"width: 500px\"/>  \n",
    "\n",
    "Let's take a repeating module in LSTMs and have a look into the structure. The module contains forget gate, input gate and output gate, as an improvement of the information control for plain RNNs.  \n",
    "Recall the transition function of plain RNNs in equation $(1)$. Assume the activation function is a sigmoid function and we can write the equation as:\n",
    "$$o^{(t)}=\\sigma(Wh^{(t-1)}+Ux^{(t)}+b)$$\n",
    "where $W$, $U$ and $b$ are recurrent weights, input weights and bias respectively.  \n",
    "Let's still use the text prediction problem mentioned above. For instance, if in the new sentence, the subject changes, the information about the previous subject might needs to be removed. This is empowered by the forget gate. \n",
    "<img src=\"https://raw.githubusercontent.com/wanniz/markdown/master/LSTM3-forget.png\" style=\"width: 300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forget gate looks the previous state and the current input, and the __sigmoid__ unit in it controls the output between 0 to 1. The output of the forget gate $f^{(t)}=\\sigma(W^fh^{(t-1)}+U^fx^{(t)}+b^f)$.\n",
    "Next, the input gate will decide which information to add in to the cell state. For example, some words in the sentence is not important for the text prediction problem, so we don't need to store them.\n",
    "<img src=\"https://raw.githubusercontent.com/wanniz/markdown/master/LSTM3-input.png\" style=\"width: 300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the __sigmoid__ unit in the input gate regulates what values need to be added, $i^{(t)}=\\sigma(W^ih^{(t-1)}+U^ix^{(t)}+b^i)$. And the __tanh__ unit creates a candidate vector $\\tilde{C}^{(t)}=\\sigma(W^Ch^{(t-1)}+U^Cx^{(t)}+b^C)$, containing all possible that could be added to the state. The multiplication of the two values is then added to the cell state via addition operation. Then the new cel state $C^{(t)}$ is updated by the output of the forget gate and the input gate: $C^{(t)}=f^{(t)}*C^{(t)}+i^{(t)}*\\tilde{C}^{(t)}$.\n",
    "Finally, the output gate is responsible to decide what information to export. In our text prediction problem, this is to decide what type of the word is apt to fill in the blank.\n",
    "<img src=\"https://raw.githubusercontent.com/wanniz/markdown/master/LSTM3-output.png\" style=\"width: 300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output gate works similarly as the input gate, where a __tanh__ unit generates a vector and the __sigmoid__ unit filters what to pass through. $o^{(t)}=\\sigma(W^oh^{(t-1)}+U^ox^{(t)}+b^o)$; $h^{(t)}=o^{(t)}*tanh(C^{(t)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-generator-with-LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After understanding the mechanism of LSTMs, let's try to implement it to build a text generator (as the one in homework3). For the application, we need the deep learning package Keras. You can install it by `pip`:\n",
    "\n",
    "    $ pip install keras\n",
    "\n",
    "For the text to analyze, we can download books whose copyright is no longer protected from [Project Gutenberg](https://www.gutenberg.org/ebooks/search/%3Fsort_order%3Ddownloads).\n",
    "Here we chose Alice’s Adventures in Wonderland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the text file and do some simple processing.  \n",
    "In this example, we will generate the text character by character, so we create a vocabulary list that stores all the unique characters in appearing in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '3', 'm', ']', 'l', 'R', 'F', ':', 'G', 'C', 'x', 'S', '\\ufeff', 'w', '“', '.', '(', 'Y', 'y', ';', 'D', 'j', 'I', 't', 'h', 'd', '_', 'L', '!', 'A', 'N', 'n', '?', 'i', '[', 's', 'K', 'H', 'q', 'g', 'u', 'z', '’', 'V', 'b', 'O', 'Z', 'J', 'a', '‘', '”', 'W', '-', 'f', 'E', 'p', '*', ')', 'e', 'r', 'c', 'P', 'X', 'M', 'v', 'k', '0', 'T', 'B', 'Q', ',', 'o', 'U']\n"
     ]
    }
   ],
   "source": [
    "with open(\"Alice.txt\", encoding=\"utf8\") as f:\n",
    "    raw_text = f.read()\n",
    "raw_text = raw_text.replace(\"\\n\",\"\")\n",
    "Voc = list(set(raw_text))\n",
    "print(Voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the  character '\\ufeff' appears in the text, since it's only related to the decode format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '3', 'm', ']', 'l', 'R', 'F', ':', 'G', 'C', 'x', 'S', '“', 'w', '.', '(', 'Y', 'y', ';', 'D', 'j', 'I', 't', 'h', 'd', '_', 'L', '!', 'A', 'N', 'n', '?', 'i', '[', 's', 'K', 'H', 'q', 'g', 'u', 'z', '’', 'V', 'b', 'O', 'Z', 'J', 'a', '‘', '”', 'W', '-', 'f', 'E', 'p', '*', ')', 'e', 'r', 'c', 'P', 'X', 'M', 'v', 'k', '0', 'T', 'B', 'Q', ',', 'o', 'U']\n"
     ]
    }
   ],
   "source": [
    "raw_text = raw_text.replace(\"\\ufeff\",\"\")\n",
    "Voc = list(set(raw_text))\n",
    "print(Voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le't see the size of our vocabulary and the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 141097\n"
     ]
    }
   ],
   "source": [
    "V_length = len(Voc)\n",
    "T_length = len(raw_text)\n",
    "print(V_length, T_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two dictionaries to look up for the index of the character in the vocabulary list or conversely. This is helpful we when do encoding of the characters to create features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_idx = {c:i for i, c in enumerate(Voc)}\n",
    "idx_ch = {i:c for i, c in enumerate(Voc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the sequence length to 100, which means we want to predict the 101 charatcer given the previous 100 characters. So we segment our text to sequence of 100 length with a step 1. And the output sequences are just corresponding input sequence shifted by one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_length = 100\n",
    "seq_in = []\n",
    "seq_out = []\n",
    "for i in range(0, T_length-S_length, 1):\n",
    "    seq_in.append(raw_text[i:i+S_length])\n",
    "    seq_out.append(raw_text[i+1:i+S_length+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140997"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = len(seq_in)\n",
    "n_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 140997 trianing examples in total. Then we need to do one-hot encoding for the sequences. (The rest codes run on cloud due to the large memory required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_in = np.zeros((n_sample, S_length, V_length))\n",
    "S_out = np.zeros((n_sample, S_length, V_length))\n",
    "for i in range(n_sample):\n",
    "    M_in = np.zeros((S_length, V_length))\n",
    "    M_out = np.zeros((S_length, V_length))\n",
    "    for j in range(S_length):\n",
    "        M_in[j][ch_idx[seq_in[i][j]]] = 1\n",
    "        M_out[j][ch_idx[seq_out[i][j]]] = 1\n",
    "    S_in[i] = M_in\n",
    "    S_out[i] = M_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the LSTMs model. Create checkpoints to save weights at different epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(None, V_length), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "filepath=\"weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(S_in, S_out, epochs=20, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights with minimum loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"weights-19-1.9227.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = numpy.random.randint(0, n_sample-1)\n",
    "pattern = seq_in[start]\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(V_length)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = idx_ch[index]\n",
    "    seq = [idx_ch[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text generated:  \n",
    "\n",
    "*Have no mistake about it: it was neither more nor less than a pig, and she felt that it would be quit e aelin that she was a little want oe toiet ano a grtpersent to the tas a little war th tee the tase oa teettee the had been tinhgtt a  at the cadl in a long tuiee aedun that sheer was a little tare gereen to be a gentle of the tabdit  soenee the gad  ouw ie the tay a tirt of toiet at the was a little anonersen, and thiu had been woite io a lott of tueh a tiie  and taede\n",
    "bot her aeain  she cere thth the bene tith the tere bane to tee toaete to tee the harter was a little tire the same oare cade an anl ano the garee and the was so seat the was a little gareen and the sabdit, and the white rabbit wese tilel an the caoe and the sabbit se teeteer,and the white rabbit wese tilel an the cade in a lonk tfne the sabdi\n",
    "ano aroing to tea the was sf teet whitg the was a little tane oo thete the sabeit  she was a little tartig to the tar tf tee the tame of the cagd, and the white rabbit was a little toiee to be anle tite thete ofsand the tabdit was the wiite rabbit, and*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
