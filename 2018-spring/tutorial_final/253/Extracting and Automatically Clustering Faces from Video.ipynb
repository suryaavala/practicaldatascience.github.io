{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Ever wanted to make Black Mirror or 1984 more of a reality?\n",
    "\n",
    "If you have a security camera being pointed somewhere - say, that mysterious image capturing device outside Rashid - you can detect and identify the faces of people who are passing by.\n",
    "\n",
    "It turns out that this is pretty useful in practice!\n",
    "\n",
    "1. Apple FaceID: label one face, face recognition for authentication\n",
    "\n",
    "2. Bank vaults: pre-label employees, sound an alarm if unrecognizable people appear\n",
    "\n",
    "3. Just creeping: you don't need to label any faces at all! With the magic of machine learning, you can cluster similar faces together automatically. You can then review the clusters manually to see who's been passing by.\n",
    "\n",
    "Option 3 is the hardest since we don't know the faces ahead of time. So we'll do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <font size=\"5\">We'll do this:</font> <img src=\"https://i.imgur.com/3V63A1s.gif\" width=\"300\" style=\"display:inline-block; margin:1em auto\">\n",
    "    <b>--></b>\n",
    "    <img src=\"https://i.imgur.com/bli4HcP.gif\" width=\"250\" style=\"display:inline-block; margin:1em auto\">\n",
    "    <b>--></b>\n",
    "    <img src=\"https://i.imgur.com/wAyzaFQ.gif\" width=\"250\" style=\"display:inline-block; margin:1em auto\">\n",
    "</div>\n",
    "<p style=\"clear: both;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "\n",
    "In this tutorial, we'll walk through extracting and clustering faces from videos with [OpenCV](https://opencv.org/) and [face_recognition](https://github.com/ageitgey/face_recognition).\n",
    "\n",
    "We'll use the following YouTube video as a data source, but it is trivial to use any video or webcam feed instead:\n",
    "\n",
    "- [Highlights: CMU Welcomes Tenth President](https://www.youtube.com/watch?v=s00G1xyKVd0)\n",
    "\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Installing the libraries](#Installing-the-libraries)\n",
    "- [Getting some videos](#Loading-data-and-plotting)\n",
    "- [A brief introduction to Haar Cascades](#A-brief-introduction-to-Haar-Cascades)\n",
    "- [Face clustering and picking representatives](#Face-clustering-and-picking-representatives)\n",
    "- [Summary](#Summary)\n",
    "- [Limitations](#Limitations)\n",
    "- [Extensions](#Extensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the libraries\n",
    "\n",
    "**Note: All the libraries used are Python 3 compatible. Feel free to use pip3 instead of pip to install.**\n",
    "\n",
    "## pytube (optional)\n",
    "\n",
    "[pytube](https://github.com/nficano/pytube/): download YouTube videos with ease. Supports many options such as audio only, different video resolutions, exposing all the available video streams.\n",
    "\n",
    "Just run ```pip install pytube```.\n",
    "\n",
    "## OpenCV\n",
    "\n",
    "[OpenCV](https://opencv.org/): leading library for open-source computer vision\n",
    "\n",
    "**Getting OpenCV**\n",
    "\n",
    "You have a couple of options, just pick one! We recommend the conda approach.\n",
    "\n",
    "1. [conda](https://conda.io/docs/)  \n",
    "    You can try installing the latest version with ```conda install opencv```, but this is currently broken for most people.  \n",
    "    Instead, you can install an older version with ```conda install -c menpo opencv3```\n",
    "\n",
    "2. [pip](https://pypi.python.org/pypi/pip)  \n",
    "    Unofficial binaries are available for opencv on pip. The \"contrib\" version includes patented algorithms/non-free modules, and can be installed with ```pip install opencv-contrib-python```. The base version can be installed as ```pip install opencv-python```. It doesn't matter which you pick for our tutorial.\n",
    "\n",
    "3. your system package manager  \n",
    "    If you use a package manager, chances are they have opencv bindings, e.g. ```brew install opencv```.\n",
    "\n",
    "4. from scratch  \n",
    "    You can otherwise build opencv from scratch, as detailed on the [official opencv website](https://docs.opencv.org/master/df/d65/tutorial_table_of_content_introduction.html).\n",
    "    \n",
    "**Getting the pretrained classifiers**\n",
    "\n",
    "OpenCV comes with some batteries included. We're interested in the pretrained object classifiers.\n",
    "\n",
    "So you'll want to keep track of the folder that OpenCV was installed to and change HAAR_PREFIX in the cell below to match.\n",
    "\n",
    "Alternatively, you can download the [OpenCV HaarCascades GitHub folder](https://github.com/opencv/opencv/tree/master/data/haarcascades) to your computer and point HAAR_PREFIX there.\n",
    "\n",
    "## dlib\n",
    "\n",
    "[dlib](http://dlib.net/): machine learning library\n",
    "\n",
    "Try ```pip install dlib```. If you're on a Mac, though, Apple's clang compiler will give you a lot of trouble.\n",
    "\n",
    "So if that didn't work, just run ```conda install -c menpo dlib```.\n",
    "\n",
    "## face_recognition\n",
    "\n",
    "[face_recognition](https://github.com/ageitgey/face_recognition): simple face recognition library\n",
    "\n",
    "Try ```pip install face_recognition```. If you get dlib errors, try ```pip install face_recognition --no-dependencies```.\n",
    "\n",
    "## Final steps\n",
    "\n",
    "Once the above libraries are installed, make sure the following cell runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os                          # filesystem functions\n",
    "\n",
    "import cv2                         # computer vision: face extraction\n",
    "import face_recognition            # computer vision: face recognition\n",
    "import matplotlib.pyplot as plt    # drawing various images in the notebook\n",
    "import pytube                      # for downloading videos from YouTube (optional)\n",
    "import random                      # random sampling for representative face\n",
    "\n",
    "from IPython import display        # useful display functions\n",
    "\n",
    "# point this to the location of your OpenCV installation\n",
    "# this must be an absolute path\n",
    "# otherwise, you will see \"error: (-215) !empty() in function detectMultiScale\"\n",
    "HAAR_PREFIX = r'/Users/.../anaconda/pkgs/opencv-3.4.1-py35_blas_openblas_200/share/OpenCV/haarcascades/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell just supports type annotations that we will be using\n",
    "\n",
    "# type annotations\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "# type aliases\n",
    "import numpy as np\n",
    "RGBImage = np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting some videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading with pytube\n",
    "\n",
    "To download a video with pytube, all we need is its YouTube link. The abovementioned video has the URL of https://www.youtube.com/watch?v=s00G1xyKVd0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded https://www.youtube.com/watch?v=s00G1xyKVd0 to video.mp4\n"
     ]
    }
   ],
   "source": [
    "def download_video(youtube_url : str, filename : str = 'video') -> str:\n",
    "    \"\"\"\n",
    "    Downloads the video at youtube_url into filename.\n",
    "    Returns full name of downloaded file, including extension.\n",
    "    \n",
    "    The filename should not include any extensions,\n",
    "    as the type of the downloaded file depends on YouTube itself.\n",
    "    We do not provide any video encoding capability,\n",
    "    look into ffmpeg if you want this feature.\n",
    "    \n",
    "    We download the first available stream returned from YouTube.\"\"\"\n",
    "    \n",
    "    yt = pytube.YouTube(youtube_url)            # we create a YouTube object\n",
    "    stream = yt.streams.first()                 # we get the first downloadable stream\n",
    "    sname = stream.default_filename\n",
    "    stream.download(filename=filename)          # and then we download it\n",
    "    extension = sname[sname.index('.'):]\n",
    "    return filename + extension\n",
    "    \n",
    "    # there are many more settings for downloading a video - audio only, quality settings, etc\n",
    "    # check out https://github.com/nficano/pytube for the full list of options\n",
    "\n",
    "\n",
    "VIDEO_URL = 'https://www.youtube.com/watch?v=s00G1xyKVd0'\n",
    "downloaded_name = download_video(VIDEO_URL)\n",
    "print(\"Downloaded {} to {}\".format(VIDEO_URL, downloaded_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the video and preparing a processing pipeline\n",
    "\n",
    "It is possible to show videos inside the Jupyter Notebook too. This isn't ideal, since the Notebook is laggy compared to Jupyter qtconsole, but it can be very helpful in a pinch.\n",
    "\n",
    "Here, we're defining a function that will display the video inside the notebook. A few key ideas:\n",
    "\n",
    "- the video is loaded from a file with [cv2.VideoCapture](https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html).\n",
    "\n",
    "    - If you provide VideoCapture with a path to a local video, it reads that video. For example, VideoCapture('video.mp4').\n",
    "    - If you provide it with a number, that becomes device ID. For example, VideoCapture(0) defaults to your webcam.\n",
    "\n",
    "\n",
    "- we accept a list of transformations to make it easy to form an image processing pipeline. Each frame of the video will have the list of transformations applied to it.\n",
    "    - For example, we're currently interested in detecting faces. So for every frame, we want to perform detectFaces(frame). However, in the future we may also want to detect cats. We can reuse our existing function by providing [detectFaces, detectCats].\n",
    "\n",
    "\n",
    "- it often isn't useful to process _every_ frame of a video. If someone's face first appears in frame X, chances are that they still appear in frame X+1. We can save time by skipping every N frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class Transformation:\n",
    "    \"\"\"\n",
    "    Class to represent an image transformation.\n",
    "    \n",
    "    It defines a transform method that allows for the chaining of transformations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def transform(self, image : RGBImage) -> RGBImage:\n",
    "        \"\"\"\n",
    "        Transforms the supplied image and returns the image after transformation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "def process_video(filename : str = None,\n",
    "                  transformations : List[Transformation] = None,\n",
    "                  show : bool = True,\n",
    "                  skip : int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Displays the video located at filename in the notebook.\n",
    "    \n",
    "    filename:\n",
    "        If no filename is provided, it displays the webcam instead.\n",
    "        \n",
    "    transformations:\n",
    "        applies all the transformations to every frame of the video.\n",
    "        If transformations are provided as [T1, T2, ..., Tn]\n",
    "        Then they are applied as Tn(...(T2(T1(frame)))).\n",
    "    \n",
    "    show:\n",
    "        if show is false, will not show the video.\n",
    "    \n",
    "    skip:\n",
    "        will skip this many frames without transforming or showing them\n",
    "    \n",
    "    \n",
    "    \n",
    "    Note that the refresh rate is very choppy on most computers.\n",
    "    This function is just a quick way to view your videos within\n",
    "    the Jupyter notebook workflow.\n",
    "    If you want your video to look nicer, look into double buffers.\n",
    "    \n",
    "    To get better performance, you should run this code from\n",
    "    jupyter qtconsole.\n",
    "    \"\"\"\n",
    "    \n",
    "    # filename of 0 corresponds to the webcam, if you have one\n",
    "    if filename is None:\n",
    "        filename = 0\n",
    "    \n",
    "    vid = cv2.VideoCapture(filename)\n",
    "\n",
    "    # https://stackoverflow.com/q/16703345\n",
    "    # Mac doesn't detect end of video properly\n",
    "    # so we need to hack around it instead of depending\n",
    "    # on VideoCapture.read's return code\n",
    "    total_frames = vid.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    cur_frame = 0\n",
    "    \n",
    "    try:\n",
    "        while cur_frame < total_frames:\n",
    "            # we read a frame of the video\n",
    "            read_success, frame = vid.read()\n",
    "\n",
    "            cur_frame = cur_frame % (skip + 1)\n",
    "            \n",
    "            # if we couldn't read a frame, we stop\n",
    "            if not read_success:\n",
    "                vid.release()\n",
    "                break\n",
    "\n",
    "            if cur_frame == 0:\n",
    "                \n",
    "                # OpenCV stores images in BGR format\n",
    "                # matplotlib expects images in RGB format\n",
    "                # therefore we need to convert the two\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # apply all our transformations to the frame\n",
    "                if transformations is not None:\n",
    "                    for t in transformations:\n",
    "                        frame = t.transform(frame)\n",
    "\n",
    "                if show:\n",
    "                    plt.axis('off')                 # remove the axis\n",
    "                    plt.title(\"Video Stream\")       # provide a title\n",
    "                    plt.imshow(frame)               # load the frame\n",
    "                    plt.show()                      # show the frame\n",
    "                    display.clear_output(wait=True) # keep the current frame until we have a new one\n",
    "\n",
    "    except KeyboardInterrupt: # we may choose to prematurely end the stream\n",
    "        vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# we can display our video by running the following cell\n",
    "process_video('video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief introduction to Haar Cascades\n",
    "\n",
    "## Background\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://imgs.xkcd.com/comics/tasks.png\" width=\"180\" style=\"display:inline-block; margin:1em auto\">\n",
    "    <img src=\"https://imgs.xkcd.com/comics/machine_learning.png\" width=\"250\" style=\"display:inline-block; margin:1em auto\">\n",
    "</div>\n",
    "<p style=\"clear: both;\">\n",
    "\n",
    "Back before TensorFlow and neural networks appeared everywhere, object recognition was a pretty difficult problem in computer vision. People came up with the concept of [Haar features](https://en.wikipedia.org/wiki/Haar-like_feature). Intuitively, Haar features are just math with weighted rectangles. You have a detection rectangle that you're moving around, and you compute pixel intensities based on adjacent rectangular regions. This is best understood through examples. Below, Haar feature C mimics the detection region for the bridge of a nose.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Prm_VJ_fig1_featureTypesWithAlpha.png/600px-Prm_VJ_fig1_featureTypesWithAlpha.png\" width=\"200\" style=\"display:block;margin:0 auto\">\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/8/8a/Haar_Feature_that_looks_similar_to_the_bridge_of_the_nose_is_applied_onto_the_face.jpg\" width=\"200\" style=\"display:block;margin:0 auto\">\n",
    "    <caption><em>Example of HAAR Features, source: [Wikipedia](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework)</em></caption>\n",
    "</div>\n",
    "\n",
    "Because the algorithm is so simple, a single Haar feature is pretty useless by itself. Instead, you'll need a LOT of Haar features, trained with a cascade classifier. A cascade classifier is just a multistage classifier that repeatedly feeds the classified result of one stage to the next.\n",
    "\n",
    "You could also have noticed that we're implicitly assuming that faces are aligned to face forward, i.e., it wouldn't work very well for faces that were turned sideways.\n",
    "\n",
    "So if this method sucks, why do we use it? The primary advantage of Haar-like features is calculation speed. Because we're just computing average intensities and areas of rectangles, computers are able to perform quick math. It's also relatively easy to parallelize.\n",
    "\n",
    "And now with all that out of the way, let's get started!\n",
    "\n",
    "## Training \n",
    "\n",
    "In the interests of tutorial length, we're not covering how to train a Haar cascade classifier. You can check the official [OpenCV](https://docs.opencv.org/3.3.0/dc/d88/tutorial_traincascade.html) tutorial for details. Essentially, you'd need a folder containing positive samples and a folder containing negative samples, e.g., one folder containing cropped images of bananas and another folder containing anything that isn't a banana. Going by advice online, you probably want at least 50 positive samples and 500 negative samples. Once you have that, OpenCV will use the following Haar features [by default](https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html):\n",
    "\n",
    "![haar features](https://docs.opencv.org/2.4/_images/haarfeatures.png)\n",
    "\n",
    "## Use\n",
    "\n",
    "For the purposes of this tutorial, we're just going to use the [inbuilt](https://github.com/opencv/opencv/tree/master/data/haarcascades) OpenCV classifiers. We're interested in the one that recognizes faces from the front angle, ```haarcascade_frontalface_default.xml```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# this is global since we only need to initialize it once\n",
    "# the haarcascade is supplied by OpenCV\n",
    "face_cascade = cv2.CascadeClassifier(HAAR_PREFIX + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "class FaceExtractingTransformation(Transformation):\n",
    "    \"\"\"\n",
    "    Transformation that extracts and saves faces from a frame.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 speed : float = 1.1,\n",
    "                 face_threshold : int = 15,\n",
    "                 min_face_size : int = 30,\n",
    "                 base_path : str = 'faces/img{}.png',\n",
    "                 save_index : int = 0,\n",
    "                 draw_face : bool = True):\n",
    "        \"\"\"\n",
    "        Creates a FaceExtracting transformation.\n",
    "        \n",
    "            speed: as speed increases, speed of facial recognition increases\n",
    "                cost: accuracy\n",
    "                note: for facial recognition, people typically use values between 1.05 and 1.4\n",
    "            \n",
    "            face_threshold: as threshold increases, false positives decreases\n",
    "                cost: losing true positives\n",
    "                note: typically in the range of 3 to 20\n",
    "                \n",
    "            min_face_size: minimum length and height of the faces found\n",
    "            base_path: path that images are saved to\n",
    "            start_index: first number in numbering for saving images\n",
    "            draw_face: true if we should draw a rectangle around the faces we extracted\n",
    "        \"\"\"\n",
    "        self.speed = speed\n",
    "        self.face_threshold = face_threshold\n",
    "        self.min_face_size = min_face_size\n",
    "        self.base_path = base_path\n",
    "        self.save_index = save_index\n",
    "        self.draw_face = draw_face\n",
    "        \n",
    "\n",
    "    def get_faces(self, image : RGBImage,\n",
    "                  speed : float,\n",
    "                  threshold : int,\n",
    "                  min_face_size : int) -> List[Tuple[int,int,int,int]]:\n",
    "        \"\"\"\n",
    "        Gets the faces from the image.\n",
    "\n",
    "        Faces are returned as bounding box coordinates, (x,y,w,h).\n",
    "\n",
    "        speed is tunable.\n",
    "            As speed increases, computation time decreases.\n",
    "            But accuracy decreases too.\n",
    "        threshold is tunable.\n",
    "            As threshold increases, the number of false positives decreases.\n",
    "            But true positives may be lost.\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray,\n",
    "                                              speed,\n",
    "                                              threshold,\n",
    "                                              minSize=(min_face_size, min_face_size))\n",
    "        return faces\n",
    "    \n",
    "    \n",
    "    def save_face(self, face : RGBImage) -> None:\n",
    "        \"\"\"\n",
    "        Saves the provided face to disk.\n",
    "        \"\"\"\n",
    "        plt.imsave(self.base_path.format(self.save_index), face)\n",
    "        self.save_index += 1\n",
    "\n",
    "    \n",
    "    def draw_rectangle(self, \n",
    "                       image : RGBImage,\n",
    "                       top_left : Tuple[int,int],\n",
    "                       bot_right : Tuple[int,int],\n",
    "                       color : Tuple[int,int,int] = (255,0,0),\n",
    "                       thickness : int = 2) -> None:\n",
    "        \"\"\"\n",
    "        Draws a rectangle touching the top_left and bot_right coordinates.\n",
    "        \"\"\"\n",
    "        if self.draw_face:\n",
    "            cv2.rectangle(image, top_left, bot_right, color, thickness)\n",
    "        \n",
    "    \n",
    "    def transform(self, image : RGBImage) -> RGBImage:\n",
    "        faces = self.get_faces(image, self.speed, self.face_threshold, self.min_face_size)\n",
    "        \n",
    "        for (x,y,w,h) in faces:\n",
    "            face = image[y:y+h, x:x+w]\n",
    "            self.save_face(face)\n",
    "            self.draw_rectangle(image, (x,y), (x+w,y+h))\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# this tends to take a while\n",
    "# locally, this generated 3.1k images\n",
    "\n",
    "# the recommended order of tuning for speed/accuracy is:\n",
    "#   0. test that it works with show=True. But generate data with show=False\n",
    "#   1. try increasing min_face_size\n",
    "#   2. try adjusting skip\n",
    "#   2. try increasing speed\n",
    "\n",
    "process_video('video.mp4', \n",
    "              transformations=[FaceExtractingTransformation()], \n",
    "              show=False,\n",
    "              skip=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/bli4HcP.gif\" width=\"300\" style=\"float:left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face clustering and picking representatives\n",
    "\n",
    "With a folder full of faces, we'd like to group faces belonging to the same people together.\n",
    "\n",
    "It is really easy to do this with the ```face_recognition``` library.\n",
    "\n",
    "For each image file,\n",
    "\n",
    "1. we load it with ```face_recognition.load_image_file(filepath)```\n",
    "2. we encode it with ```face_recognition.face_encodings(image)```. Because we used a relatively inaccurate Haar cascade to get faces, there is a chance that this fails, in which case we will ignore it and proceed to the next face.\n",
    "3. we can compare an encoded face against a list of other encoded faces with ```face_recognition.compare_faces(other_faces, current_face)```\n",
    "\n",
    "## Picking a representative face\n",
    "\n",
    "Upon successfully clustering images such that all of the images represent the same person, you'd normally just output \"yup, person X is here\" and call it a day. But we have no names! Instead, we'll aim to pick any one of their faces with equal probability. If we knew how many faces they have in total, we could just pick face number ```random.randint(0, len(faces))```. But we don't know how many faces there are in advance! (e.g. live security camera)\n",
    "\n",
    "We pull out a probability party trick here.\n",
    "\n",
    "## Reservoir sampling\n",
    "\n",
    "Suppose that your phone only has memory for one photo. You can take photos multiple times, overwriting the old one. You see an interesting row of houses in the distance. As you walk from the first house to the last, you want to take a photo of any house with uniform $\\frac{1}{n}$ probability. But you don't know how many houses there are, and you don't want to walk past the houses more than once! How can you do this?\n",
    "\n",
    "It turns out that the strategy is to take a photo of house $i$ with probability $\\frac{1}{i}$. To convince yourself of this, consider different situations for $n$.\n",
    "\n",
    "- 1 house: you definitely want to take the photo with probability 1\n",
    "- 2 houses: you want both houses to have probability half. Well, photograph the first house and with probability half replace it with the second house.\n",
    "\n",
    "And in general you can do a proof by induction to show that everything works out.\n",
    "\n",
    "## Back to our problem\n",
    "\n",
    "We'll simply need to maintain counts of each person's face as we come across them, and replace our representative face for that person with probability $\\frac{1}{\\text{number of faces for that person}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "faces = [] # encodings of faces\n",
    "face_counts = {} # counts for reservoir sampling\n",
    "face_stored = {} # int label -> filepath to representative face\n",
    "\n",
    "\n",
    "def add_faces(filepath : str) -> Union[int, None]:\n",
    "    \"\"\"\n",
    "    Loads the face located at filepath and compares it against all known faces.\n",
    "    \n",
    "    If the face could not be loaded, it returns None.\n",
    "    \n",
    "    If the face matches with an existing face, we output the match index,\n",
    "    i.e. the index of the face in our list of known faces which matched.\n",
    "    \n",
    "    Otherwise, we save the new face to our list of faces and output its index.\n",
    "    \n",
    "    In both cases, the index output is the index of the face's representative\n",
    "    in our list of known faces.\n",
    "    \"\"\"\n",
    "    image = face_recognition.load_image_file(filepath)\n",
    "    encoding = face_recognition.face_encodings(image)\n",
    "\n",
    "    if len(encoding) == 0:\n",
    "        # if we couldn't load a face\n",
    "        return None\n",
    "    else:\n",
    "        # otherwise grab the first face\n",
    "        encoding = encoding[0]\n",
    "\n",
    "    # try matching the face to existing faces, if it matched, return\n",
    "    matches = face_recognition.compare_faces(faces, encoding)\n",
    "    try:\n",
    "        first_match_index = matches.index(True)\n",
    "        return first_match_index\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # if no match, add the face and return\n",
    "    faces.append(encoding)\n",
    "    return len(faces) - 1\n",
    "\n",
    "\n",
    "# we load our faces and populate our faces, face_counts, face_stored\n",
    "for filename in os.listdir('faces'):\n",
    "    if not filename.startswith('.'):\n",
    "        filepath = 'faces/' + filename\n",
    "        face_label = add_faces(filepath)\n",
    "        \n",
    "        # if we couldn't add the face, skip it\n",
    "        if face_label is None:\n",
    "            continue\n",
    "        \n",
    "        # otherwise perform reservoir sampling\n",
    "        face_counts[face_label] = face_counts.get(face_label, 0) + 1\n",
    "        rand = random.randint(1, face_counts[face_label])\n",
    "        # only accept one of the count possibilities\n",
    "        if rand == 1:\n",
    "            face_stored[face_label] = filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing our representatives\n",
    "\n",
    "What's a convenient way to see many pictures at once?\n",
    "\n",
    "A video! Well, a photo [collage](http://answers.opencv.org/question/15589/make-a-collage-with-other-images/) works, but that gets overwhelming with too many faces. A video scales better.\n",
    "\n",
    "OpenCV includes VideoWriter, simple but limited:\n",
    "\n",
    "- keep it in AVI format\n",
    "- use the \"fourcc mp4v\" codec\n",
    "\n",
    "For actual video editing, you'd probably use [moviepy](https://zulko.github.io/moviepy/) or [ffmpeg](https://www.ffmpeg.org/) directly. The OpenCV developers aren't interested in expanding video support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote out.avi which has 36 faces.\n"
     ]
    }
   ],
   "source": [
    "output_file = 'out.avi' # must be avi\n",
    "output_codec = cv2.VideoWriter_fourcc('m', 'p','4','v') # this one tends to work better\n",
    "fps = 1 # 1 frame per second = 1 face per second\n",
    "width, height = (600, 600) # video dimensions\n",
    "out = cv2.VideoWriter(output_file, output_codec, fps, (width, height))\n",
    "\n",
    "for filepath in face_stored.values():\n",
    "    image = cv2.imread(filepath)\n",
    "    out.write(cv2.resize(image, (width, height))) # important: video and image dimensions must match\n",
    "\n",
    "out.release()\n",
    "\n",
    "print(\"Wrote {} which has {} faces.\".format(output_file, len(face_stored)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "process_video('out.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/wAyzaFQ.gif\" height=300 width=300 style=\"float:left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this tutorial, we have:\n",
    "\n",
    "1. Learned to use ```pytube``` to download YouTube videos for data\n",
    "2. Prepared an image processing pipeline for videos\n",
    "3. Learned a little theory about Haar classifiers - math with rectangles for describing faces\n",
    "4. Used the built-in Haar classifiers of ```OpenCV``` to extract all the faces from a video\n",
    "5. Learned how to recognize faces using ```face_recognition```, which uses machine learning behind the scenes\n",
    "6. Used face classification to separate a collection of faces into clusters of faces, where each cluster is of the same face\n",
    "6. Learned a little theory behind reservoir sampling, useful when you want to have uniform samples of an unknown-possibly-huge population\n",
    "7. Used reservoir sampling to pick a cluster representative\n",
    "7. Used ```OpenCV``` to create simple videos\n",
    "\n",
    "# Limitations\n",
    "\n",
    "1. We're only using the frontal face Haar feature. To *really* capture as many faces as possible, we should also find and/or train a Haar feature for side profiles of faces.\n",
    "\n",
    "2. Haar is 2000s technology. Most people are all about neural networks now. I thought it would be neat to look at classic Haar to see that it still performs reasonably well.\n",
    "\n",
    "3. I had to run this code on my computer, which is decidedly not strong enough for this. I had to use a relatively fast speed. If you had access to fancy GPUs and/or powerful CPUs, try speed=1.05 and a min_face_size of 0.\n",
    "\n",
    "# Extensions\n",
    "\n",
    "Once you have a picture of a specific face you're interested in, e.g., in your faces/ folder after running the above code: you can pass it as the argument to ```facial_recognition.compare_faces``` to check if that's the person you want.\n",
    "\n",
    "- You can check for all the known people this way (e.g. employees at a bank)\n",
    "- You can extract all the video frames in which someone appears and make that a new video (focusing on specific person)\n",
    "\n",
    "We can also preprocess the frame to improve face detection. For example, we can auto-level the image brightness to improve the lighting conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
