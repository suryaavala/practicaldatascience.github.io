{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost \n",
    "In this tutorial, I will introduce the Adaboost algorithm. The outline is as follows.\n",
    "* Introduction\n",
    "* Algorithm detail\n",
    "* Sample application\n",
    "* Algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Boosting\n",
    "Boosting is an algorithm to combine multiple weak learning algorithm to a strong learning algorithm. If we use binary classification as examples. Weak classifiers are those which only can achieve a slightly better accuracy than the random guessing by the probability. The boosting ensembles many weak classifiers together, making a strong classifiers. When the multiple weak classifiers are easy to train, which means they can be train with limited computing resource, the boosting will be very useful.\n",
    "\n",
    "## Adaboost\n",
    "Adaboost means “Adaptive Boosting”, which first created by Yoav Freund and Robert Schapire in 1995<sup>[1]</sup>. The underlying logic is to assign weights to the data samples. When the model is being trained, the model will assign more weights to data samples which are classified wrongly by the first classifier. Then it will introduce a new weak classifiers and run classification on the updated data samples, untill the mode converge to an accepeted loss or the max iterations. Also, in each iteration, the model will calculate the error rate for each weak classifier. After all iterations, the ensembled strong classifier will be a weighed summary of the weak classifiers, when the weights are calculated by their error rate. Then the classifiers with lower error rate will have a higher weights in the ensembled classifier. <br>\n",
    "Because the ensembled classifiers are conbinations of weak classifiers, they are less prone to overfitting. Weak classifiers such as one level decision tree are not likely to overfit the training dataset. Also, the ensembled model sometimes can have better accuracy than other strong classifiers. As a result, Adaboost becomes a popular ensembling algorithm in machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm detail\n",
    "#### Step 1\n",
    "First, we need to initiate the weights. If we have n data samples, we will assign 1/n to each data sample.\n",
    "W<sub>1</sub> = (w<sub>11</sub>,w<sub>1j</sub>,...,w<sub>1N</sub>)\n",
    "#### Step 2\n",
    "Then, we will iterate I rounds. In each round, we use a new weak classifier<br>\n",
    "For i = 1,2,...,I: <br>\n",
    "1. Train the weak classifier L<sub>i</sub>(x)\n",
    "2. Calculate the error rate of the classifier e<sub>i</sub>\n",
    "3. Calculate the weight of this weak classifier in the ensembled classifier a<sub>i</sub><br>\n",
    "In the Yoav Freund's paper, they used $ \\frac{1}2log(\\frac{1 - {e_i}}{e_i})$. With this equation, the classifiers with lower error rates will have a higher weights in the ensembled classifier. Though this, the ensembled classifier can be more accrate since it tend to rely more on the accurate weak classifier.\n",
    "4. Update the weights of the data samplesW<sub>i + 1</sub>\n",
    "In this sub step, we will increase the weights of those samples that are classified wrongly. So the classifier will focus more on the samples that are hard to classify.\n",
    "\n",
    "\n",
    "\n",
    "#### Step 3   \n",
    "Finally, we got our ensembled binary classifier. $ sign(\\sum_{i = 1}^{I}{a_iL_i(x)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample application\n",
    "In this section, I will use adaboost to train a classifier to classify if the network incoming trafic is malicious or benign. To analyze malicious traffic, I use the KDD-99 data<sup>[2]</sup>, which is widely used for the evaluation of the traffic classify. The attacks in the KDD-99 data are categorized in four categories. They are denial of service attack, user to root attack, remote to local attack, probing attck, and remote to local attack. Because Adaboost is suitable for binary classification. I will classify the data to malicious (1) or not (-1). <br>\n",
    "To make the classifications, there are 41 features. It includes some property features such as the protocol, service and so on. it also consits of may behavior features,which are the behavior of the same host or same services. Those features are extracted based on the domain knowledge. We are going to use all of them.\n",
    "The detail description of the data can be found [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and preprocessing\n",
    "First, I will clean the data and process it to the desired format. Since the data is procurated, I do not need to handle the missing value or outliers. But we still need to prepare the data to the format that the model can consume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(path):\n",
    "    column = []\n",
    "    with open(\"att\", 'r') as att:\n",
    "        for line in att:\n",
    "            column.append(line.split(\":\")[0])\n",
    "    column.extend(['label','category'])\n",
    "    match_dict = {'normal':'normal'}\n",
    "    with open (\"training_attack_types\", 'r') as match:\n",
    "        for line in match:\n",
    "            temp = line.split(\" \")\n",
    "            if len(temp) != 2:\n",
    "                continue\n",
    "            match_dict[temp[0].strip()] = temp[1].strip()\n",
    "    data = pd.read_csv(path,names = column)\n",
    "    data['category'] = data['label'].apply(lambda x: match_dict[x.strip('.')] if x.strip('.') in match_dict else None)\n",
    "    data = data.drop('label',1)\n",
    "    data = data.drop('service',1)\n",
    "    data = data[data.category.notnull()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, download the data from object storage\n",
    "train = data_clean('https://objectstorage.us-ashburn-1.oraclecloud.com/n/bestat/b/davidtesting/o/train2.csv')\n",
    "test = data_clean('https://objectstorage.us-ashburn-1.oraclecloud.com/n/bestat/b/davidtesting/o/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>SF</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>SF</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>dos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>SF</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>SF</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type flag  src_bytes  dst_bytes  land  wrong_fragment  \\\n",
       "0         0           tcp   SF        491          0     0               0   \n",
       "1         0           udp   SF        146          0     0               0   \n",
       "2         0           tcp   S0          0          0     0               0   \n",
       "3         0           tcp   SF        232       8153     0               0   \n",
       "4         0           tcp   SF        199        420     0               0   \n",
       "\n",
       "   urgent  hot  num_failed_logins    ...     dst_host_srv_count  \\\n",
       "0       0    0                  0    ...                     25   \n",
       "1       0    0                  0    ...                      1   \n",
       "2       0    0                  0    ...                     26   \n",
       "3       0    0                  0    ...                    255   \n",
       "4       0    0                  0    ...                    255   \n",
       "\n",
       "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                    0.17                    0.03   \n",
       "1                    0.00                    0.60   \n",
       "2                    0.10                    0.05   \n",
       "3                    1.00                    0.00   \n",
       "4                    1.00                    0.00   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.17                         0.00   \n",
       "1                         0.88                         0.00   \n",
       "2                         0.00                         0.00   \n",
       "3                         0.03                         0.04   \n",
       "4                         0.00                         0.00   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                  0.00                      0.00                  0.05   \n",
       "1                  0.00                      0.00                  0.00   \n",
       "2                  1.00                      1.00                  0.00   \n",
       "3                  0.03                      0.01                  0.00   \n",
       "4                  0.00                      0.00                  0.00   \n",
       "\n",
       "   dst_host_srv_rerror_rate  category  \n",
       "0                      0.00    normal  \n",
       "1                      0.00    normal  \n",
       "2                      0.00       dos  \n",
       "3                      0.01    normal  \n",
       "4                      0.00    normal  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>SF</td>\n",
       "      <td>105</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>254</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>SF</td>\n",
       "      <td>105</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>254</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>SF</td>\n",
       "      <td>105</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>254</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>SF</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>SF</td>\n",
       "      <td>105</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>253</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type flag  src_bytes  dst_bytes  land  wrong_fragment  \\\n",
       "0         0           udp   SF        105        146     0               0   \n",
       "1         0           udp   SF        105        146     0               0   \n",
       "2         0           udp   SF        105        146     0               0   \n",
       "3         0           udp   SF         29          0     0               0   \n",
       "4         0           udp   SF        105        146     0               0   \n",
       "\n",
       "   urgent  hot  num_failed_logins    ...     dst_host_srv_count  \\\n",
       "0       0    0                  0    ...                    254   \n",
       "1       0    0                  0    ...                    254   \n",
       "2       0    0                  0    ...                    254   \n",
       "3       0    0                  0    ...                      3   \n",
       "4       0    0                  0    ...                    253   \n",
       "\n",
       "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                    1.00                    0.01   \n",
       "1                    1.00                    0.01   \n",
       "2                    1.00                    0.01   \n",
       "3                    0.30                    0.30   \n",
       "4                    0.99                    0.01   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                          0.0                          0.0   \n",
       "1                          0.0                          0.0   \n",
       "2                          0.0                          0.0   \n",
       "3                          0.3                          0.0   \n",
       "4                          0.0                          0.0   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                   0.0                       0.0                   0.0   \n",
       "1                   0.0                       0.0                   0.0   \n",
       "2                   0.0                       0.0                   0.0   \n",
       "3                   0.0                       0.0                   0.0   \n",
       "4                   0.0                       0.0                   0.0   \n",
       "\n",
       "   dst_host_srv_rerror_rate  category  \n",
       "0                       0.0    normal  \n",
       "1                       0.0    normal  \n",
       "2                       0.0    normal  \n",
       "3                       0.0    normal  \n",
       "4                       0.0    normal  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.reset_index(drop=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the data is as follows. We notice the \"protocol_type\", \"flag\", \"land\", and \"service\" are categorical variables. We need to change them to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['duration', 'protocol_type', 'flag', 'src_bytes', 'dst_bytes', 'land',\n",
       "       'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
       "       'num_compromised', 'root_shell', 'su_attempted', 'num_root',\n",
       "       'num_file_creations', 'num_shells', 'num_access_files',\n",
       "       'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n",
       "       'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',\n",
       "       'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
       "       'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
       "       'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
       "       'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
       "       'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
       "       'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that the category will be processed to +1 and -1 as the binary class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['category'] = train['category'].apply(lambda x: -1 if x == 'normal' else 1)\n",
    "test['category'] = test['category'].apply(lambda x: -1 if x == 'normal' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to check if the data is imbalanced. Imbalanced data can lead to bias in binary classification problem. In this dataset, we notice the data is almost balanced. Adaboost is sensitive to imbalance data. So we need to make the train data balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of the normal traffic data point is 53.46 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Percentage of the normal traffic data point is %.2f \" %( 100 * len(train[train['category'] == -1]) / len(train) )) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of the normal traffic data point is 50.00 \n"
     ]
    }
   ],
   "source": [
    "# start modified from https://stackoverflow.com/questions/45839316/pandas-balancing-data\n",
    "g = train.groupby('category')\n",
    "train = g.apply(lambda x: x.sample(g.size().min())).reset_index(drop=True)\n",
    "# finish modified from https://stackoverflow.com/questions/45839316/pandas-balancing-data\n",
    "print(\"Percentage of the normal traffic data point is %.2f \" %( 100 * len(train[train['category'] == -1]) / len(train) )) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical variables are transformed to the binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, drop_first=True)\n",
    "test = pd.get_dummies(test, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost model training\n",
    "In this section, I will use the processed data to train an adaboost classifier. Also I will train 2 decition tree classifier model as benchmarks. <br>\n",
    "As I mentioned, the adaboost ensemble multiple weak classifiers, in this experienment, the weak classifier we use is decision tree stumps(decision tree with max_depth as 1). Many machine learning libarary has implemented the adaboost classifier. Since we use scikit-learn a lot in other assignments, we will use this liabarary as an example.<br>\n",
    "I also train a decision tree stump to display how much the ensembled model improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "trainY = np.array(train['category'])\n",
    "trainX = np.array(train.drop('category', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark1 = DecisionTreeClassifier(max_depth=1)\n",
    "abc = AdaBoostClassifier(benchmark1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_model = abc.fit(trainX, trainY)\n",
    "benchmark1_model = benchmark1.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training accuracy of decision stump is 0.9219\n",
      "the training accuracy of ensembled model is 0.9851\n"
     ]
    }
   ],
   "source": [
    "benchmark1_acc = benchmark1_model.score(trainX,trainY)\n",
    "abc_acc = abc_model.score(trainX,trainY)\n",
    "print (\"the training accuracy of decision stump is %.4f\" % benchmark1_acc)\n",
    "print (\"the training accuracy of ensembled model is %.4f\" % abc_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that on trainging dataset, the accuracy of ensembled is slightly better than the simple dicision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "In this section, I use test data to evaluate how the model perform. To evaluate the performance of the model, I use both accuracy and auc score as measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "testY = np.array(test['category'])\n",
    "testX = np.array(test.drop('category', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test accuracy of decision stump is 0.4161\n",
      "the test accuracy of ensembled model is 0.9808\n"
     ]
    }
   ],
   "source": [
    "benchmark1_acc_t = benchmark1_model.score(testX,testY)\n",
    "abc_acc_t = abc_model.score(testX,testY)\n",
    "print (\"the test accuracy of decision stump is %.4f\" % benchmark1_acc_t)\n",
    "print (\"the test accuracy of ensembled model is %.4f\" % abc_acc_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_abc = abc_model.predict(testX)\n",
    "pred_b1 = benchmark1_model.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I calculate te auc score of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# reference: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n",
    "def auc_score(pred):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(testY, pred, pos_label=1)\n",
    "    return metrics.auc(fpr, tpr)\n",
    "# reference: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n",
    "score_abc = auc_score(pred_abc)\n",
    "score_b1 = auc_score(pred_b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use matplotlib to draw a bar chart on auc score to two classifiers to show the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ff17da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(np.array([1,2]), (score_abc, score_b1), 0.4,color='lightskyblue')\n",
    "ax.set_xticks(np.array([1,2]))\n",
    "ax.set_xticklabels(('Adaboost','Decision Stump'))\n",
    "ax.set_title('AUC score of the model on test set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart above shows that the adaboost performance much better than the decision stump classifier on the test dataset. The reason why the decision stump perform much worse is because it overfits the data and fail to classify the new testing data. If we increase the depth of the decision tree, the accuracy and the aoc score will be better, however, this chart still dispaly how Adaboost can make weak classifiers such as the decision stump to a strong classifier. It is also much less prone to overfitting than the decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Adaboost\n",
    "In this last section, I will implement a simple version of adaboost algorithm to help you understand the algorithm details more easily. Because the focus of this implementations are Adaboost algorithm, I will still use the decision tree classifiers as the weak classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "class Adaboost():\n",
    "    \n",
    "    classifier = None\n",
    "    iterations = 100\n",
    "    classifier_list = []\n",
    "    significance_list = []\n",
    "    def __init__(self, classifier, iterations):\n",
    "        self.classifier = classifier\n",
    "        self.iterations = iterations\n",
    "        self.classifier_list = []\n",
    "        self.significance_list = []\n",
    "        \n",
    "    def train(self, data, label, verbose=False):\n",
    "        print ()\n",
    "        print (\"model training started\")\n",
    "        if len(data) != len(label):\n",
    "            print (\"data length does not match, abort\")\n",
    "            return\n",
    "        weights = np.array([1] * len(data)).T\n",
    "        weights = weights / weights.sum()\n",
    "        for i in range(0, self.iterations):\n",
    "            # train the weak classifier\n",
    "            classifier = self.classifier.fit(data, label, sample_weight=weights)\n",
    "            self.classifier_list.append(copy.deepcopy(classifier))\n",
    "            # print (classifier)\n",
    "            # calculate the error rate of the weak calssifier\n",
    "            prediction = classifier.predict(data)\n",
    "            correct = [0] * len(label)\n",
    "            wrong = [0] * len(label)\n",
    "            for j in range(0, len(label)):\n",
    "                if label[j] == prediction[j]:\n",
    "                    correct[j] = 1\n",
    "                else:\n",
    "                    wrong[j] = -1\n",
    "            # note that the error is weighted\n",
    "            error = - np.dot(np.array(wrong),(weights.T))\n",
    "            if verbose == True:\n",
    "                print(\"The weighted error of classifier \"+ str(i) + \" is \" + str(error))\n",
    "\n",
    "            # Calculate the weight of this weak classifier in the ensembled classifier \n",
    "            significance = math.log((1 - error) / error) / 2\n",
    "            self.significance_list.append(significance)\n",
    "            if verbose == True:\n",
    "                print(\"The importance of classifier \"+ str(i) + \" is \" + str(significance))\n",
    "\n",
    "            # update the weights\n",
    "            updated_para = np.exp(- significance * (np.array(correct) + np.array(wrong)).T )\n",
    "            weights = weights * updated_para\n",
    "            #normalize the weights\n",
    "            weights = weights / weights.sum()\n",
    "        print (\"model training finished\")\n",
    "        print ()\n",
    "\n",
    "    def predict(self, data, correct_label=None):\n",
    "        sumprob = np.array([0.0] * len(data))\n",
    "        \n",
    "        for i in range(0, len(self.classifier_list)):\n",
    "            #print(self.classifier_list[i].predict(data))\n",
    "            # add all weak classifiers\n",
    "            sumprob += self.significance_list[i] * self.classifier_list[i].predict(data)\n",
    "        # print(sumprob)\n",
    "        # sign(sum(all weak classifiers))\n",
    "        res = []\n",
    "        for j in sumprob:\n",
    "            if j > 0:\n",
    "                res.append(1)\n",
    "            else:\n",
    "                res.append(-1)\n",
    "        if not correct_label is None:\n",
    "            cnt = 0\n",
    "            for k in range(0, len(correct_label)):\n",
    "                if correct_label[k] != res[k]:\n",
    "                    cnt += 1\n",
    "            #print(cnt)\n",
    "            print (\"The accuracy of the prediction is \"+ str(1 - cnt / len(correct_label)))\n",
    "        return np.array(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementation, we can use the same data to train the model and get the accuracy to veify that the implementation is correcct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train the model\n",
      "\n",
      "model training started\n",
      "model training finished\n",
      "\n",
      "The accuracy of the prediction is 0.9727187446699642\n",
      "The accuracy of the prediction is 0.9768286007526514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Adaboost(benchmark1,50)\n",
    "print (\"train the model\")\n",
    "test.train(trainX, trainY)\n",
    "\n",
    "test.predict(trainX, trainY)\n",
    "test.predict(testX, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and more resources\n",
    "This tutorial introduce the theory and the applications of the Adaboost algorithm. Adaboost is one of the most widely used boosting algorithm. Common appliances include face detection and recoginition, because the preformance of it is good interms of accuracy and runtime. I hope this tutorial can work as a first step of adaboost and boosting. If you are interested in diving deeper into it. You can check these papaers:\n",
    "* [multi class Adaboost](https://www.intlpress.com/site/pub/files/_fulltext/journals/sii/2009/0002/0003/SII-2009-0002-0003-a008.pdf)\n",
    "* [face detection application](http://www.inoa.it/home/cosimo/EI/adaboostforfacedetection.pdf)\n",
    "\n",
    "\n",
    "\n",
    "<br> Apart from the Adaboost, other boosting algorithms also play very important role in the current macine learning area. Many Kaggle winner team will also use boosting such as Adaboost and [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) to ensemble the classifiers and achieve a high rank in the competition. [xgboost](https://github.com/dmlc/xgboost) is a library that most Kaggle winners are using to ensemble the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "[1] Freund, Y. (1995). Boosting a weak learning algorithm by majority. Information and computation, 121(2), 256-285.\n",
    "\n",
    "[2] Cost-based Modeling and Evaluation for Data Mining With Application to Fraud and Intrusion Detection: Results from the JAM Project by Salvatore J. Stolfo, Wei Fan, Wenke Lee, Andreas Prodromidis, and Philip K. Chan. \n",
    "\n",
    "[3] Hastie, T., Rosset, S., Zhu, J., & Zou, H. (2009). Multi-class adaboost. Statistics and its Interface, 2(3), 349-360.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
