{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "This tutorial will introduce some basic ideas concerning Convolutional Neural Network and how to implement an CNN model to solve supervised learning problem, Twitter feeds sentiment analysis, using Tensorflow convolutional neural network module. \n",
    "As the input data generated in our daily life has included more high dimensional data like image and text rather than only structured data like numbers, we need deep learning model to solve complex practical problems like image classification. For some traditional model like linear regression, it will be hard to project a high dimensional data into a vector without loosing some information or keep the number of free parameters in a reasonable level, while in deep learning, we can handle this in a state-of-the-art way. Convolutional neural network is an important part of deep learning. Following is a picture illustrate how CNN can classify the images.\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*oB3S5yHHhvougJkPXuc8og.gif\">\n",
    "We can see from the image that there are a fixed input layer and fixed output layer and also multiple hidden layers. Although CNN firstly introduced to solve image classification problem, it is also proved efficient subsequently on the semantic parse for natural language process(1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "This tutorial will show how to build a simple convolutional neural network model to identify the sentiment information concerning Twitter feeds, taking advantage of [Tensorflow](https://www.tensorflow.org/tutorials/deep_cnn) and [pretrained google Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) model. The pretrained word vector file can be downloaded [here](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "This tutorial will use Twitter Sentiment Analysis Data Set from [Ibrahim Naji's blog](http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/). Because there will be a lot of mannual working to do the annotation, I just used the free annotated dataset available on Internet, this csv file contains nearly 1.6 million records with the Twitter feeds text content and sentiment flag(0 stand for negative, 1 stand for positive).\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Convolutional Neural Network Overview](#Convolutional-Neural-Network-Overview)\n",
    "- [Libraries installation](#Installing-the-libraries)\n",
    "- [Loading and preprocessing data](#Loading-data-and-preprocessing)\n",
    "- [Model implementation details](#Model-implementation)\n",
    "- [Code integration](#Code-integration)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Reference](#Reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.mdpi.com/information/information-07-00061/article_deploy/html/images/information-07-00061-g001.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the picture above, we can have a brief overview of convolutional neural network. \n",
    "First comes the input layer, this layer is usually a high dimensional data, for example a couple of 2-D matrix can form a 3-D input layer. When it comes to convolutional layer, there is a filter, which works like a flashlight, it has fixed size in our case. The filter will go the path we configured through all the data in input layer and form a smaller size of data layer, the process is just like the gif below. Then comes the pooling layer, this layer is mainly for downsampling strategy which will further reduce data set, different from the convolutional layer, pooling layer ysyally has no overlapping and is also used to avoid overfitting. After we have down the most featured part of CNN, we will have two fully connected layer similar to normal neural network. Note: there may be many convolutional layers and pooling layers according to the model design, while for tutorial purpose, only one convolutional layer and one polling layer will are included in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before build the model with tensorflow, many libraries and packages need to be installed to continue with the following process.  You can install Tensorflow and gensim use anaconda:\n",
    "\n",
    "### To install tensorflow, you need to execute the following codes.\n",
    "Create tensorflow conda environment:\n",
    "\n",
    "$conda create -n tensorflow pip python=3.5 \n",
    "\n",
    "Active conda environment:\n",
    "\n",
    "$activate tensorflow(the prompt will change to tensorflow)\n",
    "\n",
    "Install tensorflow in conda environment(First is CPU-only and second is GPU version):\n",
    "\n",
    "$pip install --ignore-installed --upgrade tensorflow \n",
    "\n",
    "$pip install --ignore-installed --upgrade tensorflow-gpu\n",
    "\n",
    "CPU-only version is enough in the scope of this tutorial to execute model training and evaluation.\n",
    "\n",
    "### To install gensim, you need to execute the following code:\n",
    "$ conda install -c anaconda gensim\n",
    "Note: to use the pretrained word vectore model, you still need to download the google pretrained model in this [site](https://code.google.com/archive/p/word2vec/) and put that file under the same folder with this file.\n",
    "After tou have installed all the needed packages and files, make sure that the following code work for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "from tensorflow.contrib import learn\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For we have installed and loaded all the libraries we need, let's load our data set first and do some preprocessing. The file downloaded from this [site](http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/), the click the Twitter Sentiment Analysis Dataset hyerlink in that page(in the beginning of the fourth paragraph). Then a Sentiment-Analysis-Dataset.zip file will be downloaded and upzip it, there is only one file 'Sentiment Analysis Dataset.csv' inside it. Put the csv file in this dictionary so that we can load the data using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Sentiment Analysis Dataset.csv',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we run the code, we see there are two warning message saying the two line of bad data is ignore. After looking into the original dataset, we found that it is because of some parse errors, for we have had enough training set, so we can just ignore the two records and go on with our data exploration and preprocessing with current data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ItemID  Sentiment SentimentSource  \\\n",
      "0       1          0    Sentiment140   \n",
      "1       2          0    Sentiment140   \n",
      "2       3          1    Sentiment140   \n",
      "3       4          0    Sentiment140   \n",
      "4       5          0    Sentiment140   \n",
      "\n",
      "                                       SentimentText  \n",
      "0                       is so sad for my APL frie...  \n",
      "1                     I missed the New Moon trail...  \n",
      "2                            omg its already 7:30 :O  \n",
      "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
      "4           i think mi bf is cheating on me!!!   ...  \n"
     ]
    }
   ],
   "source": [
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the printed head 10 records we can see that only the second and last column are needed to train our model. In addition, for the input layer format in Tensorflow, y is a list of length n, where n is the number of classes in this model, in our case, we just have two classes(potitive and engative). So we still need to encode the 'Sentiment' column to a list with length 2([0,1] stand for negative, [1,0] stand for positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#list which store the twitter \n",
    "feed_list = []\n",
    "\n",
    "# list which store the labels of every record\n",
    "label = []\n",
    "# record the largest length of one feed\n",
    "twitter_length = 0\n",
    "# Iterate through the dataset to get the columns we want and encode the label column\n",
    "for index, row in dataset.iterrows():\n",
    "    text = row['SentimentText']\n",
    "    # first trnasfer all the letters to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # write regular expression to clean the unstructured data and only get the words inside it\n",
    "    text = text.replace(\"\\'s\", \"\")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    #join words of one twitter feeds with space and append it to feed_list\n",
    "    clean = regex.sub(' ', text)\n",
    "    words = nltk.word_tokenize(clean)\n",
    "    twitter_length = max(twitter_length,len(words))\n",
    "    feed_list.append(' '.join(words))\n",
    "    #encode label\n",
    "    if row['Sentiment'] == 0:\n",
    "        label.append([0,1])\n",
    "    else:\n",
    "        label.append([1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CNN model can only deal with high dimensional numbers, we need to transform each word to a vector and then form a 3-D array and store them in a tensor(the trainable unit in Tensorflow). As we mentioned above, we are going to use the Google pretrained model to get the word vector. If some word dose not existed in this pretrained model, then we are going to use certain noise word to replace the word. On th other hand, because the size of input data has to be the same for every  record, we need to fill every feed to the same size, the largest text length, 'twitter_length'(the number we get in data preprocessing). Again we will use certain noise word to fill the sentence.(in this tutorial we use 'the'). Then we are ready to implement the code to get the input tensor for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initiate the pretrained word vector model\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model is the pretrained model. We pass it as parameter so we just need to load the model once\n",
    "# feeds is a list of twitter feeds we get from data loading part\n",
    "def get_vectors(model,feeds):\n",
    "    # high dimensional data which will be assigned to input tensor\n",
    "    final_input = []\n",
    "    for sentence in feeds:\n",
    "        #the matrix of one certain feed\n",
    "        mid_input = []\n",
    "        i = 0\n",
    "        for word in sentence.split(' '):\n",
    "            i = i + 1\n",
    "            if word in model.vocab:\n",
    "                mid_input.append(model.word_vec(word))\n",
    "            else:\n",
    "                mid_input.append(model.word_vec('the'))\n",
    "        #fill the feed to the largest length\n",
    "        while i < twitter_length:\n",
    "            mid_input.append(model.word_vec('the'))\n",
    "            i = i+1\n",
    "        final_input.append(mid_input)\n",
    "    return final_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have get all the knowledge and input data we need to feed our CNN model(hopefully), then we configure the hidden layers inside our network, including number of hidden layers, filter size, pooling size. Regarding features of our data and experience, we will have three hiden layers totally: one convolutional layer, one pooling layer and one fully connected layer. Then for every layer, we need to initiate weights for every neuron. The configuration code is below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional & Pooling Layer\n",
    "In convolutional layer, we are going to implement the convolution to our input data. How tensorflow doing so is first flat the high dimensional data, including both the input data and weight matrix and then right multiply weight matrix with flatted input matrix. In our case, we would like to set the filter length to 3, because 3-gram seems enough to get the sentiment for certain sentenece, and to simplify the model and speed up the training process, we are going to set the filter width to 300, so the filter in convolutional layer will be look like this:<img src = \"http://www.wildml.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM.png\">\n",
    "note that there are three size of filter in the image: 2,3 and 4, while in our case, we only consider the filter length 3. And we are using Relu to calculate the output of each filter. \n",
    "For the pooling layer, just like the image above, we will use a filter length of 'twitter_length-2', the number of filters can be defined by user, it defined how many filters will be applied to the input data, the following is the code sample of the two layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer configuration\n",
    "# You can set filter_number to any number you want.\n",
    "# Bigger the filter_number, more complex will the model be. The model will also fit better to training data\n",
    "filter_number = 300\n",
    "filter_length = 3\n",
    "filter_width = 300\n",
    "conv_layer = {'filter_length' : filter_length ,'filter_width': filter_width,'out_channel':filter_number}\n",
    "pool_layer = {'filter_length':filter_number}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT EXECUTE\n",
    "\"\"\"\n",
    "# NOTE: This chunck is only for illustrating the use of functions. Please do not run it.\n",
    "\n",
    "# input_data is the high dimensional data we get by get_vectors method.\n",
    "# input_data is of size[batch,twitter_length,300,1]\n",
    "# weight is the randomely genrated weight of certain size\n",
    "# weight is of size[filer_length*filter_width*in_channel,out_channel]\n",
    "conv_mid = tf.nn.conv2d(input_data,weight,strides=[1, 1, 1, 1],padding='VALID')\n",
    "# After conv2d function we can get a relatively smaller high dimensional data compared to input_data\n",
    "# conv_mid is of size[batch,twitter_length-filter_length+1,1,filter_number]\n",
    "# Apply Relu after add bias\n",
    "# bias will be initiated as a constant with size [filter_number]\n",
    "conv_out = tf.nn.relu(tf.nn.bias_add(conv_mid, bias))\n",
    "# The max_pool function will generate a tensor of size[1,1,1,filter_number]\n",
    "pool_mid = tf.nn.max_pool(conv_out,ksize=[1,twitter_length-filter_length+1, 1, 1],strides=[1, 1, 1, 1],padding='VALID')\n",
    "# For there are many redundancy on the dimension(many 1s in shape), so we decide to flat the tensor before we process to the output layer\n",
    "pool_out = tf.reshape(pool_mid, [-1, filter_number])\n",
    "# Then finally we get a tensor of size[1,filter_number] and are ready to go to output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of relevant functions can be found below:\n",
    "\n",
    " [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
    " \n",
    " [tf.nn.relu](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n",
    " \n",
    " [tf.nn.max_pool](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool)\n",
    " \n",
    " [tf.reshape](https://www.tensorflow.org/api_docs/python/tf/reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Layer\n",
    "The output layer is a fully connected layer and will generate a tuple(two numbers), each indicating the scores each class get considering the input data. Then we use argmax function to get the predicted label of input data. We are also going to calculate the accuracy and error in this layer for model development and evaluation. The code sample is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT EXECUTE\n",
    "\"\"\"\n",
    "# NOTE: This chunck is only for illustrating the use of functions. Please do not run it.\n",
    "\n",
    "# This function will calculate the final results\n",
    "# weight is randomly initiated with size [filter_number,2]\n",
    "# bias is also randomly generated with size [2]\n",
    "results = tf.nn.xw_plus_b(pool_out, weight, bias)\n",
    "# use argmax to get the predicted label(index) of the input data\n",
    "# For a batch of data with n records, predictions will be a list of index\n",
    "predictions = tf.argmax(scores, 1, name='predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of relevant functions can be found below:\n",
    "\n",
    " [tf.nn.xw_plus_b](https://www.tensorflow.org/api_docs/python/tf/nn/xw_plus_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are going to build the training, development and evaluation function. The purpose of development is to evaluation the model during the process of training, so we can tunning our model better. Then before going to training and evaluation, we need to define several tensors needed to be optimized over time. In our case, we choose to use gradient decend to do batch optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT EXECUTE\n",
    "\"\"\"\n",
    "# NOTE: This chunck is only for illustrating the use of functions. Please do not run it.\n",
    "\n",
    "\n",
    "# Here we use softmax cross entropy to calculate the loss of the model\n",
    "# This function will calculate the probability error in our task\n",
    "#First we calculate the sum of loss of every record compared to label\n",
    "losses = tf.nn.softmax_cross_entropy_with_logits(labels = label, logits = results) \n",
    "# Calculate the mean loss to calculate the gradient of every trainable variable(weights)\n",
    "loss = tf.reduce_mean(losses)\n",
    "# get the tensor the same size of predictions, 1 indicates correct prediction, vice versa\n",
    "correct_number = tf.equal(predictions, tf.argmax(label, 1))\n",
    "\n",
    "# calculate the number of records classified correctly/total number of records\n",
    "# This function first reduce the dimension and then do an average operation\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_number, 'float'))\n",
    "\n",
    "# Define training process\n",
    "# Variables used to track the step, start from 0\n",
    "step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "# In this case we are using AdamOptimizer to implement batch gradient decend\n",
    "optimizer = tf.train.AdamOptimizer(1e-8)\n",
    "\n",
    "#Calculate the gradients according to average error\n",
    "gradients = optimizer.compute_gradients(loss)\n",
    "\n",
    "# Optimize all the trainable variables using the gradients calculated above\n",
    "training = optimizer.apply_gradients(gradients, global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of relevant functions can be found below:\n",
    "\n",
    "[tf.nn.softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "\n",
    "[tf.reduce_mean](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)\n",
    "\n",
    "[tf.train.AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have defined all the tensors needed in training and evaluation process, we will continue with the training, development and evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x is batch of input data, y is the encoded label\n",
    "# Loss and accu are the tensors which store the losses and accuracy of model over the time\n",
    "# Training are the traning tensor which define the training behavior\n",
    "# Step is the tensor which record the current step\n",
    "def train(x, y,loss,accu,training,step):\n",
    "    # Define the input data\n",
    "    feed_dict = {input_x: x,input_y: y}\n",
    "    \n",
    "    # Execute the training process with input data defined above and get the output\n",
    "    _, step, l, a = sess.run([training, step, loss, accu],feed_dict)\n",
    "    \n",
    "    # print the output(Summary)\n",
    "    print(\"step {}, loss {:g}, accuracy {:g}\".format(step, l, a))\n",
    "\n",
    "def development(x, y, loss,accu,step):\n",
    "     # Define the development data set\n",
    "    feed_dict = {input_x: x,input_y: y}\n",
    "    \n",
    "    # Calculate the loss and accuracy on development dataset\n",
    "    s, l, a = sess.run([step,loss,accu],feed_dict)\n",
    "    \n",
    "    # Print the summaries\n",
    "    print(\"step {}, loss {:g}, acc {:g}\".format(s,l,a))\n",
    "\n",
    "# Predictions is the tensor which defines the predict behavior of the model\n",
    "# x is the raw text of feeds\n",
    "# y is the label of feeds(optional)\n",
    "# model is the pretrained word vector model\n",
    "def evaluation(x, predictions, model, y = None):\n",
    "    # Calculate the prediction of the evaluation dataset\n",
    "    preds = np.array(predictions.eval(feed_dict={input_x: np.array(get_vectors(model,x))}))\n",
    "    \n",
    "    # print the formatted output\n",
    "    if y == None:\n",
    "        print(np.column_stack((np.array(x), preds)))\n",
    "    else:\n",
    "        print(np.column_stack((np.array(x), preds, np.argmax(y,1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For we have get all the modules we want in our model, we are going to integrated them and do a one time running session. There are some things need to be noted:\n",
    "1: Because a large data set training will cost a lot of time. In this case, we are choosing a batch size of 1000 from the original dataset, and we will run totally 50 batchs. So for training we will feed 50*1000 records into our model. For further use or extension, you can definitly randomely select different batch size and different batchs from the original data set. \n",
    "2: We will choose another 1000 development data set except training data to get some fair statistics on accuracy.(training : develop = 9 : 1)\n",
    "3: For evaluation data, you can create it by youself or just randomly choose from the original data. When you input the data manually, remember to put as certain format as in sample. It will output a readable table where first column is the raw text, the is prediction, last column is the true label of that data(if applicable)\n",
    "4: Inside the whole graph, we have to build all the trainable variables like weight and bias to tensors. There are usually two ways to generate tensors, one is through tf.Variables(), another is through tf.placeholder()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, loss 0.672146, accuracy 0.64\n",
      "step 2, loss 0.687244, accuracy 0.547\n",
      "step 3, loss 0.591995, accuracy 0.716\n",
      "step 4, loss 0.546992, accuracy 0.77\n",
      "step 5, loss 0.742128, accuracy 0.665\n",
      "step 6, loss 0.949542, accuracy 0.531\n",
      "step 7, loss 0.899836, accuracy 0.433\n",
      "step 8, loss 0.683185, accuracy 0.568\n",
      "step 9, loss 0.68601, accuracy 0.563\n",
      "step 10, loss 0.632335, accuracy 0.681\n",
      "\n",
      "Evaluation:\n",
      "step 10, loss 0.758563, acc 0.5758\n",
      "\n",
      "step 11, loss 0.784145, accuracy 0.598\n",
      "step 12, loss 0.778575, accuracy 0.569\n",
      "step 13, loss 0.839259, accuracy 0.474\n",
      "step 14, loss 0.677135, accuracy 0.563\n",
      "step 15, loss 0.643198, accuracy 0.638\n",
      "step 16, loss 0.646102, accuracy 0.62\n",
      "step 17, loss 0.627565, accuracy 0.642\n",
      "step 18, loss 0.711754, accuracy 0.583\n",
      "step 19, loss 0.796025, accuracy 0.518\n",
      "step 20, loss 0.762611, accuracy 0.544\n",
      "\n",
      "Evaluation:\n",
      "step 20, loss 0.676938, acc 0.6002\n",
      "\n",
      "step 21, loss 0.684499, accuracy 0.596\n",
      "step 22, loss 0.634603, accuracy 0.639\n",
      "step 23, loss 0.613054, accuracy 0.67\n",
      "step 24, loss 0.588428, accuracy 0.68\n",
      "step 25, loss 0.626011, accuracy 0.645\n",
      "step 26, loss 0.665794, accuracy 0.612\n",
      "step 27, loss 0.613656, accuracy 0.644\n",
      "step 28, loss 0.623412, accuracy 0.641\n",
      "step 29, loss 0.62241, accuracy 0.653\n",
      "step 30, loss 0.572628, accuracy 0.691\n",
      "\n",
      "Evaluation:\n",
      "step 30, loss 0.571183, acc 0.6994\n",
      "\n",
      "step 31, loss 0.575904, accuracy 0.681\n",
      "step 32, loss 0.576883, accuracy 0.705\n",
      "step 33, loss 0.600636, accuracy 0.668\n",
      "step 34, loss 0.594505, accuracy 0.684\n",
      "step 35, loss 0.610323, accuracy 0.666\n",
      "step 36, loss 0.604581, accuracy 0.667\n",
      "step 37, loss 0.571988, accuracy 0.713\n",
      "step 38, loss 0.586953, accuracy 0.681\n",
      "step 39, loss 0.556274, accuracy 0.718\n",
      "step 40, loss 0.588209, accuracy 0.681\n",
      "\n",
      "Evaluation:\n",
      "step 40, loss 0.567835, acc 0.6882\n",
      "\n",
      "step 41, loss 0.548966, accuracy 0.73\n",
      "step 42, loss 0.570386, accuracy 0.687\n",
      "step 43, loss 0.565737, accuracy 0.709\n",
      "step 44, loss 0.596146, accuracy 0.668\n",
      "step 45, loss 0.555826, accuracy 0.72\n",
      "step 46, loss 0.559942, accuracy 0.705\n",
      "step 47, loss 0.550167, accuracy 0.709\n",
      "step 48, loss 0.563424, accuracy 0.701\n",
      "step 49, loss 0.555854, accuracy 0.703\n",
      "step 50, loss 0.517457, accuracy 0.745\n",
      "\n",
      "Evaluation:\n",
      "step 50, loss 0.536342, acc 0.726\n",
      "\n",
      "[['is so sad for my apl friend' '1' '1']\n",
      " ['i missed the new moon trailer' '0' '1']\n",
      " ['omg its already 7 30 o' '0' '0']\n",
      " ['omgaga im sooo im gunna cry ive been at this dentist since 11 i was suposed 2 just get a crown put on 30mins'\n",
      "  '1' '1']\n",
      " ['i think mi bf is cheating on me t t' '1' '1']\n",
      " ['or i just worry too much' '1' '1']\n",
      " ['juuuuuuuuuuuuuuuuussssst chillin' '0' '0']\n",
      " ['sunny again work tomorrow tv tonight' '0' '1']\n",
      " ['handed in my uniform today i miss you already' '1' '0']\n",
      " ['hmmmm i wonder how she my number' '1' '0']\n",
      " ['i must think about positive' '0' '1']\n",
      " ['thanks to all the haters up in my face all day 112 102' '0' '0']\n",
      " ['this weekend has sucked so far' '1' '1']\n",
      " ['jb isnt showing in australia any more' '0' '1']\n",
      " ['ok thats it you win' '0' '1']\n",
      " ['lt this is the way i feel right now' '1' '1']\n",
      " ['awhhe man im completely useless rt now funny all i can do is twitter http myloc me 27hx'\n",
      "  '1' '1']\n",
      " ['feeling strangely fine now im gon na go listen to some semisonic to celebrate'\n",
      "  '1' '0']\n",
      " ['huge roll of thunder just now so scary' '0' '1']\n",
      " ['i just cut my beard off it only been growing for well over a year im gon na start it over shaunamanu is happy in the meantime'\n",
      "  '1' '1']\n",
      " ['very sad about iran' '1' '1']\n",
      " ['wompppp wompp' '0' '1']\n",
      " ['youre the only one who can see this cause no one else is following me this is for you because youre pretty awesome'\n",
      "  '0' '0']\n",
      " ['lt sad level is 3 i was writing a massive blog tweet on myspace and my comp shut down now it all lost lays in fetal position'\n",
      "  '1' '1']\n",
      " ['headed to hospitol had to pull out of the golf tourny in 3rd place i think i re ripped something yeah that'\n",
      "  '1' '1']\n",
      " ['boring whats wrong with him please tell me' '1' '1']\n",
      " ['cant be bothered i wish i could spend the rest of my life just sat here and going to gigs seriously'\n",
      "  '1' '1']\n",
      " ['feeeling like shit right now i really want to sleep but nooo i have 3 hours of dancing and an art assignment to finish'\n",
      "  '1' '1']\n",
      " ['goodbye exams hello alcohol tonight' '0' '0']\n",
      " ['i didnt realize it was that deep geez give a girl a warning atleast'\n",
      "  '1' '1']\n",
      " ['i hate it when any athlete appears to tear an acl on live television'\n",
      "  '1' '1']\n",
      " ['i miss you guys too i think im wearing skinny jeans a cute sweater and heels not really sure what are you doing today'\n",
      "  '1' '1']\n",
      " ['meet your meat http bit ly 15ssci' '0' '1']\n",
      " ['my horsie is moving on saturday morning' '1' '1']\n",
      " ['no sat off need to work 6 days a week' '1' '1']\n",
      " ['really dont like doing my room its so boring sick of doing my wardrobe out cant waiit till i have my walk in one yay'\n",
      "  '1' '1']\n",
      " ['sox floyd was great but relievers need a scolding' '1' '1']\n",
      " ['times by like a million' '0' '1']\n",
      " ['uploading pictures on friendster' '0' '0']\n",
      " ['what type of a spaz downloads a virus my brother that who msn is now fucked forever'\n",
      "  '1' '1']\n",
      " ['amp amp fightiin wiit the babes' '0' '1']\n",
      " ['so i wrote something last week and i got a call from someone in the new york office http tumblr com xcn21w6o7'\n",
      "  '0' '0']\n",
      " ['enough said' '1' '1']\n",
      " ['do i need to even say it do i well here i go anyways chris cornell in chicago tonight'\n",
      "  '1' '0']\n",
      " ['health class what a joke' '0' '0']\n",
      " ['ginaaa lt 3 go to the show tonight' '0' '0']\n",
      " ['spiral galaxy ymptweet it really makes me sad when i look at muslims reality now'\n",
      "  '1' '1']\n",
      " ['all time low shall be my motivation for the rest of the week' '1' '1']\n",
      " ['and the entertainment is over someone complained properly rupturerapture experimental you say he should experiment with a melody'\n",
      "  '0' '1']\n",
      " ['another year of lakers that neither magic nor fun' '0' '1']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "with tf.Graph().as_default():\n",
    "    # Configure the model\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True,log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # Begin the session\n",
    "    with sess.as_default():\n",
    "        # Define the input data\n",
    "        input_x = tf.placeholder(tf.float32, [None, twitter_length,300], name='input_x')\n",
    "        input_y = tf.placeholder(tf.float32, [None, 2], name='input_y')\n",
    "        # Extend the input data because Tensorflow CNN model will only accept Rank 4 input data \n",
    "        input_expanded = tf.expand_dims(input_x, -1)\n",
    "       \n",
    "        # Convolutional Layer\n",
    "        filter_shape = [conv_layer['filter_length'], conv_layer['filter_width'], 1, filter_number]\n",
    "        conv_w = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1),name='conv_w')\n",
    "        conv_b = tf.Variable(tf.constant(0.1, shape=[filter_number]),name='conv_b')\n",
    "        conv_mid = tf.nn.conv2d(input_expanded,conv_w,strides=[1, 1, 1, 1],padding='VALID')\n",
    "        conv_out = tf.nn.relu(tf.nn.bias_add(conv_mid, conv_b))\n",
    "        \n",
    "        # Pooling Layer\n",
    "        pool_mid = tf.nn.max_pool(conv_out,ksize=[1, twitter_length - conv_layer['filter_length'] + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID',name='pool')\n",
    "        pool_out = tf.reshape(pool_mid, [-1, pool_layer['filter_length']])\n",
    "       \n",
    "        # Output Layer\n",
    "        out_w = tf.get_variable('out_w',shape=[filter_number, 2],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        out_b = tf.Variable(tf.constant(0.1, shape=[2]), name='out_b')\n",
    "        outputs = tf.nn.xw_plus_b(pool_out, out_w , out_b, name='outputs')\n",
    "        predictions = tf.argmax(outputs, 1, name='predictions')\n",
    "       \n",
    "        # Help neuron in training and evaluation process\n",
    "        losses = tf.nn.softmax_cross_entropy_with_logits(labels = input_y, logits = outputs) #  only named arguments accepted            \n",
    "        loss = tf.reduce_mean(losses)\n",
    "        correct = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'), name='accuracy')\n",
    "        step = tf.Variable(0, name=\"step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        gradients = optimizer.compute_gradients(loss)\n",
    "        training = optimizer.apply_gradients(gradients, global_step=step)\n",
    "       \n",
    "        # Start model running\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        echo = 50\n",
    "        location = 0\n",
    "        while echo > 0:\n",
    "            end = location + 1000\n",
    "            x_input = np.array(get_vectors(model,feed_list[location:end]))\n",
    "            train(x_input, label[location:end],loss,accuracy,training,step)\n",
    "            current_step = tf.train.global_step(sess,step)\n",
    "            if current_step % 10 == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                development(np.array(get_vectors(model,feed_list[50000:55000])), label[50000:55000], loss, accuracy,step)\n",
    "                print(\"\")\n",
    "            echo -= 1\n",
    "            location = location + 1000\n",
    "        evaluation(feed_list[:50],predictions,model,label[:50])\n",
    "        \n",
    "        # When you want to input your own data\n",
    "        # evaluation(['i really like that cat'],predictions,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have completed our model, while when we are looking at the development and evaluation summary data, there are still some problems:\n",
    "\n",
    "It seems like the accuracy is increasing batch by batch, while there also can have overfitting problems when we has a large amount of training data or when our model is too complex. There are many methodologies about how to avoid the overfitting problem. The dropout method, especially in CNN, will drop certain part of neuron to make the model simpler to avoid overfitting. Sample code about the dropout schema inside the hidden layers can be found here [code with dropout schema](https://www.tensorflow.org/tutorials/layers). \n",
    "\n",
    "In addition to overfitting, we can also increase the model performance through random training data selection or cross validation which may have a higher requirement on hardware. \n",
    "\n",
    "From the output of the evaluation, we can found that those which are classified incorrectly are usually words which dose not exist in our pretrained model. So it may be a better choice to use a non-static model(train the word vector over time), for the words appeared in twitter feeds are usually freestyle.\n",
    "\n",
    "Even there are some limitations on this model, I learned a lot from the research process and still learned a very efficient way to handle high dimensional data. In the future we may even extend our input into image, audio and even video. Through CNN we can find more possibilities of data science application in our daily life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)Shen, X. He, J. Gao, L. Deng, G. Mesnil. 2014.Learning Semantic Representations Using Convolu-\n",
    "tionalNeuralNetworksforWebSearch. InProceedings of WWW 2014.\n",
    "(2)K., & Y. (2014, September 03). Convolutional Neural Networks for Sentence Classification. Retrieved March 31, 2018, from https://arxiv.org/abs/1408.5882 (This tutorial is nearly a code implementation of this research paper)\n",
    "(3)Britz, D. (2016, February 05). Implementing a CNN for Text Classification in TensorFlow. Retrieved March 31, 2018, from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}