{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTM to generate poems of Percy Bysshe Shelley by Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allowing computer to write poems that similar to one poet is interesting. One way to finish this task is to use N-gram model. However, with the development of deep learning, people find that using recurrent neural network (RNN) to finish this task is much more efficient. In this tutorial, I will introduce you about how to use long-short-term-memory (LSTM), which is a kind of RNN, to train the computer to write poems. We will use pytorch as tools to finish this task. Since Percy Bysshe Shelley is one of my favorite poet, so let start with him!\n",
    "\n",
    "The tutorial will contain these parts:\n",
    "1. Some basic concepts about RNN and LSTM, which can help people who know nothing about neural network to understand the basic things.\n",
    "1. How to collect poems of your favorite poet? This part will introduce you how to use beautiful soup to catch poems from web and use NLTK to clean it.\n",
    "2. How to transform the data into a form that is suitable to feed in a LSTM network? This part will introduce you how to use Dataset and Dataloader of pytorch to preprocess the training data and generate batch.\n",
    "3. How to build and train the LSTM model?\n",
    "4. How to use the model to write poems?\t\n",
    "\n",
    "Attention: This tutorial just tries to tell you how to build a basic LSTM model, so I leave out some important parts, such as dividing data into training and validation set and evaluating model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to RNN and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural network (RNN) is a kind of Neural Network that takes sequence data as input and share parameters over each iteration. Long-short-term-memory (LSTM) is a transformation of RNN which contains three gates to simulate the memory of humans. This kind of neural network is good at deal with data whose current state has dependency on previous states.\n",
    "\n",
    "To understand it better. You can see:\n",
    "1. RNN: http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns\n",
    "2. LSTM: http://colah.github.io/posts/2015-08-Understanding-LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data collection and clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to imitate Percy Bysshe Shelley's poems, we need to collect some training data. So let's using BeautifulSoup to crawl multiple poems. BeautifulSoup is a package of python that can generate a HTML tree so that we can get the context or attributes of a tag easily. We collect the poems from https://www.poemhunter.com/poems.\n",
    "\n",
    "Here are the steps that we collect the data.\n",
    "1. We need to see the source code to find which part of the HTML contains the poems that we need. Here, according to my manual analyzation in Chrome, I know that the poems are under the second 'p' of each page. By this, we can get one poem. Then we need to find the link of the the next poem, so that we can find the next poems by iteration. The function get_poem() finish this function.\n",
    "2. The get_poems() does the iteration to find all the poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Get one poem and find the next poem.\n",
    "# html: The url of the web page that contains the poem.\n",
    "# return:\n",
    "# poem: The text of one poem.\n",
    "# next_html: The next url that contains next poem.\n",
    "def get_poem(html):\n",
    "    html = requests.get(html).content.decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    count = 0\n",
    "    poem = None\n",
    "    for p in soup.find_all(\"p\"):\n",
    "        count += 1\n",
    "        if count == 2:\n",
    "            poem = p.text\n",
    "            break\n",
    "    html_prefix = \"https://www.poemhunter.com\"\n",
    "    html_body = soup.find(\"li\", class_=\"next\")\n",
    "    if html_body:\n",
    "        html_body = html_body.find(\"a\")[\"href\"]\n",
    "        next_html = html_prefix + html_body\n",
    "        return poem, next_html\n",
    "    else:\n",
    "        return poem, None\n",
    "\n",
    "# Do iterations to collect all the poems.\n",
    "def get_poems(start_html):\n",
    "    poems = []\n",
    "    next_html = start_html\n",
    "    count = 0\n",
    "    while True:\n",
    "        poem, next_html = get_poem(next_html)\n",
    "        poems.append(poem)\n",
    "        if not next_html:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "        count += 1\n",
    "    return poems, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 3 raw poems:\n",
      "['\\r\\n                        I met a traveller from an antique landWho said: `Two vast and trunkless legs of stoneStand in the desert. Near them, on the sand,Half sunk, a shattered visage lies, whose frown,And wrinkled lip, and sneer of cold command,Tell that its sculptor  well those passions readWhich yet survive, stamped on these lifeless things,The hand that mocked them and the heart that fed.And on the pedestal these words appear --\"My name is Ozymandias, king of kings:Look on my works, ye Mighty, and despair!\"Nothing beside remains. Round the decayOf that colossal wreck, boundless and bareThe lone and level sands stretch far away.\\' \\r\\n                        \\n', '\\r\\n                        Listen, listen, Mary mine,To the whisper of the Apennine,It bursts on the roof like the thunder’s roar,Or like the sea on a northern shore,Heard in its raging ebb and flowBy the captives pent in the cave below.The Apennine in the light of dayIs a mighty mountain dim and gray,Which between the earth and sky doth lay;But when night comes, a chaos dreadOn the dim starlight then is spread,And the Apennine walks abroad with the storm,Shrouding... \\r\\n                        \\n', '\\r\\n                        Serene in his unconquerable mightEndued[,] the Almighty King, his steadfast throneEncompassed unapproachably with powerAnd darkness and deep solitude an aweStood like a black cloud on some aery cliff Embosoming its lightning—in his sightUnnumbered glorious spirits trembling stoodLike slaves before their Lord—prostrate aroundHeaven’s multitudes hymned everlasting praise. \\r\\n                        \\n']\n",
      "I got 148 poems.\n"
     ]
    }
   ],
   "source": [
    "# Here are the poems that we collected.\n",
    "start_html = \"https://www.poemhunter.com/poem/ozymandias/\"\n",
    "poems, count = get_poems(start_html)\n",
    "print(\"The first 3 raw poems:\")\n",
    "print(poems[:3])\n",
    "print(\"I got \" + str(count) + \" poems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save raw dataset\n",
    "import pickle\n",
    "# pickle.dump(poems, open(\"poems\" + \".pickle\", \"wb\" ))\n",
    "poems = pickle.load(open(\"poems\" + \".pickle\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "However, from the data we can see that there still exists some \"bad\" characters in our texts, such as \"\\r\\n\", \"II\". We need to remove these kind of words. To be more details, we only need to remain the a-z0-9 characters and punctuation and then split the text into words in lower case. Then, we join all the words into a big list. Also, in order to train the model. We need to transform words text data into digits, which means each word can be represent by a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'met', 'a', 'traveller', 'from', 'an', 'antique', 'landwho', 'said', ':', 'two', 'vast', 'and', 'trunkless', 'legs', 'of', 'stonestand', 'in', 'the', 'desert', 'near', 'them', ',', 'on', 'the', 'sand', ',', 'half', 'sunk', ',', 'a', 'shattered', 'visage', 'lies', ',', 'whose', 'frown', ',', 'and', 'wrinkled', 'lip', ',', 'and', 'sneer', 'of', 'cold', 'command', ',', 'tell', 'that', 'its', 'sculptor', 'well', 'those', 'passions', 'readwhich', 'yet', 'survive', ',', 'stamped', 'on', 'these', 'lifeless', 'things', ',', 'the', 'hand', 'that', 'mocked', 'them', 'and', 'the', 'heart', 'that', 'fed', 'and', 'on', 'the', 'pedestal', 'these', 'words', 'appear', '--', 'my', 'name', 'is', 'ozymandias', ',', 'king', 'of', 'kings', ':', 'look', 'on', 'my', 'works', ',', 'ye', 'mighty', ',']\n"
     ]
    }
   ],
   "source": [
    "# First Step:\n",
    "# Clean the text.\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def clean_text(poems):\n",
    "    word_list = []\n",
    "    # replace all whitespace with a single space\n",
    "    poems_content = [re.sub(r\"\\s+\", \" \", p).lower() for p in poems]\n",
    "\n",
    "    # add spaces before all punctuation, so they are separate tokens\n",
    "    punctuation = set(re.findall(r\"[^\\w\\s]+\", \" \".join(poems_content))) - {\"-\", \"'\"}\n",
    "    \n",
    "    for c in punctuation:\n",
    "        poems_content = [p.replace(c, \" \" + c + \" \") for p in poems_content]\n",
    "        \n",
    "    poems_content = [re.sub(r\"\\s+\", \" \", p).lower().strip() for p in poems_content]\n",
    "    \n",
    "    # remove word that is not a-z0-9,!?();:-\n",
    "    poems_content = [re.sub(r\"[^\\sa-z0-9,!?();:-]\", \"\", p) for p in poems_content]\n",
    "    \n",
    "    for poem in poems_content:\n",
    "        token_poem = word_tokenize(poem)\n",
    "        word_list += token_poem\n",
    "\n",
    "    return word_list\n",
    "\n",
    "word_list = clean_text(poems)\n",
    "print(word_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13741\n",
      "13741\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 18, 24, 22, 25, 26, 22, 2, 27, 28, 29, 22, 30, 31, 22, 12, 32, 33, 22, 12, 34, 15, 35, 36, 22, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 22, 47, 23, 48, 49, 50, 22, 18, 51, 38, 52, 21, 12, 18, 53, 38, 54, 12, 23, 18, 55, 48, 56, 57, 58, 59, 60, 61, 62, 22, 63, 15, 64, 9, 65, 23, 59, 66, 22, 67, 68, 22]\n",
      "13741\n"
     ]
    }
   ],
   "source": [
    "# Second Step:\n",
    "def construct_dictionary_and_word_transform(word_list):\n",
    "    word2index = {}\n",
    "    index2word = {}\n",
    "    index = 0\n",
    "    index_word_list = []\n",
    "    for word in word_list:\n",
    "        if word not in word2index:\n",
    "            word2index[word] = index\n",
    "            index2word[index] = word\n",
    "            index += 1\n",
    "        index_word_list.append(word2index[word])\n",
    "    return word2index, index2word, index_word_list, len(index2word)\n",
    "\n",
    "word2index, index2word, index_word_list, dic_length = construct_dictionary_and_word_transform(word_list)\n",
    "print(len(word2index))\n",
    "print(len(index2word))\n",
    "print(index_word_list[0:100])\n",
    "print(dic_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Using Dataset and Dataloader to prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In previous section, we had finish the basic processing of text and convert them into digits. However, since we want to train a neural network, we need more data preparation. In neural network, we always collect the data and then pack them as batchs before training. This is because creating a batch is the same as pack many single vector as a huge matrix, which can take the advantage of matrix multiplication and the acceleration of GPU. \n",
    "\n",
    "To learn more about batch, you can see:\n",
    "https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pytorch, it provide us with two classs that help us create our own dataset and batch clearly and easily.\n",
    "First, let's start with Dataset.\n",
    "\n",
    "The fuction of Dataset is that it overide the function __getitem__() to allow Dataloader to automatically generate a batch. The example here is easy, since we just need to directly get one item (one item is consist of one training data and its label). However, for some complex problem, you need to make some futher modification of your data, you can feel free to deal with them in the Dataset.\n",
    "\n",
    "PS: For this certain problem, the label of one data is the next data of it.\n",
    "\n",
    "Example:\n",
    "\n",
    "data  : I    love you . \n",
    "\n",
    "label : love you  .\n",
    "\n",
    "I -> love, love -> you, you -> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class PoemsDataset(data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        self.bptt = 35\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        i *= self.bptt\n",
    "        return self.data[i : i + self.bptt], self.data[i + 1: i + 1 + self.bptt]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - 1) // self.bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn to the Dataloader. Taking a dataset as an input, dataloader can automatically generate batch. However, sometimes we are not very satisfied with it's batch, so we need to make some modification. The way we do that is overide the **collate_fn()**.\n",
    "\n",
    "Here, according to the definition of the LSTM model of pytorh, we need to transpose each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The shape of a batch data is [batch_size, data, label]\n",
    "# Here we do not need do anything.\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Dataset and Dataloader to create our batch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: \n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    12\n",
      "   30    31    22    12    32    33    22    12    34    15    35    36    22\n",
      "   12    18    53    38    54    12    23    18    55    48    56    57    58\n",
      "   73    74    18    75    38    76    77    22    78    12    79    80    12\n",
      "   18    94    95    18    96    97    98    22    99    95    18   100    23\n",
      "  111    15   112     2    68   113   114    12   115    22   116   117    18\n",
      "   18    91   132   133   134    18   135    22   136   137    17   138   139\n",
      "   23   152   153   154   155    39   156    17   138   157   158   159   160\n",
      "    2   177    22   178   179   180   181    23   118   182   178    22   152\n",
      "   22   172   194   195   196   197   198   199    22    59   165   200   201\n",
      "\n",
      "Columns 13 to 25 \n",
      "   13    14    15    16    17    18    19    20    21    22    23    18    24\n",
      "   37    38    39    40    41    42    43    44    45    46    22    47    23\n",
      "   59    60    61    62    22    63    15    64     9    65    23    59    66\n",
      "   81    82    83    84    85    86    22    86    22    87    88    22    89\n",
      "    2   101   102    22   103    17    39   104   105    12   106    18   107\n",
      "  118    12   119   120   121   122   123   124   125   126    22     2   127\n",
      "  140    22    18   141    63    22   138   142   143   144   134   145   146\n",
      "  161   162   163   164   165   166   167    97   168   169   170   171   172\n",
      "  183   184   183   185   122   123    22   180   186   172   164   187   188\n",
      "  202    22   199    61   173   174   122    92   198   203   204   205   206\n",
      "\n",
      "Columns 26 to 34 \n",
      "   22    25    26    22     2    27    28    29    22\n",
      "   48    49    50    22    18    51    38    52    21\n",
      "   22    67    68    22    12    69    70    71    72\n",
      "   18    90    15    18    91    22    92    93    23\n",
      "  108    17    18   109   110    18    91    17    18\n",
      "  128    18   114   129   130    61   131    22    12\n",
      "   12   147   148     5   149    95     2   150   151\n",
      "  173   174    22   175    61    92     2   176    17\n",
      "   22   189   190    22    12   191    70   192   193\n",
      "  207    22   208    22    10    12   209    22   210\n",
      "[torch.LongTensor of size 10x35]\n",
      "\n",
      "Label: \n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    1     2     3     4     5     6     7     8     9    10    11    12    13\n",
      "   31    22    12    32    33    22    12    34    15    35    36    22    37\n",
      "   18    53    38    54    12    23    18    55    48    56    57    58    59\n",
      "   74    18    75    38    76    77    22    78    12    79    80    12    81\n",
      "   94    95    18    96    97    98    22    99    95    18   100    23     2\n",
      "   15   112     2    68   113   114    12   115    22   116   117    18   118\n",
      "   91   132   133   134    18   135    22   136   137    17   138   139   140\n",
      "  152   153   154   155    39   156    17   138   157   158   159   160   161\n",
      "  177    22   178   179   180   181    23   118   182   178    22   152   183\n",
      "  172   194   195   196   197   198   199    22    59   165   200   201   202\n",
      "\n",
      "Columns 13 to 25 \n",
      "   14    15    16    17    18    19    20    21    22    23    18    24    22\n",
      "   38    39    40    41    42    43    44    45    46    22    47    23    48\n",
      "   60    61    62    22    63    15    64     9    65    23    59    66    22\n",
      "   82    83    84    85    86    22    86    22    87    88    22    89    18\n",
      "  101   102    22   103    17    39   104   105    12   106    18   107   108\n",
      "   12   119   120   121   122   123   124   125   126    22     2   127   128\n",
      "   22    18   141    63    22   138   142   143   144   134   145   146    12\n",
      "  162   163   164   165   166   167    97   168   169   170   171   172   173\n",
      "  184   183   185   122   123    22   180   186   172   164   187   188    22\n",
      "   22   199    61   173   174   122    92   198   203   204   205   206   207\n",
      "\n",
      "Columns 26 to 34 \n",
      "   25    26    22     2    27    28    29    22    30\n",
      "   49    50    22    18    51    38    52    21    12\n",
      "   67    68    22    12    69    70    71    72    73\n",
      "   90    15    18    91    22    92    93    23    18\n",
      "   17    18   109   110    18    91    17    18   111\n",
      "   18   114   129   130    61   131    22    12    18\n",
      "  147   148     5   149    95     2   150   151    23\n",
      "  174    22   175    61    92     2   176    17     2\n",
      "  189   190    22    12   191    70   192   193    22\n",
      "   22   208    22    10    12   209    22   210    18\n",
      "[torch.LongTensor of size 10x35]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "dataset = PoemsDataset(index_word_list)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=False, drop_last=True)\n",
    "\n",
    "count = 0\n",
    "for (data, label) in data_loader:\n",
    "    if count == 1:\n",
    "        break\n",
    "    print(\"Data: \")\n",
    "    print(data)\n",
    "    print(\"Label: \")\n",
    "    print(label)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build and train a LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will try to build one simple LSTM model to train the data in order to help you know how to use pytorch to build a basic neural network model. Here, you can see some codes name embedding (encode) and decode, these part of codes condense each word (digits) into a small embedding, which can make the training space more dense. \n",
    "\n",
    "To see more about embedding, you can see:\n",
    "http://deeplearning.net/tutorial/rnnslu.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the code that:\n",
    "A simple neural network in pytorch is consist of a __init__ function and a __forward__ function. The first function define the basic architecture of the neural and the second function define the operation when the neural network implement the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "    # A model with an encoder, a LSTM module, and a decoder.\n",
    "    def __init__(self, vocab_size, hidden_size, nlayers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.encoder = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.LSTM = nn.LSTM(hidden_size, hidden_size, nlayers)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "        self.decoder.weight = self.encoder.weight\n",
    "\n",
    "    def forward(self, data, h):\n",
    "        embedding = self.encoder(data)\n",
    "        o, h = self.LSTM(embedding, h)\n",
    "        decoded = self.decoder(o.view(o.size(0) * o.size(1), o.size(2)))\n",
    "        return decoded.view(o.size(0), o.size(1), decoded.size(1)), h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we build the model, lets begin to train it. For a model, you need to specify its loss function, hidden_size, layers and so on. Here for the simplicity, this parameters are all set very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, batch_size, learning_rate, dic_length):\n",
    "    model = LSTMModel(vocab_size=dic_length, hidden_size=100, nlayers=1)\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    h = None\n",
    "    for (data, label) in data_loader:\n",
    "        model.zero_grad()\n",
    "        o, h = model(Variable(data.t().contiguous()).long(), h)\n",
    "        loss = loss_function(o.view(-1, dic_length), Variable(label))\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if batch % 100 == 0 and batch > 0:\n",
    "            cur_loss = total_loss[0] / 100\n",
    "            print('loss:{}'.format(cur_loss))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = train(data_loader, batch_size, 0.001, dic_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using LSTM Model to genereate poems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the exciting time comes! Let's using our trained model to generate poems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward means how many words we want to generate\n",
    "# start_sequence allow us to choose some word to begin\n",
    "def generation(model, forward, start_sequence, dic_length):\n",
    "    model.eval()\n",
    "\n",
    "    result = [0] * forward\n",
    "    i = 0\n",
    "    hidden = None\n",
    "    single_input = Variable(torch.rand(1, 1).mul(dic_length).long(), volatile=True)\n",
    "\n",
    "    for word_idx in start_sequence:\n",
    "        single_input.data.fill_(int(word_idx))\n",
    "        output, hidden = model(single_input, hidden)\n",
    "\n",
    "    word_weights = output.squeeze().data.div(1).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    single_result[i] = word_idx\n",
    "    i += 1\n",
    "    single_input.data.fill_(word_idx)\n",
    "\n",
    "    for nextword_idx in range(forward - 1):\n",
    "        output, hidden = model(single_input, hidden)\n",
    "        word_weights = output.squeeze().data.div(1).exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        single_result[i] = word_idx\n",
    "        i += 1\n",
    "        single_input.data.fill_(word_idx)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generation(model, 10, [0, 2], dic_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "\n",
    "[1] https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "\n",
    "[2] http://deeplearning.net/tutorial/rnnslu.html\n",
    "\n",
    "[3] https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
    "\n",
    "[4] http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns\n",
    "\n",
    "[5] http://colah.github.io/posts/2015-08-Understanding-LSTMs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}