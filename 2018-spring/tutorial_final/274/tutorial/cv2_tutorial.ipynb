{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products.\n",
    "\n",
    "The library has more than 2500 optimized algorithms, which includes a comprehensive set of both classic and state-of-the-art computer vision and machine learning algorithms. These algorithms can be used to detect and recognize faces, identify objects, classify human actions in videos, track camera movements, track moving objects, extract 3D models of objects, produce 3D point clouds from stereo cameras, stitch images together to produce a high resolution image of an entire scene, find similar images from an image database, remove red eyes from images taken using flash, follow eye movements, recognize scenery and establish markers to overlay it with augmented reality, etc.\n",
    "\n",
    "It has C++, Python, Java and MATLAB interfaces and supports Windows, Linux, Android and Mac OS. This tutorial will particularly focus on its python interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Content\n",
    "This tutorial aims at getting you familiar with OpenCV. We will show how to use OpenCV to do basic operations to images and videos. Later, we will leverage OpenCV existing classifiers to do face detection.\n",
    "\n",
    "Finally, we will analyze a speech video of Trump to see how OpenCV applies to real life video data. Specially, we would combine what we have learned to do face detection and tracking to the video and present the result in a decent way. \n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Installing the library](#Installing-the-library)\n",
    "- [Loading, Displaying and Saving images](#Loading,-Displaying-and-Saving-images)\n",
    "- [Loading video and Displaying frame-by-frame](#Loading-video-and-Displaying-frame-by-frame)\n",
    "- [Drawing Functions in OpenCV](#Drawing-Functions-in-OpenCV)\n",
    "- [Accessing Image](#Accessing-Image)\n",
    "- [Arithmetic Operations](#Arithmetic-Operations)\n",
    "- [Face Detection using Haar Cascades](#Face-Detection-using-Haar-Cascades)\n",
    "- [Example Application: Mark Trump's face from his speech video](#Example-Application:-Mark-Trump's-face-from-his-speech-video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the library\n",
    "Before getting started, you need to install OpenCV. Using pip is the best choice:\n",
    "\n",
    "    $ pip install opencv-contrib-python\n",
    "\n",
    "Once you have installed the library, you can check it by print the version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading, Displaying and Saving images\n",
    "Now since we've installed OpenCV, let's look at some basic functions. First, we use `cv2.imread()` to read an image.\n",
    "\n",
    "`cv2.imread()` has two arguments. The first one stands for a relative or full path of the image and the second argument specifies the way the image would be read. The value could be `1`, `0`, `-1`, representing `cv.IMREAD_COLOR`, `cv.IMREAD_GRAYSCALE` and `cv.IMREAD_UNCHANGED` respectively.\n",
    "\n",
    "Suppose we want to read an image in the working directory named \"cat.jpg\" without any change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('cat.jpg', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After loading the image, we can use `cv2.imshow()` method to display the image in a new window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.imshow('example_window', img)\n",
    "cv2.waitKey(0) # wait infinitely\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"cat.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can use `cv2.imwrite()` to save the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('cat2.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading video and Displaying frame-by-frame\n",
    "Often, the data we could collect is in the format of video. In this case, we should read the video and break it into many frames. OpenCv provides extremely easy way to do these operations. To read a video, we have to create a `VideoCapture` object using `cv2.VideoCapture()`. Its argument can be either a device index or the path of a video file. To save a video, we need to create a `VideoWriter` object using `cv2.VideoWriter()` and then constantly write frames to the object. In the method, we should specify file path, [FourCC](http://www.fourcc.org/codecs.php) code, the number of frames per second(fps) and frame size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create VideoCapture object & get frame width and height\n",
    "cap = cv2.VideoCapture('trump.wmv')\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "# Define FourCC and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "out = cv2.VideoWriter('trump_out.avi', fourcc, 30.0, (frame_width, frame_height))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    # read the next frame \n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "        \n",
    "    # you can make some changes to the frame here\n",
    "    \n",
    "    # write the frame\n",
    "    # out.write(frame) #de-comment and re-run to check saving function\n",
    "    \n",
    "    # display the frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):# press Q to quit\n",
    "        break\n",
    "\n",
    "# release everything\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Drawing Functions in OpenCV\n",
    "Sometimes, we need to use lines to mark down the information we want to present and OpenCV provides some useful drawing functions for us. For example, when we detect a human face in an image, we may use a rectangle to mark its boundary and generate a new image for visualization. Following are the examples of `cv2.line()`, `cv2.rectangle()` and `cv2.circle()` with straightforward comments.\n",
    "\n",
    "It should be noted that all the drawing functions do `in-place modification` to the source image. Therefore, if you do need the origin one for further use, remember to make copies before calling drawing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create a black image\n",
    "img = np.zeros((512,512,3), np.uint8)\n",
    "img_line = img.copy()\n",
    "img_rect = img.copy()\n",
    "img_circle = img.copy()\n",
    "img_text = img.copy()\n",
    "\n",
    "# Display the original black image\n",
    "cv2.imshow('black_image', img)\n",
    "cv2.waitKey(500)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Draw a diagonal white line with thickness of 5 px, Display it\n",
    "# Parameters: image object, start point, end point, color, thickness \n",
    "img_line = cv2.line(img_line,(0,0),(512,512),(255,255,255),5)\n",
    "\n",
    "cv2.imshow('white_line', img_line)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Draw a blue rectangle with thickness of 3px, Display it\n",
    "# Parameters: image object, top-left point, bottom-right point, color, thickness\n",
    "img_rect = cv2.rectangle(img_rect,(0,0),(128,128),(255,0,0),3)\n",
    "cv2.imshow('blue_rect', img_rect)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Draw a circle\n",
    "# Parameters: image object, center, radius, color, thickness(-1 means filling the closed shape inside)\n",
    "img_circle = cv2.circle(img_circle,(100,100), 50, (0,0,255), -1)\n",
    "cv2.imshow('red_circle', img_circle)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Image\n",
    "Image is just one format of data. After loading an image to memory, we can directly access and modify pixel values of image by row and column coordinates. A BGR image is three-dimensional so it would return a vector of size 3 given (x,y). You could also use (x,y,z) to get the specific value of B, G or R. For a grayscale image, only one number which stands for intensity would return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "255\n"
     ]
    }
   ],
   "source": [
    "pixel = img[50,50]\n",
    "print(pixel)\n",
    "print(type(pixel))\n",
    "\n",
    "# access only green pixel\n",
    "g_pixel = img_line[3,3,1]\n",
    "print(g_pixel)\n",
    "\n",
    "# modify a pixel value\n",
    "img_line[3,3,1] = 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a matrix three dimensional vector, an image has its shape and other properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n",
      "786432\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "# returns a tuple of number of rows, columns and channels (if image is color)\n",
    "print(img.shape)\n",
    "\n",
    "# Total number of pixels\n",
    "print(img.size)\n",
    "\n",
    "# Image datatype\n",
    "print(img.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we want to manipulate part of the whole picture. By numpy indexing, we can easily retreive take out a region of an image and then put it in another region of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_res = img_rect.copy()\n",
    "#cv2.imwrite('before.png',img_res)\n",
    "region = img_rect[0:129,0:129]\n",
    "\n",
    "img_res[383:512,383:512] = region\n",
    "#cv2.imwrite('after.png', img_res)\n",
    "cv2.imshow('image', img_res)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic Operations\n",
    "Sometimes, we want to combine two images into one single image. Then we should use `c2.add()` method. You may directly use `+` to do that but OpenCV addition is a saturated operation instead of a modulo one, which means number beyond 255 would simply become 255 rather than mod 255. Normally, a saturated operation will provide a better result. We could also add them with different weights using `cv2.addWeighted()`. Then the following equation `dst = α * img1 + β * img2 + γ` will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img1 = cv2.imread('emoji.jpg')\n",
    "img2 = cv2.imread('bg.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"emoji.jpg\" align=\"left\" height=\"30%\" width=\"30%\"> <img src=\"bg.jpg\" align=\"center\" height=\"30%\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_img = cv2.add(img1, img2)\n",
    "blend_img = cv2.addWeighted(img1,0.2,img2,0.8,0)\n",
    "\n",
    "cv2.imshow('added image',add_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"added_image.png\" width=\"30%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.imshow('blended image',blend_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"blended_image.jpg\" width=\"30%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection using Haar Cascades\n",
    "So far, we have learned how load data, access data and do some basic operations to our data. Now, let's do something more interesting. \n",
    "\n",
    "Suppose we have an image of Trump and we want to detect the position of his face. To achieve this, we can leverage the advantage of machine learning. First we need to train a machine learning model. Then, put the image into the model as input and it will give us the result we want. For object detection, using  `Haar feature-based cascade` classifiers is an effective way. The principle of the method could be learned from Paul Viola and Michael Jones' paper, `\"Rapid Object Detection using a Boosted Cascade of Simple Features\"` in 2001 or a video [here](https://www.youtube.com/watch?v=WfdYYNamHZ8). Fortunately, OpenCV comes with some existing detectors using Haar Cascades for facing detections, which are stored in `opencv/data/haarcascades/` folder. To use them, we should first load these detectors files. Then load our image in `grayscale mode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_face = cv2.imread('face.png',0)\n",
    "cv2.imshow('blended image',img_face)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"face.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use `detectMultiScale()` method to detect faces in the image. It will return a list of tuples, each tuple in the form of (x,y,w,h) as a rectangle. As you may wonder how to select parameters for `detectMultiScale()` method, detailed tutorial is accessible from [here](http://www.bogotobogo.com/python/OpenCV_Python/python_opencv3_Image_Object_Detection_Face_Detection_Haar_Cascade_Classifiers.php), which may take some time to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[540 120 228 228]\n"
     ]
    }
   ],
   "source": [
    "faces = face_cascade.detectMultiScale(img_face, 1.3, 5)\n",
    "for face in faces:\n",
    "    print(face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of different position, background and size of training data, the performances of default models provided by OpenCV are not always as good as we expected. Sometimes, we do have to train a different model for our own task. Its full details are [here](https://docs.opencv.org/2.4/doc/user_guide/ug_traincascade.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Application: Mark Trump's face from his speech video\n",
    "Now we want to see how OpenCV can help us do analysis in real life. In general, the original data we can get in real life is always in the format of videos. Therefore, the following content will show a simple example of how to mark Trump's face from his speech video and generate a new video with face detection boundaries. The steps are:\n",
    "- Loading the video\n",
    "- Breaking the video into frames\n",
    "- Detecting faces in a frame\n",
    "- Marking the result of detection by drawing functions\n",
    "- Displaying and saving frames\n",
    "\n",
    "Marked frames will be like this:\n",
    "<img src=\"trump_marked.jpg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark(frame, face_cas):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # bgr to gray mode\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        frame = cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),5)\n",
    "    \n",
    "def process_video(r_path, w_path, face_cas):\n",
    "    cap = cv2.VideoCapture(r_path)\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    out = cv2.VideoWriter(w_path, fourcc, 30.0, (frame_width, frame_height))\n",
    "    cnt = 0\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret == False:\n",
    "            break\n",
    "\n",
    "        # detect faces in the frame and mark\n",
    "        mark(frame, face_cas)\n",
    "        if cnt == 0:\n",
    "            cnt = 1\n",
    "            cv2.imwrite('trump_marked.jpg',frame)\n",
    "        # write the frame\n",
    "        # out.write(frame)\n",
    "\n",
    "        # display the frame\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):# press Q to quit\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return\n",
    "\n",
    "# pre-processing starts\n",
    "in_path = 'trump.wmv'\n",
    "out_path = 'out.avi'\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "process_video(in_path, out_path, face_cascade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and references\n",
    "This tutorial simply introduced how to play with images and videos using OpenCV and how to use OpenCV interfaces to do face detection. You may want to explore more about OpenCV or want to know the fundamental principle of face detection. Further, you may want to know how to use OpenCV to track objects. Much more detail are available from the following links.\n",
    "\n",
    "1. OpenCV: https://opencv.org/\n",
    "2. Face detection and tracking method: https://www.youtube.com/watch?v=WfdYYNamHZ8\n",
    "3. Object Tracking using OpenCV: https://www.learnopencv.com/object-tracking-using-opencv-cpp-python/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
