{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big data processing is obviously an important part of data science.By making use of big data, we could get some interesting insights and more reliable results.\n",
    "<img src=http://chuantu.biz/t6/269/1522551559x-1566683256.png />\n",
    "\n",
    "<br> The above figure shows the Google search trend of Big Data(Red) and Data Science(Blue), which shows an increase search interest in these areas over years.\n",
    "\n",
    "<br>This tutorial will introduce you to one popular programming model **MapReduce** which focuses on processing large dataset in parallel.\n",
    "By introducing basic concepts, showing several examples to how to program with MapReduce in detail, hopefully, you will get a good understanding of the framework and feel more comfortable to use the framework in your future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Idea of MapReduce in a Programmer's View\n",
    "MapReduce was created with the idea of making parallel programming of large dataset simpler in mind. It used to call for a large amount of code to make computation parallel among thousands of machines, distribute data among them and figure out ways to recover from failure. With the emergence of MapReduce programming model, things become much easier. A programmer only needs to care about code that directly related to computation based on the dataset without worrying about issues caused by performing computing among a cluster of machines.\n",
    " \n",
    " <br> Before learning to program with Hadoop MapReduce (an implementation of MapReduce), it would be helpful to know about the basic workflow of a MapReduce job.\n",
    "<img src=http://chuantu.biz/t6/269/1522551484x-1404812823.png />\n",
    "<center><figcaption>MapReduce workflow overview (Referenced from [Advanced Cloud Computing](https://www.cs.cmu.edu/~15719/) course of CMU)</figcaption></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> MapReduce jobs will be divided into map tasks and reduce tasks. Map task will conduct user-defined map function while reduce task will execute reduce functions. The input and output of a MapReduce job will be stored in a distributed file system, **HDFS**. The main workflow of a MapReduce job can be viewed as follows. \n",
    "\n",
    "+ **Map Phase**: Each map task will conduct map function on its own portion of input -- taking in a <font color=INDIANRED>(key, value)</font> pair and emit one or several intermediate <font color=INDIANRED>(key, value)</font> pairs. It will sort keys within the mapper and store the output on the local disk. \n",
    "\n",
    "\n",
    "+ **Reduce Phase**:\n",
    "  + **Shuffle stage**:  <font color=INDIANRED>(key, value)</font> pairs will be sent to corresponding reducers for further processing. Usually, the reducer for a key in different mappers is decided by `hash(key) % number_of_reducers`. This process is time-consuming as it usually includes reading from disk from remote machines. This process will guarantee that each key will be assigned to exactly one reducer.\n",
    "  + **Merge & Sort stage**: <font color=INDIANRED>(key, value)</font> pairs from different mappers will be merged and sorted in the reducer. The way of merge same key can be done by specifying a combiner function.\n",
    "  + **Reduce stage**: Each reducer will conduct reduce functions on one or more keys. It will take in the output <font color=INDIANRED>(key, value)</font> pairs from the mapper which have been merged and sort by key. The output of reduce function will be <font color=INDIANRED>(key, value)</font> pair as well. These <font color=INDIANRED>(key, value)</font> pairs produced by reduce functions will be written into HDFS as the final output of a MapReduce job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each stage, tasks could be processed in parallel. For examples, different mappers will run in parallel to process its own split. However, only after all mappers finish their tasks can a reducer start executing its work. Only after shuffle/ merge & sort finished in one particular reducer, can the reducer start executing reduce function on its own input partition.\n",
    "<br>Understanding basic concepts of MapReduce will help you design, optimize and debug your own program. With the help of MapReduce, you only need to design the data processing process by figuring out your map functions and your reduce functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " #### Map Function \n",
    "HDFS will store files in 64MB blocks by default and each block of an input file will become an **input split** for MapReduce workflow. Each mapper will process one input split by executing map function on it.\n",
    "<br> The map function takes in a <font color=INDIANRED>(key, value)</font> pair. Sometimes it is a <font color=INDIANRED>(, line)</font> pair where no key is specified and the value will be a line of the input file. The function could generate intermediate <font color=INDIANRED>(key, value)</font> pairs.\n",
    " \n",
    " #### Reduce Function \n",
    " The reduce function will take in <font color=INDIANRED>(key, value)</font> pairs produced by map function and produce the final <font color=INDIANRED>(key, value)</font> pairs that will be stored in HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a Hadoop MapReduce Streaming \n",
    "After the brief introduction of MapReduce work overview, let's jump into a classic \"WordCount\" example and show the Hadoop MapReduce streaming (one implementation of MapReduce) on our local machine.\n",
    "\n",
    "<br> We are simulating the MapReduce workflow on a local machine. Instead of having input and output on HDFS or local disk of the mapper, we will take input from standard input and output to standard output. \n",
    "\n",
    "<br> **Note that most codes cannot run directly in the notebook, follow instructions and run python files in the directory using commands**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a simple design for my map and reduce function. For the map function, it takes in the line, and for every word, it emits a (word,1) pair. For the reducer, aggregate all value for the same key together to get the final count of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the map function saved in `WordCountmapper.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "##please change the first commented line to the right location of your python2/3 (3 preferred) interpreter\n",
    "import sys\n",
    "#reading line from standard input\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        #write result to standard output\n",
    "        print ('{0}\\t{1}'.format(word, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the reducer function is saved in `WordCountReducer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "##please change the first commented line to the right location of your python2/3 (3 preferred) interpreter\n",
    "import sys\n",
    "word_count_dict = {}\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    if word in word_count_dict:\n",
    "        word_count_dict[word] += count\n",
    "    else:\n",
    "        word_count_dict[word] = count\n",
    "\n",
    "for word in word_count_dict.keys():\n",
    "    print('{0}\\t{1}'.format(word, word_count_dict[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could simulate on your local machine with the command<br>\n",
    "`echo \"foo foo quux labs foo bar quux\" | ./WordCountmapper.py | sort | ./WordCountReducer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labs\t1\n",
      "quux\t2\n",
      "foo\t3\n",
      "bar\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "command = 'echo \"foo foo quux labs foo bar quux\" | ./WordCountmapper.py | sort | ./WordCountReducer.py'\n",
    "output = subprocess.check_output(command, shell=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we only simulate the file locally with standard input and output. **Note that 'WordCountmapper.py' and 'WordCountReducer.py' is still a right implementation for word_count running on real clusters.**\n",
    "If you are interested, please check out the [introduction video of CMU's Cloud Computing course](https://youtu.be/39tUXYjW4co) which shows you how to run your code on clusters step by step.\n",
    "<br> It is always a good practice to simulate runs on small dataset locally before you launch large data set and run on real clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the above reduce function could achieve word count, however, it doesn't take into consideration that the keys are sorted before executing the reduce function. We could optimize the function to save space and avoid memory problem of large data sets. The following is another implementation which considers the pairs are sorted in advance. The function is saved in `WordCountReducer2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "curr_word = None\n",
    "word = None\n",
    "curr_count = 0\n",
    "word_count_dict = {}\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    if not curr_word:\n",
    "        curr_word = word\n",
    "\n",
    "    if word == curr_word:\n",
    "        curr_count += count\n",
    "    else:\n",
    "\n",
    "        print('{0}\\t{1}'.format(curr_word, curr_count))\n",
    "        curr_word = word\n",
    "        curr_count = count\n",
    "\n",
    "#last can be combined with previous ones\n",
    "if curr_word == word:\n",
    "    print('{0}\\t{1}'.format(curr_word, curr_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could simulate on your local machine with the command <br>\n",
    "`echo \"foo foo quux labs foo bar quux\" | ./WordCountmapper.py | sort | ./WordCountReducer2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\t1\n",
      "foo\t3\n",
      "labs\t1\n",
      "quux\t2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "command = 'echo \"foo foo quux labs foo bar quux\" | ./WordCountmapper.py | sort | ./WordCountReducer2.py'\n",
    "output = subprocess.check_output(command, shell=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of MRJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous example, you may find out it's not easy to write map and reduce function even for a simple job. You could only conduct one MapReduce job in each run. If your program consists of many MapReduce steps, you have to set up clusters multiple times.\n",
    "\n",
    "<br> Here I would like to introduce you to a new python library called `mrjob` which make it much easier and intuitive to write programs that consist of several MapReduce steps. First, let's get it installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install mrjob`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'd like to show you how a WordCount program is written using MRJob. The program is saved in `./wordcount.py`. The input file is the same as before but saved in `./input.txt`.\n",
    "Run the program by command line tool in the working directory <br>\n",
    "```python wordcount.py input.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            # write result to standard output\n",
    "            yield word, 1\n",
    "\n",
    "    def reducer(self, word, count):\n",
    "        yield (word,sum(count))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bar\"\t1\n",
      "\"foo\"\t3\n",
      "\"labs\"\t1\n",
      "\"quux\"\t2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "command = 'python wordcount.py input.txt'\n",
    "output = subprocess.check_output(command, shell=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ With `mrjob` , all you have to do is writing mapper, combiner, reducer function in a very intuitive way. I'm not going to introduce combiner function in detail here, but will give you referece at the end.\n",
    "  + For the mapper, the input is (key,value) pair has already been parsed. Note that value is raw input you have to convert it to the datatype you want.\n",
    "  + For the reducer, the input is (key,values) where key is generated by previous mapper and values are python `generator` yielded by all mappers that contain all values of a given key.\n",
    "  \n",
    "  In the word count example, map function converts a raw input line to several `(word,1)` pairs. The reducer function     will take care of certain key and all its values in the generator, use ` sum ` to aggregate all values.\n",
    "  \n",
    "+ `mrjob` can make you write all MapReduce steps in one single file\n",
    "\n",
    "+ It could help you test locally, run on a Hadoop cluster, run on Amazon EMR and run on Google Cloud Dataproc on an easy configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Application: Calculate Movie Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I'm going to use a more complex example to show how to write a Hadoop MapReduce program in python with the use of `mrjob`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing similarity between two movies can help us do things like recommendations. Similarity can be defined in many ways. The similarity I use is the [Pearson's correlation coefficient](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient)(which has been used in HW2 RD) of same user's rating.\n",
    "\n",
    "By measuring the correlation coefficient between different movies, the program output the topK movie pairs which are highly similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset I use is from [movielens_100k](https://grouplens.org/datasets/movielens/100k/) which collects real user's rating for movies. The dataset contains 100k ratings from 1000 users on 1700 movies. \n",
    "<br>The input data is in u.data, each line is one record of (user_id   movie_id    rating   timestamp)\n",
    "<br>The movie info is in u.item, each line is one record of (movie id | movie title |...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Steps of my design:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Step one:\n",
    "  + mapper ( `mapper_extract_data` ): Read line from the u.data and emit (user_id,(movie_id,rating)) pair\n",
    "  + reducer ( `reducer_combine_singleUser_ratings` ): Put all rating from one user in a list, produce (user_id,    [(movie_id1,rating1),(movie_id2,rating2),(movie_id3,rating3),...]) pair\n",
    "\n",
    "\n",
    "+ Step two:\n",
    "  + mapper ( `mapper_generate_rating_pairs` ): The input will be the output of the last step. It will combine the (movie_id, rating) pair by two in the list. Produces rating pairs ((m1,m2),(r1,r2)) of the same user. Note that an order is maintained to make ratings on same movie pairs from different users be treated as same key and go into one reducer. This step will generate a huge dataset to shuffle.\n",
    "  + reducer( `reducer_combine_singleUser_ratings` ): Combine all rating pairs of the same movie pairs. Calculate the correlation between two movies using these ratings. Note that if there are no more than 10 users rate both movies, we should eliminate the results. The output will be (None, ((m1,m2),coefficient))\n",
    "\n",
    "+ Step three:\n",
    "  + reducer( `find_topK_moviepairs` ): Before conducting reducer, an initializer is used to create a dictionary that map `movie_id` into `movie_name`. All input pairs will go into one machine as they all have `key=None` which enables global ranking. Note that some different movie_ids are corresponding the same movie such as 266 and 680, we should eliminate such results.\n",
    "  \n",
    "The steps configuration are in steps function.\n",
    "Also, I add an argument when I run the function by `configure_args`\n",
    "<br> The file is saved as `./movierating.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import combinations\n",
    "\n",
    "class MRMovieSimilarity(MRJob):\n",
    "\n",
    "    #(user_id,movie_id,rating,timestamp) -> (userid,(movie_id,rating))\n",
    "    def mapper_extract_data(self, _, record):\n",
    "        user_id, movie_id, rating, timestamp = record.split()\n",
    "        yield user_id, (movie_id, rating)\n",
    "\n",
    "    # (userid,(movie_id,rating)) -> (userid,[(m1,r1),(m2,r2)...])\n",
    "    def reducer_combine_singleUser_ratings(self, user_id, rating_info):\n",
    "        all_ratings = []\n",
    "        for movie_id, rating in rating_info:\n",
    "            all_ratings.append((movie_id, rating))\n",
    "        yield None, all_ratings\n",
    "\n",
    "    # (userid,[(m1,r1),(m2,r2)...]) -> ((m1,m2),(r1,r2))...\n",
    "    def mapper_generate_rating_pairs(self,_,all_ratings):\n",
    "        for movie1info,movie2info in combinations(all_ratings,2):\n",
    "            m1 = movie1info[0]\n",
    "            r1 = movie1info[1]\n",
    "            m2 = movie2info[0]\n",
    "            r2 = movie2info[1]\n",
    "            if m1 < m2:\n",
    "                yield (m1,m2),(r1,r2)\n",
    "            if m2 < m1:\n",
    "                yield (m2,m1),(r2,r1)\n",
    "\n",
    "    def calculate_correlation(self,x,y):\n",
    "        n = len(x)\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "\n",
    "        x_sum = x.sum()\n",
    "        y_sum = y.sum()\n",
    "\n",
    "        x_sqr = x ** 2\n",
    "        y_sqr = y ** 2\n",
    "\n",
    "        x_sqr_sum = x_sqr.sum().item()\n",
    "        y_sqr_sum = y_sqr.sum().item()\n",
    "        xy = x * y\n",
    "        xy_sum = xy.sum()\n",
    "\n",
    "        res = (n * xy_sum - x_sum * y_sum) / (\n",
    "        math.sqrt(n * x_sqr_sum - x_sum ** 2) * math.sqrt(n * y_sqr_sum - y_sum ** 2))\n",
    "\n",
    "        return res;\n",
    "\n",
    "    # (m1,m2), cosine similarity score\n",
    "    def reducer_cosine_similarity(self,movie_pair,rating_pair):\n",
    "        rating_1 = []\n",
    "        rating_2 = []\n",
    "        for r1,r2 in rating_pair:\n",
    "            rating_1.append(float(r1))\n",
    "            rating_2.append(float(r2))\n",
    "        if len(rating_1) > 10:\n",
    "            #sim = dot(rating_1, rating_2) / (norm(rating_1) * norm(rating_2))\n",
    "            sim = self.calculate_correlation(rating_1,rating_2)\n",
    "            yield None,(movie_pair,sim)\n",
    "\n",
    "    def map_movie_names(self,movie_pair):\n",
    "        if  self.name_dict[movie_pair[0]] == self.name_dict[movie_pair[1]]:\n",
    "            return None\n",
    "        return self.name_dict[movie_pair[0]],self.name_dict[movie_pair[1]]\n",
    "\n",
    "    def find_topK_moviepairs(self,_,sim_info):\n",
    "        K = 20\n",
    "        cnt = 0\n",
    "        sorted_siminfo = sorted(list(sim_info), key=lambda a: float(a[1]), reverse=True)\n",
    "        for movie_pair,score in sorted_siminfo:\n",
    "            if self.map_movie_names(movie_pair):\n",
    "                yield self.map_movie_names(movie_pair),score\n",
    "                cnt += 1\n",
    "            if cnt >= K:\n",
    "                break\n",
    "\n",
    "    def get_movies_names(self):\n",
    "        self.name_dict = {}\n",
    "        with open('u.item',encoding = \"ISO-8859-1\") as f:\n",
    "            for line in f:\n",
    "                fields = line.split('|')\n",
    "                self.name_dict[(fields[0])] = fields[1]\n",
    "\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(MRMovieSimilarity, self).configure_args()\n",
    "        self.add_file_arg('--items', help='u.item (which contains movie info) path')\n",
    "\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper_extract_data, reducer=self.reducer_combine_singleUser_ratings),\n",
    "                MRStep(mapper=self.mapper_generate_rating_pairs, reducer = self.reducer_cosine_similarity),\n",
    "                MRStep( reducer_init=self.get_movies_names, reducer=self.find_topK_moviepairs)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMovieSimilarity.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python3 to run the program \n",
    "```\n",
    " python3 ./movierating.py ./u.data --items ./u.item\n",
    "```\n",
    "It takes about 10 minutes to run on local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Jackie Chan's First Strike (1996)\", \"Wings of Desire (1987)\"]\t0.9728148411099612\n",
      "[\"Fire Down Below (1997)\", \"Shadow Conspiracy (1997)\"]\t0.966190518580404\n",
      "[\"Air Bud (1997)\", \"Free Willy 3: The Rescue (1997)\"]\t0.9658676311477036\n",
      "[\"Tin Drum, The (Blechtrommel, Die) (1979)\", \"True Romance (1993)\"]\t0.9624980365728483\n",
      "[\"Chain Reaction (1996)\", \"Shadow Conspiracy (1997)\"]\t0.958569328934402\n",
      "[\"Sling Blade (1996)\", \"Some Folks Call It a Sling Blade (1993)\"]\t0.9579176872327142\n",
      "[\"Event Horizon (1997)\", \"Wes Craven's New Nightmare (1994)\"]\t0.9510703097740938\n",
      "[\"Hamlet (1996)\", \"Raise the Red Lantern (1991)\"]\t0.9386740768532977\n",
      "[\"Once Upon a Time... When We Were Colored (1995)\", \"Rosewood (1997)\"]\t0.9343666883698424\n",
      "[\"Local Hero (1983)\", \"Raise the Red Lantern (1991)\"]\t0.9340289249886088\n",
      "[\"Young Guns (1988)\", \"Beverly Hills Ninja (1997)\"]\t0.928950669521923\n",
      "[\"War, The (1994)\", \"Outbreak (1995)\"]\t0.9280125662443223\n",
      "[\"D3: The Mighty Ducks (1996)\", \"Batman Forever (1995)\"]\t0.9264365695804326\n",
      "[\"Fly Away Home (1996)\", \"Nell (1994)\"]\t0.9242987424932899\n",
      "[\"Chain Reaction (1996)\", \"Trigger Effect, The (1996)\"]\t0.9159337782355624\n",
      "[\"Koyaanisqatsi (1983)\", \"Night on Earth (1991)\"]\t0.9151820253560947\n",
      "[\"Excess Baggage (1997)\", \"Postman, The (1997)\"]\t0.9099637547345425\n",
      "[\"That Darn Cat! (1997)\", \"Cats Don't Dance (1997)\"]\t0.9098399179564544\n",
      "[\"War, The (1994)\", \"Dead Poets Society (1989)\"]\t0.9097222222222222\n",
      "[\"Pocahontas (1995)\", \"Congo (1995)\"]\t0.909661734959589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this may cause error on your machine, you have to specify your python3 interpreter \n",
    "# make sure you have numpy and mrjob in that environment\n",
    "# like \"/Users/momo/anaconda/envs/MRtutorial/bin/python ./movierating.py ./u.data --item ./u.item\"\n",
    "command = 'python3 ./movierating.py ./u.data --item ./u.item'\n",
    "output = subprocess.check_output(command, shell=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an amazon AWS account, you could run this program on real EMR clusters as well. See [quick reference](https://pythonhosted.org/mrjob/guides/emr-quickstart.html) for help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Thoughts on MapReduce\n",
    "+ Each MapReduce step will read input and write output with HDFS which is time-consuming. As a result, it's not very suitable for processing iterative jobs. (Use Spark instead)\n",
    "+ MapReduce jobs may suffer from the imbalance workload at different reducers. \n",
    "+ Pay Attention to memory settings when running MapReduce job on clusters to eliminate out-of-memory error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. MapReduce Paper: https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf\n",
    "2. Mrjob documentations: https://pythonhosted.org/mrjob/index.html\n",
    "3. Set up EMR clusters to run MapReduce program in python: https://youtu.be/39tUXYjW4co\n",
    "4. Another python MR library (lower level and more flexible): https://github.com/klbostee/dumbo/wiki/Short-tutorial\n",
    "5. MapReduce combiners: https://www.tutorialspoint.com/map_reduce/map_reduce_combiners.htm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
