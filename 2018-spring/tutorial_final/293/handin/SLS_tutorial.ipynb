{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will introduce one approximate algorithm for optimization problem: Stochastic Local Search. <br>\n",
    "<br>\n",
    "Lots of practical problems in data science are about using the data to do some classificaton or regression. One good example is that in \"homework 3\" of 15688, we make a linear regression based on real-time bus data from Pittsburgh to make TrueTime prediction on arrival time.<br>\n",
    "<br>\n",
    "Almost all classification or regression problems are seeking for a optimal solution. However, the hardness of directly computing the optimal answer would be NP-hard. So people start to introduce some approximate algorithms to find approximate solution with the provable guarantees on the distance of the returned solution to the optimal one.<br>\n",
    "<br>\n",
    "So this tutorial would cover one useful approximate algorithm: Stochastic Local Search.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tutorial Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this tutorial, we will cover how to use this stochastic local search in real problem. One of the example would be about how to use stochastic local search to compute the parameter of one Bayesian network and how the noise parameter would influence the search progress and result. The other example would be more about applying this stochastic local search in an actual regression problem, which is .......<br>\n",
    "<br>\n",
    "The following topics would be covered in this tutorial:\n",
    "- [Motivation of SLS](#Motivation-of-SLS)\n",
    "- [How SLS algorithm work](#How-SLS-algorithm-work)\n",
    "- [Python implement of SLS](#Pyhton-implement-of-SLS)\n",
    "- [Example application: linear regression](#Example-application:-linear-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation of SLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most approximate algorithms are designed as greedy approximate algorithms. Greedy approximate algorithms work by making a local optimal step each time with the hope of finding the global optimal. However if the object function has multiple local maximun or minimum, in which case the global maximum or minimum would be different from the local optimal. Then the greedy approximate algorithm would most likely fall into local optimal. <br><br>\n",
    "For example, one of the most commen used greedy approximate algorithms would be gradient decent. Gradient decent takes iterative steps to reach the maximun or minimum value of object fucniton. An example progress looks like the following:<br>\n",
    "<img src=\"./gradient_decent.png\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we start our progress in point Q, it is easy to use gradient decent to find global minimum. Each iteration of gradient decent is going towards the direction that makes object function smaller. However, when we are choosing P as start point, the gradient decent would finially ends with local optimal instead of global optimal.<br><br>\n",
    "So one thing about the gradient decent is that the initial value have great influence on whether this algorithm would ends with global optimal. Since there is no way for us to know exactly whether a start point is a \"good start point\", it is very likely to fall into local optimal.<br><br>\n",
    "SLS algorithm could solve the problem by introducing a random step in iterative steps. The goal of introducing this random step is to have your parameter move randomly with the hope of it jumping out of this local optimal.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How SLS algorithm work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Decent\n",
    "Before we head to the SLS algorithm, let's have a look of gradent decent algorithm first, since it is used as one part of the SLS algorithm.<br><br>\n",
    "   Given parameter set **{Max_Iterations, Min_error, learning rate, training_examples, traininglabels}**<br><br>\n",
    "   Assign our training weights **w** with initial values<br><br>\n",
    "   Repeat following steps until reaching the Max_Iterations or reaching the Min_error:<br><br>\n",
    "      comput our model output: **y' = X*w**<br><br>\n",
    "      update training weight: **w**:= **w** + learning_rate * $\\sum_{}{} x*(y'-traininglabels)$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLS\n",
    "Comparing to gradent decent, the main defference of SLS is that it adds a random step in the algorithm. Given a probability p, algorithm would choose whether to take a random step or a greedy step.<br><br>\n",
    "We define the random step as randomly flip on bit of a randomly chosen variable. More specificly, we'll convert the float value into it's binary bits. Then randomly choose on bit to flip, eg, 0->1, 1->0. By doing this, we successfully assign our weight marteix a random value.<br><br>\n",
    "While in greedy step, we'll use gradient decent to find a local optimal. \n",
    "So our SLS algorithm now works like this:<br><br>\n",
    "   1. Initialize all parameters uniformly random.<br><br>\n",
    "   2. Then take the stochastic local search step, which has a probability of p going the random step, probability of 1-p going the gradent decent step. <br><br>\n",
    "   3. Repeat the stochastic local search step until SLS algorithm reaches our STOP_CONSTRAIN.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The workflow of SLS algorithm<br><br>\n",
    "  algorithm parameter sets is **{MAX-TRIES, MAX-FLIPS, NOISE PROBALITY: p, OBJECT FUNCTION: f, SOLUTION PARAMETER: b}**<br><br>\n",
    "    TRIES = 0<br><br>\n",
    "    **while** TRIES < MAX-TRIES **do** { <br><br>\n",
    "    Initialize b == (0,1) uniformly at random <br><br>\n",
    "    **while** STEPS < MAX-STEPS **do** { <br><br>\n",
    "       RANDOM = flip a biased coin using noise parameter p<br><br>\n",
    "       **if** RANDOM then <br><br>\n",
    "       **else** using gradent decent to find nearest local optimal result<br><br>\n",
    "       **if** f(b) > f' **then** {f' = f(b); b' = b}<br><br>\n",
    "       **if**  MEET THE STOP_CONSTRAINTS **then** return b'<br><br>\n",
    "       }\n",
    "    TRIES = TRIES + 1<br><br>\n",
    "    }<br><br>\n",
    "    return b' <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supplyment: \n",
    "b' is used to store the current best solution. STOP_CONSTRAINTS is the constraints we set as the iteration stop requirement. When parameter of our current solution meet the STOP_CONSTRAINTS, this iteration would stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the workflow, whether the random step would execute depends on the probability of nosie parameter p, which is set in range of [0,1]. The goal of doing this random step is to make our algorithm possible to jump out of a local optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyhton implement of SLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from struct import pack,unpack\n",
    "import matplotlib.pyplot as plt\n",
    "def biased_flip(some_list, probabilities):  \n",
    "    \"\"\"\n",
    "    input:  some_list: the values we want to output biased, in this case, it is 0 or 1\n",
    "            probabilities: the probability vector determining which value would be picked as output\n",
    "    \"\"\"\n",
    "    x = random.uniform(0,1)  \n",
    "    cumulative_probability = 0.0  \n",
    "    for item, item_probability in zip(some_list, probabilities):  \n",
    "        cumulative_probability += item_probability  \n",
    "        if x < cumulative_probability:break  \n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_step(w):\n",
    "    #choose variable we want to assign a random value\n",
    "    i=random.randint(0, len(w))\n",
    "    variable=w[i,0]\n",
    "    #randomly flip a bit on choosen variable's binary value\n",
    "    fs=pack('f', variable)\n",
    "    bval = list(unpack('BBBB', fs))\n",
    "    index=random.randint(0,3)\n",
    "    B=bval[index]\n",
    "    v=random.randint(0,7)\n",
    "    if B&(255-pow(2,v))==B:\n",
    "        B=B+pow(2,v)\n",
    "    else:\n",
    "        B=B-pow(2,v)\n",
    "    bval[index]=B\n",
    "    fs = pack('BBBB', *bval)\n",
    "    new_variable=unpack('f',fs)\n",
    "    w[i,0]=new_variable[0]\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_decent(x, y, learning_rate, weight_initial, maxIterations):\n",
    "    \"\"\"\n",
    "    function: using gradient decent to find local optimal\n",
    "    input:  x: training data\n",
    "            y: training label\n",
    "            learning_rate: learning rate of each iteration, in range of [0,1]\n",
    "            weight_initial: start weight\n",
    "            maxIterations: maximum iteration number\n",
    "    output: w: weight matrix of local optimal\n",
    "    \"\"\"\n",
    "    iteration=0\n",
    "    m=weight_initial.shape[0]\n",
    "    w=weight_initial\n",
    "    y_model=np.dot(x, w)\n",
    "    er=y_model-y\n",
    "    while iteration<maxIterations and np.sum(er**2)>0.1:\n",
    "        for i in range(m):\n",
    "            for j in range(x.shape[0]):\n",
    "                w[i][0] = w[i][0]-learning_rate*er[j,0]*x[j,i]\n",
    "        y_model=np.dot(x, w)\n",
    "        er=y_model-y\n",
    "        iteration +=1\n",
    "    return w\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def greedy_step(w, training_examples, training_labels):\n",
    "    \"\"\"\n",
    "    function: using gradient decent to find local optimal in greedy step\n",
    "    \"\"\"\n",
    "    learning_rate = 0.0001\n",
    "    max_iterations = 50000\n",
    "    return gradient_decent(training_examples, training_labels, learning_rate, w, max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SLS(training_examples, training_labels, MAX_TRIES, MAX_STEPS, noise_parameter, stop_constraint):\n",
    "    \"\"\"\n",
    "    function: using SLS to do a linear regression\n",
    "    \"\"\"\n",
    "    TRIES=0\n",
    "    w_current_optimal=random.random([training_examples.shape[1],training_labels.shape[1]])\n",
    "    while (TRIES < MAX_TRIES):\n",
    "        #initialize weight uniformly at random\n",
    "        weight=random.random([training_examples.shape[1],training_labels.shape[1]])\n",
    "        STEPS=0\n",
    "        while (STEPS < MAX_STEPS):\n",
    "            STEPS+=1\n",
    "            i=biased_flip([0,1],[noise_parameter, 1-noise_parameter])\n",
    "            if i==0:            \n",
    "                weight=random_step(weight)\n",
    "            if i==1:\n",
    "                weight=greedy_step(weight, training_examples, training_labels)\n",
    "            if np.sum((training_labels-np.dot(training_examples, w_current_optimal))**2) > np.sum((training_labels-np.dot(training_examples, weight))**2):\n",
    "                w_current_optimal=weight\n",
    "            if np.sum((training_labels-np.dot(training_examples, w_current_optimal))**2) < stop_constraint:\n",
    "                return w_current_optimal\n",
    "        TRIES+=1\n",
    "    return w_current_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example application: linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll use one simple linear regression as an example to see how SLS algorithm works. We'll see how area of house relate to it's price. Given data: <br><br>\n",
    "area of house: 150, 200, 250, 300, 350, 400, 600<br><br>\n",
    "price of house: 6450, 7450, 8450, 9450, 11450, 15450,18450<br><br>\n",
    "What we are trying to do here is to train a linear regression model to fit our training data properly. <br><br>\n",
    "Before we start training, let's first use min-max normalization to normalize our data into [0,1].<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_examples=np.array([[150], [200], [250], [300], [350], [400], [600]])\n",
    "training_labels=np.array([[6450], [7450], [8450], [9450], [11450], [15450], [18450]])\n",
    "\n",
    "def min_max_normalization(x):\n",
    "    \"\"\"\n",
    "    function: use min-max normalization to normalize data into [0,1]\n",
    "    input: data to be normalized\n",
    "    output: normalized data\n",
    "    \"\"\"\n",
    "    return np.array([(float(i)-min(x))/float(max(x)-min(x)) for i in x])\n",
    "\n",
    "training_e = min_max_normalization(training_examples)\n",
    "training_l = min_max_normalization(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model we want to train is y=w'X+b. \"b\" is the bias. To make \"b\" as a part of training, we add a \"1\" into all trainging examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array((1,1,1,1,1,1,1))\n",
    "training_data = np.column_stack((training_e,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put our training_data and training_l into SLS algorithm and check out how well this weight matrix fits our training data.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = SLS(training_data, training_l, 2, 20, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(training_e, training_l, 'r*')\n",
    "plt.plot(training_e, weight[0,0]*training_e+weight[1,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see how noise parameter influence our training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we all noticed, there is a noise parameter p which determines the probability of whether random step takes place. Let's change our \"p\" to see how it influences.<br><br>\n",
    "First, let's modify our SLS algorithm to save our loss function output everytime we make a step.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noise_parameter_SLS(training_examples, training_labels, MAX_STEPS, stop_constraint):\n",
    "    \"\"\"\n",
    "    function: finding how noise pparameter(probability) influence training\n",
    "    input: \n",
    "    \"\"\"\n",
    "    noise = [0, 0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "    line_color = ['b','y' 'g', 'r', 'k', 'c']\n",
    "    plt.gca().set_color_cycle(['red', 'green', 'blue', 'yellow', 'cyan', 'magenta'])\n",
    "    TRIES=0\n",
    "    w_current_optimal=random.random([training_examples.shape[1],training_labels.shape[1]])\n",
    "    for k in range(len(noise)):\n",
    "        #initialize weight uniformly at random\n",
    "        noise_parameter=noise[k]\n",
    "        loss=[]\n",
    "        itera=[]\n",
    "        weight=random.random([training_examples.shape[1],training_labels.shape[1]])\n",
    "        STEPS=0\n",
    "        while (STEPS < MAX_STEPS):\n",
    "            STEPS+=1\n",
    "            itera.append(STEPS)\n",
    "            i=biased_flip([0,1],[noise_parameter, 1-noise_parameter])\n",
    "            if i==0:            \n",
    "                weight=random_step(weight)\n",
    "            if i==1:\n",
    "                weight=greedy_step(weight, training_examples, training_labels)\n",
    "            loss.append(np.sum((training_labels-np.dot(training_examples, weight))**2)/training_examples.shape[0])\n",
    "            plt.plot(itera, loss)\n",
    "            if np.sum((training_labels-np.dot(training_examples, w_current_optimal))**2) > np.sum((training_labels-np.dot(training_examples, weight))**2):\n",
    "                w_current_optimal=weight\n",
    "            if np.sum((training_labels-np.dot(training_examples, w_current_optimal))**2) < stop_constraint:\n",
    "                break\n",
    "    plt.legend(['noise=0', 'noise=0.1', 'noise=0.3', 'noise=0.5', 'noise=0.7', 'noise=1.0'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_parameter_SLS(training_data, training_l, 100, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set our noise parameter as 0, 0.1, 0.3, 0.5, 0.7, 1, one example output would be as follows: (Due to the uncertainty of going random step or greedy step, running output may be different from following example.)<br><br>\n",
    "<img src=\"./noise_parameter.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see, when noise parameter is larger, the whole training went more randomly. However, larger noise parameter did means that it is more likely to jump out of local optimal. When noise parameter is set as 0, SLS algorithm is completely greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When multi local optimal exists, this tutorial introduces one possible way of jumping out of local optimal. One thing for sure, given infinite time(steps or tries), SLS would find the global optimal. However, when given selected steps or tries, no one knows when or whether this SLS algorithm would find global optimal. But returned solution of SLS is at least not worse than pure greedy search.<br><br>\n",
    "If your are interested, more disscussions and details on SLS are available from the following links.<br><br>\n",
    "1. Understanding the role of noise in stochastic local search: Analysis and experiments https://www.sciencedirect.com/science/article/pii/S0004370208000040\n",
    "2. Stochastic Local Search https://www.cs.put.poznan.pl/mkomosinski/materialy/optymalizacja/extras/StochasticLocalSearch.pdf\n",
    "3. video about SLS algorithm https://www.youtube.com/watch?v=E0l8xIXYSY4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
