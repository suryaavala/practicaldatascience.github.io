{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will introduce some ideas in neural networks such as fine-tuning. The corresponding practical practice in this tutorial involves fine-tuning a convolutional neural network(CNN) to perform 3D object classification based on the paper Multi-view Convolutional Neural Networks for 3D Shape Recognition.\n",
    "The goal of this tutorial is to implement an MVCNN 3D object classifier. To make things clearer, let's first look at the architecture in the paper. The dataset is composed by different objects with 12 images from 12 aspects. After training each images via CNN1, we select one with the largest tensor as the input of CNN2.\n",
    "<img  src=\"arch.png\">\n",
    "\n",
    "The tutorial is composed by two parts. First we need to implement a single-view classifier which only uses the view from one aspect corresponding to the CNN1 part. And then we implement the multi-view classifier based on the data from 12 aspects of view corresponding to the CNN1 part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, you'll need to install some libraries that we will use to set up the environment. First you need to intall Anaconda Python on your computer(the version of mine is 4.4.9) and then install the Keras library(2.0.2) and its dependency TensorFlow(1.0.0) with conda command.Keras is an open source neural network library written in Python.TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and also used for machine learning applications such as neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    conda install tensorflow\n",
    "    conda install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training of CNNs, computers with GPU could be more effcient. However, the purpose of this tutorial is to give you a glance of the usage of keras and tensorflow in training CNNs, you can just use your local machine to train one epoch if you only have CPU equipped.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "import glob\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and loading data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we use in this tutorial is the same with the one used in the paper, the modelnet-40. This file contains the 40 categories of CAD models used to train our deep network. You can download the dataset [here](http://3dshapenets.cs.princeton.edu).Then put the dataset under the project folder. Here is the structure of the dataset. \n",
    "<img  src=\"dataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data, first we need to read the name of files. There are 40 kinds of models in the dataset. To make life easier, we can only use some of them by changing the nclasses parameter(the default number is 40)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the sub-directories of a directory\n",
    "def subdirs(dirname):\n",
    "    return [x for x in os.listdir(dirname) if os.path.isdir(os.path.join(dirname, x))]\n",
    "\n",
    "def data_filenames(subset, src_dir='modelnet40', nclasses = 40):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - subset:       Either 'train' or 'test'\n",
    "    - src_dir:      The name of dataset\n",
    "    - nclasses:     The number of classes.(default is 40)\n",
    "    \"\"\"\n",
    "    classes = sorted(subdirs(src_dir))\n",
    "    ans = []\n",
    "    i = 0\n",
    "    for (icls, cls) in enumerate(classes):\n",
    "        \n",
    "        subset_dir = os.path.join(src_dir, cls, subset)\n",
    "        model_dirs = subdirs(subset_dir)\n",
    "        for model_dir in model_dirs:\n",
    "            filenames = glob.glob(os.path.join(src_dir, cls, subset, model_dir, '*.png'))\n",
    "            ans.append((icls, filenames))\n",
    "        i+=1\n",
    "        if i == nclasses:\n",
    "            break\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the number of file to test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710\n"
     ]
    }
   ],
   "source": [
    "print(len(data_filenames('test',nclasses=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The dataset is composed by various image so that we need to implement a function the read images with the help of keras library. Also we need to call the preprocess function of ResNet50 model for each image. The reason we use ResNet50 model for inputing images is that we will apply this model in the further training work. The target size of the image is (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image(filename, target_size=(224, 224)):\n",
    "    x = image.load_img(filename, target_size=target_size)\n",
    "    x = image.img_to_array(x)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   ..., \n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]]\n",
      "\n",
      "  [[-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   ..., \n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]]\n",
      "\n",
      "  [[-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   ..., \n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]]\n",
      "\n",
      "  ..., \n",
      "  [[-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   ..., \n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]]\n",
      "\n",
      "  [[-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   ..., \n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]]\n",
      "\n",
      "  [[-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   ..., \n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]\n",
      "   [-103.93900299 -116.77899933 -123.68000031]]]]\n"
     ]
    }
   ],
   "source": [
    "print(read_image('ModelNet40/airplane/train/airplane_0001.off/airplane_0001.0.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to implement a generator to returns images and classes from ModelNet-40 in size one batches.\n",
    "\n",
    "First we read all the name of both the test and train files in the modelnet40 dataset with the function data_filenames.\n",
    "\n",
    "The generator yields by default an infinite number of elements which each have the form (x, y), where x is an input for supervised training and y is an output. \n",
    "\n",
    "If single is True (single view mode) then x is a 4D numpy array with shape 1 x h x w x 3 which ramdomly chose for 12 views of the object, representing an input image(h for height, w for weight), and y is a 2D numpy array of shape 1 x nclasses, where nclasses is the number of classes in ModelNet-40 (defined as the global nclasses, equal to 40). \n",
    "\n",
    "If single is False (multiple view mode) then x is a list of arrays representing different views of the same model: the list has length nviews (defined as the global nviews, equal to 12): each view is an numpy array of an image with shape 1 x h x w x 3.So that the x is a 5D numpy array with shape 12 x 1 x h x w x 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(subset, single=True, frac=1.0, nclasses = 40):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - subset:       Either 'train' or 'test'\n",
    "    - single:       If true, return one image and class at a time in the format (img, cls) which for the single-view classifier\n",
    "                    If false, return (x, cls), where x is a list of images for all (12) views which for the culti-view classifier\n",
    "    - frac:         Fraction of dataset to load (use frac < 1.0 for quick tests).\n",
    "    - nclasses:     The number of classes.(default is 40)\n",
    "    \"\"\"\n",
    "    filenames = data_filenames(subset)\n",
    "    def generator_func():\n",
    "        while 1:\n",
    "            random.shuffle(filenames)\n",
    "            for (cls, view) in filenames[:int(len(filenames)*frac)]:\n",
    "                cls_array = np.zeros((1, nclasses), 'float32')\n",
    "                cls_array[0, cls] = 1.0\n",
    "                cls = cls_array\n",
    "                if single:\n",
    "                    filename = random.choice(view)\n",
    "                    yield (read_image(filename), cls)\n",
    "                else:\n",
    "                    yield ([read_image(view_elem) for view_elem in view], cls)\n",
    "    return (generator_func(), len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, to make a simple test, we set the frac to be 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When single is True:\n",
      "dataset size:2468\n",
      "1\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.]]\n",
      "When single is False:\n",
      "dataset size:2468\n",
      "12\n",
      "[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "(g, dataset_size) = data_generator('test', single=True, frac = 0.01)\n",
    "(x1, y1) = g.__next__()\n",
    "print('When single is True:')\n",
    "print('dataset size:' + str(dataset_size))\n",
    "print(len(x1))\n",
    "print(y1)\n",
    "\n",
    "(g2, dataset_size2) = data_generator('test', single=False, frac = 0.01)\n",
    "(x2, y2) = g2.__next__()\n",
    "print('When single is False:')\n",
    "print('dataset size:' + str(dataset_size))\n",
    "print(len(x2))\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, we can get the data generator for both train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9843\n",
      "2468\n"
     ]
    }
   ],
   "source": [
    "(train_generator,dataset_size_train) = data_generator('train')\n",
    "(validation_generator, dataset_size_val) = data_generator('test')\n",
    "print(dataset_size_train)\n",
    "print(dataset_size_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single view classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will implement a single-view classifier step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural network training, fine-tuning is the process to make the result more precise by adjusting the weights of the models. In our tutorial, we will fine tune the existed CNN model and here we will use the ResNet-50 which is already included in Keras.applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a base model using pretrained ResNet-50 without the  fully-connected layer at the top of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model = applications.ResNet50(weights = 'imagenet',include_top=False, input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we need to add a flat layer followed by a dense layer with 40 outputs and softmax as the activation.\n",
    "\n",
    "Here we will use the sequential model in keras which is a linear stack of layers. The sequential model is relatively simple and fast. There is no connection between two layers. For this single-view classifier, we only need to use the sequential model.\n",
    "\n",
    "The softmax function is often used in the final layer of a neural network-based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(40, activation='softmax'))\n",
    "model = Model(inputs= base_model.input, outputs= top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes if we train all the layers the result is not as satisfied as we set some top layers not trainable. So here we will not finetuned all the layers but set p fraction of layers to be not trainable. I use p = 0.5 in this tutorial but you can make more experiment to find the best p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_not_train(model, p):\n",
    "    for layer in model.layers[:int(len(model.layers)*p)]:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_not_train(model,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we need to compile the model. Here we use the categorical_crossentropy loss and accuracy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we will call the keras.model function fit_generator() with both the train and test generator. Here we also need to set the number epoch which is the total number of iterations of training on the data. Notice here we do not need to run it until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=9843, epochs=20, validation_data=<generator..., validation_steps=2468)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1035/9843 [==>...........................] - ETA: 6129s - loss: 3.1274 - acc: 0.2512"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-f5d415024e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_size_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1875\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=dataset_size_train,\n",
    "    epochs=20,\n",
    "    validation_data = validation_generator,\n",
    "    nb_val_samples=dataset_size_val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can have a look of the result above which I run on my local machine without GPU. The ETA is more than 6000s. The inefficiency due to the generator only generate one image a time which means the batch size is 1. If we use larger batch size, it brings advantages to training speed. However, we also have to take the size of batch into consideration. The higher the batch size, the more memory space we'll need. Here we will set the size to 16.\n",
    "\n",
    "Here we will build another function to generate the batch data with the generator above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(generator, batch_size):\n",
    "    def generator_func(generator, batch_size):\n",
    "        while True:\n",
    "            i = batch_size\n",
    "            xx = np.empty((batch_size, 224,224,3))\n",
    "            yy = np.empty((batch_size, 40))\n",
    "            while i:\n",
    "                (x,y) = generator.__next__()\n",
    "                xx[batch_size-i]= x[0]\n",
    "                yy[batch_size-i] = y[0]\n",
    "                i-=1\n",
    "            yield xx,yy\n",
    "    return (generator_func(generator, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next is to use the new generator as the test/train data of the fit_generator. One thing we need to be careful is that the step_per_epoch and nb_val_samples need to be divided by the size of batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=19, epochs=20, validation_data=<generator..., validation_steps=4)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "model.fit_generator(\n",
    "    batch_generator(train_generator, batch_size),\n",
    "    steps_per_epoch=dataset_size_train //batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=batch_generator(validation_generator, batch_size),\n",
    "    nb_val_samples=dataset_size_val //batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multi view classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we will implement a rather complex network with multiple views of a object. For each object in the dataset, it has 12 aspects of view. At the first part, we only user one of them to recognize the 3D object which it is obvious not accurate enough comparing with using all of them. In part 1, we use the Sequential Model but here we will use the functional api of keras. The functional api is the way to solve complex problems. The reason we use it here is that we need to share the same layers in CNN2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the architecture showed in the image above, it has two CNNs. For CNN1 we will truncate ResNet50. The reason I choose 34 as the number of stop layer is from the [research]('https://arxiv.org/pdf/1512.03385.pdf') of ResNet the 34-ResNet works best. Be careful here for each object we have 12 images and each of them should be a separate instance but share the CNN1 with same parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_instances = []\n",
    "base_model = applications.ResNet50(weights = 'imagenet',include_top=False)\n",
    "models = []\n",
    "input_t = Input((224,224,3))\n",
    "for i in range(0,12):\n",
    "    input_instances.append(Input(shape=(224,224,3)))\n",
    "    models.append(Model(inputs = base_model.input, outputs=base_model.layers[34].output)(input_instances[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CNN2, the input is the largest tensor of the 12 output from CNN1. Then we build one Conv2D layer. The Conv2D is 2D convolution layer which is often used in image-related problem. And then we use batch normalization to prevent gradients vanishing and improve accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.normalization import  BatchNormalization\n",
    "from keras.layers.merge import maximum\n",
    "max_tensor = maximum(models)\n",
    "x = Conv2D(filters=40, kernel_size = 5)(max_tensor)\n",
    "x = BatchNormalization()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the same as Part I, we add a flat layer followed by a dense layer with 40 outputs and softmax as the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Conv2D(filters=40, kernel_size = 5)(max_tensor)\n",
    "x = BatchNormalization()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(40, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs= input_instances, outputs= x)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve performace, we will alse batch the data. Be careful here the single parameter of data generator is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train_generator,dataset_size_train) = data_generator('train',single = False)\n",
    "(validation_generator, dataset_size_val) = data_generator('test', single = False)\n",
    "\n",
    "def batch_generator(generator, batch_size):\n",
    "    def generator_func(generator, batch_size):\n",
    "        while True:\n",
    "            i = 0\n",
    "            xx=[]\n",
    "            for t in range(12):\n",
    "                xx.append(numpy.empty((batch_size,224,224,3)))\n",
    "            yy = numpy.empty((batch_size,40))\n",
    "            while i < batch_size:\n",
    "                (x,y) = generator.__next__()\n",
    "                k = 0\n",
    "                while k < 12:\n",
    "                    xx[k][i] = x[0]\n",
    "                    yy[i] = y[0]\n",
    "                    k+=1\n",
    "                i+=1\n",
    "            yield xx,yy\n",
    "    return (generator_func(generator, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the fit_generator function to train the model like part1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "model.fit_generator(\n",
    "    batch_generator(train_generator, batch_size),\n",
    "    steps_per_epoch=dataset_size_train //batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=batch_generator(validation_generator, batch_size),\n",
    "    nb_val_samples=dataset_size_val //batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I introduce a process of implementing the classifier of 3D object. After this tutorial, you should have the knowledge of the following few things:\n",
    "    \n",
    "    1.A basic understanding of two kinds of keras model, sequential model and functional api, and the difference between them.\n",
    "    2.Different types of layers in Keras such as Dense layer, Flatten layer, Conv2D and so on.   \n",
    "    3.Some concepts of deep learning such as fine-tuning\n",
    "\n",
    "Moreover, I will give you some results from aws. It takes too much time to run until convergency so that I will give you the result of the first epoch as reference.\n",
    "For single-view classifier, the accuracy after first epoch is 55%.\n",
    "For multi-view classifier, the accuracy after first epoch is 58%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.Keras Library: https://keras.io\n",
    "    2.Deep Residual Learning for Image Recognition: https://arxiv.org/pdf/1512.03385.pdf\n",
    "    3.The dataset modelnet-40: http://3dshapenets.cs.princeton.edu\n",
    "    4.Multi-view Convolutional Neural Networks for 3D Shape Recognition: https://arxiv.org/abs/1505.00880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
