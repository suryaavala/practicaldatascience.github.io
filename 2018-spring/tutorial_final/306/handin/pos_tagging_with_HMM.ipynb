{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech Tagging with HMM\n",
    "This is a tutorial on part-of-speech tagging task in natural language processing. POS tagging is an important task for language processing since they can give us a lot of distributional information that are useful to further infer essential things like syntax of a sentence. \n",
    "\n",
    "**This tutorial focuses mainly on training a English POS tagger from a tagged data set using Hidden Markov Models and solve using the viterbi algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "- [What is a POS tag?](#What-is-a-POS-tag?)\n",
    "- [Why are POS tags important?](#Why-are-POS-tags-important?)\n",
    "- [Installing Libraries and Loading Data](#Installing-Libraries-and-Loading-Data)\n",
    "- [How do we tag the parts-of-speech?](#How-do-we-tag-the-parts-of-speech?)\n",
    "- [How well do modern POS taggers perform?](#How-well-do-modern-POS-taggers-perform?)\n",
    "- [Other tasks using HMMs](#Other-tasks-using-HMMs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a POS tag?\n",
    "One may think there is a fixed set of classes of POS. However, in reality there are actually multiple standards of part-of-speech classes. Below is a list of POS tags from  [the Penn Treebank](https://web.archive.org/web/19970614160127/http://www.cis.upenn.edu:80/~treebank/).\n",
    "\n",
    "| | **Alphabetical list of part-of-speech tags used in the Penn Treebank Project:**             | \n",
    "|----------|-------------|\n",
    "| CC      | Coordinating conjunction|\n",
    "| CD | Cardinal number |\n",
    "| DT | Determiner |\n",
    "| EX | Existential there |\n",
    "| FW | Foreign word |\n",
    "| IN | Preposition or subordinating conjunction | \n",
    "| JJ | Adjective | \n",
    "| JJR | Adjective, comparative | \n",
    "| JJS | Adjective, superlative |\n",
    "| LS | List item marker |\n",
    "| MD | Modal |\n",
    "| NN | \tNoun, singular or mass |\n",
    "| NNP |Proper noun, singular |\n",
    "| NNPS |Proper noun, plural |\n",
    "| PDT | Predeterminer |\n",
    "| POS\t|Possessive ending|\n",
    "| PRP |Personal pronoun|\n",
    "| PRP\\$ | Possessive pronoun|\n",
    "| RB\t|Adverb|\n",
    "| RBR |Adverb, comparative|\n",
    "| RBS |Adverb, superlative|\n",
    "| RP |Particle|\n",
    "| SYM| Symbol|\n",
    "| TO |to|\n",
    "| UH |Interjection|\n",
    "| VB |Verb, base form|\n",
    "| VBD |Verb, past tense|\n",
    "| VBG |Verb, gerund or present participle|\n",
    "| VBN |Verb, past participle|\n",
    "| VBP |Verb, non-3rd person singular present|\n",
    "| VBZ |Verb, 3rd person singular present|\n",
    "| WDT |Wh-determiner|\n",
    "| WP |Wh-pronoun|\n",
    "| WP\\$ |Possessive wh-pronoun|\n",
    "| WRB |Wh-adverb|\n",
    "\n",
    "As we can see, the classes used in PTB are not the same as the eight categories we usually suppose. For example, for the Noun class we all know, the PTB has it separated into a lot of different classes including NN, NNP, NNPS... When doing computations, Part of speech classes are usually more find-grained in order to perform more tasks accurately. For further information on POS, one can checkout [this](https://web.stanford.edu/~jurafsky/slp3/10.pdf) pdf file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are POS tags important?\n",
    "Because in NLP, if one wants to parse a sentence and make further analysis or generate a sentence, one needs to extract the grammatical syntax of that sentence. To do that, we will need part of speeches. \n",
    "Take sentence-parsing as a example. Many parsing algorithms (for example, the Earley's algorithm), use (probabilistic) [context-free grammar](https://en.wikipedia.org/wiki/Context-free_grammar) to do sentence parsing. Since the grammar rules are largely based on parts-of-speech. Indeed, POS tagging is a very important underlying task.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Libraries and Loading Data\n",
    "Libraries:\n",
    "This tutorial will only use built-in libraries like os, collections. One can also install nltk (natural language tool-kit) and its dependencies following the [instructions](https://www.nltk.org/install.html). Some of steps in this tutorial can be done using functions provided by nltk, and this tutorial provides instructions on calling them.\n",
    "\n",
    "Data:\n",
    "In order to perform POS tagging, we need training data (not surprising). This means that we will need human-annotated corpus. We will use the a small sample of Penn Treebank's data, downloaded from [Kaggle](https://www.kaggle.com/nltkdata/penn-tree-bank/version/5). The tagged source is in the folder \"tagged\", the first thing we need to do is to concat them together into a collection of tagged text. The following two blocks are instructions on cleaning and formatting the raw tagged text.\n",
    "\n",
    "Side note: \"Treebanks\" usually serve as a good source for annotated data. Treebanks are mostly corpus annotated by people. They are rich in information since they contains reliable syntactic information about texts. However, treebanks are not golden standards. People make mistakes too. Also since the making a treebank is costly, a lot of treebanks are actually annotated long times ago. However, they are relative reliable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pierre/NNP Vinken/NNP ]\n",
      ",/, \n",
      "[ 61/CD years/NNS ]\n",
      "old/JJ ,/, will/MD join/VB \n",
      "[ the/DT board/NN ]\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "import os, nltk\n",
    "taggedRaw = \"\"\n",
    "for i in range(199):\n",
    "    fileName =  \"wsj_\"+ str(i+1).zfill(4)+\".pos\"\n",
    "    with open (os.path.join(\"tagged\", fileName), \"r\") as f:\n",
    "        taggedRaw = taggedRaw + f.read()\n",
    "print(taggedRaw.strip()[1:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have successfully loaded the file. As we can see, the tags and the raw text words are separated by a \"/\". Let's parse that to make a list of (word, tag) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre/NNP', 'Vinken/NNP', '\\n,/,', '61/CD', 'years/NNS']\n",
      "Processed: \n",
      "\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "# replace the \"[]\" s\n",
    "taggedRaw = taggedRaw.replace(\"[\", \" \")\n",
    "taggedRaw = taggedRaw.replace(\"]\", \" \")\n",
    "splitted = taggedRaw.split(\" \")\n",
    "# from observation we can see there are \"====\" separators in some files\n",
    "splitted = list(filter(lambda x: ((not x.isspace()) and (not x==\"\") and (\"=\" not in x)), splitted))\n",
    "print(splitted[0:5])\n",
    "# alternatively, this can be done by nltk using nltk.tag.str2tuple function\n",
    "splitted = list(map((lambda x: tuple(x.strip().split(\"/\"))), splitted)) \n",
    "print(\"Processed: \\n\")\n",
    "print(splitted[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, nltk has built-in tagged words one can use. Below is some nltk built-in word tags from the Brown corpus. It also has a built-in string to tuple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
      "('Pierre', 'NNP')\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.brown.tagged_words())\n",
    "print(nltk.tag.str2tuple('Pierre/NNP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we tag the parts-of-speech?\n",
    "There are a lot of POS tagging techniques. We can use Hidden Markov Models to complete this task, or Maximum Entropy Markov Models, and these days there are even part-of-speech taggers using neural networks. In this tutorial, we will focus on bigram HMM, a simple but powerful model.\n",
    "\n",
    "\n",
    "[Hidden Markov Models](https://en.wikipedia.org/wiki/Hidden_Markov_model) is a is a statistical Markov model that lets us predict the hidden states given an observation. It is a model where the system being modeled is assumed to be a Markov process with unobserved (i.e. hidden) states. <BR> Here, we can use HMM because we can see the word classes as the hidden states, and the sentence as the observation.<BR>\n",
    "Click [here](https://web.stanford.edu/~jurafsky/slp3/9.pdf) to red more to understand HMMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Hidden Markov Model\n",
    "A HMM must have these components:\n",
    "![alt text](https://i.imgur.com/OSsES16.png \"States info from the textbook\")\n",
    "(this image is from Speech and Language Processing. Daniel Jurafsky & James H. Martin)\n",
    "\n",
    "\n",
    "The observation is just the texts. What we need to compute is a transition probability matrix (A) and an emission probability sequence (B). \n",
    "\n",
    "This tutorial will break the computation into four steps. \n",
    "- record transition observation count\n",
    "- normalize the transition matrix\n",
    "- record the emission observation\n",
    "- normalize the emission sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to compute the transition probability matrix. \n",
    "To do that we should first record the observations. We can represent the count of observation by making a matrix `transitionMatrix` such that `transitionMatrix[i][j]` represent the number of occurrence of tag i followed by j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "transitionMatrix = defaultdict(dict)\n",
    "# keep track of the vocabulary, easier for smoothing\n",
    "seen = set()\n",
    "# mark the special symbols\n",
    "UNKNOWN_SYMBOL = \"UNK\"\n",
    "INIT_SYMBOL = \"INIT\"\n",
    "FINAL_SYMBOL = \"FINAL\"\n",
    "\n",
    "# initialize\n",
    "prevTag = INIT_SYMBOL\n",
    "\n",
    "\n",
    "for pair in splitted:\n",
    "    try:\n",
    "        (token, tag) = pair\n",
    "        # This smoothing trick is taken from David Bamman.\n",
    "        if token not in seen:\n",
    "            seen.add(token)\n",
    "            token = UNKNOWN_SYMBOL\n",
    "            \n",
    "        # initialize the inner layer of that tag\n",
    "        if prevTag not in transitionMatrix:\n",
    "            transitionMatrix[prevTag] = defaultdict(int)\n",
    "        # increment the transition observation\n",
    "        transitionMatrix[prevTag][tag] += 1\n",
    "        \n",
    "        # update the source tag\n",
    "        prevTag = tag\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "# deal with the stop\n",
    "if prevTag not in transitionMatrix:\n",
    "    transitionMatrix[prevTag] = defaultdict(int)\n",
    "transitionMatrix[prevTag][FINAL_SYMBOL] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a 2d dictionary where `transitionMatrix[i][j]` represent the number of occurrence of tag i followed by tag j. The next thing we need to do is normalize it to get a probMatrix such that $\\forall i, \\sum _{j} \\text{ProbMatrix[i][j]} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "transitionProbabilities = copy.deepcopy(transitionMatrix)\n",
    "for (prevTag) in transitionMatrix:\n",
    "    totalCount = 0\n",
    "    for (tag) in transitionMatrix[prevTag]:\n",
    "        totalCount += transitionMatrix[prevTag][tag]\n",
    "    for (tag) in transitionMatrix[prevTag]:\n",
    "        transitionProbabilities[prevTag][tag] = transitionMatrix[prevTag][tag]/totalCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we need to compute the emission probabilities. We will store it also in a 2d layered dictionary such that `emission[i][j]` represents the probability that the token j is emitted given the tag i.\n",
    "\n",
    "First we will also count the observations. The structure of the code is very similar to the transition observation count code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissionMatrix = defaultdict(dict)\n",
    "seen = set()\n",
    "prevTag = INIT_SYMBOL\n",
    "for pair in splitted:\n",
    "    try:\n",
    "        (token, tag) = pair\n",
    "        if token not in seen:\n",
    "            seen.add(token)\n",
    "            token = UNKNOWN_SYMBOL\n",
    "        if tag not in emissionMatrix:\n",
    "            emissionMatrix[tag] = defaultdict(int)\n",
    "        emissionMatrix[tag][token] += 1\n",
    "        prevTag = tag\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we also need to normalize this such that the emission probabilities are actual probabilities instead of observation count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissionProbabilities = copy.deepcopy(emissionMatrix)\n",
    "for tag in emissionMatrix:\n",
    "    totalCount = 0\n",
    "    for token in emissionMatrix[tag]:\n",
    "        totalCount += emissionMatrix[tag][token]\n",
    "    for token in emissionMatrix[tag]:\n",
    "        emissionProbabilities[tag][token] = emissionMatrix[tag][token]/totalCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have already computed the information needed for a Hidden Markov model.\n",
    "Instead of storing the model in the global environment, one can also save it as a output file and read from there in the future.\n",
    "Below is a quick check so that it's clearer what the matrices look like. They are in the form ( tag_i, tag_j, transition probability tag_i-> tag_j) and ( tag_i, token_j, emission probability tag_i-> word_j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition: i, j, p i->j\n",
      "[('INIT', 'NNP', 1.0), ('NNP', 'NNP', 0.38257173219978746), ('NNP', ',', 0.1532412327311371), ('NNP', 'CD', 0.020403825717321997), ('NNP', 'VBZ', 0.03645058448459086)]\n",
      "\n",
      "\n",
      "Emission: i, j, p i->j\n",
      "[('NNP', 'UNK', 0.25823591923485656), ('NNP', 'Vinken', 0.00010626992561105207), ('NNP', 'Kent', 0.0007438894792773645), ('NNP', 'New', 0.01689691817215728), ('NNP', 'Lorillard', 0.0003188097768331562)]\n"
     ]
    }
   ],
   "source": [
    "trans = []\n",
    "\n",
    "for prevtag in transitionProbabilities:\n",
    "    for tag in transitionProbabilities[prevtag]:\n",
    "        trans.append((prevtag, tag, transitionProbabilities[prevtag][tag]))\n",
    "emi = []\n",
    "for tag in emissionProbabilities:\n",
    "    for token in emissionProbabilities[tag]:\n",
    "        emi.append((tag, token, emissionProbabilities[tag][token]))\n",
    "\n",
    "print(\"Transition: i, j, p i->j\\n\" + str(trans[0:5]))\n",
    "print(\"\\n\")\n",
    "print(\"Emission: i, j, p i->j\\n\" + str(emi[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the Viterbi Algorithm\n",
    "\n",
    "Now we already have a HMM. We will need to solve for the best hidden states sequence given new observations. Here, new observations are new raw texts we need to tag. Luckily we do not need to invent an algorithm on-the-fly. We can use [the viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm). The core idea of the viterbi algorithm is that when there are two paths that both reach a state, we go with the path with higher probability. <br>This animation is an illustration of this process.\n",
    "We first has a starting state, and (Hs and Fs) are the hidden states we want to get given the annotated observations. The probability on every arrow is the product of three things: \n",
    "1. the probability of the previous states sequence \n",
    "2. the transition probability from the state on the arrow's left to the state on the right (i.e. the probability we move to the target state from the previous state)\n",
    "3. the emission probability (i.e. the probability that we get this observation given the state)<br>\n",
    "as we traverse the observations, we can record probabilities effectively. Also, if we keep pointers from states to states, we will be able to retrieve the predicted states from the end to the beginning.\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/7/73/Viterbi_animated_demo.gif \"States info from the textbook\")\n",
    "(this animation if from the wikipedia page of the viterbi algorithm)\n",
    "\n",
    "To implement the algorithm, let's first transform the probabilities into the log  space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Read in the HMM and store probs as log probs\n",
    "logTransition =defaultdict(dict)\n",
    "logEmission = defaultdict(dict)\n",
    "for prevtag in transitionProbabilities:\n",
    "    for tag in transitionProbabilities[prevtag]:\n",
    "        logTransition[prevtag][tag] = math.log(transitionProbabilities[prevtag][tag])\n",
    "for tag in emissionProbabilities:\n",
    "    for token in emissionProbabilities[tag]:\n",
    "        logEmission[tag][token] =math.log(emissionProbabilities[tag][token])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to collect a set of states, and a set of vocabulary in order to process unseen words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = set()\n",
    "vocabulary = set()\n",
    "for prevTag in logTransition:\n",
    "    states.add(prevTag)\n",
    "states.add(FINAL_SYMBOL)\n",
    "for tag in emissionProbabilities:\n",
    "    vocabulary = vocabulary.union(emissionProbabilities[tag].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the algorithm, we will need a trellis of \"width the same as the len(observation) and height the same as the number of states\". Also we need backpointers that allows us to trace back the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation is adapted and modified from an implementation of this algorithm by Noah Smith.\n",
    "def viterbi(sentence):\n",
    "    # get the collection of tags (states in the HMM context)\n",
    "\n",
    "    # instead of using split(), one can also use nltk.word_tokenize() if all dependencies are installed.\n",
    "    observations = [\" \"] + sentence.split() + [FINAL_SYMBOL]\n",
    "    trellis = defaultdict(dict)\n",
    "    backpointers = defaultdict(dict)\n",
    "    for state in transitionProbabilities[INIT_SYMBOL].keys():\n",
    "        trellis[0][state] = 0\n",
    "    for i in range(1, len(observations)):\n",
    "        if observations[i] not in vocabulary:\n",
    "            observations[i] = UNKNOWN_SYMBOL\n",
    "\n",
    "    # traverse to fill the trellis\n",
    "    for i in range(1, len(observations) + 1):\n",
    "        # iterate through possible current states\n",
    "        for current in states:\n",
    "            # iterate through possible prev states\n",
    "            for previous in states:\n",
    "                try:\n",
    "                    if (i == len(observations)):   # special treatment for the final state\n",
    "                        p = trellis[i-1][previous] + logTransition[previous][current] \n",
    "                    else:\n",
    "                        # the product of the three probabilities, in log space\n",
    "                        p = trellis[i-1][previous] + logTransition[previous][current] + logEmission[current][observations[i]]\n",
    "                    # mark the better previous state\n",
    "                    if (i in trellis and current in trellis[i]):\n",
    "                        if (p > trellis[i][current]):\n",
    "                            trellis[i][current] = p # Viterbi probability\n",
    "                            backpointers[i][current] = previous # link the states\n",
    "                    else:\n",
    "                        trellis[i][current] = p\n",
    "                        backpointers[i][current] = previous\n",
    "                except:\n",
    "                    continue\n",
    "    # find the best of the last\n",
    "    lastTag = None\n",
    "    currentBest = 0\n",
    "    for possibleTag in trellis[len(observations)]:\n",
    "        p = trellis[len(observations)][possibleTag]\n",
    "        if not lastTag or  p > currentBest:\n",
    "            currentBest = p\n",
    "            lastTag = possibleTag\n",
    "\n",
    "    if lastTag:\n",
    "        res = []\n",
    "        for i in range(1, len(observations)):\n",
    "            res = [lastTag] + res\n",
    "            lastTag = backpointers[len(observations) - i ][lastTag]\n",
    "        res.pop()\n",
    "        return (\" \".join(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try tag three sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP VBD DT JJ NN .\n",
      "NN IN PRP .\n",
      "VBP PRP VBP PRP .\n"
     ]
    }
   ],
   "source": [
    "print(viterbi(\"I am a good person .\"))\n",
    "print(viterbi(\"look at her !\"))\n",
    "print(viterbi(\"do you like it ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags makes a lot of sense, right? Although there are some mistakes like it marked the look as a noun, most verbs are indeed marked as VB-, nouns are NN-, a is Determiner and so on (One can refer back to the list of classes for checking if they are not familiar). Given that we trained the model on such a small data set. It's pretty good.<br>\n",
    "Side note: nltk also supports a [HMM tagger](https://www.nltk.org/_modules/nltk/tag/hmm.html), you can explore it if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well do modern POS taggers perform?\n",
    "As you can see, our tagger trained from such a small training dataset using bigrams performs not bad. In fact, modern POS taggers can perform very well. The accuracy can exceed 95% so POS tagginig is generally perceived as a solved task. However, correctly determine the parts-of-speech is only a start. There is a long way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tasks using HMMs\n",
    "\n",
    "Hidden Markov Models are powerful and have a lot of different applications since we have a lot of tasks involving inferring states. For example, it can be powerful in decryption, gene prediction, time series predictions and so on. This is a valuable and general model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
