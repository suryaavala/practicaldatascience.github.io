{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import operator \n",
    "from sklearn.datasets import load_wine\n",
    "import nltk\n",
    "import random \n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from scipy.sparse import csr_matrix  \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## K Nearst Neighbors Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K nearest neighbors is a simple but powerful classification algorithm. The idea behind this meodel is very straightforward. Given a positive integer K and a test instance $x_0$, KNN starts by identifing the K points in the training data that are closest to $x_0$, represented by $N_0$. The measure of closeness is choosen depending on the needs of the problem.\n",
    "\n",
    "Each k-closest neighbor takes on a certain value. To determine the value of $x_0$, the algorithm estimates the conditional probability for class j as the fraction of points in $N_0$ whose values are equal to j:\n",
    "\n",
    "\\begin{equation}\n",
    "Pr( Y = j| X = x_0 ) = \\frac{1}{K} \\sum_{i \\in N_0} I(y_i =j)\n",
    "\\end{equation}\n",
    "\n",
    "It then assignes the test observation $x_0$ with the value with the largest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN algorithm is very powerful because it does not assume anything about the data other than a distance measure that can be calculated consistently between any two instances. is It is a instance-based lazy learning algorithm. \n",
    "\n",
    "Instance-based algorithms are those that model the problem using rows of the data to make predictive decisions. The kNN algorithm is an extreme form of instance-based methods since all training observations are retained as part of the model.\n",
    "\n",
    "Lazy learning refers to the fact that KNN does not build a model until the time a classification is required. A disadvantage of this is that it can be computationally expensive to repeat the same or similar searches over larger training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will write a function to calculate Euclidean distance. We will use this measure of distance to implement KNN using sample dataset example later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclideanDistance(x1, x2):\n",
    "    distance = 0\n",
    "    for i in range(len(x1)):\n",
    "        distance += pow((x1[i] - x2[i]), 2)\n",
    "    return math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the distance function, we can find the k elements of the training set that are closest to our test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNeighbors(train, X, k):\n",
    "    distances = []\n",
    "    for i in range(len(train)):\n",
    "        dist = euclideanDistance(train[i], X)\n",
    "        distances.append((train[i], dist))\n",
    "    distances.sort(key = operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for i in range(k):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have located the most similar neighbors for a test instance, we can predict the response based on those neighbors. The way to acomplish this is to allow each neighbor to vote for their class attribute, and take the majority vote as the prediction.\n",
    "getVote is a function that returns the majority vote from a number of neighbors. It assumes the class is the last attribute for each neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getVote(neighbors):\n",
    "    classVotes = {}\n",
    "    for i in range(len(neighbors)):\n",
    "        response = neighbors[i][-1]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function tells us how accurate our prediction is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAccuracy(test, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(test)):\n",
    "        if test[i][-1] == predictions[i]:\n",
    "            correct += 1\n",
    "    return (correct/(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example Using Wine Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wine dataset is a small sample dataset that comes with python's scikit-learn library. We will use this dataset to demonstrate the KNN classification algorithm shown above. \n",
    "This dataset the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. There are 178 instances, each representing the quantities of 13 constituents found in each of the three types of wines. \n",
    "The attributes are\n",
    "- Alcohol \n",
    "- Malic acid \n",
    "- Ash \n",
    "- Alcalinity of ash \n",
    "- Magnesium \n",
    "- Total phenols \n",
    "- Flavanoids \n",
    "- Nonflavanoid phenols \n",
    "- Proanthocyanins \n",
    "- Color intensity \n",
    "- Hue \n",
    "- OD280/OD315 of diluted wines \n",
    "- Proline \n",
    "\n",
    "For more information on this dataset, see [here](https://archive.ics.uci.edu/ml/datasets/wine).\n",
    "\n",
    "As we are using Euclidean distance as our measure of similarity, it is very important to normalize the value of our input features. Too much variation in scale between different features will skew the prediction results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_wine()\n",
    "wine_features = dataset['data']\n",
    "wine_features_normed = wine_features / wine_features.max(axis=0)\n",
    "wine_target = np.array([dataset['target']])\n",
    "wine_dataset = np.concatenate((wine_features_normed, wine_target.T), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into test and train datasets \n",
    "trainingSet = []\n",
    "testSet = []\n",
    "split = 0.67\n",
    "for x in range(len(wine_dataset)-1):\n",
    "        if random.random() < split:\n",
    "            trainingSet.append(wine_dataset[x])\n",
    "        else:\n",
    "            testSet.append(wine_dataset[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "predictions=[]\n",
    "k = 5\n",
    "for x in range(len(testSet)):\n",
    "    neighbors = getNeighbors(trainingSet, testSet[x], k)\n",
    "    result = getVote(neighbors)\n",
    "    predictions.append(result)\n",
    "    #print('predicted =' + repr(result) + ', actual =' + repr(testSet[x][-1]))\n",
    "accuracy = getAccuracy(testSet, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, KNN with k = 5 gives perfect classification results. This dataset is a well posed problem with \"well behaved\" class structures and no missing values, which allows us to get good results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using KNN on Sparse Matrices for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little bit of tweaking of our original code, we can use KNN to classify text. The example here is taken from the tweets dataset used in homework 3, text precessing. Using KNN, we can classify tweets into two groups using their owener's political affiliation. \n",
    "First we follow the same steps in homework 3 to generate a sparse bag-of-words TF-IDF feature matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    text = text.lower()\n",
    "    text = \" \" + text + \" \"\n",
    "    text = text.replace(\"'s\", ' ')\n",
    "    text = text.replace(\"-\", ' ')\n",
    "    text = text.replace(\"'\", '')\n",
    "    \n",
    "    text_map =  string.punctuation.replace(\"'\",\"\")\n",
    "    blank = []\n",
    "    for i in range(len(text_map)):\n",
    "        blank.append(\" \")\n",
    "    blank = ''.join(blank)\n",
    "    \n",
    "    translator = str.maketrans(text_map, blank)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    token_list = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            token_list.append(lemmatizer.lemmatize(token))\n",
    "        except:\n",
    "            token_list = token_list\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    res = df.copy()\n",
    "    tokens = []\n",
    "    for i in range(len(res['text'])):\n",
    "        tokens.append(process(res['text'].iloc[i], lemmatizer))\n",
    "    res['text'] = tokens\n",
    "    return res\n",
    "    pass\n",
    "\n",
    "processed_tweets = process_all(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rare_words(processed_tweets):\n",
    "    words = []\n",
    "    for i in range(len(processed_tweets)):\n",
    "        words += processed_tweets['text'][i]\n",
    "    \n",
    "    countDict = Counter(words)\n",
    "    rare_words = [word for word, occurrences in countDict.items() if occurrences <= 1]\n",
    "    #Counter(words)\n",
    "    return sorted(rare_words)\n",
    "    pass\n",
    "\n",
    "rare_words = get_rare_words(processed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(processed_tweets, rare_words):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords += rare_words\n",
    "    \n",
    "    tfidf = sklearn.feature_extraction.text.TfidfVectorizer(stop_words = stopwords)\n",
    "    \n",
    "    tweets_string = processed_tweets['text'].apply(lambda x: str(\" \".join(x)))\n",
    "    X = tfidf.fit_transform(tweets_string)\n",
    "    \n",
    "    return tfidf, X\n",
    "    pass\n",
    "\n",
    "(tfidf, X) = create_features(processed_tweets, rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_labels(processed_tweets):\n",
    "    screen_name = processed_tweets['screen_name']\n",
    "    rep = ['realDonaldTrump', 'mike_pence','GOP']\n",
    "    label = []\n",
    "    for i in range(len(screen_name)):\n",
    "        if screen_name[i] in rep:\n",
    "            label.append(0)\n",
    "        else:\n",
    "            label.append(1)\n",
    "    return np.array(label)\n",
    "    pass\n",
    "\n",
    "y = create_labels(processed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With X as the sparse bag-of-words TF-IDF matrix and y as the classification result, we can split our dataset to generate a train and a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test \\\n",
    "    = train_test_split(X, y, test_size = 0.33, random_state = 244)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As X_train and X_test are csr sparse matrices, we want to be very careful when calculating their distances. As Euclidean distance can be written as \n",
    "\\begin{equation}\n",
    "|a^2 - b^2| = a^2 -2ab + b^2\n",
    "\\end{equation}\n",
    "\n",
    "we can first multiply each matrix by itself, and then use the dot product to get our result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kNN_Sparse(train, test, k):  \n",
    "    # calculate the square sum of each vector  \n",
    "    train_sq = train.multiply(train).sum(1)  \n",
    "    test_sq = test.multiply(test).sum(1)  \n",
    "      \n",
    "    # calculate the dot product\n",
    "    distance = test.dot(train.transpose()).todense()  \n",
    "      \n",
    "    # calculate the distance  \n",
    "    num_test, num_train = distance.shape  \n",
    "    distance = np.tile(test_sq, (1, num_train)) + np.tile(train_sq.T, (num_test, 1)) - 2 * distance  \n",
    "      \n",
    "    # get the k neighbors\n",
    "    neighbors_index = np.argsort(distance)[:, 0:k]  \n",
    "    neighbors = np.zeros((num_train, k), np.float64)  \n",
    "    for i in range(num_test):  \n",
    "        neighbors[i] = distance[i, neighbors_index[i]]  \n",
    "      \n",
    "    return neighbors, neighbors_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with above, we allow each neighbor to vote for their class, and assign the majority vote value to the test instance. We can also evaluate the accuracy of this model by comparing our predictions with y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "k = 5\n",
    "for i in range(X_test.shape[0]):\n",
    "    neighbors,neighbors_index = kNN_Sparse(X_train, X_test[i:i+1], k)\n",
    "    for j in neighbors_index.tolist():\n",
    "        response = y_train[j]\n",
    "        data = Counter(response)\n",
    "        result = max(response, key = data.get)\n",
    "        predictions.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8446312839376423"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getAccuracy_sparse(testSet, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(testSet)):\n",
    "        if testSet[i] == predictions[i]:\n",
    "            correct += 1\n",
    "    return (correct/(len(testSet)))\n",
    "\n",
    "getAccuracy_sparse(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy with k = 5 is 0.84. This is not a bad result for such a simple and straightforward lazy learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theory and characteristic part of this tutorial used information from the [textbook](http://www-bcf.usc.edu/~gareth/ISL/) _An Introduction to Statistical Learning with Applications in R_ (ISLR) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, 7th edition. \n",
    "\n",
    "The idea for using KNN on text classfication comes from the papaer _KNN with TF-IDF based Framework for Text Categorization_ by Bruno Trstenjak,Sasa Mikac, and Dzenana Donko, linked [here](https://www.sciencedirect.com/science/article/pii/S1877705814003750). \n",
    "\n",
    "The wine dataset is from [UCI's machine learning repository](https://archive.ics.uci.edu/ml/datasets/wine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
