{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic detection with LDA model on ubuntu dialog corpus\n",
    "Topic detection is such a task where the model will determine several topics that are relevant to the given document. \n",
    "Before we talk about LDA, I would like to give some basic idea about topic model.\n",
    "\n",
    "## Topic Model\n",
    "In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (Latent Dirichlet Allocation)\n",
    "LDA (Latent Dirichlet Allocation) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.\n",
    "\n",
    "### LDA is a generative model\n",
    "LDA is a generative model. It assumes a article is generated from a mix of hidden topics, and each model contains a potential words that can be drawn from.\n",
    "\n",
    "Of course, LDA can also be used for discriminative task. But let's first look at the generative process.\n",
    "\n",
    "So literally we have two distribution:\n",
    "1. the topic model distribution p(topic|θ)\n",
    "2. a word distrubition given a topic model p(word|topic).\n",
    "\n",
    "Notice that θ is a hyper-pamameter\n",
    "\n",
    "Then to generate a article, the process is here:\n",
    "```\n",
    "Chooseparameter θ ～ p(θ);\n",
    "For each of the N words:\n",
    "Choose a topic ～ p(topic|θ);\n",
    "Choose a word ～ p(word|topic);\n",
    "```\n",
    "\n",
    "And to classify the topic given an article, we choose the topic that has the highest probability to generate this article. It's similar to maximum posterior probability.\n",
    "And notice that **the words order doesn't matter.** So actually the LDA model fail to capture some semantic features regarding words order. But LDA model is still powerful, especially in topic detection task, where words order is not a necessary feature to take into account.\n",
    "\n",
    "### Discriminative Task\n",
    "When we talk about topic detection, it's a discriminative task. And the way LDA modal to do this is just to find maximum likelihood of potential topics, using Gibbs sampling. It's similar to maximum posterior probability.\n",
    "\n",
    "A more detailed explanation can be found [here](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "\n",
    "### Unsupervised model\n",
    "\n",
    "LDA is an unsupervised model, in that it doesn't require any label. In simple word, LDA do clustering among bunch of documents.\n",
    "\n",
    "As a result, the topics in model doesn't have a specific \"name\", instead, each topic is characterized as a word vector representing their probabilities.\n",
    "\n",
    "And in our task, we want to determine the relevant topics given the document. A naive way is to choose some keyword from topic word vectors with high probabilities.\n",
    "\n",
    "### another problem to notice\n",
    "\n",
    "And another problem is that, it's likely that given a dialog, we might suggest a keyword not in the article! This might look wried.\n",
    "\n",
    "But on the other hand, this is a good thing, as this model consider the hidden semantic structure. For example, a simple dialog Hey, I failed to use 'sudo apt install python3'. A keyword python3 can be easily detected, but a human being with some linux knowledge should be able to tell it's a ubuntu-like distribution. And the LDA model might be able to do this.\n",
    "\n",
    "And here is the possible situation:\n",
    "\n",
    "User input: Hey, I failed to use `sudo apt install python3`\n",
    "\n",
    "Response: your problem might be relative to this topics: `python3, ubuntu, apt, install`\n",
    "\n",
    "And the word `ubuntu` looks reasonable even if it doesn't appear in the dialog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus: Ubuntu dialog (dataset)\n",
    "\n",
    "The corpus we use here is [Ubuntu dialog](http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/), which is a multi-turn dialog, introduced by [this paper](https://arxiv.org/abs/1506.08909)\n",
    "\n",
    "Notice that the corpus already has 2.0 version, but we still use 1.0 here.\n",
    "\n",
    "We should download the [raw data](http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/ubuntu_dialogs.tgz), as the first step in data science is to deal with dirty data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The lib you need in this tutorial\n",
    "```\n",
    "flashtext\n",
    "gensim\n",
    "stop_words\n",
    "```\n",
    "All these can be easily installed using `pip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess\n",
    "\n",
    "combine all dialogs into one text file, one dialog one line, retaining conversation only, removing punctuation, removing stopword\n",
    "\n",
    "\n",
    "## notes in preprocessing\n",
    "### stop word\n",
    "get stop words from a python lib called stop_words\n",
    "\n",
    "### flashtext\n",
    "flashtext is a powerful python lib used for replacing keyword. We might want to remove stop word, and any punctuation. \n",
    "\n",
    "However, filtering out stop word might be time consuming using Regex,the runtime is linear to the size of the keyword set you want to filer out. A good tool for this is [flashtext](https://github.com/vi3k6i5/flashtext), a word matching model based on prefix-tree. Its performance remain O(n), where n is the length of sentence, while Regex might go up to O(mn) where m is the stopword size, which means the flashtext is performance-insentitive regarding the size of keyword set. It reduce my processing time significantly. \n",
    "\n",
    "### stemming\n",
    "Stemming means combining similar word into one. For example, **install** and **installs**\n",
    "\n",
    "This is extremely important, because when building the model, it's intuitively to transform all words to their Semitic root, which is better to build the vector space.\n",
    "\n",
    "Here I use the Stemmer from [NLTK](http://www.nltk.org/)\n",
    "I found that the stemmer might trim the word at some wrong position. But it's fine to construct word-vector space.\n",
    "\n",
    "### tokenize\n",
    "basically you can write a simple Regex to do this, or even string.split() will do. But I to gain better performance, I use tokenizer from NLTK too. The regex I use for tkenizer is **r'\\w\\S+\\w'**, which retain a word start with character and end with character, this can filter out punctuation, but keep sth. like **don't**  or **pre-installed**, where punctuation appear inside a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import codecs\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stop_words import get_stop_words\n",
    "from flashtext import KeywordProcessor\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import functools\n",
    "import string\n",
    "\n",
    "files = glob('./dialogs/**/*.tsv')\n",
    "\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = set(get_stop_words('en'))\n",
    "# create punctuationi to filter\n",
    "punc = set(string.punctuation)\n",
    "    \n",
    "    \n",
    "\n",
    "def processing(i, file_list, stopword, punc):\n",
    "    tk = RegexpTokenizer(r'\\w\\S+\\w')\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    with codecs.open('whole_dialogs_stem_%d' % i, 'w', 'utf-8') as out:\n",
    "        for fi in file_list:\n",
    "            with codecs.open(fi, 'r', 'utf-8') as f:\n",
    "                words = functools.reduce(lambda x, y: x + y,\n",
    "                                         map(tk.tokenize, [line.strip().split('\\t')[-1].lower() for\n",
    "                                                           line in f]))\n",
    "                words = [p_stemmer.stem(\n",
    "                    w) for w in words if w not in stopword and w not in punc]\n",
    "                out.write(' '.join(words) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You might want to use multi process to do it, otherwise it will take a long time.\n",
    "\n",
    "### Also, I would like to recommend a funny tools called `tqdm`, which can show the process bar. It's helpful when you're running long task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def div_list(l, n):\n",
    "    length = len(l)\n",
    "    t = length // n\n",
    "    quaters = [t * i for i in range(0, n)]\n",
    "    ran = range(0, n - 1)\n",
    "    result = [l[quaters[i]:quaters[i + 1]] for i in ran]\n",
    "    result.append(l[quaters[n - 1]:len(l)])\n",
    "    return result\n",
    "\n",
    "process_num = 8\n",
    "p = Pool()\n",
    "div_files = div_list(files, process_num)\n",
    "for i in range(process_num):\n",
    "    p.apply_async(processing, args=(\n",
    "        i, div_files[i], stopword_processor))\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "output_list = glob('./whole_dialogs_stem_*')\n",
    "for output in glob('./whole_dialogs_stem_*'):\n",
    "    os.system('cat %s >> whole_dialogs_stem' % output)\n",
    "    os.system('rm %s' % output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 20 minutes in my laptop to run. For convenience, I output all processed corpus to a single file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce to gensim\n",
    "(Gensim)[https://radimrehurek.com/gensim/index.html] is powerful topic modelling lib in python. It provide rich models which can be easily used off-the-shelf. What's more, it provides some useful tool to construct corpus with word vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we load the data and construct the corpus as a dictionary. The data fed into dictionary should be a list of document, where each document is a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "with open('whole_dialogs_stem', 'r', encoding='utf-8') as f:\n",
    "    raw_corpus = [line.split() for line in f]\n",
    "dictionary = corpora.Dictionary(raw_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then you might want to filter out some rare words, as well as some words that apprear too frequently. These words might significantly slow down the training process, and also make some negative effect to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=3, no_above=0.2, keep_n=100000)\n",
    "dictionary.save('ubuntu.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we construct the corpus as word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in raw_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is how a document looks like. Notice that the word order doesn't matter anymore, only frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And let's train a LDA model\n",
    "we can use `LdaMulticore` to take advantage of multi processor in your machine. The model training need a corpus, a dictionary lookup, and more importantly, you have to specifiy `num_topics`, it's a hyper parameter that to be tune. A good idea is to train the model iteratively, until the similarity among topics is minimized. But this might take a long time. \n",
    "\n",
    "Also you can search the parameter based on `Perplexity vs num_topics`\n",
    "\n",
    "For tutorial purpose, I just use `num_topics = 50` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LdaMulticore(corpus, id2word=dictionary, workers=3,\n",
    "                            num_topics=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to save the model. Also remember the dictionary we saved before, it will be needed in the inference as a lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ubuntu.lda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the words distribution in topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44,\n",
       "  '0.096*\"file\" + 0.043*\"directori\" + 0.041*\"folder\" + 0.028*\"copi\" + 0.025*\"permiss\" + 0.025*\"home\" + 0.020*\"chang\" + 0.018*\"sudo\" + 0.014*\"save\" + 0.014*\"creat\"'),\n",
       " (36,\n",
       "  '0.074*\"gnome\" + 0.041*\"desktop\" + 0.040*\"kde\" + 0.020*\"want\" + 0.018*\"distro\" + 0.017*\"xubuntu\" + 0.014*\"differ\" + 0.013*\"uniti\" + 0.013*\"app\" + 0.012*\"xfce\"'),\n",
       " (40,\n",
       "  '0.017*\"well\" + 0.015*\"thing\" + 0.014*\"think\" + 0.013*\"realli\" + 0.013*\"time\" + 0.012*\"yeah\" + 0.012*\"one\" + 0.012*\"good\" + 0.011*\"now\" + 0.010*\"lol\"'),\n",
       " (33,\n",
       "  '0.113*\"question\" + 0.090*\"window\" + 0.086*\"linux\" + 0.077*\"ask\" + 0.036*\"answer\" + 0.016*\"want\" + 0.012*\"hello\" + 0.011*\"one\" + 0.011*\"quick\" + 0.010*\"guy\"'),\n",
       " (32,\n",
       "  '0.082*\"script\" + 0.032*\"python\" + 0.028*\"firewal\" + 0.025*\"bash\" + 0.023*\"rule\" + 0.022*\"iptabl\" + 0.019*\"block\" + 0.019*\"write\" + 0.017*\"echo\" + 0.016*\"firestart\"'),\n",
       " (34,\n",
       "  '0.049*\"bug\" + 0.043*\"free\" + 0.024*\"report\" + 0.017*\"ipod\" + 0.014*\"linux\" + 0.013*\"good\" + 0.012*\"thank\" + 0.012*\"buy\" + 0.011*\"softwar\" + 0.011*\"sync\"'),\n",
       " (23,\n",
       "  '0.044*\"link\" + 0.039*\"googl\" + 0.029*\"look\" + 0.028*\"thank\" + 0.024*\"read\" + 0.020*\"debian\" + 0.019*\"find\" + 0.017*\"search\" + 0.016*\"forum\" + 0.014*\"guid\"'),\n",
       " (14,\n",
       "  '0.035*\"compiz\" + 0.030*\"window\" + 0.027*\"mous\" + 0.026*\"theme\" + 0.026*\"set\" + 0.024*\"keyboard\" + 0.020*\"effect\" + 0.020*\"press\" + 0.019*\"chang\" + 0.017*\"button\"'),\n",
       " (2,\n",
       "  '0.073*\"control\" + 0.047*\"turn\" + 0.044*\"volum\" + 0.033*\"vnc\" + 0.021*\"remot\" + 0.020*\"level\" + 0.017*\"set\" + 0.017*\"speaker\" + 0.016*\"servic\" + 0.015*\"desktop\"'),\n",
       " (5,\n",
       "  '0.143*\"file\" + 0.041*\"name\" + 0.037*\"find\" + 0.019*\"packag\" + 0.018*\"deb\" + 0.018*\"search\" + 0.017*\"locat\" + 0.015*\"thank\" + 0.014*\"program\" + 0.013*\"want\"'),\n",
       " (0,\n",
       "  '0.093*\"key\" + 0.033*\"nick\" + 0.031*\"regist\" + 0.028*\"msg\" + 0.016*\"sign\" + 0.016*\"privat\" + 0.015*\"gpg\" + 0.015*\"identifi\" + 0.013*\"power\" + 0.012*\"public\"'),\n",
       " (3,\n",
       "  '0.049*\"card\" + 0.037*\"wireless\" + 0.033*\"driver\" + 0.026*\"devic\" + 0.022*\"modul\" + 0.017*\"laptop\" + 0.016*\"ndiswrapp\" + 0.016*\"see\" + 0.015*\"wifi\" + 0.013*\"usb\"'),\n",
       " (11,\n",
       "  '0.090*\"share\" + 0.053*\"window\" + 0.051*\"samba\" + 0.040*\"printer\" + 0.036*\"mac\" + 0.033*\"print\" + 0.023*\"xmm\" + 0.018*\"vim\" + 0.017*\"network\" + 0.017*\"linux\"'),\n",
       " (18,\n",
       "  '0.100*\"bit\" + 0.038*\"64bit\" + 0.037*\"9.10\" + 0.035*\"9.04\" + 0.033*\"version\" + 0.025*\"amd64\" + 0.024*\"run\" + 0.024*\"32bit\" + 0.021*\"10.04\" + 0.016*\"karmic\"'),\n",
       " (26,\n",
       "  '0.055*\"apach\" + 0.041*\"php\" + 0.039*\"mysql\" + 0.022*\"apache2\" + 0.019*\"record\" + 0.019*\"xgl\" + 0.018*\"databas\" + 0.015*\"localhost\" + 0.014*\"lamp\" + 0.011*\"bind\"'),\n",
       " (9,\n",
       "  '0.072*\"machin\" + 0.054*\"virtual\" + 0.033*\"virtualbox\" + 0.029*\"encrypt\" + 0.026*\"run\" + 0.024*\"box\" + 0.022*\"offic\" + 0.015*\"guest\" + 0.014*\"vbox\" + 0.014*\"openoffic\"'),\n",
       " (27,\n",
       "  '0.071*\"repo\" + 0.065*\"repositori\" + 0.048*\"enabl\" + 0.042*\"add\" + 0.033*\"edgi\" + 0.032*\"packag\" + 0.032*\"univers\" + 0.026*\"main\" + 0.021*\"sourc\" + 0.018*\"sources.list\"'),\n",
       " (47,\n",
       "  '0.067*\"vmware\" + 0.036*\"delet\" + 0.026*\"hack\" + 0.025*\"trash\" + 0.024*\"clear\" + 0.022*\"fuck\" + 0.020*\"viru\" + 0.018*\"empti\" + 0.017*\"shit\" + 0.015*\"cach\"'),\n",
       " (19,\n",
       "  '0.058*\"download\" + 0.058*\"compil\" + 0.055*\"sourc\" + 0.040*\"make\" + 0.026*\"build\" + 0.025*\"man\" + 0.024*\"page\" + 0.023*\"packag\" + 0.022*\"configur\" + 0.020*\"code\"'),\n",
       " (31,\n",
       "  '0.196*\"error\" + 0.059*\"messag\" + 0.045*\"fail\" + 0.020*\"say\" + 0.018*\"give\" + 0.015*\"torrent\" + 0.014*\"got\" + 0.014*\"problem\" + 0.013*\"warn\" + 0.012*\"found\"')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic detection with LDA model\n",
    "Finally, let's see how the model works with the inference with new document.\n",
    "\n",
    "As it's unsupervised learning, there is no label for evaluating. Generally people use perplexity and some other metrics to measure how well the model perform.\n",
    "\n",
    "But here I would just try to use the model to inference some key word in the given document.\n",
    "\n",
    "#### To make sure the model works in predicting new data, we should make the same preprocess to the  document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tk = RegexpTokenizer(r'\\w\\S+\\w')\n",
    "p_stemmer = PorterStemmer()\n",
    "dictionary = corpora.Dictionary.load('ubuntu.dict')\n",
    "model = models.LdaMulticore.load('ubuntu.lda')\n",
    "\n",
    "def preprocess(fi, stopword, punc, tk, stemmer, dictionary):\n",
    "    with codecs.open(fi, 'r', 'utf-8') as f:\n",
    "        words = functools.reduce(lambda x, y: x + y,\n",
    "                                 map(tk.tokenize, [line.strip().split('\\t')[-1].lower() for\n",
    "                                                   line in f]))\n",
    "        words = [stemmer.stem(\n",
    "            w) for w in words if w not in stopword and w not in punc]\n",
    "        return dictionary.doc2bow(words)\n",
    "    \n",
    "def inference(doc, model, k=3):\n",
    "    # get the top k topic\n",
    "    topics = sorted(model[doc], key=lambda x: x[1], reverse=True)[:k]\n",
    "    return topics\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dialog:\n",
      "2008-09-03T06:57:00.000Z\tbrettley\t\tif i installed another linux opperating system next to ubuntu, would it take the GRUB i have now and modify it?\n",
      "\n",
      "2008-09-03T06:59:00.000Z\twols_\tbrettley\tit would install anotehr bootloader\n",
      "\n",
      "2008-09-03T07:00:00.000Z\tbrettley\twols_\tlike what?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_file = './test/100002.tsv'\n",
    "print('original dialog:')\n",
    "with open(test_file,'r') as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "doc = preprocess(test_file, en_stop, punc, tk, p_stemmer, dictionary)\n",
    "topics = inference(doc, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top 3 topics, with their probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, 0.7095178708078098),\n",
       " (11, 0.12502156021403896),\n",
       " (35, 0.08712723564481913)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see what keyword we can find in the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "topic 12, prob: 0.709518\n",
      "{'boot': 0.12062512636333775, 'grub': 0.052740154034447956, 'window': 0.03675319373613759, 'live': 0.03537946195971994, 'livecd': 0.01556302143835399, 'will': 0.01335677179709427, 'bio': 0.011869683427816443, 'now': 0.01146358867856986, 'option': 0.011268242272930003, 'want': 0.010371099908555202}\n",
      "--------------------------------------------------\n",
      "topic 11, prob: 0.125022\n",
      "{'share': 0.09041290024881912, 'window': 0.05304941127631381, 'samba': 0.0511428194971934, 'printer': 0.039550153515780714, 'mac': 0.03575682170535402, 'print': 0.03329300179646669, 'xmm': 0.022617994471666004, 'vim': 0.017858619265137876, 'network': 0.017451469127535375, 'linux': 0.017051113616151126}\n",
      "--------------------------------------------------\n",
      "topic 35, prob: 0.087127\n",
      "{'send': 0.05996920976719629, 'raid': 0.043209108945681196, 'mail': 0.04018007224764992, 'email': 0.02659783011425958, 'map': 0.013692063313678489, 'softwar': 0.012529430848190325, 'md5sum': 0.011701613580852376, 'serial': 0.010712457565584237, 'postfix': 0.009947900931653594, 'array': 0.009709612132322982}\n"
     ]
    }
   ],
   "source": [
    "for t in topics:\n",
    "    LDA_keywords = sorted(model.get_topic_terms(t[0]), key=lambda x:x[1], reverse=True)\n",
    "    LDA_keywords = dict((dictionary[i[0]],i[1]) for i in LDA_keywords)\n",
    "    print('-'*50)\n",
    "    print('topic %d, prob: %f'%(t[0],t[1]))\n",
    "    print(LDA_keywords)\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment result\n",
    "I've tried out some testcase, the result seems reasonable but not very good as I expect.\n",
    "You're free to play some testcase with the model. \n",
    "\n",
    "\n",
    "I've tried training the model with different parameters, but couldn't find a very good one. Actually people are always having some controversy over LDA, mainly due to two weakness:\n",
    "* hard to tune the paramater, somehow empirical (or you can use complicated method for parameters searching, but very time-consuming and not realistic in real production)\n",
    "* the topics themselve are generated by clustering, it's sometimes hard for human being to understand.\n",
    "\n",
    "The reason why I want to try LDA is when I first look at their [official example](https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation), which give reasonable results. But it seems that the model doesn't play very well with the ubuntu corpus.\n",
    "\n",
    "I find a [good paper](https://arxiv.org/abs/1608.08176) on discussing what's the problem of LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### some discussion here:\n",
    "\n",
    "1. As mentioned above, the experiment doesn't produce very good result. But when choosing complicated machine learning model, we always undertake the risk that the experiment might not give a perfect outcome. And even we can tune the model with specific dataset, and spend long time on tunning the params, it doesn't make much sense in real production.\n",
    "\n",
    "2. As a unsupervised learning model (clustering), it's actually not intuitive to use it for classification task. Because we don't even have a label for each topic the model generate. And how the topic looks like is totallly determined by the algorithm itself. Even it make much sense in the mathematical perspective, but it might not be a good model in real world.\n",
    "\n",
    "3. Another reason why LDA doesn't perform well in ubuntu dialog corpus it that (i guess) the dialog is quite short, with only few word that can represent the feature of the topic. I have a project using LDA for topic classification of newspaper articles, which make much more sense than ubuntu corpus. I suspect whether LDA is a good model for such a corpus. But the outcome looks somehow reasonable so it's a meaningful experiment.\n",
    "\n",
    "4. After all, I hope I can give you some ideas on topic modeling, as well as some preprocess approach and the way we use it for topic detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
