{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This tutorial will introduce you to PyTorch framework for analyzing free text documents. You will learn how to model the language used in the Federalist Papers in RNN, which is a more efficient way than the simple N-gram.\n",
    "\n",
    "The objective and dataset are the same with CMU 15688 Spring 2017's Homework3 Natural Language Processing part.\n",
    "You can check the detail by downloading handout of https://autolab.andrew.cmu.edu/courses/15388-s18/assessments/homework3.\n",
    "\n",
    "<b>Please go to https://drive.google.com/open?id=1EQWp3hAQ9R1y4SfkExvt2gqw1S3jl3ER with login gmail with andrew.cmu.edu and download \"data.npy\" and \"new.pkl\" then put them under the same directory of tutorial.ipynb</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "In this tutorial, we will show how to train a model and generate specific style text in Python, specifically using [Pytorch](http://pytorch.org/)\n",
    "\n",
    "You'll use a copy of the Federalist Papers downloaded from Project Guttenberg, available here: http://www.gutenberg.org/ebooks/18\n",
    "\n",
    "The dataset is proprocessed as npy file thus you can load it easily.\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Installing the libraries](#Installing-the-libraries)\n",
    "- [Loading data](#Loading-data)\n",
    "- [N-gram recap](#N-gram-recap)\n",
    "- [From machine learning to deep learning](#From-machine-learning-to-deep-learning)\n",
    "- [From RNN to LSTM](#From-RNN-to-LSTM)\n",
    "- [RNNs using PyTorch](#RNNs-using-PyTorch)\n",
    "- [Summary and references](#Summary-and-references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, you'll need to install the libraries that we will use. We assume you have conda, so you will use conda to install. You can install PyTorch by following the instructiton of http://pytorch.org/.\n",
    "If you are using Linux without cuda* or MAC OS, simply use:\n",
    "\n",
    "    $ conda install pytorch-cpu torchvision -c pytorch\n",
    "\n",
    "If you are using other OS or you have a nvidia gpu card, just follow the instructiton of http://pytorch.org/ to install. PyTorch does not officially support Windows, so we recommend you to use Linux or MAC OS.\n",
    "\n",
    "*: cuda is parallel computing platform and application programming interface model created by Nvidia, which can accelerate your float calculation. However, it needs you have a nvidia gpu card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you will use numpy to load proprocessed Federalist Papers dataset. \"papers\" is a list containing 86 acticles as string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "papers = np.load(\"data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing field, n-gram can be used for text generation, as we learned in the class. Due to limitation of space, this tutorial will not go into the detail of it. LanguageModel model in Homework3 Natural Language Processing part will be used here. Please review n-gram and homework before you start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nlp import LanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After load LanguageModel in nlp.py, let us check the function in it. They are a little bit different from the homework3. \n",
    "\n",
    "first(): generate a n-1 long text. The text should appear in somewhere of train text.\n",
    "\n",
    "sample(length, first): take the text length and first text, then return generated random samples of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has been performed by some individual citizen of preeminent wisdom and approved integrity . minos , we learn , was the primitive founder of the government of crete , as zaleucus was of that of the locrians . theseus first , and after him draco and solon , instituted the government of athens . lycurgus was the lawgiver of sparta . the foundation of the original government of rome was laid by romulus , and the work completed by two of his elective successors , numa and tullius hostilius . on the abolition of royalty the consular administration was substituted by brutus , who stepped forward with a project for such a reform , which , he alleged , had been prepared by tullius hostilius , and to which his address obtained the assent and ratification of the senate and people . this remark is applicable to confederate governments also . amphictyon , we are told , was the author of that which bore his name . the achaean league received its first birth from achaeus , and its second from aratus . what degree of agency these reputed lawgivers might have in their respective establishments , or how far\n"
     ]
    }
   ],
   "source": [
    "l_hamilton = LanguageModel(papers,10)\n",
    "first = l_hamilton.first()\n",
    "n_gram = l_hamilton.sample(200,first)\n",
    "print(n_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, N-gram has some limitations. First, the memory usage and time complexity will increase rapidly with increasing n. Second, N-gram can only remember exact n word, thus the important context before or after it cannot be taken into account. Third, if you want to change the number of N, you must train the model again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From machine learning to deep learning\n",
    "\n",
    "\n",
    "\n",
    "How to create a text with the same style of writing? Whether n-gram or RNN, the essence is to predict the next word by observing the context, then add newly generated words to the context, and loops until generates the expected length. In this process, a function of \"predicting the next word\" is the key to the problem. You may recall the definition of machine learning in 15688 class. We are told \"learning\" is actually fitting hypothesis function with sample and label, and then predicting label of new sample. Machine learning is very good at such \"learn and predict\" task. And deep learning is a subclass of machine learning, which work particularly excellent in image, text, sound and so on. We use different deep learning models, i.e. neural networks, to solve different problems. Here we use RNN, a special kind of neural network. To understand RNN, you need some preliminary knowledge about deep learning. Let's first introduce the neural network.\n",
    "\n",
    "\n",
    "\n",
    "<b>Neural network</b>: you may have heard of it in many places, but not sure its exact definition. First of all, what is a neuron? [Perceptron](https://en.wikipedia.org/wiki/Perceptron) is a typical kind of neuron. Let's take it as example.\n",
    "\n",
    "![Perceptron](img/Perceptron-diagram.jpg)\n",
    "<center>Figure 1. Perceptron source: http://harveycohen.net/image/perceptron.html</center>\n",
    "\n",
    "A neuron take the outputs of predecessor neurons as inputs, then do some calculation (e.g. for perceptron, calculate linear sum). Then activation function will decide whether or not activate (you can image the light is on) itself by comparing the calculation result with threshold.\n",
    "\n",
    "Single layer perceptron is one of the simplest neural network, but it can also learn and solve quite complex problems. However, it does not terminate if the learning set is not [linearly separable](https://en.wikipedia.org/wiki/Linear_separability), which is an unsolvable problems.\n",
    "\n",
    "A neural network is consisted of many connected neurons as below.\n",
    "\n",
    "<img src=\"img/neural-network.png\" width=\"40%\">\n",
    "<center>Figure 2. neural network source: http://neuralnetworksanddeeplearning.com/chap1.html</center>\n",
    "\n",
    "The details of neural network are quite trivial, so we recommond you check it on https://en.wikipedia.org/wiki/Artificial_neural_network\n",
    "\n",
    "Here we only talk about the components help you understand the contents of this tutorial.\n",
    "\n",
    "In theory, multiple layers neural networks can express arbitrary functions. Once the hyperparameters(e.g. the number of layers, the number of neurons) are decided by human, we need to find a set of parameter(e.g. weights of every layer) which can optimize the neural networks to fit the train set. This problem is actcully [optimization problem](https://en.wikipedia.org/wiki/Optimization_problem). [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) or its variants are usually used to solve this problem in the field of deep learning.\n",
    "\n",
    "Since you can simply think neural networks as a kind of function, let's first take naive function such as $y = ax^2 + bx + c (a>0)$ as example. We want to find the minimum of this function.\n",
    "\n",
    "<img src=\"img/gd.png\" width=\"30%\">\n",
    "<center>Figure 3. Gradient descent source: https://www.quora.com/What-is-Stochastic-Gradient-Descent</center>\n",
    "\n",
    "We calculate the current gradient every iteration, then step forward on the opposite of that gradient. If everything goes well (e.g step length is appropriate and function is convex), we will finally find the minimum after finite iteration.\n",
    "\n",
    "But the gradient of neural networks cannot be calculated directly. So we use [chain rule](https://en.wikipedia.org/wiki/Chain_rule) to calculate every partial derivative with respect of parameters. This method is called [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation). \n",
    "<img src=\"img/giphy.gif\" width=\"40%\">\n",
    "<center>Figure 4. feed forward back propagation source: https://giphy.com/explore/neural-networks</center>\n",
    "This is critical for understanding neural network, so we recommond you read the algorithm to get intuitive feeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From RNN to LSTM\n",
    "Traditional neural networks are not good at memorize context. The relationship of order and time between sample and sample can not be learned. So it's considered be not suitable for solving natural language processing problem.\n",
    "\n",
    "RNN[(Recurrent neural network)](https://en.wikipedia.org/wiki/Recurrent_neural_network) address this issue. Loops in RNN allow information to persist.\n",
    "\n",
    "<img src=\"img/RNN-unrolled.png\" width=\"50%\">\n",
    "<center>Figure 5. RNN source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</center>\n",
    "\n",
    "A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor[[1]](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). Thus the \"memory\" of processed sample can be used for processing current sample.\n",
    "\n",
    "Naive recurrent neural network is difficult to capture long term correlation, because it can not handle exploding gradient problem and vanishing gradient problem. Both of them results for inappropriate learning.\n",
    "\n",
    "\n",
    "Using LSTM[(Long Short-Term Memory)](https://en.wikipedia.org/wiki/Long_short-term_memory) can solve this problem well.\n",
    "\n",
    "<img src=\"img/LSTM3-chain.png\" width=\"50%\">\n",
    "<center>Figure 6. RNN source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</center>\n",
    "\n",
    "\n",
    "Every LSTM cell persist and pass not only hidden output but also cell state, which is kind of like a conveyor belt. This change makes long-term information easily flow along cell state unchanged. The LSTM also have the ability to remove or add information to the cell state, carefully regulated by structures called gates[[1]](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). This improvement moderates exploding gradient problem and vanishing gradient problem of RNN. Please check http://colah.github.io/posts/2015-08-Understanding-LSTMs/ for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load numpy and torch package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load processed npy file as dataset and save it save \"papers\".\n",
    "We split each paper into word list as \"paper_list\". \"words\" is the set of unique words which appear in the dataset. \"word_dict\" gives these word numeric label, makes it easier for RNN training later, since neural networks cannot process string directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211879\n"
     ]
    }
   ],
   "source": [
    "papers = np.load(\"data.npy\")\n",
    "\n",
    "def word_map(papers):\n",
    "    paper_list = [paper.split() for paper in papers]\n",
    "    corpus = sum(paper_list, [])\n",
    "    print(len(corpus))\n",
    "    words = list(set(corpus))\n",
    "    words.sort()\n",
    "    word_dict = {word: i for i, word in enumerate(words)}\n",
    "    return paper_list, words, word_dict\n",
    "\n",
    "paper_list, words, word_dict = word_map(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the length of \"corpus\", \"paper_list\", \"words\", \"word_dict\". As you can seen, there are 211879 words, 86 acticles, 8686 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 8686 8686\n"
     ]
    }
   ],
   "source": [
    "print(len(paper_list), len(words), len(word_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"word_to_number\" changes word list into number list. \"number_to_word\" changes number list into word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_number(word_list, word_dict):\n",
    "    return [word_dict[word] for word in word_list]\n",
    "\n",
    "def number_to_word(number_list, words):\n",
    "    return [words[number] for number in number_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we changes\n",
    "\n",
    "\"after an unequivocal experience of the inefficacy of the subsisting federal government , you are called upon to deliberate on\"\n",
    "\n",
    "into\n",
    "\n",
    "[312, 456, 8148, 3184, 5442, 7817, 4232, 5442, 7817, 7552, 3350, 3699, 5, 8678, 607, 1107, 8274, 7900, 2149, 5476]\n",
    "\n",
    "and recover it for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[312, 456, 8148, 3184, 5442, 7817, 4232, 5442, 7817, 7552, 3350, 3699, 5, 8678, 607, 1107, 8274, 7900, 2149, 5476]\n",
      "after an unequivocal experience of the inefficacy of the subsisting federal government , you are called upon to deliberate on\n"
     ]
    }
   ],
   "source": [
    "#take the first 20 words of the first article as example\n",
    "test = word_to_number(paper_list[0][:20], word_dict)\n",
    "print(test)\n",
    "test = number_to_word(test, words)\n",
    "print(\" \".join(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we do some further preprocessing for generating data set and data loader. You need to understand  some necessary definations below.\n",
    "\n",
    "<b>Batch</b>: In deep learning, we usually chunk the raw dataset into many same size matrixs to accelerate parallel floating point calculation. Such matrix is called a batch. Here, batch size is 18 and batch length is 149. Thus there are 79 batchs (211878 = 18 $*$ 149 $*$ 79). Usually you can set any appropriate numbers as you like.\n",
    "\n",
    "<b>Input and output</b>: Input and output are basically the same article fragment. The only difference between them is the output is one word \"faster\" than input. for instance, if the input is [312, 456, 8148, 3184, 5442], then the output is [456, 8148, 3184, 5442, 7817]\n",
    "\n",
    "<b>data_set and data_loader</b>: torch.utils.data.Dataset and torch.utils.data.DataLoader class encapsulate batch and put them into neural network. They have many convenient function you can customize, and you can even override the function of them. Here we just use simplest data_set and data_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####preprocessing#####\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "BATCH_LEN = 149\n",
    "BATCH_SIZE = 18\n",
    "\n",
    "def make_batch(corpus, batch_len):\n",
    "    batches = len(corpus) // batch_len\n",
    "    array = corpus[:batches * batch_len]\n",
    "    return np.array(corpus).reshape(batches, batch_len)\n",
    "def preprocessing(paper_list):\n",
    "    np.random.shuffle(paper_list)\n",
    "    #shuffle the paper list every time to guarantee randomness\n",
    "    corpus = np.concatenate(paper_list)\n",
    "    #concatenate them together into a long list\n",
    "    corpus = word_to_number(corpus, word_dict)\n",
    "    inputs = np.array(make_batch(corpus[:-1], BATCH_LEN))\n",
    "    #inputs will discard the last word\n",
    "    targets = np.array(make_batch(corpus[1:], BATCH_LEN))\n",
    "    #targets will discard the first word\n",
    "    #inputs, targets shape: 1422 * 149\n",
    "    data_set = TensorDataset(torch.from_numpy(inputs), torch.from_numpy(targets))\n",
    "    data_loader = torch.utils.data.DataLoader(data_set, batch_size=BATCH_SIZE)\n",
    "    return data_loader\n",
    "#one batch 18 * 149... 79 batchs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use 3 layers lstm RNN. Before lstm, we use embedding for dense vector representations of the word to make learning results better. After lstm, a linear layer is used for output. This model and hyperparameters are adapted from https://arxiv.org/pdf/1708.02182.pdf .\n",
    "\n",
    "Finding a perfect model and hyperparameters is not intuitive, so just remember we use 3 layers lstm RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####model######\n",
    "from torch import nn\n",
    "\n",
    "EMBEDDING_DIM = 400\n",
    "HIDDEN_DIM = 1150\n",
    "\n",
    "class TextModel(nn.Module):\n",
    "    def __init__(self, wordcount, embedding_dim, hidden_dim):\n",
    "        super(TextModel, self).__init__()\n",
    "        #create each layer in sequence\n",
    "        self.wordcount = wordcount\n",
    "        self.embedding = nn.Embedding(num_embeddings=wordcount, embedding_dim=embedding_dim)\n",
    "        self.rnns = nn.ModuleList([\n",
    "            nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True),\n",
    "            nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True),\n",
    "            nn.LSTM(input_size=hidden_dim, hidden_size=embedding_dim, batch_first=True)])\n",
    "        self.projection = nn.Linear(in_features=embedding_dim, out_features=wordcount)\n",
    "\n",
    "    #forward function will take train text as input, then calculate and pass the hidden results in sequence.\n",
    "    #if variable forward is greater than 0, forward length text will be generated.\n",
    "    def forward(self, input, forward=0):\n",
    "        h = input.long()\n",
    "        h = self.embedding(h)\n",
    "        states = []\n",
    "        for rnn in self.rnns:\n",
    "            h, state = rnn(h)\n",
    "            states.append(state)\n",
    "        h = self.projection(h)\n",
    "        logits = h\n",
    "        if forward > 0:\n",
    "            outputs = []\n",
    "            h = torch.max(logits[:, -1:, :], dim=2)[1]\n",
    "            for i in range(forward):\n",
    "                h = self.embedding(h)\n",
    "                for j, rnn in enumerate(self.rnns):\n",
    "                    h, state = rnn(h, states[j])\n",
    "                    states[j] = state\n",
    "                h = self.projection(h)\n",
    "                outputs.append(h)\n",
    "                h = torch.max(h, dim=2)[1]\n",
    "            logits = torch.cat([logits] + outputs, dim=1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Variable](http://pytorch.org/tutorials/beginner/former_torchies/autograd_tutorial.html) is a wrapper around a Tensor(basic arithmetic type in Pytorch) which can help back propagation. Remember we should always check if we can use cuda to accelerate calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####Variable######\n",
    "from torch.autograd import Variable\n",
    "def to_variable(tensor):\n",
    "    # Tensor -> Variable (on GPU if possible)\n",
    "    if torch.cuda.is_available():\n",
    "        # Tensor -> GPU Tensor\n",
    "        tensor = tensor.cuda()\n",
    "    return torch.autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function is a function tell us how good our prediction is by comparing it with expected output. Here we use CrossEntropyLoss3D. Because\n",
    "\n",
    "<center>$Perplexity(text)=2^{Entropy(text)}$</center>\n",
    "\n",
    ", as we minimize the loss of CrossEntropyLoss3D, the perplexity also decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####loss function#####\n",
    "class CrossEntropyLoss3D(nn.CrossEntropyLoss):\n",
    "    def forward(self, input, target):\n",
    "        return super(CrossEntropyLoss3D, self).forward(input.view(-1, input.size()[2]), target.long().view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During each training iteration,\n",
    "\n",
    "step 1: put current batch into RNN to generate prediction\n",
    "\n",
    "step 2: use loss function to calculate loss\n",
    "\n",
    "step 3: use loss.backward to back propagation\n",
    "\n",
    "step 4: use optimizer.step to update weights.\n",
    "\n",
    "Since training is usually time-cosuming, here we only train for 10 iterations (if it is still too slow, just use ctrl-c to stop it). As you can see, the loss keeps going down, from 9 to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "epoch0iteration0\n",
      "Variable containing:\n",
      " 9.0664\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "##################################################\n",
      "##################################################\n",
      "epoch0iteration1\n",
      "Variable containing:\n",
      " 9.0087\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "##################################################\n",
      "##################################################\n",
      "epoch0iteration2\n",
      "Variable containing:\n",
      " 8.6229\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "##################################################\n",
      "##################################################\n",
      "epoch0iteration3\n",
      "Variable containing:\n",
      " 7.9662\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "##################################################\n",
      "##################################################\n",
      "epoch0iteration4\n",
      "Variable containing:\n",
      " 7.4917\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "##################################################\n",
      "##################################################\n",
      "epoch0iteration5\n",
      "Variable containing:\n",
      " 7.0940\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "##################################################\n",
      "##################################################\n",
      "epoch0iteration6\n",
      "Variable containing:\n",
      " 6.8526\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "##################################################\n",
      "##################################################\n",
      "epoch0iteration7\n",
      "Variable containing:\n",
      " 6.6658\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "##################################################\n",
      "##################################################\n",
      "epoch0iteration8\n",
      "Variable containing:\n",
      " 6.5120\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "##################################################\n",
      "##################################################\n",
      "epoch0iteration9\n",
      "Variable containing:\n",
      " 6.3707\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "##################################################\n",
      "##################################################\n",
      "epoch0iteration10\n",
      "Variable containing:\n",
      " 6.3282\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "##################################################\n",
      "training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextModel(\n",
       "  (embedding): Embedding(8686, 400)\n",
       "  (rnns): ModuleList(\n",
       "    (0): LSTM(400, 1150, batch_first=True)\n",
       "    (1): LSTM(1150, 1150, batch_first=True)\n",
       "    (2): LSTM(1150, 400, batch_first=True)\n",
       "  )\n",
       "  (projection): Linear(in_features=400, out_features=8686)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####train#####\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 1\n",
    "def train():\n",
    "    model = TextModel(len(words),EMBEDDING_DIM,HIDDEN_DIM)\n",
    "    loss_fn = CrossEntropyLoss3D()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    #Adam is a kind of method of gradient descent. Commonly, we can use SGD, Adam, or RMSprop.\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        loss_fn = loss_fn.cuda()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        data_loader = preprocessing(paper_list)\n",
    "        for iteration, (input, label) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(to_variable(input), forward=0)\n",
    "            loss = loss_fn(prediction, to_variable(label))\n",
    "            print(\"#\"*50)\n",
    "            print(\"epoch{}iteration{}\".format(epoch,iteration))\n",
    "            print(loss)\n",
    "            print(\"#\"*50)\n",
    "            if iteration == 10:\n",
    "                break\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    print(\"training finished\")\n",
    "    return model\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions below are used for generating text. We trained the model above for 20 epochs by using a GTX 1080 computer for 30 minutes. It should perfermance well since the validation loss is less than 2. You will load the trained model from \"new.pkl\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(model, sequence_length, inp=None):\n",
    "    model.eval()\n",
    "    logits = model(inp, forward=sequence_length)\n",
    "    classes = torch.max(logits, dim=2)[1]\n",
    "    return classes[:, -sequence_length:]\n",
    "\n",
    "def generation(inp, forward):\n",
    "    filepath = \"new.pkl\"\n",
    "    model = TextModel(len(words),EMBEDDING_DIM,HIDDEN_DIM)\n",
    "    #create a empty model\n",
    "    if torch.cuda.is_available():\n",
    "        model.load_state_dict(torch.load(filepath))\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(filepath, lambda storage, loc: storage))\n",
    "        model = model.cpu()\n",
    "    #load rnn weights from new.pkl\n",
    "    #load weights to model\n",
    "    inp = Variable(torch.from_numpy(inp))\n",
    "    generated = generate(model, forward, inp).data.cpu().numpy()\n",
    "    return generated\n",
    "\n",
    "def to_text(preds, words):\n",
    "    #change number list into article\n",
    "    return [\" \".join(words[c] for c in line) for line in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate test using both n-gram and rnn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram output: \n",
      "has been performed by some individual citizen of preeminent wisdom and approved integrity . minos , we learn , was the primitive founder of the government of crete , as zaleucus was of that of the locrians . theseus first , and after him draco and solon , instituted the government of athens . lycurgus was the lawgiver of sparta . the foundation of the original government of rome was laid by romulus , and the work completed by two of his elective successors , numa and tullius hostilius . on the abolition of royalty the consular administration was substituted by brutus , who stepped forward with a project for such a reform , which , he alleged , had been prepared by tullius hostilius , and to which his address obtained the assent and ratification of the senate and people . this remark is applicable to confederate governments also . amphictyon , we are told , was the author of that which bore his name . the achaean league received its first birth from achaeus , and its second from aratus . what degree of agency these reputed lawgivers might have in their respective establishments , or how far\n",
      "200\n",
      "##################################################\n",
      "rnn output: \n",
      "and extensive justice . it has been remarked , that even in the two states which seem to have been framed by the same act is also at all times an indispensable discussion of that object . it cannot be confessed that this will always be as many men as well as under other occasions , not only out of their own experience , but as us as well as external men upon their present address , would they not only excite their friendship for her own neighbors . but the latter will not admit of some of their own frontier at the disposal of the community , that they were disposed by his liberties in the hands of it . if we consider this propensity , if we had so strong in our mind , as the payment of popular government , in their political capacities , to liberty and to property , to be made at all times to secure the final instruments of foreign corruption . it must be confessed that these observations tend to ascertain a change of men , which is of no nature , and that it has not been before yet to have\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(\"n-gram output: \")\n",
    "print(n_gram)\n",
    "print(len(n_gram.split()))\n",
    "print(\"#\"*50)\n",
    "\n",
    "rnn_first = [first.split()]\n",
    "rnn_first = [word_to_number(i, word_dict) for i in rnn_first]\n",
    "rnn = generation(np.array(rnn_first),200)\n",
    "rnn =  to_text(rnn, words)[0]\n",
    "print(\"rnn output: \")\n",
    "print(rnn)\n",
    "print(len(rnn.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and references\n",
    "\n",
    "This tutorial shows another approach to model the language in Pytorch. Much more detail are available from the following links.\n",
    "\n",
    "1. rnn and lstm: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "2. model: https://arxiv.org/pdf/1708.02182.pdf\n",
    "3. neural network: https://en.wikipedia.org/wiki/Artificial_neural_network\n",
    "4. back propagation: https://en.wikipedia.org/wiki/Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
