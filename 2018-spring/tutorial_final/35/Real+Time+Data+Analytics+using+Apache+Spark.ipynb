{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Big Data Analytics Using Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will introduce you to the concepts of real time big data analytics and how it is achieved with the help of Apache Spark, along with a high level arhitectural view of the data pipeline created when dealing with a live stream of data. Before that, it throws some light on the business requirements and the motivation behind companies moving from \"store and process data models\" towards \"stream and compute data models\". Finally, with a real world example, I show how to build a Machine Learning Model on the fly with an incoming data stream and use it for the data coming immediately afterwards.\n",
    "\n",
    "So far in this cource, we have been dealing with static datasets, that are stored in csv files. We had also requested up-to-date data from APIs, but essentially,  we were dealing with pieces of data that were not changing in real time. This is called as batch analytics. In this, a typical e-commerce company will have transactions occcur all over the day and stored in the order management system, and once in a day, the data is aggregated and put into HDFS. Everyday, the analysis is performed on the data from the previous day. However, most of the business goals in todays world are highly information sensitive and require more than just static dataset analysis. Consider a user logging system that records events like system errors, warnings, security related events, etc. Such events should immediately trigger a set of preventive/ curative measures instead of getting reported the next day when it is too late!\n",
    "\n",
    "# What is RTDA?\n",
    "\n",
    "RTDA stands for Real Time Data Analytics. It is the analysis of data as soon as it is produced, i.e. , with 0 millisecond latency- number of customers visiting a pagelink, customers buying a particular product on an e-commerce website, number of tweets produced every second. Near Real Time Analytics is analyzing the data in a given timeframe (1 minute, 5 seconds), which is close to the time it is produced. The time frame is defined by the time sensitivity of the business requirement. For instance, for an Intensive Care Unit (ICU) sub-system monitoring the vitals of a patient, it is not pragmatic to raise an alarm with a delay of 1 minute from the time when data is produced. Similarly, finance and cyber-security require fraud detection in real or 'near real' time.\n",
    "\n",
    "RTDA finds useful application in-\n",
    "\n",
    "- Application health monitoring- reporting errors, warnings, number of clicks on a link\n",
    "- User Sentiment Analysis- response on social media regarding a new feature/technology in your product\n",
    "- High Frequency Stock Trading- instead of referring to historical data, refers to present data\n",
    "\n",
    "Find more about real time analytics here https://www.em360tech.com/tech-news/real-world-use-cases-real-time-data-analytics/. Find more business applications of RTDA here https://www.quora.com/What-are-some-good-examples-of-realtime-analytics.\n",
    "\n",
    "# Modelling Real Time Data For Analytics\n",
    "\n",
    "The real time data can be seen as a stream of events, where each event has a timestamp indicating when it occured, and some data indicating what event occured. This stream of data can be used for aggregating statistics, applying existing Machine Learning Models on the events as they occur or for building Machine Learning Models on the fly with the incoming stream of data and applying them on the data following immediately afterwards. The third type of analysis is used when we don't want the model to be a day old, not even an hour old, instead, we want the model to be built on the most recent data at any time. How do we accomplish this?\n",
    "\n",
    "# Introducing Spark!\n",
    "\n",
    "[<img src=\"https://spark.apache.org/docs/2.2.0/img/streaming-arch.png\">](https://spark.apache.org/docs/2.2.0/img/streaming-arch.png)\n",
    "\n",
    "Spark is a distributed computing framework. It is a fast, easy, general-purpose big data processing engine and has built-in modules for high frequency data streaming, machine learning and graph processing. Datasets today are enormous and need to be stored distributively on a cluster of servers. Essentially, spark converts a continuous stream of data into a timed sequence of discrete datasets called RDDs (Resilient Distributed Datasets ) and the amount of data going into a dataset is governed by the definition of the timeframe for which you want spark to monitor the data. The dataset becomes ready for any analysis as soon as it is completely filled according to the time window and the sequence of these datasets are called DStreams or discretised streams.\n",
    "\n",
    "Below is how the data pipeline looks like.\n",
    "\n",
    "\n",
    "\n",
    "[<img src=\"https://spark.apache.org/docs/2.2.0/img/streaming-flow.png\">](https://spark.apache.org/docs/2.2.0/img/streaming-flow.png)\n",
    "\n",
    "\n",
    "We can see the stream of continuous data flowing into the spark streaming engine and getting converted into a sequence of discretised datasets. We can perform transformational operations like map, filter, union, reduce, forEachRDD on these DStreams, store them as text/Hadoop files or can simply print them. It will take the current elements of the DStream as input and, based on whatever operations you perform on the DStream, the elements will be replaced by the output of those operations. \n",
    "\n",
    "[<img src=\"https://3.bp.blogspot.com/-ICu9V1GM9z4/Wij6KutEZfI/AAAAAAAABQs/xEjJ3IAWKW8mKSolTrRYMLeJxdduY80BwCLcBGAs/s1600/figure5-28.png\">](https://3.bp.blogspot.com/-ICu9V1GM9z4/Wij6KutEZfI/AAAAAAAABQs/xEjJ3IAWKW8mKSolTrRYMLeJxdduY80BwCLcBGAs/s1600/figure5-28.png)\n",
    "\n",
    "The documentation for Spark is provided here https://spark.apache.org/docs/0.9.1/scala-programming-guide.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a real life use case for analysing real time data using Spark. The following example takes in the live stream of tweets from the twitter API and analyses it to find out the Top 10 most hot topics on twitter. We will go, step by step, over the whole process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setting up Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the stream of events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First of all, we need to provide a stream of events. We need to make sure that the packages we're going to use are available to the notebook. Python provides a package called 'tweepy' which stands for 'twitter for python' that enables us to use the streaming API that twitter provides. The documentation for tweepy can be found here http://docs.tweepy.org/en/v3.4.0/streaming_how_to.html. Use the following pip or conda command to install the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation cells have been left without running, purposefully. \n",
    "#The first time, this step will install the packages. Once run, they generate the output 'Requirement already satisfied' in the subsequent runs\n",
    "\n",
    "! pip install tweepy\n",
    "# \"\"\"or\"\"\"\n",
    "!conda install -c conda-forge tweepy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a live stream of tweets, register on Twitter Apps and create your own twitter application in the developer options as we did previously for the Yelp API in hw1. Twitter API requires you to authenticate yourself in order to make any calls for data streaming. Go to the 'Keys and Access Tokens' tab in your twitter app. You must store your authentication credentials (key and access tokens) in a .txt file away from version control. Now you are fully-equipped to proceed forward. Below is the logic and the code of the first part, i.e.- process 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - In the main program, first authenticate yourself. \n",
    " - Then, create a stream of tweets and filter it to contain only those elements that are pertaining to the United States, i.e., have 'u.s.' in their text. So we will get the Top 10 hot topics related to United States. \n",
    " - Whenever data arrives, tweepy calls the **on_data()** method. In the on_data(self, data) method, the argument data is the whole tweet object that contains all the information about the tweet for eg.- who created the tweet, when the tweet got created, all the hashtags in the tweet and the tweet itself. For each tweet that contains 'u.s.' in it, the on_data() method will be called and the tweets will get printed by the **print(data.rstrip())** command. \n",
    " - In the instance of an error, tweepy calls the **on_error()** method that prints the status of the call. \n",
    "\n",
    "Below is the code for the complete process and the output is the tweet objects containing the word 'u.s.' in them. Since, you are recieving a live stream of incoming data, you will have to mannually interrupt the Kernel for this cell in order to terminate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\":\"Fri Mar 30 00:47:14 +0000 2018\",\"id\":979520390891540486,\"id_str\":\"979520390891540486\",\"text\":\"RT @ThisWeekABC: EXCLUSIVE: EPA Administrator Scott Pruitt has spent much of his first year in Washington living in a townhouse one block f\\u2026\",\"source\":\"\\u003ca href=\\\"http:\\/\\/twitter.com\\\" rel=\\\"nofollow\\\"\\u003eTwitter Web Client\\u003c\\/a\\u003e\",\"truncated\":false,\"in_reply_to_status_id\":null,\"in_reply_to_status_id_str\":null,\"in_reply_to_user_id\":null,\"in_reply_to_user_id_str\":null,\"in_reply_to_screen_name\":null,\"user\":{\"id\":801078727899774976,\"id_str\":\"801078727899774976\",\"name\":\"Gary L Loving\",\"screen_name\":\"GaryLLoving\",\"location\":\"1100 N. Stonewall Ave, Oklahoma City, OK\",\"url\":\"http:\\/\\/nursing.ouhsc.edu\",\"description\":\"Interim Dean, OU Fran & Earl Ziegler College of Nursing (Tweets are my own and don't represent the position of the University or the College)\",\"translator_type\":\"none\",\"protected\":false,\"verified\":false,\"followers_count\":53,\"friends_count\":141,\"listed_count\":2,\"favourites_count\":364,\"statuses_count\":407,\"created_at\":\"Tue Nov 22 15:03:46 +0000 2016\",\"utc_offset\":-25200,\"time_zone\":\"Pacific Time (US & Canada)\",\"geo_enabled\":false,\"lang\":\"en\",\"contributors_enabled\":false,\"is_translator\":false,\"profile_background_color\":\"F5F8FA\",\"profile_background_image_url\":\"\",\"profile_background_image_url_https\":\"\",\"profile_background_tile\":false,\"profile_link_color\":\"1DA1F2\",\"profile_sidebar_border_color\":\"C0DEED\",\"profile_sidebar_fill_color\":\"DDEEF6\",\"profile_text_color\":\"333333\",\"profile_use_background_image\":true,\"profile_image_url\":\"http:\\/\\/pbs.twimg.com\\/profile_images\\/905867060378353664\\/pI__7ZWb_normal.jpg\",\"profile_image_url_https\":\"https:\\/\\/pbs.twimg.com\\/profile_images\\/905867060378353664\\/pI__7ZWb_normal.jpg\",\"profile_banner_url\":\"https:\\/\\/pbs.twimg.com\\/profile_banners\\/801078727899774976\\/1504811892\",\"default_profile\":true,\"default_profile_image\":false,\"following\":null,\"follow_request_sent\":null,\"notifications\":null},\"geo\":null,\"coordinates\":null,\"place\":null,\"contributors\":null,\"retweeted_status\":{\"created_at\":\"Thu Mar 29 15:26:56 +0000 2018\",\"id\":979379387413028864,\"id_str\":\"979379387413028864\",\"text\":\"EXCLUSIVE: EPA Administrator Scott Pruitt has spent much of his first year in Washington living in a townhouse one\\u2026 https:\\/\\/t.co\\/3qZsOempSS\",\"display_text_range\":[0,140],\"source\":\"\\u003ca href=\\\"http:\\/\\/www.socialflow.com\\\" rel=\\\"nofollow\\\"\\u003eSocialFlow\\u003c\\/a\\u003e\",\"truncated\":true,\"in_reply_to_status_id\":null,\"in_reply_to_status_id_str\":null,\"in_reply_to_user_id\":null,\"in_reply_to_user_id_str\":null,\"in_reply_to_screen_name\":null,\"user\":{\"id\":450941680,\"id_str\":\"450941680\",\"name\":\"This Week\",\"screen_name\":\"ThisWeekABC\",\"location\":null,\"url\":\"http:\\/\\/abcnews.com\\/ThisWeek\",\"description\":\"The official Twitter account of @ABC News' This Week, hosted by @GStephanopoulos.\",\"translator_type\":\"none\",\"protected\":false,\"verified\":true,\"followers_count\":153642,\"friends_count\":1157,\"listed_count\":1988,\"favourites_count\":79,\"statuses_count\":56549,\"created_at\":\"Fri Dec 30 21:09:43 +0000 2011\",\"utc_offset\":-14400,\"time_zone\":\"Eastern Time (US & Canada)\",\"geo_enabled\":true,\"lang\":\"en\",\"contributors_enabled\":false,\"is_translator\":false,\"profile_background_color\":\"C0DEED\",\"profile_background_image_url\":\"http:\\/\\/pbs.twimg.com\\/profile_background_images\\/378800000062384509\\/6ddb92be9bca39051ed8f8b7c952feec.jpeg\",\"profile_background_image_url_https\":\"https:\\/\\/pbs.twimg.com\\/profile_background_images\\/378800000062384509\\/6ddb92be9bca39051ed8f8b7c952feec.jpeg\",\"profile_background_tile\":false,\"profile_link_color\":\"0084B4\",\"profile_sidebar_border_color\":\"FFFFFF\",\"profile_sidebar_fill_color\":\"DDEEF6\",\"profile_text_color\":\"333333\",\"profile_use_background_image\":true,\"profile_image_url\":\"http:\\/\\/pbs.twimg.com\\/profile_images\\/883070351298711553\\/Ervy92xV_normal.jpg\",\"profile_image_url_https\":\"https:\\/\\/pbs.twimg.com\\/profile_images\\/883070351298711553\\/Ervy92xV_normal.jpg\",\"profile_banner_url\":\"https:\\/\\/pbs.twimg.com\\/profile_banners\\/450941680\\/1501252646\",\"default_profile\":false,\"default_profile_image\":false,\"following\":null,\"follow_request_sent\":null,\"notifications\":null},\"geo\":null,\"coordinates\":null,\"place\":null,\"contributors\":null,\"is_quote_status\":false,\"extended_tweet\":{\"full_text\":\"EXCLUSIVE: EPA Administrator Scott Pruitt has spent much of his first year in Washington living in a townhouse one block from the U.S. Capitol that is co-owned by wife of a top energy lobbyist. https:\\/\\/t.co\\/LpvJruFmuk https:\\/\\/t.co\\/zwnrA4lhTy\",\"display_text_range\":[0,217],\"entities\":{\"hashtags\":[],\"urls\":[{\"url\":\"https:\\/\\/t.co\\/LpvJruFmuk\",\"expanded_url\":\"https:\\/\\/abcn.ws\\/2Gi2ZSp\",\"display_url\":\"abcn.ws\\/2Gi2ZSp\",\"indices\":[194,217]}],\"user_mentions\":[],\"symbols\":[],\"media\":[{\"id\":979379385924046849,\"id_str\":\"979379385924046849\",\"indices\":[218,241],\"media_url\":\"http:\\/\\/pbs.twimg.com\\/media\\/DZd0XM5X0AEW-S7.jpg\",\"media_url_https\":\"https:\\/\\/pbs.twimg.com\\/media\\/DZd0XM5X0AEW-S7.jpg\",\"url\":\"https:\\/\\/t.co\\/zwnrA4lhTy\",\"display_url\":\"pic.twitter.com\\/zwnrA4lhTy\",\"expanded_url\":\"https:\\/\\/twitter.com\\/ThisWeekABC\\/status\\/979379387413028864\\/photo\\/1\",\"type\":\"photo\",\"sizes\":{\"large\":{\"w\":992,\"h\":414,\"resize\":\"fit\"},\"thumb\":{\"w\":150,\"h\":150,\"resize\":\"crop\"},\"medium\":{\"w\":992,\"h\":414,\"resize\":\"fit\"},\"small\":{\"w\":680,\"h\":284,\"resize\":\"fit\"}}}]},\"extended_entities\":{\"media\":[{\"id\":979379385924046849,\"id_str\":\"979379385924046849\",\"indices\":[218,241],\"media_url\":\"http:\\/\\/pbs.twimg.com\\/media\\/DZd0XM5X0AEW-S7.jpg\",\"media_url_https\":\"https:\\/\\/pbs.twimg.com\\/media\\/DZd0XM5X0AEW-S7.jpg\",\"url\":\"https:\\/\\/t.co\\/zwnrA4lhTy\",\"display_url\":\"pic.twitter.com\\/zwnrA4lhTy\",\"expanded_url\":\"https:\\/\\/twitter.com\\/ThisWeekABC\\/status\\/979379387413028864\\/photo\\/1\",\"type\":\"photo\",\"sizes\":{\"large\":{\"w\":992,\"h\":414,\"resize\":\"fit\"},\"thumb\":{\"w\":150,\"h\":150,\"resize\":\"crop\"},\"medium\":{\"w\":992,\"h\":414,\"resize\":\"fit\"},\"small\":{\"w\":680,\"h\":284,\"resize\":\"fit\"}}}]}},\"quote_count\":205,\"reply_count\":171,\"retweet_count\":989,\"favorite_count\":963,\"entities\":{\"hashtags\":[],\"urls\":[{\"url\":\"https:\\/\\/t.co\\/3qZsOempSS\",\"expanded_url\":\"https:\\/\\/twitter.com\\/i\\/web\\/status\\/979379387413028864\",\"display_url\":\"twitter.com\\/i\\/web\\/status\\/9\\u2026\",\"indices\":[116,139]}],\"user_mentions\":[],\"symbols\":[]},\"favorited\":false,\"retweeted\":false,\"possibly_sensitive\":false,\"filter_level\":\"low\",\"lang\":\"en\"},\"is_quote_status\":false,\"quote_count\":0,\"reply_count\":0,\"retweet_count\":0,\"favorite_count\":0,\"entities\":{\"hashtags\":[],\"urls\":[],\"user_mentions\":[{\"screen_name\":\"ThisWeekABC\",\"name\":\"This Week\",\"id\":450941680,\"id_str\":\"450941680\",\"indices\":[3,15]}],\"symbols\":[]},\"favorited\":false,\"retweeted\":false,\"filter_level\":\"low\",\"lang\":\"en\",\"timestamp_ms\":\"1522370834454\"}\n",
      "{\"created_at\":\"Fri Mar 30 00:47:15 +0000 2018\",\"id\":979520394070654976,\"id_str\":\"979520394070654976\",\"text\":\"RT @Gun_Shots: Legal U.S. gun owner have 300 million guns and probably a trillion rounds of ammo. Seriously folks, if we were the problem,\\u2026\",\"source\":\"\\u003ca href=\\\"http:\\/\\/twitter.com\\/download\\/iphone\\\" rel=\\\"nofollow\\\"\\u003eTwitter for iPhone\\u003c\\/a\\u003e\",\"truncated\":false,\"in_reply_to_status_id\":null,\"in_reply_to_status_id_str\":null,\"in_reply_to_user_id\":null,\"in_reply_to_user_id_str\":null,\"in_reply_to_screen_name\":null,\"user\":{\"id\":292732274,\"id_str\":\"292732274\",\"name\":\"Tailgunner Rich\",\"screen_name\":\"wizguy123\",\"location\":\"L.A.\\/L.B. area. So.Cal.\",\"url\":null,\"description\":\"Former Marine. Uppity blue collar conservative. I operate heavy equiptment. People behaving badly. Culture wars. A world set aflame & in ashes. #History #cats\",\"translator_type\":\"none\",\"protected\":false,\"verified\":false,\"followers_count\":518,\"friends_count\":289,\"listed_count\":63,\"favourites_count\":24057,\"statuses_count\":142112,\"created_at\":\"Wed May 04 04:03:04 +0000 2011\",\"utc_offset\":null,\"time_zone\":null,\"geo_enabled\":true,\"lang\":\"en\",\"contributors_enabled\":false,\"is_translator\":false,\"profile_background_color\":\"C0DEED\",\"profile_background_image_url\":\"http:\\/\\/abs.twimg.com\\/images\\/themes\\/theme1\\/bg.png\",\"profile_background_image_url_https\":\"https:\\/\\/abs.twimg.com\\/images\\/themes\\/theme1\\/bg.png\",\"profile_background_tile\":false,\"profile_link_color\":\"1DA1F2\",\"profile_sidebar_border_color\":\"C0DEED\",\"profile_sidebar_fill_color\":\"DDEEF6\",\"profile_text_color\":\"333333\",\"profile_use_background_image\":true,\"profile_image_url\":\"http:\\/\\/pbs.twimg.com\\/profile_images\\/950415363841261568\\/RwEtLUap_normal.jpg\",\"profile_image_url_https\":\"https:\\/\\/pbs.twimg.com\\/profile_images\\/950415363841261568\\/RwEtLUap_normal.jpg\",\"profile_banner_url\":\"https:\\/\\/pbs.twimg.com\\/profile_banners\\/292732274\\/1517785791\",\"default_profile\":true,\"default_profile_image\":false,\"following\":null,\"follow_request_sent\":null,\"notifications\":null},\"geo\":null,\"coordinates\":null,\"place\":null,\"contributors\":null,\"retweeted_status\":{\"created_at\":\"Thu Mar 29 20:07:46 +0000 2018\",\"id\":979450062752247808,\"id_str\":\"979450062752247808\",\"text\":\"Legal U.S. gun owner have 300 million guns and probably a trillion rounds of ammo. Seriously folks, if we were the\\u2026 https:\\/\\/t.co\\/OUczJ09fFb\",\"source\":\"\\u003ca href=\\\"http:\\/\\/www.gun-shots.net\\/\\\" rel=\\\"nofollow\\\"\\u003eGun Shots\\u003c\\/a\\u003e\",\"truncated\":true,\"in_reply_to_status_id\":null,\"in_reply_to_status_id_str\":null,\"in_reply_to_user_id\":null,\"in_reply_to_user_id_str\":null,\"in_reply_to_screen_name\":null,\"user\":{\"id\":368545981,\"id_str\":\"368545981\",\"name\":\"Gun Shots\",\"screen_name\":\"Gun_Shots\",\"location\":null,\"url\":\"http:\\/\\/www.gun-shots.net\\/\",\"description\":\"Gun Shots provides information on selecting, purchasing, maintaining and using your firearms.\",\"translator_type\":\"none\",\"protected\":false,\"verified\":false,\"followers_count\":105026,\"friends_count\":83405,\"listed_count\":775,\"favourites_count\":119,\"statuses_count\":20479,\"created_at\":\"Mon Sep 05 20:25:38 +0000 2011\",\"utc_offset\":10800,\"time_zone\":\"Athens\",\"geo_enabled\":false,\"lang\":\"en\",\"contributors_enabled\":false,\"is_translator\":false,\"profile_background_color\":\"F1F1F1\",\"profile_background_image_url\":\"http:\\/\\/pbs.twimg.com\\/profile_background_images\\/325545407\\/wrapper-bg.png\",\"profile_background_image_url_https\":\"https:\\/\\/pbs.twimg.com\\/profile_background_images\\/325545407\\/wrapper-bg.png\",\"profile_background_tile\":true,\"profile_link_color\":\"566A0C\",\"profile_sidebar_border_color\":\"C7C7C7\",\"profile_sidebar_fill_color\":\"EBEBEB\",\"profile_text_color\":\"333333\",\"profile_use_background_image\":false,\"profile_image_url\":\"http:\\/\\/pbs.twimg.com\\/profile_images\\/1530293865\\/gunshots_normal.png\",\"profile_image_url_https\":\"https:\\/\\/pbs.twimg.com\\/profile_images\\/1530293865\\/gunshots_normal.png\",\"default_profile\":false,\"default_profile_image\":false,\"following\":null,\"follow_request_sent\":null,\"notifications\":null},\"geo\":null,\"coordinates\":null,\"place\":null,\"contributors\":null,\"is_quote_status\":false,\"extended_tweet\":{\"full_text\":\"Legal U.S. gun owner have 300 million guns and probably a trillion rounds of ammo. Seriously folks, if we were the problem, you'd know it.\\n\\nhttps:\\/\\/t.co\\/RJPv2ygjEe\",\"display_text_range\":[0,163],\"entities\":{\"hashtags\":[],\"urls\":[{\"url\":\"https:\\/\\/t.co\\/RJPv2ygjEe\",\"expanded_url\":\"http:\\/\\/www.gun-shots.net\\/legal-u-s-gun-owner-300-million-guns-probably-trillion-rounds-ammo-seriously-folks-problem-youd-know.shtml\",\"display_url\":\"gun-shots.net\\/legal-u-s-gun-\\u2026\",\"indices\":[140,163]}],\"user_mentions\":[],\"symbols\":[]}},\"quote_count\":6,\"reply_count\":7,\"retweet_count\":153,\"favorite_count\":230,\"entities\":{\"hashtags\":[],\"urls\":[{\"url\":\"https:\\/\\/t.co\\/OUczJ09fFb\",\"expanded_url\":\"https:\\/\\/twitter.com\\/i\\/web\\/status\\/979450062752247808\",\"display_url\":\"twitter.com\\/i\\/web\\/status\\/9\\u2026\",\"indices\":[116,139]}],\"user_mentions\":[],\"symbols\":[]},\"favorited\":false,\"retweeted\":false,\"possibly_sensitive\":false,\"filter_level\":\"low\",\"lang\":\"en\"},\"is_quote_status\":false,\"quote_count\":0,\"reply_count\":0,\"retweet_count\":0,\"favorite_count\":0,\"entities\":{\"hashtags\":[],\"urls\":[],\"user_mentions\":[{\"screen_name\":\"Gun_Shots\",\"name\":\"Gun Shots\",\"id\":368545981,\"id_str\":\"368545981\",\"indices\":[3,13]}],\"symbols\":[]},\"favorited\":false,\"retweeted\":false,\"filter_level\":\"low\",\"lang\":\"en\",\"timestamp_ms\":\"1522370835212\"}\n",
      "{\"created_at\":\"Fri Mar 30 00:47:15 +0000 2018\",\"id\":979520394674683906,\"id_str\":\"979520394674683906\",\"text\":\"RT @thedailybeast: One of the \\\"most influential women in technology\\\" has been indicted for \\\"a scheme to defraud the U.S. Department of Stat\\u2026\",\"source\":\"\\u003ca href=\\\"http:\\/\\/twitter.com\\/download\\/android\\\" rel=\\\"nofollow\\\"\\u003eTwitter for Android\\u003c\\/a\\u003e\",\"truncated\":false,\"in_reply_to_status_id\":null,\"in_reply_to_status_id_str\":null,\"in_reply_to_user_id\":null,\"in_reply_to_user_id_str\":null,\"in_reply_to_screen_name\":null,\"user\":{\"id\":20016443,\"id_str\":\"20016443\",\"name\":\"Chicago News Bench\\u2122\",\"screen_name\":\"ChiNewsBench\",\"location\":\"Houston, TX\",\"url\":\"https:\\/\\/www.facebook.com\\/chicagonewsbench\",\"description\":\"Fighting the Left since 1972. Journalist, former Asst Media Director ACU in D.C. #MAGA #UWBadgers\",\"translator_type\":\"none\",\"protected\":false,\"verified\":false,\"followers_count\":6619,\"friends_count\":5842,\"listed_count\":347,\"favourites_count\":16858,\"statuses_count\":129273,\"created_at\":\"Wed Feb 04 00:22:22 +0000 2009\",\"utc_offset\":-18000,\"time_zone\":\"America\\/Chicago\",\"geo_enabled\":false,\"lang\":\"en\",\"contributors_enabled\":false,\"is_translator\":false,\"profile_background_color\":\"000000\",\"profile_background_image_url\":\"http:\\/\\/pbs.twimg.com\\/profile_background_images\\/661706901600604160\\/xq_CGKN7.jpg\",\"profile_background_image_url_https\":\"https:\\/\\/pbs.twimg.com\\/profile_background_images\\/661706901600604160\\/xq_CGKN7.jpg\",\"profile_background_tile\":true,\"profile_link_color\":\"DD0000\",\"profile_sidebar_border_color\":\"FFFFFF\",\"profile_sidebar_fill_color\":\"EFEFEF\",\"profile_text_color\":\"333333\",\"profile_use_background_image\":true,\"profile_image_url\":\"http:\\/\\/pbs.twimg.com\\/profile_images\\/961362384160878592\\/x5WMpcCe_normal.jpg\",\"profile_image_url_https\":\"https:\\/\\/pbs.twimg.com\\/profile_images\\/961362384160878592\\/x5WMpcCe_normal.jpg\",\"profile_banner_url\":\"https:\\/\\/pbs.twimg.com\\/profile_banners\\/20016443\\/1522273378\",\"default_profile\":false,\"default_profile_image\":false,\"following\":null,\"follow_request_sent\":null,\"notifications\":null},\"geo\":null,\"coordinates\":null,\"place\":null,\"contributors\":null,\"retweeted_status\":{\"created_at\":\"Fri Mar 30 00:45:06 +0000 2018\",\"id\":979519854385532928,\"id_str\":\"979519854385532928\",\"text\":\"One of the \\\"most influential women in technology\\\" has been indicted for \\\"a scheme to defraud the U.S. Department of\\u2026 https:\\/\\/t.co\\/D3NI8qathw\",\"source\":\"\\u003ca href=\\\"http:\\/\\/www.socialflow.com\\\" rel=\\\"nofollow\\\"\\u003eSocialFlow\\u003c\\/a\\u003e\",\"truncated\":true,\"in_reply_to_status_id\":null,\"in_reply_to_status_id_str\":null,\"in_reply_to_user_id\":null,\"in_reply_to_user_id_str\":null,\"in_reply_to_screen_name\":null,\"user\":{\"id\":16012783,\"id_str\":\"16012783\",\"name\":\"The Daily Beast\",\"screen_name\":\"thedailybeast\",\"location\":null,\"url\":\"http:\\/\\/thedailybeast.com\",\"description\":\"Independent. Intelligent. Influential.\",\"translator_type\":\"none\",\"protected\":false,\"verified\":true,\"followers_count\":1166560,\"friends_count\":939,\"listed_count\":23838,\"favourites_count\":1738,\"statuses_count\":250443,\"created_at\":\"Wed Aug 27 15:24:02 +0000 2008\",\"utc_offset\":-18000,\"time_zone\":\"Quito\",\"geo_enabled\":true,\"lang\":\"en\",\"contributors_enabled\":false,\"is_translator\":false,\"profile_background_color\":\"363636\",\"profile_background_image_url\":\"http:\\/\\/pbs.twimg.com\\/profile_background_images\\/662622637\\/orsgxv91k6lu6jq3zunq.jpeg\",\"profile_background_image_url_https\":\"https:\\/\\/pbs.twimg.com\\/profile_background_images\\/662622637\\/orsgxv91k6lu6jq3zunq.jpeg\",\"profile_background_tile\":true,\"profile_link_color\":\"FF0000\",\"profile_sidebar_border_color\":\"FFFFFF\",\"profile_sidebar_fill_color\":\"F6F5EF\",\"profile_text_color\":\"000000\",\"profile_use_background_image\":true,\"profile_image_url\":\"http:\\/\\/pbs.twimg.com\\/profile_images\\/862673271212441600\\/u_DNSQ_Q_normal.jpg\",\"profile_image_url_https\":\"https:\\/\\/pbs.twimg.com\\/profile_images\\/862673271212441600\\/u_DNSQ_Q_normal.jpg\",\"profile_banner_url\":\"https:\\/\\/pbs.twimg.com\\/profile_banners\\/16012783\\/1521045740\",\"default_profile\":false,\"default_profile_image\":false,\"following\":null,\"follow_request_sent\":null,\"notifications\":null},\"geo\":null,\"coordinates\":null,\"place\":null,\"contributors\":null,\"is_quote_status\":false,\"extended_tweet\":{\"full_text\":\"One of the \\\"most influential women in technology\\\" has been indicted for \\\"a scheme to defraud the U.S. Department of State of $1.231 million\\\" https:\\/\\/t.co\\/fLK7kho8PL\",\"display_text_range\":[0,164],\"entities\":{\"hashtags\":[],\"urls\":[{\"url\":\"https:\\/\\/t.co\\/fLK7kho8PL\",\"expanded_url\":\"https:\\/\\/thebea.st\\/2GEP2NF\",\"display_url\":\"thebea.st\\/2GEP2NF\",\"indices\":[141,164]}],\"user_mentions\":[],\"symbols\":[]}},\"quote_count\":0,\"reply_count\":0,\"retweet_count\":5,\"favorite_count\":0,\"entities\":{\"hashtags\":[],\"urls\":[{\"url\":\"https:\\/\\/t.co\\/D3NI8qathw\",\"expanded_url\":\"https:\\/\\/twitter.com\\/i\\/web\\/status\\/979519854385532928\",\"display_url\":\"twitter.com\\/i\\/web\\/status\\/9\\u2026\",\"indices\":[117,140]}],\"user_mentions\":[],\"symbols\":[]},\"favorited\":false,\"retweeted\":false,\"possibly_sensitive\":false,\"filter_level\":\"low\",\"lang\":\"en\"},\"is_quote_status\":false,\"quote_count\":0,\"reply_count\":0,\"retweet_count\":0,\"favorite_count\":0,\"entities\":{\"hashtags\":[],\"urls\":[],\"user_mentions\":[{\"screen_name\":\"thedailybeast\",\"name\":\"The Daily Beast\",\"id\":16012783,\"id_str\":\"16012783\",\"indices\":[3,17]}],\"symbols\":[]},\"favorited\":false,\"retweeted\":false,\"filter_level\":\"low\",\"lang\":\"en\",\"timestamp_ms\":\"1522370835356\"}\n",
      "\n",
      " --------------------------------------------------------------------------------------\n",
      "********************Process interrupted manually**********************\n",
      "\n",
      " --------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tweepy\n",
    "from tweepy import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "consumer_key = 'xxx........xxx'\n",
    "# \"\"\" Here you want to put your own consumer key. After running the cell, I replaced mine with xxx...xxx.\"\"\"\n",
    "\n",
    "consumer_secret = 'xxx............xxx'\n",
    "# \"\"\"Here you want to put your own consumer_secret. After running the cell, I replaced mine with xxx...xxx.\"\"\"\n",
    "\n",
    "access_token = 'xxx.............xxx'\n",
    "# \"\"\"Here you want to put your own access_token. After running the cell, I replaced mine with xxx...xxx.\"\"\"\n",
    "\n",
    "access_token_secret = 'xxx............xxx'\n",
    "# \"\"\"Here you want to put your own access_token_secret. After running the cell, I replaced mine with xxx...xxx.\"\"\"\n",
    "\n",
    "class StdOutListener(StreamListener):\n",
    "  \n",
    "    def on_data(self, data):\n",
    "        print(data.rstrip())\n",
    "        return True\n",
    "  \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        listener=StdOutListener()\n",
    "        authentic = OAuthHandler(consumer_key, consumer_secret)\n",
    "        authentic.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "        DStream = Stream(authentic, listener)\n",
    "        DStream.filter(track=['u.s.'])\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n --------------------------------------------------------------------------------------\")\n",
    "    print(\"********************Process interrupted manually**********************\")\n",
    "    print(\"\\n --------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the DStream output is just getting printed. However, we don't want to just print this DStream, instead, we want to send this DStream to the Spark engine running at localhost 9999, to perform some computation on it. Also, it is very important to understand that we need two parallel processes-- one that creates the DStream and sends it to the Spark engine and another that collects those DStreams on the spark server synchronously and performs computation on them and stores them in Spark SQL database. These two processes should run simultaneously in order to accomplish data analytics in real time, which is, unfortunately, not possible using the jupyther notebook. Hence, in order to accomplish this synchronisation, we need to save the above code (process 1) seperately as a python file tutorial_part_1.py and run it simultaneously with the process 2. To send the output data on the localhost 9999, use the following command on the terminal/ bash-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Installation cells have been left without running purposefully. \n",
    "#Once run, they generate the output 'Requirement already satisfied' in the subsequent runs\n",
    "\n",
    "python tutorial_part_1.py | nc -lk 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The output of the process 1 is a stream of Json string objects. We just need the 'text' part of this string. \n",
    " \n",
    "- To convert a stream of json strings into a stream of json objects, **map()** function is used. For each tweet in the sttream of tweets, the extractTweetText function is called, which returns the text part of the tweet, if one exists. Since, each of the tweets is a json object, first we need to parse it using **json.loads()**.The map function will take each element of the DStream, which is a json string and replaces it with the text component of each tweet. \n",
    "- Next, split the text of each tweet into words using the **flatmap()** function, in order to find out the hashtagged words among those. The difference between the  map () and the flatmap() functions is that the flatmap() can produce multiple outputs from each element of the stream. Hence, every DStream element will have multiple outputs corresponding to the words in the tweet text. \n",
    "- The **filter()** function removes the non-desirable elements, i.e. it only results in those elements that pass through the filter function, rest being filtered out. So, we use the filter function to filter-in only the words containing hashtags. \n",
    "- The next **map()** function results in a tuple containing lowercase hashtag and count 1 as the output for every element, so that, we can reduce these 1s and count them across the datasets. \n",
    "- The **reduceByKey()** function compares and adds the 1s for each of the hashtags, where the keys are same, and thus, reduces the stream by its 'Key'. \n",
    "- The third **map()** function returns a TagCount objects containing the hashtag and the number of time the hashtag occurs for each of the elements of the DStream. Now save each of the transformed DStreams into a table so as to perform SQL queries in them.\n",
    "- The **forEachRDD()** function performs a logic with each RDD in the DStream and outputs the result. The logic we perform here is converting each of the DStream into a Data Frame and save it as a table 'tag_counts'. \n",
    "\n",
    "Spark also provides a union function. The readers are encouraged to try using the union function as well, by taking two streams- twitter and facebook and combine them using the union function and perform similar analysis.\n",
    "\n",
    "To start the collection of data, start the Spark context. Connect to the SQl context and query the same tag_counts context using the SQL select query to get the top 10 hashtags and print them for 100 times. Below is the code for process 2. In order to get this working, start the process 2 simultaneously with running the process 1 in the tutorial_part_1.py file. Below that is the output that tells the most hot topics related to 'u.s.' according to the tweets that occured at the very same time I ran these cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure for process 2 is explained below with the code. The output displayed is the hot topics v/s their count, using the matplotlib library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Installation cells have been left without running purposefully. \n",
    "#Once run, they generate the output 'Requirement already satisfied' in the subsequent runs\n",
    "\n",
    "! pip install pandas=0.19.1 --yes\n",
    "# or\n",
    "! conda install pandas=0.19.2 --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "import  datetime\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "# \"\"\" First of all, connect to the port where we are sending our stream of events.\n",
    "# In the main program, the host (\"localhost\") and the port number (9999) are provided as an argument to the connection.\n",
    "# Create a spark context and give it an application name, that can be seen on the spark monitoring dashboard. \n",
    "# Spark context is just an object under which the connection to the spark engine is made.\"\"\"\n",
    "\n",
    "TagCount = namedtuple('TagCount', (\"tag\",\"count\"))\n",
    "\n",
    "def extractTweetText(tweetJson):\n",
    "    if('text' in tweetJson):\n",
    "        return tweetJson['text']\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "host = \"localhost\"\n",
    "port = 9999\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) !=3:\n",
    "        print(\"Usage: trending_tags.py localhost 9999\", file = sys.stderr)\n",
    "        exit(-1)\n",
    "\n",
    "    sc = SparkContext(appName=\"SparkStreamingTwitterHotTopics\")\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "    windowTimeFrame=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The process 2 code borrows the implementation of this method.\n",
    "\n",
    "def functionToCreateContext():\n",
    "    sc = SparkContext(...)  # new context\n",
    "    ssc = StreamingContext(...)\n",
    "    lines = ssc.socketTextStream(...)  # create DStreams\n",
    "    \n",
    "    ssc.checkpoint(checkpointDirectory)  # set checkpoint directory\n",
    "    return ssc\n",
    "\n",
    "#Alternately you can use a spark conf object too. Use any one of the two methods.\n",
    "\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "#This is a small example of the spark reduce() function to show how it works\n",
    "\n",
    "data = [10, 20, 30, 40, 50]\n",
    "distData = sc.parallelize(data)\n",
    "reducedRDD = distData.reduce(lambda a, b: a + b)\n",
    "print(reducedRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[5] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text file RDDs can be created using SparkContext’s textFile method. \n",
    "\n",
    "lines = sc.textFile(\"data.txt\")\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)\n",
    "#  to use lineLengths again later, use below code to retain its value\n",
    "lineLengths.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (2, 'aa'), (3, 'aaa')]\n"
     ]
    }
   ],
   "source": [
    "#This is a small example of the spark saveAsSequence() function to show how it works\n",
    "\n",
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))\n",
    "rdd.saveAsSequenceFile(\"dataRdd.txt\")\n",
    "sortedRDD = sorted(sc.sequenceFile(\"dataRdd.txt\").collect())\n",
    "print(sortedRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'tag'> Column<b'count'>\n",
      "#russia's 3\n",
      "#putin 1\n",
      "#ukraine 1\n",
      "#russia 1\n",
      "#sjubase 1\n",
      "#puertoricans 1\n",
      "#snow 1\n",
      "#cold 1\n",
      "#hellno 1\n",
      "#no 1\n",
      "--------------\n",
      "2018-03-29 20:48:00.783929\n",
      "--------------\n",
      "Column<b'tag'> Column<b'count'>\n",
      "#russia's 3\n",
      "#ukraine 1\n",
      "#cold 1\n",
      "#sjubase 1\n",
      "#hellno 1\n",
      "#no 1\n",
      "#https://www.pol… 1\n",
      "#alisadr 1\n",
      "#puertoricans 1\n",
      "#putin 1\n",
      "--------------\n",
      "2018-03-29 20:48:15.922416\n",
      "--------------\n",
      "Column<b'tag'> Column<b'count'>\n",
      "#russia's 3\n",
      "#putin 1\n",
      "#ukraine 1\n",
      "#russia 1\n",
      "#sjubase 1\n",
      "#puertoricans 1\n",
      "#snow 1\n",
      "#cold 1\n",
      "#hellno 1\n",
      "#no 1\n",
      "--------------\n",
      "2018-03-29 20:48:31.050300\n",
      "--------------\n",
      "Column<b'tag'> Column<b'count'>\n",
      "#russia's 3\n",
      "#ukraine 1\n",
      "#snow 1\n",
      "#cold 1\n",
      "#sjubase 1\n",
      "#hellno 1\n",
      "#no 1\n",
      "#https://www.pol… 1\n",
      "#alisadr 1\n",
      "#putin 1\n",
      "--------------\n",
      "2018-03-29 20:48:46.259474\n",
      "--------------\n",
      "Column<b'tag'> Column<b'count'>\n",
      "#russia's 3\n",
      "#puertoricans 1\n",
      "#putin 1\n",
      "#ukraine 1\n",
      "#russia 1\n",
      "#no 1\n",
      "#snow 1\n",
      "#cold 1\n",
      "#sjubase 1\n",
      "#hellno 1\n",
      "--------------\n",
      "2018-03-29 20:49:01.391971\n",
      "--------------\n",
      "Column<b'tag'> Column<b'count'>\n",
      "#russia's 3\n",
      "#ukraine 1\n",
      "#cold 1\n",
      "#sjubase 1\n",
      "#hellno 1\n",
      "#no 1\n",
      "#https://www.pol… 1\n",
      "#alisadr 1\n",
      "#puertoricans 1\n",
      "#putin 1\n",
      "--------------\n",
      "2018-03-29 20:49:16.516267\n",
      "--------------\n",
      "\n",
      " --------------------------------------------------------------------------------------\n",
      "********************Process interrupted manually**********************\n",
      "\n",
      " --------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now we continue our process 2...\n",
    "\n",
    "ssc = StreamingContext(sc, windowTimeFrame)\n",
    "    \n",
    "#\"\"\"Next, create a streaming context by passing to it the spark context and provide the time window interval \n",
    "#    for which you want the processing to happen everytime. Then, the spark context gets connected to the localhost on port 9999 \n",
    "#     and the spark streaming is pointed to the stream we have created on this port using process 1. \n",
    "#     The returned DStreamed is stored in the tweetsDStream so that stream manipulation operations can be performed on it.\"\"\"\n",
    "\n",
    "tweetsDSream = ssc.socketTextStream(host, port)\n",
    "\n",
    "jsonTweets = tweetsDSream.map(lambda  whole_tweet:extractTweetText(json.loads(whole_tweet)))\n",
    "    \n",
    "splitStream = jsonTweets.flatMap(lambda text: text.split(\" \"))\n",
    "    \n",
    "hashTweets = splitStream.filter(lambda  text: text.startswith(\"#\"))\n",
    "    \n",
    "tupleTweets = hashTweets.map(lambda word: (word.lower(),1))\n",
    "    \n",
    "reducedTweets = tupleTweets.reduceByKey(lambda a,b: a+b)\n",
    "    \n",
    "countTweets = reducedTweets.map(lambda rec: TagCount(rec[0], rec[1]))\n",
    "    \n",
    "tableTweets = countTweets.foreachRDD(lambda rdd: rdd.toDF().registerTempTable(\"tag_counts\"))\n",
    "    \n",
    "ssc.start() \n",
    "sqlContext = SQLContext(sc)\n",
    "count = 0\n",
    "try:\n",
    "    while count < 100:\n",
    "        time.sleep(15)\n",
    "        count=count + 1\n",
    "        hotTopics = sqlContext.sql('select tag, count from tag_counts order by count desc limit 10')\n",
    "        print(hotTopics['tag'], hotTopics['count'])\n",
    "        for row in hotTopics.collect():\n",
    "            print(row.tag, row['count'])\n",
    "        print('--------------')\n",
    "        print (datetime.datetime.now())\n",
    "        print('--------------')\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n --------------------------------------------------------------------------------------\")\n",
    "    print(\"********************Process interrupted manually**********************\")\n",
    "    print(\"\\n --------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This tutorial demonstrated some application of what is possible with Real Time Data Analytics using Spark. Please visit the references for more information on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "\n",
    "1) https://cloudxlab.com/blog/real-time-analytics-dashboard-with-apache-spark-kafka/\n",
    "\n",
    "2) https://www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=IMW14704USEN\n",
    "\n",
    "3) https://static1.squarespace.com/static/55007c24e4b001deff386756/t/55f590a9e4b044a1a342598f/1442156713857/B131+-+Patel,+Nixon.pdf\n",
    "\n",
    "4) https://spark.apache.org/docs/0.9.1/scala-programming-guide.html\n",
    "\n",
    "5) https://spark.apache.org/docs/latest/streaming-programming-guide.html\n",
    "\n",
    "6) https://www.youtube.com/watch?v=MS4-CjZCGsM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
