{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "logging.basicConfig(level=logging.WARN, format='%(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System on Resource Poor Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have your ever crashed your sever trying to generate features for your model? \n",
    "\n",
    "### Too much data to fit in memory?\n",
    "\n",
    "### Let's solve this here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have ever use Netflix, YouTube, Hulu or any othe streaming service, after you finish watching a video you had most likely seen a set of recommendations, showing you other similar videos. This is done by their recommendation system and is a core part of their platform because it allow their users to continue enjoying the service, one video after the next, without having to previously decide exactly what they want to watch. In short Recommendation Systems are important.\n",
    "\n",
    "In this tutorial I'll show you how to create a \"content-based\" movie recommendation system, even if you only have a single machine with limited memory RAM available.\n",
    "\n",
    "The focus of this tutorial would be on how to batch and serialize information to allow the model to be trained, rather than creating a highly accurate recommendation system. But you can use these same concepts in your more complex systems.\n",
    "\n",
    "##### Why do I want to learn this?\n",
    "\n",
    "1. Recommendation Systems are a really popular part of todays applications.\n",
    "2. You will be able to work with huge datasets just using your own laptop.\n",
    "\n",
    "##### What is the problem?\n",
    "When working with a content-based recommendation system the are two main memory intensive operations.\n",
    "    1. Calculating a huge number of comparisons\n",
    "        Why? \n",
    "        - movielens20m (popular movie recommendations dataset) contains 27,000 unique movies\n",
    "        - You compare each movie against each other: over 700 MM comparisons\n",
    "        - A comparison involves tens of operations depending how many features you are using to describe each movie.\n",
    "    2. Doing big merges in memory. In general highly accurate content-based recommendation systems you want to use several fields in order to improve your knowledge about the movie. For instance you want to include:\n",
    "        - Title\n",
    "        - Author\n",
    "        - Cast\n",
    "        - Plot\n",
    "        - Etc.\n",
    "\n",
    "##### What are we suggesting?\n",
    "You can \"divide and conquer\" in order to solve this problem.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our theory is that we are using too much memory at once. Thus we will create a function to easily let us know how much memory we are consuming at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GBs Used: 0.088036 \n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "def get_memory_usage(pid = None, log=False):\n",
    "    ''' Logs how many gigabytes of RAM this process is using'''\n",
    "    if pid is None:\n",
    "        pid = os.getpid()\n",
    "    process = psutil.Process(pid)\n",
    "    memory = (process.memory_info().rss / (1024 ** 3))\n",
    "    if log:\n",
    "        logging.warning(\"GBs Used: %f \" % (memory))\n",
    "        \n",
    "    return memory\n",
    "\n",
    "memory = get_memory_usage(log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "We will be using the openly available and widely used movielens 20m dataset. So let's make a function that will read that dataset and give us one of its fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(field, filepath='movies.csv', id_field='movieId'):\n",
    "    # Read data from CSV but only keep the field requested \n",
    "    cursor = pd.read_csv(filepath)[[id_field, field]].fillna('')\n",
    "    \n",
    "    # Ignore all the punctuation characters\n",
    "    regex = '[%s]' % re.escape(string.punctuation)\n",
    "    cursor[field] = cursor[field].apply(lambda x: re.sub(regex,' ', x))\n",
    "    \n",
    "    # Remove duplicated spaces\n",
    "    regex_spaces = '\\s+' \n",
    "    cursor[field] = cursor[field].apply(lambda x: re.sub(regex_spaces,' ', x).strip())\n",
    "    \n",
    "    # Return data as numpy array\n",
    "    return cursor.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data looks like this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 'Adventure Animation Children Comedy Fantasy'],\n",
       "       [2, 'Adventure Children Fantasy'],\n",
       "       [3, 'Comedy Romance'],\n",
       "       ..., \n",
       "       [131258, 'Adventure'],\n",
       "       [131260, 'no genres listed'],\n",
       "       [131262, 'Adventure Fantasy Horror']], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data('genres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features \n",
    "Now that we can get data. We will use it to generate features. \n",
    "\n",
    "For that we need 2 things: Indexing and Scoring.\n",
    "\n",
    "\n",
    "1 - INDEXING: which is where we calculate the importance of words in each movie.\n",
    "\n",
    "    Ex. \n",
    "        How relevant is the term 'Magic' in 'Harry Potter'\n",
    "\n",
    " In this particular problem we will use TFIDF to calculate the importance of each word. But you can use any vectorizer you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def index(matrix):\n",
    "    # Save the IDs for later\n",
    "    ids = matrix[:, 0].astype(int)\n",
    "    \n",
    "    # Extract the values as a list\n",
    "    data = matrix[:, 1]\n",
    "    \n",
    "    # TODO: Explain parameters values / magic numbers\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "    \n",
    "    # Train the model\n",
    "    t0 = time()\n",
    "    tfidf_matrix = vectorizer.fit_transform(data)\n",
    "    logging.debug(\"Indexing Duration: %f\" % (time() - t0))\n",
    "    logging.debug(\"n_samples: %d, n_features: %d\" % tfidf_matrix.shape)\n",
    "    \n",
    "    return tfidf_matrix, ids #, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2 - SCORING, which is where we need to calculate how similar two movies are based on the previously generated index. For this system we will use cosine similarity. But you can feel free to use other distance metrics.\n",
    "\n",
    "    Ex.\n",
    "        Is 'Magic' extremely important in both 'Harry Potter' and 'Twilight'?\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def cosine_similarity(matrix):\n",
    "    '''\n",
    "     Given a Matrix NxM, where N is the number of items and M is the number of features, \n",
    "     return the cosine similarity between each pair of items.\n",
    "     \n",
    "    :param matrix: Scipy Sparse matrix\n",
    "    :return: Scipy Sparse matrix NxN where every cell is the similarity of its indexes\n",
    "    '''\n",
    "\n",
    "    t0 = time()\n",
    "    \n",
    "    product = matrix @ matrix.T\n",
    "    \n",
    "    matrix_square = matrix.copy()\n",
    "    matrix_square.data **= 2\n",
    "    \n",
    "    square_sum = matrix_square.sum(axis=1)\n",
    "    square_sum = np.sqrt(square_sum)\n",
    "    \n",
    "    norm = square_sum @ square_sum.T\n",
    "    score = product / norm\n",
    "    \n",
    "    duration = time() - t0\n",
    "    \n",
    "    logging.debug(\"Scoring Duration: %f\" % duration)\n",
    "    logging.debug(\"n_samples: %d, n_related_samples: %d\" % score.shape)\n",
    "\n",
    "    return csr_matrix(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous function will use the cosine similarity score to rank for each movie, the other 27,000 movies. For instance, it will tell us how similar is each of the 27,000 movie in the dataset to \"Harry Potter\".\n",
    "\n",
    "But do we really need to save the 27,000 recommendations of a movies if at the end of the day, we will most likely only recommend 5 of them. Probably not. So let's create a function to truncate this data.\n",
    "\n",
    "Let's just keep the top K elements that are most similar for each of the movies. And at the same time we will convert the data from a matrix to a DataFrame where each row is: id1, id2, score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_k(matrix, movie_ids, k, movie_ids_row=None):\n",
    "    \n",
    "    # Extract the data from the sparse matrix\n",
    "    rows, cols = matrix.nonzero()\n",
    "    scores = matrix.data\n",
    "\n",
    "    # Parse from matrix index to movielend_id\n",
    "    if movie_ids_row is None:\n",
    "        print('warning')\n",
    "        movie_ids_row = movie_ids\n",
    "    \n",
    "    rows_movielens = movie_ids_row[rows]\n",
    "    cols_movielens = movie_ids[cols]\n",
    "\n",
    "    # Filter Out movie recommending themselves\n",
    "    # Ex. Harry Potter most similar movie will always be Harry Potter\n",
    "    mask = rows_movielens != cols_movielens\n",
    "    rows_movielens = rows_movielens[mask]\n",
    "    cols_movielens = cols_movielens[mask]\n",
    "    scores = scores[mask]\n",
    "\n",
    "    # Concat columns as a single panda dataframe\n",
    "    pre_frame = np.rec.fromarrays((rows_movielens, cols_movielens, scores), names=('id1','id2','score'))\n",
    "    result = pd.DataFrame(pre_frame)\n",
    "    \n",
    "    # Get only the top K elements for each movieid\n",
    "    # We are assuming that a really small similarity in a field won't \n",
    "    # cause a significant difference in the last result.\n",
    "    result = result.set_index('id2').groupby('id1')['score'].nlargest(k)\n",
    "    result = result.reset_index()\n",
    "    result = result.set_index(['id1', 'id2'])\n",
    "    logging.info('  %s\\t %d/%d saved/found', 'TFIDF', result.shape[0], len(scores))\n",
    "        \n",
    "    return result.reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to calculate the similarity of any two movies for a particular field, such as 'genres'. We just need to:\n",
    "1. Get the data\n",
    "```\n",
    "data = get_data('genres')\n",
    "```\n",
    "2. Index that data\n",
    "```\n",
    "tfidf_matrix, ids = index(data)\n",
    "```\n",
    "3. Calculate the similarity\n",
    "```\n",
    "similarity_scores = similarity(tfidf_matrix)\n",
    "```\n",
    "4. Get the best K recommendations\n",
    "```\n",
    "recommendations = get_top_k(similarity_scores, movie_ids, 100)\n",
    "```\n",
    "\n",
    "Let's try this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b0a65ccada5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 3. Calculate the similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msimilarity_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 4. Get the best K recommendations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-babea437bc17>\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(matrix)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msquare_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_sum\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0msquare_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "field = 'genres'\n",
    "\n",
    "# 1. Get the data\n",
    "data = get_data(field)\n",
    "\n",
    "# 2. Index that data\n",
    "tfidf_matrix, movie_ids = index(data)\n",
    "\n",
    "# 3. Calculate the similarity\n",
    "similarity_scores = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# 4. Get the best K recommendations\n",
    "recommendations = get_top_k(similarity_scores, movie_ids, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! We got a Memory Error. \n",
    "\n",
    "Right now we are trying to do all 700 MM comparisons at the same time and our RAM it's like NO!!!!!! I know RAM is too much for you. We need to chill. We are doing too much computation at the same time and also most of those values are useless. Let's be honest we don't need to know that 'Harry Potter' and 'Toy Story' have a 0.00001 cosine similarity in order to obtain the 'Harry Potter' top 5 similar movies.\n",
    "\n",
    "What we do?\n",
    "\n",
    "1. TRIMMING: Remove really small values\n",
    "\n",
    "2. BATCHING: Only ompare a subset of movies at the same time. \n",
    "\n",
    "    - PRO: A smaller subset will allow us to run our program in machines with smaller RAMs \n",
    "    - CON: Using smaller subsets will make our program slower. \n",
    "    Thus we want to tune this parameter to use as much memory as you have available but not go over board.\n",
    "\n",
    "You can consider both of this number hyperparameter, and tune it based on your particular scenario. \n",
    "For the subset size, you would want to start with a really big number and progressively decresing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that calcualte the cosine similarity between two matrixes, but removes all the values which are less than a specified threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, squareform\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def distance_batch(m1,\n",
    "                   m2,\n",
    "                   cap):\n",
    "    \n",
    "    # Calculate Distance\n",
    "    t0 = time()\n",
    "    \n",
    "    product = m1 @ m2.T\n",
    "    \n",
    "    # First Matrix\n",
    "    matrix_square = m1.copy()\n",
    "    matrix_square.data **= 2\n",
    "    \n",
    "    square_sum = matrix_square.sum(axis=1)\n",
    "    square_sum1 = np.sqrt(square_sum)\n",
    "    \n",
    "    # Second Matrix\n",
    "    matrix_square = m2.copy()\n",
    "    matrix_square.data **= 2\n",
    "    \n",
    "    square_sum = matrix_square.sum(axis=1)\n",
    "    square_sum2 = np.sqrt(square_sum)\n",
    "    \n",
    "    # Get Denominator\n",
    "    norm = square_sum1 @ square_sum2.T\n",
    "    \n",
    "    score = product / norm\n",
    "    \n",
    "    \n",
    "    duration = time() - t0\n",
    "    \n",
    "    result = np.array(score)\n",
    "    \n",
    "    # Remove super small values\n",
    "    result[result < cap] = 0\n",
    "\n",
    "    # Make the matrix sparse - Uses less RAM\n",
    "    return csr_matrix(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be our magical function, it will take the big tfidf matrix, it will work with only batches of \"batch_size\" at the time. \n",
    "\n",
    "It will also call our trimming function, so that we remove all the values which are smalle than \"cap\".\n",
    "\n",
    "Finally it will only return our top K elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_cosine_similarity(tfidf_matrix, movie_ids, batch_size=1000, k=100, cap=0.5):\n",
    "    '''\n",
    "     Given a Matrix NxM, where N is the number of items and M is the number of features, \n",
    "     return the cosine similarity between each pair of items.\n",
    "     \n",
    "    :param index: Numpy matrix\n",
    "    :return: Sparse matrix NxN where every cell is the similarity of its indexes\n",
    "    '''\n",
    "    \n",
    "    t0 = time()\n",
    "    matrix = tfidf_matrix\n",
    "    \n",
    "    # Calculate Similarity\n",
    "    for i in range(0, matrix.shape[0], batch_size):\n",
    "        limit = min(i+batch_size, matrix.shape[0])\n",
    "        m1 = matrix[i:limit,:]\n",
    "        \n",
    "        # Calculate Distance\n",
    "        dist_matrix = distance_batch(m1, matrix, cap)\n",
    "        \n",
    "        # Extract TOP K result\n",
    "        p = get_top_k(dist_matrix, movie_ids, k, movie_ids_row=movie_ids[i:limit])\n",
    "        \n",
    "        # Temporarily save to a local file\n",
    "        p.to_pickle('distance_%i.tmp' % (i))\n",
    "    \n",
    "    # Append All Similarities\n",
    "    frames = []\n",
    "    for i in range(0, matrix.shape[0], batch_size):\n",
    "        frames.append(pd.read_pickle('distance_%i.tmp' % (i)))\n",
    "    result = pd.concat(frames, axis=0)\n",
    "    \n",
    "    logging.debug('Duration: %f', time()-t0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go again. But this time we will use our `batch_cosine_similarity` instead of our preview naive `cosine_similarity` function.\n",
    "\n",
    "In the result the `score` will tell us how simlar it's. For the 'genres' case: a 0 value means they dont share any genre if they have a value of 1 it means all their genres are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of the Data\n",
      "\n",
      "   id1   id2  score\n",
      "0    1  2294    1.0\n",
      "1    1  3114    1.0\n",
      "2    1  3754    1.0\n",
      "3    1  4016    1.0\n",
      "4    1  4886    1.0\n"
     ]
    }
   ],
   "source": [
    "def run(q, field='genres', batch_size=500, k=100, cap=0.1):\n",
    "\n",
    "    # 1. Get the data\n",
    "    data = get_data(field)\n",
    "\n",
    "    # 2. Index that data\n",
    "    tfidf_matrix, movie_ids = index(data)\n",
    "\n",
    "    # 3. Calculate the similarity\n",
    "    recommendations = batch_cosine_similarity(tfidf_matrix, movie_ids, batch_size, k, cap)\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "genres_recommendation = run('genres')\n",
    "\n",
    "print('\\nSample of the Data\\n')\n",
    "print(genres_recommendation.head())\n",
    "del genres_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GBs Used: 3.610821 \n"
     ]
    }
   ],
   "source": [
    "# Clear Memory\n",
    "%reset -f array\n",
    "m = get_memory_usage(log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single field is two simple. What if we combine both the similarity of the title and the genre?\n",
    "\n",
    "True our memory won't be happy about keeping all that in memory. What we will do here is temporarily save the results to the disk.For this we will use pickle because it allows to save things to disk in a easy, fast and lean way (2/3 smaller than csv).\n",
    "\n",
    "Once we calculate the recommendations for each field, we read the recommendations from disk, join them together by giving an weight to each field and return our model.\n",
    "\n",
    "How to assign weights vary, some systems use an heuristic approach using trial and error and manually analyzing the results obtained with each set of weights. Other systems use a machine learning approach, trying to learn this weight based on explicit user feedback. For the sake of simplicity we will use the same weight for each of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GBs Used: 3.610821 \n",
      "GBs Used: 4.344379 \n",
      "GBs Used: 4.262871 \n",
      "GBs Used: 4.345444 \n"
     ]
    }
   ],
   "source": [
    "def join_recommendations(fields, weights, k = 5):\n",
    "    \n",
    "    for field in fields: \n",
    "        logging.debug('Writting FIELD: %s', field)\n",
    "        get_memory_usage(log=True)\n",
    "        recommendation = run(field, batch_size = 1000)\n",
    "        if recommendation is not None:\n",
    "            recommendation.to_pickle('%s_rec.tmp' % field)\n",
    "            del recommendation\n",
    "\n",
    "    recs = pd.DataFrame(columns=['id1', 'id2'])\n",
    "    for field in fields: \n",
    "        logging.debug('Reading FIELD: %s', field)\n",
    "        get_memory_usage(log=True)\n",
    "        rec = pd.read_pickle('%s_rec.tmp' % field)\n",
    "        rec.columns = ['id1', 'id2', field]\n",
    "        recs = recs.merge(rec, how='outer', on=['id1', 'id2'])\n",
    "\n",
    "    recs = recs.fillna(0)    \n",
    "        \n",
    "    recs['score'] = 0\n",
    "    \n",
    "    for field, weight in zip(fields, weights): \n",
    "        recs['score'] += weight * recs[field]\n",
    "        \n",
    "    ml = recs.set_index('id2').groupby('id1')['genres'].nlargest(k)\n",
    "    ml = ml.reset_index()\n",
    "    return ml\n",
    "    \n",
    "rec = join_recommendations(['title', 'genres'], [0.5,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's translate those numbers into words. And find recommendations for a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_movie_id(title, movie_titles):\n",
    "    return movie_titles[movie_titles[:,1] == title][0][0]\n",
    "\n",
    "def get_movie_titles(ids, movie_titles):\n",
    "    return movie_titles[np.in1d(movie_titles[:,0],ids)]\n",
    "\n",
    "def get_recommendations(title, recs, movie_titles):\n",
    "    m_id = get_movie_id(title, movie_titles)\n",
    "    rec_ids = recs[recs.id1 == m_id].id2.values\n",
    "    \n",
    "    return get_movie_titles(rec_ids, movie_titles)\n",
    "\n",
    "movie_titles = get_data('title')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the recommendations for `Toy Story`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2294, 'Antz 1998'],\n",
       "       [3114, 'Toy Story 2 1999'],\n",
       "       [3754, 'Adventures of Rocky and Bullwinkle The 2000'],\n",
       "       [4016, 'Emperor s New Groove The 2000'],\n",
       "       [4886, 'Monsters Inc 2001']], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('Toy Story 1995', rec, movie_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about for the famous `Journey ot the Center of the Earth`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2046, 'Flight of the Navigator 1986'],\n",
       "       [7743, 'Explorers 1985'],\n",
       "       [9004, 'D A R Y L 1985'],\n",
       "       [31389, 'Dr Who and the Daleks 1965'],\n",
       "       [51698, 'Last Mimzy The 2007']], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('Journey to the Center of the Earth 1959', rec, movie_titles)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you dont have to worry about crashing your machine with memory limitation. You can generate so many more features.\n",
    "\n",
    "You can generate features for:\n",
    "\n",
    "    Different fields:\n",
    "        - Title\n",
    "        - Genres\n",
    "        - Cast\n",
    "        - Etc.\n",
    "    Using different algorithms:\n",
    "        - Simple Term Frecuency\n",
    "        - TFIDF\n",
    "        - BM25F\n",
    "        - Etc.\n",
    "    Using different distances:\n",
    "        - Jaccard\n",
    "        - Cosine\n",
    "        - Etc.\n",
    "\n",
    "Merge all this features together and then play around to see which combination works best for your scenario.\n",
    "\n",
    "You can even implement a supervised learning model, interviewing users and asking them to explicitly tell you if two movies are similar.\n",
    "\n",
    "We wanted to keep this tutorial KISS (Keep It Short and Simple) so we will end here. But now that you have the tools, let your imagination run wild, try lots of combinations and have fun regardless of what your memory RAM wants to say. ;)\n",
    "\n",
    "#DFTBA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
 }