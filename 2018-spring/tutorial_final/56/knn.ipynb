{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will introduce the k-nearest neighbors (KNN) classification algorithm, and some of its practical applications.\n",
    "\n",
    "Machine learning and data science are two deeply connected fields. In this tutorial, we will see how to complete the data science pipeline, beginning with data collection, and ending with utilization of KNN to tell us something meaningful about the data.\n",
    "\n",
    "The tutorial begins with a brief introduction to basic machine learning concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Content\n",
    "\n",
    "- [What is Machine Learning?](#What-is-Machine-Learning?)\n",
    "- [KNN Classification Algorithm](#KNN-Classification-Algorithm)\n",
    "- [Practical Applications](#Practical-Applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, let's take a closer look at what \"machine learning\" means.\n",
    "\n",
    "_\"Machine learning explores the study and construction of algorithms that can learn from and make predictions on data\"_ - Wikipedia\n",
    "\n",
    "Basically, people like to observe and make predictions about the universe. Unfortunately, as humans, we can only take in so much information at once. This is where computers (_**machines**_) come in! They can handle large data sets, and perform computations much more quickly than we can.\n",
    "\n",
    "So, we observe what happens in the world, and try to describe it in a way that machines will understand (\"study and construction of algorithms\"). Once they have _**learned**_ what the question is, we can feed them tons of data and they will start to make predictions.\n",
    "\n",
    "There are three main types of machine learning problems. They are as follows:\n",
    "- Classification\n",
    "    * Example: Is an email spam or not spam?\n",
    "- Regression\n",
    "    * Example: Given someone's weight, can you predict their height?\n",
    "- Clustering \n",
    "    * Example: Each person has a course list. Can you predict their year?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of what type of problem we are trying to solve, we need to do a bit of setup to get started.\n",
    "\n",
    "**1. Define your question**\n",
    "\n",
    "We start with a question. This is the problem we are trying to solve. We saw some examples of this in the section above!\n",
    "\n",
    "**2. Gather training data**\n",
    "\n",
    "Experts in any field generally have a lot of experience. Well, in order for our machine learning model to become an expert in answering our question, we need to give it a lot of experience doing that!\n",
    "\n",
    "This is what we call \"training data\". This data is special in that we know what our model should output on each data point. For example, if we are trying to decide whether or not an email is spam, our training data might be of the form:\n",
    "(email contents (text), spam (bool)).\n",
    "\n",
    "Make sure that your training data set is large enough, otherwise the model might not be accurate.\n",
    "\n",
    "**3. Split into train and test**\n",
    "\n",
    "We have all this data for which we already know the correct behavior of the model. This data would be great to figure out how accurate our model is as well!\n",
    "\n",
    "Often, we split the training data set into training data, and testing data. Training data is used to actually create the model, while testing data is used later to verify its accuracy.\n",
    "\n",
    "You get to decide how much data to test or train with. Sometimes, people go with 80% train, 20% test but it is up to you! The more data that is involved in creating the model the more accurate it will be.\n",
    "\n",
    "**4. Decide on an algorithm**\n",
    "\n",
    "Algorithms are at the heart of machine learning and data science. There are a million ways to solve the same problem, but the quality of the results (among other measures) is dependent on picking the right algorithm.\n",
    "\n",
    "If you are using Python, a great resource to explore machine learning algorithms is `sklearn`. This package contains a plethora of machine learning algorithms that are easy to test and play with.\n",
    "\n",
    "For the rest of this tutorial, we will explore the k-nearest neighbors (KNN) algorithm.\n",
    "\n",
    "**5. Data Pre-Processing and Feature Selection**\n",
    "\n",
    "Sometimes, the data you get may not be in a useable form. For example, maybe your dataset consists of images, but your algorithm takes in a vector. Here, you would need a way to convert images into something your algorithm can understand.\n",
    "\n",
    "Additionally, data can come with a ton of extra information that you might not care about. For example, a Twitter profile contains a handle, bio, and tweets. If we only care about analyzing handles, we do not need the extra data and it would just slow the computation down. Picking what parts of our data we \"care\" about is _feature selection_\n",
    "\n",
    "**6. Train your model**\n",
    "\n",
    "Now that we have an algorithm, we feed our training data to the algorithm. As the computer sees more and more data, it begins to get a sense for how to answer our question.\n",
    "\n",
    "In other words, the computer builds a model out of the training data.\n",
    "\n",
    "**7. Test your model**\n",
    "\n",
    "Now we can use our testing data to see how accurate our model is. To do this, we remove the answer from that data and give it to the model. Then, we ask the model our _question_ (from #1!).\n",
    "\n",
    "Since we still do know the right answer for these data, we can figure out how often our machine is correct vs incorrect. Depending on the results, we might iterate on steps 2-6 a few more times.\n",
    "\n",
    "**8. You're good to go!**\n",
    "\n",
    "Woo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classification Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know how people often say \"you are the average of your closest friends\"? This is kind of what KNN does.\n",
    "\n",
    "For any new point, call it `newPoint`, the algorithm looks at the _k_ data points that are closest to `newPoint` (neighbors). Then, the _k_ neighbors vote. `newPoint` is classified as the most commonly cast vote.\n",
    "\n",
    "So we've given an intuitive description of the algorithm, but it is imprecise. A few questions you may have may be:\n",
    "1. What is _k_?\n",
    "2. How is \"closest\" defined?\n",
    "3. What does \"vote\" mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is k?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question is actually quite a bit more complex than it sounds. In the context of KNN, _k_ is a _parameter_. This means that for any implementation of KNN, the algorithm will ask you to provide _k_.\n",
    "\n",
    "So how do we decide on a _k_?\n",
    "\n",
    "If we pick a _k_ that is too large, it will be hard to distinguish the true correct answer. It's like when you are trying to make a decision with too many people. There can sometimes be so many different opinions that it is hard to pick out one to go with.\n",
    "\n",
    "If we pick a _k_ that is too small, it may not be accurate. This is the other side of the scenario we just looked at. Suppose we try to make a big decision with just three people. Have we really listened to enough voices to trust this decision?\n",
    "\n",
    "Remember how we said that algorithms are at the heart of machine learning? Here is another great example of that. There are tons of different algorithms that we can use to decide _k_.\n",
    "\n",
    "A simple (but not always optimal) way to decide is to make `k = sqrt(number of possible answers)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is \"closest\" defined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to define \"closest\".\n",
    "\n",
    "One common way is by calculating the Euclidian distance between two data points. In this case, you would need to figure out how to represent your data such that this is possible.\n",
    "\n",
    "Vectors would be a useful data form here, because computers understand vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does \"vote\" mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know how we said \"you are the average of your closest friends\"? Suppose we wanted to figure out whether you like pineapples on pizza or not.\n",
    "\n",
    "Well, we might go to your closest friends and poll them on their preferences. Then, we could guess your preference based on the most common answer that they gave. (As a sidenote, pineapples _definitely_ belong on pizza.)\n",
    "\n",
    "This is essentially what KNN does too! Our training data set comes with labels already, right? So after we find the k nearest neighbors, we look at their labels. Then, we take the most common label and classify the new data point as that.\n",
    "\n",
    "You can decide on the exact way you want to pick an answer from the votes. Here, we just pick the most common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few built-in modules. \n",
    "\n",
    "`math` is a module that contains many helpful functions.\n",
    "\n",
    "`csv` helps us handle .csv format files. csv files are often used to store data.\n",
    "\n",
    "We will not use any additional packages, but `sklearn` is a good option as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some setup before we begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that our data is of the form:\n",
    "\n",
    "`(label, (x,y))`\n",
    "\n",
    "label: a string representing the correct classification of the data point\n",
    "\n",
    "(x,y): floats representing the data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time for code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to read in our dataset and process it. Depending on the format of your data, you may need a function `processData(data, features)` that goes through the dataset and extracts only the features you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "    # filePath (string): path to some csv file\n",
    "# returns:\n",
    "    # a list of values in the csv\n",
    "def readCSV(filepath):\n",
    "    result = []\n",
    "    with open(filepath, 'r', encoding = 'utf-8') as file:\n",
    "        f = csv.reader(file)\n",
    "    \n",
    "    return result\n",
    "        \n",
    "# parameters:\n",
    "    # filePath (string): contains the location of the dataset\n",
    "    # trainPercent (float): number between 0 and 1, representing percentage of data to be used for training\n",
    "# returns:\n",
    "    # a tuple of lists (training, testing)\n",
    "def makeData(filePath, trainPercent):\n",
    "    training = []\n",
    "    testing = []\n",
    "    \n",
    "    contents = readCSV(filePath)\n",
    "    # contents = processData(contents, features) # comment this in if data needs pre-processing\n",
    "    numData = len(contents)\n",
    "    numTrain = round(numData*trainPercent) # round so that it is a whole number\n",
    "    \n",
    "    lenTrainSet = 0\n",
    "    for line in contents:\n",
    "        if lenTrainSet < numTrain:\n",
    "            training.append(line)\n",
    "            lenTrainSet += 1\n",
    "        else:\n",
    "            testing.append(line)\n",
    "    \n",
    "    return (training, testing)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our distance function. Here, we will implement Euclidian distance between points in 2D space, but this is interchangeable with any other definition of distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "    # ((x1, y1), (x2,y2)): represents two points in floats\n",
    "# returns:\n",
    "    # euclidian distance as a float\n",
    "def distance(point1, point2):\n",
    "    (x1,y1) = point1\n",
    "    (x2,y2) = point2\n",
    "    d = (x1-x2)**2 + (y1-y2)**2\n",
    "    return math.sqrt(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will find our k nearest neighbors using the distance function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "    # k: integer representing number of neighbors to be selected\n",
    "    # trainData: list of training data\n",
    "    # newPoint: new data point of the form (label, (x,y))\n",
    "# returns:\n",
    "    # list of  k neighbors\n",
    "def findNeighbors(k, trainData, newPoint):\n",
    "    dist = []\n",
    "    (newLabel, pointNew) = newPoint\n",
    "    for data in trainData:\n",
    "        (label, point) = data\n",
    "        dist.append((label, distance(pointNew, point)))\n",
    "    dist = sorted(dist, key=itemgetter(1))\n",
    "    \n",
    "    neighbors = dist[0:k]\n",
    "    \n",
    "    result = []\n",
    "    for n in neighbors:\n",
    "        (label, p) = n\n",
    "        result.append(label)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will let them vote!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "    # neighbors: list of neighbors\n",
    "# returns:\n",
    "    # result of neighbor vote\n",
    "def vote(neighbors):\n",
    "    d = dict()\n",
    "    for neighbor in neighbors:\n",
    "        d[neighbor] = d.get(neighbor,0) + 1\n",
    "    \n",
    "    bestCount = 0\n",
    "    bestGuess = None\n",
    "    for key in d:\n",
    "        currGuess = d[key]\n",
    "        if (bestGuess == None) or (currGuess > bestCount):\n",
    "            bestCount = key\n",
    "            bestGuess = currGuess\n",
    "    return bestGuess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally... Let's put this all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = makeData('data.csv', 0.8) # create model\n",
    "\n",
    "# parameters:\n",
    "    # k: integer representing number of neighbors to be selected\n",
    "    # newPoint: new data point of the form (label, (x,y))\n",
    "# returns:\n",
    "    # classification of newPoint\n",
    "def knn(k, newPoint):\n",
    "    neighborSet = findNeighbors(k, train, newPoint)\n",
    "    classification = vote(neighborSet)\n",
    "    return classification\n",
    "    \n",
    "knn(5, (3.0, 4.0)) # example call to knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are curious about how accurate we are, we can check that by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "    # testData: list of testing data\n",
    "# returns:\n",
    "    # (output, label): tuple of lists where output = guess classification, label = real classification\n",
    "def classifyTestingData(testData):\n",
    "    output = []\n",
    "    labels = []\n",
    "    for data in testData:\n",
    "        (label, point) = data\n",
    "        labels.append(label)\n",
    "        classified = knn(5, point)\n",
    "        output.append(classified)\n",
    "    \n",
    "    return (output, labels)\n",
    "\n",
    "# parameters:\n",
    "    # testData: list of testing data\n",
    "    # k: integer representing number of neighbors to be selected\n",
    "# returns:\n",
    "    # float between 0 and 1 that represents percentage of correct results\n",
    "def accuracyCheck(testData, k):\n",
    "    (guessLabel, realLabel) = classifyTestingData(testData)\n",
    "    assert(guessLabel == realLabel) # sanity check\n",
    "    \n",
    "    numPoints = len(guessLabel)\n",
    "    correctGuesses = 0\n",
    "    for i in range(numPoints):\n",
    "        if guessLabel[i] == realLabel[i]:\n",
    "            correctGuesses += 1\n",
    "    \n",
    "    return correctGuesses / numPoints\n",
    "        \n",
    "accuracyCheck(test, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief aside: my model is 100% accurate. Is that good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...Actually, no! This is indicative of an issue called _overfitting_.\n",
    "\n",
    "Because your model is trained on a specific set of data, it can actually fit the data _too_ well. Since there is no way to train the model on all of the data, ever, this is an issue. If your model fits this specific set of data too well, then it may not perform well on other data sets that look different.\n",
    "\n",
    "Of course, an accuracy that is too low is also not desirable.\n",
    "\n",
    "This is why it is important to double check for accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you had a hand-written number and you wanted the computer to recognize what it was. We can use KNN to solve this problem! Let's talk about one approach to this problem here:\n",
    "\n",
    "You can imagine creating a bounding box around the number, where pen-strokes are black and everything else within the box is white. So, divide this box into some number of smaller boxes (100x150 for example). Each small box is colored either white or black.\n",
    "\n",
    "Computers are really good at working with vectors. So, let's turn this grid into a vector such that white boxes are 0s and black boxes are 1s. So, we end up with something that looks like this:\n",
    "\n",
    "[[white, black, white]  \n",
    " [white, black, white]  \n",
    " [white, black, white]  \n",
    " [white, black, white]]\n",
    "\n",
    "=> [0,1,0,0,1,0,0,1,0,0,1,0]\n",
    "\n",
    "Then, we can graph this and find the closest points by Euclidian distance like we did before.\n",
    "\n",
    "But there's an issue. There are so many dimensions! How would we ever graph this? One technique we could use is PCA. This stands for principal component analysis. The algorithm picks out the most important features of the vector. Let's see how this could work given some data point `newPoint`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(2)\n",
    "pca.fit(newPoint)\n",
    "X = pca.transform(newPoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling PCA with 2 as the parameter results in a new vector in 2 dimensions.\n",
    "\n",
    "In order to integrate this with the code we wrote above, all we need to do is define `processData` and pick k!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def processData(data, features):\n",
    "    pca = PCA(features)\n",
    "    \n",
    "    result = []\n",
    "    for d in data:\n",
    "        pca.fit(d)\n",
    "        result.append(pca.transform(d))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def makeData(filePath, trainPercent):\n",
    "    training = []\n",
    "    testing = []\n",
    "    \n",
    "    contents = readCSV(filePath)\n",
    "    # contents = processData(contents, features) # comment this in if data needs pre-processing\n",
    "    numData = len(contents)\n",
    "    numTrain = round(numData*trainPercent) # round so that it is a whole number\n",
    "    \n",
    "    lenTrainSet = 0\n",
    "    for line in contents:\n",
    "        if lenTrainSet < numTrain:\n",
    "            training.append(line)\n",
    "            lenTrainSet += 1\n",
    "        else:\n",
    "            testing.append(line)\n",
    "    \n",
    "    return (training, testing)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loan eligibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you had a set of people for which you knew their loan defaulting habits. Given a new person, can you figure out how likely he or she is to default on a loan?\n",
    "\n",
    "You can with KNN!\n",
    "\n",
    "Here it would be important to pick appropriate features. There are a lot of characteristics of a person, but not all of them would be relevant in determining loan defaulting habits.\n",
    "\n",
    "Again, all we need to integrate this with our code above is to define `processData` and pick k! Here is some pseudocode to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(data, features):\n",
    "    result = []\n",
    "    for d in data:\n",
    "        newData = filter(lambda x: x = featuresWeWant, d)\n",
    "        result.append(newData)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is useful in many other practical applications. Just some of the possibilities are:\n",
    "- facial recognition\n",
    "- detecting fradulent credit card activity\n",
    "- credit ratings\n",
    "- detecting suspicious activity in surveillance footage\n",
    "\n",
    "As you can see, KNN is a simple yet powerful tool to use in extracting meaningful results from data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
