{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial is intended to introduce you to the concept of hyperparamter tuning. In particular, it will focus on a relatively new algorithm called LIPO that is notable because it has no hyperparamters, and is also proven to be better than random search in a lot of real situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the libraries\n",
    "\n",
    "For this tutorial we're going to use two different libraries: ```scikit-learn```, and ```dlib```. ```Scikit-learn``` gives us a wide range of machine learning algorithms, as well as some good hyperparameter tuning algorithms to benchmark against. It does not implement LIPO however, so we're also going to use ```dlib``` which does have an implementation. To install both using conda:\n",
    "\n",
    "```\n",
    "conda install scikit-learn\n",
    "conda install -c menpo dlib\n",
    "```\n",
    "\n",
    "Note that this is installing dlib from a different channel, so it's possible issues might arise in the future with this step.\n",
    "\n",
    "We can now make sure this works by importing both ```dlib``` and ```scikit-learn```, as well as ```numpy``` to manipulate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "A hyperparameter is a parameter of a machine learning algorithm that determines something about how the algorithm will run. In particular, hyperparameters are set before the algorithm runs and do not change value during the run. Some algorithms don't have any hyperparameters; a least squares regression, for example, requires nothing up front. In comparison, support vector classifier with an RBF kernel has two significant hyperparameters, and a decision tree classifier has many more. When using any algorithm with hyperparameters, which values are used is very important, as they significantly impact the performance of the algorithm. Choosing hyperparameters, however, is a fairly difficult problem, especially for algorithms that have a large number of them.\n",
    "\n",
    "As an example, let's use the aforementioned support vector classifier to classify irises based on their petal and sepal dimensions. There is a sample dataset with these features built into ```scikit-learn```; we can load it and look at the first few elements of both the data (feautres) and targets (actual classes). Looking at the frequency of each class label we can see that we have 50 of each type of iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "\n",
      "[0 0 0 0 0]\n",
      "\n",
      "[[ 0  1  2]\n",
      " [50 50 50]]\n"
     ]
    }
   ],
   "source": [
    "iris_data = datasets.load_iris()\n",
    "print(iris_data.data[:5])\n",
    "print()\n",
    "print(iris_data.target[:5])\n",
    "print()\n",
    "print(np.asarray(np.unique(iris_data.target, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a few different support vector classifiers with different hyperparameters to see how they can change performance. For brevity's sake we'll just look at three different values for the ```C``` hyperparameter. In essence, this hyperparameter specifies how bad misclassifying something is.\n",
    "\n",
    "Note that to determine the accuracy of each classifier we're using a stratified 10-fold cross validation. This means we divide the data into 10 smaller sets, each with roughly the same number of each type of flower. Then for each smaller set we train the classifier on the other 9 sets, before scoring it on the holdout set. The overall accuracy is the average of the 10 accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n",
      "0.9800000000000001\n",
      "0.9133333333333333\n"
     ]
    }
   ],
   "source": [
    "svc_1_classifier = svm.SVC(C=.0001)\n",
    "svc_1_accuracy = cross_val_score(svc_1_classifier, iris_data.data, iris_data.target, cv=10)\n",
    "\n",
    "svc_2_classifier = svm.SVC(C=1)\n",
    "svc_2_accuracy = cross_val_score(svc_2_classifier, iris_data.data, iris_data.target, cv=10)\n",
    "\n",
    "svc_3_classifier = svm.SVC(C=10000)\n",
    "svc_3_accuracy = cross_val_score(svc_3_classifier, iris_data.data, iris_data.target, cv=10)\n",
    "\n",
    "print(svc_1_accuracy.mean())\n",
    "print(svc_2_accuracy.mean())\n",
    "print(svc_3_accuracy.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the choice of hyperparameter makes a big difference! It's very possible that by adjusting it further we could do even better than 98% accuracy, and that's not even considering possible adjustments to the second hyperparameter, ```gamma```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple optimization methods\n",
    "\n",
    "The middle value for ```C``` above is just the default in ```scikit-learn```, and the other values were chosen somewhat randomly. In this case it's likely that the value we found to be best is a good choice, but our criteria for saying that is that it's close to the maximum possible accuracy. It's very possible to imagine a situation where we've hit a local maximum rather than a global, or where the global maximum lies between two of our (again, randomly) chosen values.\n",
    "\n",
    "## Grid search\n",
    "\n",
    "Luckily there are a few simple ways to pick hyperparameters that are better than the guess-and-check we employed above. One of those is grid search, which involves searching across the cartesian product of sets of values for each hyperparameter in question. We can observe this using our three values for ```C``` plus two other values in between, and picking 5 values for ```gamma``` that seem reasonable. We'll use ```scikit-learn```'s built in ```GridSearchCV``` and once again measure accuracy with stratified 10-fold cross validation.\n",
    "\n",
    "Note that we wrap our support vector classifier in the ```GridSearchCV```. When we call ```fit``` it tries all the parameter combinations and picks the best one, which we then score ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9800000000000001\n",
      "\n",
      "{'C': 100, 'gamma': 0.01}\n"
     ]
    }
   ],
   "source": [
    "grid_search_parameters = {\n",
    "    'C': [0.0001, 0.01, 1, 100, 10000],\n",
    "    'gamma': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_search_svc = svm.SVC()\n",
    "grid_search_classifier = GridSearchCV(grid_search_svc, grid_search_parameters, cv=10)\n",
    "grid_search_classifier.fit(iris_data.data, iris_data.target)\n",
    "\n",
    "grid_search_accuracy = cross_val_score(\n",
    "    grid_search_classifier.best_estimator_,\n",
    "    iris_data.data,\n",
    "    iris_data.target,\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "print(grid_search_accuracy.mean())\n",
    "print()\n",
    "print(grid_search_classifier.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that we managed to match our accuracy from before. But to some degree we were guided by the default values from ```scikit-learn```. Grid search can be useful, but it requires some knowledge of what values might work well for each hyperparameter.\n",
    "\n",
    "## Random Search\n",
    "\n",
    "It might seem unintuitive, but randomly choosing hyperparameters is actually a relatively effective strategy. One reason why this is the case is that varying any single hyperparameter (like one would do for grid search) often doesn't change accuracy by very much, meaning that a lot of the time spent by grid search is wasted. As we saw above, we have a decent idea of what the hyperparameters should be for the data that we're using, so grid search could still be more effective here, but we can demonstrate that random search can also do a good job. Note that we still have to provide a distribution for each of the hyperparameters, but that this is just as simple if not simpler than picking values like we did for grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9866666666666667\n",
      "\n",
      "{'C': 51.617731761813545, 'gamma': 0.008101361429248288}\n"
     ]
    }
   ],
   "source": [
    "random_search_distributions = {\n",
    "    'C': scipy.stats.expon(scale=100),\n",
    "    'gamma': scipy.stats.expon(scale=.1)\n",
    "}\n",
    "\n",
    "random_search_svc = svm.SVC()\n",
    "random_search_classifier = RandomizedSearchCV(\n",
    "    random_search_svc,\n",
    "    random_search_distributions,\n",
    "    cv=10,\n",
    "    n_iter=25)\n",
    "random_search_classifier.fit(iris_data.data, iris_data.target)\n",
    "\n",
    "random_search_accuracy = cross_val_score(\n",
    "    random_search_classifier.best_estimator_,\n",
    "    iris_data.data,\n",
    "    iris_data.target,\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "print(random_search_accuracy.mean())\n",
    "print()\n",
    "print(random_search_classifier.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, random search does even better than grid search here. We also only examined 25 different sets of hyperparameters, which is the exact same number that we used for grid search.\n",
    "\n",
    "Assuming you can pick appropriate distributions for each hyperparameter, random search can be very effective. However, if that isn't the case it might not work so well. For example, if we picked a random number for ```gamma``` between 0 and 10, it's likely our result wouldn't have been nearly as good. So we need to know both a range and a distribution within that range.\n",
    "\n",
    "# LIPO\n",
    "\n",
    "So far none of our algorithms for choosing hyperparameters have looked at the actual function we're trying to optimize, they've just been various ways to choose the hyperparameters themselves. They take all the function outputs (accuracies) and pick the inputs (hyperparameters) that maximize that output. But it turns out by looking at the inputs and outputs together we can make smarter decisions about what additional inputs to try. This is what LIPO does.\n",
    "\n",
    "Central to the idea of LIPO are Lipschitz functions, or functions that for any pair of points on their graphs have a slope that is bounded in magnitude by a real number, known as the Lipschitz constant. It's easy to imagine that the function we're trying to optimize, accuracy as a function of the hyperparameters, is a Lipschitz function.\n",
    "\n",
    "It's also simple to see that if we know the Lipschitz constant for a function we can take our set of known points and bound the function at any point. For example, in the image below the black points are known, and the green lines have slope equal to the Lipschitz constant:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lipschitz constant](https://1.bp.blogspot.com/-J6B_BQWCR8o/WkFzO5qfFyI/AAAAAAAAA18/slBcjvnaupoNUlueG-I_V9BVxAwWfGDQwCEwYBhgL/s1600/g4175.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the above image in mind, the core of LIPO is relatively intuituve: at every step we evaluate the function at the point with the highest maximum bound and then adjust our bounds based on the new point. But how do we know what value to use as the Lipschitz constant? In the paper presenting LIPO, the authors provide a simple method that they show works well: simply taking the largest slope seen so far as the Lipschitz constant.\n",
    "\n",
    "It turns out that there are some other minor issues with LIPO. One is that it's possible for the observed Lipschitz constant to be infinity. The solution to this is fairly complicated, but involves adding noise at points where it's likely an infinite slope would exist. Another problem is that LIPO is not particularly good at converging once the area of the maximum has been identified. Since this is an issue with the core of the algorithm, ```dlib``` implements a version that switches to another method once LIPO identifies the area of the maximum.\n",
    "\n",
    "In the implementation below we can see one of the major benefits of using LIPO: all we have to provide are bounds for each hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9933333333333334\n",
      "\n",
      "16.52036505133443 0.011262444232475159\n"
     ]
    }
   ],
   "source": [
    "def f(C, gamma):\n",
    "    LIPO_classifier = svm.SVC(C=C, gamma=gamma)\n",
    "    LIPO_accuracy = cross_val_score(LIPO_classifier, iris_data.data, iris_data.target, cv=10)\n",
    "    return LIPO_accuracy.mean()\n",
    "\n",
    "[C, gamma], LIPO_accuracy = dlib.find_max_global(f, [.001, .001], [1000, 1000], 25)\n",
    "\n",
    "print(LIPO_accuracy)\n",
    "print()\n",
    "print(C, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, LIPO does just as well on this particular problem as random search. Looking specifically at the hyperparameters that it finds are optimal, the values are very similar to the ones found by random search.\n",
    "\n",
    "# Benchmarking\n",
    "\n",
    "The test that we did so far was relatively simple, as evidenced by the fact that all three of our hyperparameter tuning methods managed to achieve very high (98%+) accuracy with just a few iterations. In particular, the optimal values for the hyperparameters were close to the ```scikit-learn``` defaults, meaning that grid search worked better than it otherwise might.\n",
    "\n",
    "In addition to the Iris dataset, ```scikit-learn``` includes a dataset of hand-written digits. We can use some code similar to what we wrote above to benchmark the different hyperparameter tuning methods against each other. We begin by loading and examining the data. Each image is 8x8 so we have 64 features for each digit, and we can see that there are roughly equal numbers of each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      "  15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "   0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "   0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      " [ 0.  0.  0. 12. 13.  5.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  0.\n",
      "   3. 15. 16.  6.  0.  0.  0.  7. 15. 16. 16.  2.  0.  0.  0.  0.  1. 16.\n",
      "  16.  3.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  1. 16. 16.  6.\n",
      "   0.  0.  0.  0.  0. 11. 16. 10.  0.  0.]\n",
      " [ 0.  0.  0.  4. 15. 12.  0.  0.  0.  0.  3. 16. 15. 14.  0.  0.  0.  0.\n",
      "   8. 13.  8. 16.  0.  0.  0.  0.  1.  6. 15. 11.  0.  0.  0.  1.  8. 13.\n",
      "  15.  1.  0.  0.  0.  9. 16. 16.  5.  0.  0.  0.  0.  3. 13. 16. 16. 11.\n",
      "   5.  0.  0.  0.  0.  3. 11. 16.  9.  0.]\n",
      " [ 0.  0.  7. 15. 13.  1.  0.  0.  0.  8. 13.  6. 15.  4.  0.  0.  0.  2.\n",
      "   1. 13. 13.  0.  0.  0.  0.  0.  2. 15. 11.  1.  0.  0.  0.  0.  0.  1.\n",
      "  12. 12.  1.  0.  0.  0.  0.  0.  1. 10.  8.  0.  0.  0.  8.  4.  5. 14.\n",
      "   9.  0.  0.  0.  7. 13. 13.  9.  0.  0.]\n",
      " [ 0.  0.  0.  1. 11.  0.  0.  0.  0.  0.  0.  7.  8.  0.  0.  0.  0.  0.\n",
      "   1. 13.  6.  2.  2.  0.  0.  0.  7. 15.  0.  9.  8.  0.  0.  5. 16. 10.\n",
      "   0. 16.  6.  0.  0.  4. 15. 16. 13. 16.  1.  0.  0.  0.  0.  3. 15. 10.\n",
      "   0.  0.  0.  0.  0.  2. 16.  4.  0.  0.]]\n",
      "\n",
      "[0 1 2 3 4]\n",
      "\n",
      "[[  0   1   2   3   4   5   6   7   8   9]\n",
      " [178 182 177 183 181 182 181 179 174 180]]\n"
     ]
    }
   ],
   "source": [
    "digit_data = datasets.load_digits()\n",
    "print(digit_data.data[:5])\n",
    "print()\n",
    "print(digit_data.target[:5])\n",
    "print()\n",
    "print(np.asarray(np.unique(digit_data.target, return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.751209710760296\n",
      "\n",
      "{'C': 100, 'gamma': 0.01}\n"
     ]
    }
   ],
   "source": [
    "grid_search_parameters = {\n",
    "    'C': [0.0001, 0.01, 1, 100, 10000],\n",
    "    'gamma': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_search_svc = svm.SVC()\n",
    "grid_search_classifier = GridSearchCV(grid_search_svc, grid_search_parameters, cv=10)\n",
    "grid_search_classifier.fit(digit_data.data, digit_data.target)\n",
    "\n",
    "grid_search_accuracy = cross_val_score(\n",
    "    grid_search_classifier.best_estimator_,\n",
    "    digit_data.data,\n",
    "    digit_data.target,\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "print(grid_search_accuracy.mean())\n",
    "print()\n",
    "print(grid_search_classifier.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9816944170412627\n",
      "\n",
      "{'C': 37.34961130566053, 'gamma': 0.001381501094060212}\n"
     ]
    }
   ],
   "source": [
    "random_search_distributions = {\n",
    "    'C': scipy.stats.expon(scale=100),\n",
    "    'gamma': scipy.stats.expon(scale=.1)\n",
    "}\n",
    "\n",
    "random_search_svc = svm.SVC()\n",
    "random_search_classifier = RandomizedSearchCV(\n",
    "    random_search_svc,\n",
    "    random_search_distributions,\n",
    "    cv=10,\n",
    "    n_iter=25)\n",
    "random_search_classifier.fit(digit_data.data, digit_data.target)\n",
    "\n",
    "random_search_accuracy = cross_val_score(\n",
    "    random_search_classifier.best_estimator_,\n",
    "    digit_data.data,\n",
    "    digit_data.target,\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "print(random_search_accuracy.mean())\n",
    "print()\n",
    "print(random_search_classifier.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9822593887926752\n",
      "\n",
      "999.9999999999998 0.0015964154273407744\n"
     ]
    }
   ],
   "source": [
    "def f(C, gamma):\n",
    "    LIPO_classifier = svm.SVC(C=C, gamma=gamma)\n",
    "    LIPO_accuracy = cross_val_score(LIPO_classifier, digit_data.data, digit_data.target, cv=10)\n",
    "    return LIPO_accuracy.mean()\n",
    "\n",
    "[C, gamma], LIPO_accuracy = dlib.find_max_global(f, [.001, .001], [1000, 1000], 25)\n",
    "\n",
    "print(LIPO_accuracy)\n",
    "print()\n",
    "print(C, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are very much in line with what we'd expect. Grid search does fairly poorly, while random search and LIPO both do very well.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "LIPO certainly isn't perfect; despite the excellent results obtained in the very limited tests we ran above, there are still flaws. That being said, it does work well in many situations (and the authors of the original LIPO paper proved as much). Perhaps even more importantly it's easy to use. The lack of hyperparameters makes it an excellent first (and perhaps last, depending on the quality of the results) option when looking to optimize hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:15388]",
   "language": "python",
   "name": "conda-env-15388-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
