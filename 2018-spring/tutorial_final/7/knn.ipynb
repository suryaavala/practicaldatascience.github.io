{

 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The Kth Nearest Neighbor Algorithm or KNN for short is regarded as a simple yet effective algorithm primarily used for classification in supervised learning problems. Due to its simplicity, it is one of the most widely used learning algorithms and can even out perform many more complicated algorithms in a wide variety of problems. This tutorial serves as an in-depth guide to learning the foundations of KNN, understanding the pros and cons of the algorithm, and ultimately implementing the algorithm from scratch to tackle real world examples. Furthermore, we will analyze our performance on these examples and see ways to improve the algorithm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the KNN algorithm, we must first understand the problem it is trying to solve. The classical KNN algorithm is usually used for classification and falls under supervised learning. In other words we are trying to use information in the training data to predict or classify the target's group or class. An example of this type of problem might be predicting the gender of a person given hair length, height, and weight. Supervised learning involves using a data set containing training examples with associated correct labels. We use these labels to infer a function or pattern in the data to classify a new test datum. But how does KNN use this training information to classify and predict the class of the test data?\n",
    "    Well, the KNN achieves this task in a simple and naive way. The underlying premise of the KNN algorithm is that data in the same group or class have similar values for the variables associated with it. Looking back to our gender example, we might see that women tend to have longer hair and less height and weight then men might. This information is then leveraged to classify a new point as either male or female. The KNN algorithm begins by taking the the new point that needs to be classified and finding the k-most similar points/neighbors from the training data. Then it uses a majority vote of the classes its neighbors fall into to ultimately decide the class of the point. Assume we have a person who is 5'6\", 135 pounds and has a hair length of 12\". Using our KNN algorithm for a k-value of 5, we would then find the 5 people who are most similar to the person's gender we are trying to find. Say of these 5 neighbors, 4 are Female and 1 is Male. The algorithm would then classify that person's gender as Female. The inquisitive reader might ask what metric or guideline should be used to find the k \"most similar points/neighbors\". The answer would be that similarity between points would have to be some distance metric of which Euclidean distance is commonly used. Euclidean distance is defined as $d(x,x′)=\\sqrt{(x_1−x′_1)^{2}+(x_2−x′_2)^{2}+…+(x_n−x′_n)^{2}}$. Other popular distance metrics include the Manhattan distance, Hamming distance, and Chebyshev distance. Using different metrics can influence the results and success of the algorithm as we will see later in an example problem.\n",
    "    Now that we have learned the basics of how the KNN algorithm works in theory, let us code the algorithm from scratch to deepen our understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin by implementing a metric for comparing how similar two points are to each othen. We will be using the Euclidean distance, the straight line distance between two points in an Euclidean space. The formula for the distance between the two points can be given by the function $d(x,x′)=\\sqrt{(x_1−x′_1)^{2}+(x_2−x′_2)^{2}+…+(x_n−x′_n)^{2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "#Euclidean distance\n",
    "def euclideanDistance(x,x_i): \n",
    "    #define distance to be initially 0\n",
    "        distance = 0\n",
    "        # for every explanatory variable calculate the difference between the two points\n",
    "        # adding the square of the difference to the distance.\n",
    "        for feature in range(len(x)-1):\n",
    "            distance += (x[feature] - x_i[feature])**2 \n",
    "        #return the sqrt of the sum of squared differences \n",
    "        return math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next it is important to note that the KNN function does not need to be trained and thus does not have a training function. This is because it is a lazy learner a property we will discuss later in this tutorial. As such we will now implement the actual \"meat\" of the algorithm, classifying our test data. This will be done in the predict function. We will begin by first calculating the distances between every point and the point we are predicting our class for. Then we will select the k points/neighbors that have the least distance between the test point. We will do this by sorting the list in ascending order and getting the index of the k smallest distances. We will then store all the classes of these points in a list and find the class which occurs the most amount of times. In case of a tie between classes we will select the tieing class that occured first in the list. We will then return the class that occurs the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def predict(x_train, x_test, y_train, k):\n",
    "    #create a list to keep track of the distances between x_test and the points in x_train.\n",
    "    distList = []\n",
    "    classList = []\n",
    "    #iterate through every training point and store the euclidean distance between x_test and \n",
    "    #the point in the distance list as a list with distance and its index.\n",
    "    for i in range(len(x_train)):\n",
    "        distList.append([euclideanDistance(x_test,x_train[i]),i])\n",
    "    #sort the nested list by distance\n",
    "    distList.sort()\n",
    "    # get the index for the first k elements of the list and index y_train to append its class to the list of classes.\n",
    "    for j in range(k):\n",
    "        index = distList[j][1]\n",
    "        classList.append(y_train[index])\n",
    "    #create a counter object passing in the list of classes\n",
    "    c = Counter(classList)\n",
    "    #return the most common class.\n",
    "    return c.most_common(1)[0][0]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KNN:\n",
    "    def __init__(self,x_train,x_test,y_train,y_test,k):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.k = k\n",
    "    def train(self):\n",
    "        #do nothing! remember that the KNN is a lazy learner!\n",
    "        pass\n",
    "    def euclideanDistance(self,x,x_i):\n",
    "        distance = 0\n",
    "        for feature in range(len(x)-1):\n",
    "            distance += (x[feature] - x_i[feature])**2\n",
    "        return math.sqrt(distance)\n",
    "    def predict(self,x_test):\n",
    "        distList = []\n",
    "        classList = []\n",
    "        for i in range(len(self.x_train)):\n",
    "            distList.append([self.euclideanDistance(x_test,self.x_train[i]),i])\n",
    "        distList.sort()\n",
    "        for j in range(self.k):\n",
    "            index = distList[j][1]\n",
    "            classList.append(self.y_train[index])\n",
    "        c = Counter(classList)\n",
    "        return c.most_common(1)[0][0]\n",
    "    def knncomplete(self):\n",
    "        classifications = []\n",
    "        for i in range(len(self.x_test)):\n",
    "            classifications.append(self.predict(x_test[i,:]))\n",
    "        return classifications\n",
    "    def accuracy(self):\n",
    "        classifications = self.knncomplete()\n",
    "        return sum(1 for x,y in zip(classifications,self.y_test) if x == y) / len(self.y_test)\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Iris Flower Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris flower data is commonly used as a benchmark for testing implementations of classifying algorithms. The dataset was first introduced by British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problem. The multivariate dataset contains 50 samples from 3 species of iris. The dataset has 4 explanatory variables - length of petals, width of petals, length of sepal, and width of sepal. We will be using these variables to ultimately classify a flower into one the 3 species of iris. We will begin by loading the data into a pandas data frame as shown below. Since the data does not have column names let us first define these to pass into pd.read_csv while we load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepalLength</th>\n",
       "      <th>sepalWidth</th>\n",
       "      <th>petalLength</th>\n",
       "      <th>petalWidth</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepalLength  sepalWidth  petalLength  petalWidth        Class\n",
       "0          5.1         3.5          1.4         0.2  Iris-setosa\n",
       "1          4.9         3.0          1.4         0.2  Iris-setosa\n",
       "2          4.7         3.2          1.3         0.2  Iris-setosa\n",
       "3          4.6         3.1          1.5         0.2  Iris-setosa\n",
       "4          5.0         3.6          1.4         0.2  Iris-setosa"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "names = ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'Class']\n",
    "df = pd.read_csv('/Users/arjuncomputerscience/Desktop/iris.data', header=None, names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will split the data into training and test data and instantiate our KNN class object. We will then call the accuracy function to see how accurate the model was on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "x = numpy.array(df.loc[:,['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth']])\n",
    "y = numpy.array(df['Class'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=23)\n",
    "kNN = KNN(x_train,x_test,y_train,y_test,4)\n",
    "kNN.accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the algorithm achieved a 96 percent accuracy in classifying the species of iris given the sepal length and width and the petal length and width. A 96 percent accuracy is pretty good, but we will later explore a better statistic for model validation and comparison and see how to further improve our model by tuning our model parameter k. Let us look at some pros and cons of KNN before talking about algorithm improvements and solving our second example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros and Cons of KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROS:\n",
    "The first major pro for the KNN model is that it is very intuitive and easy to understand and implement. As such it can be used very quickly and as a first approach algorithm in classification problems. Furthermore, we see that the KNN model makes no assumptions about the data. There is no assumptions about the underlying distribution making the algorithm non-parametric. Thus, this algorithm can be used for a larger subset of problems and beat models that might be making assumptions that the data does not infact adhere to. Another major pro of the KNN algorithm is that it is a lazy-learner. This means the KNN algorithm only generalizes the training data when a query to predict a new point is made. As such the KNN model needs no training and has the entire dataset stored. An eager learner is the opposite of a lazy learner and stores a model or distribution learned from the data before making a prediction. Thus, these types of models have to be trained before making a prediction. The last pro we will discuss is the algorithms ability to easily classify into multiple categories and not just binary ones as some other algorithms are hardcoded to.\n",
    "\n",
    "## Cons:\n",
    "The first major con is the downside to being a lazy learning alorithm. Since no training is needed, the testing/predicting phase is computationally intensive. Note that the algorithm has to go through all the data each time it needs to make a prediction. This is not ideal in industry settings. Furthermore, a big downside to the KNN algorithm is class imbalance issues. If there are a lot of data classified under one class with less classified under another, the algorithm's majority voting system can be exploited when classifying, selecting the most common class more often. Another big downside to the KNN algorithm is data with high dimensionality. When this occurs the distance between points can be less significant. The distance between the closest and farthest neighbor becomes a lot smaller. The result of this is poorer accuracy and classification from the algorithm.\n",
    "\n",
    "However it should ultimately be noted that the KNN algorithm is still widely used, and effective in many settings. There are also some ways to minimize the impact of cons as we will see later in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2

}