{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot using Tensorflow Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can we teach a machine to talk?**\n",
    "\n",
    "In this tutorial, I will apply [Sequence to Sequence Learning](https://arxiv.org/pdf/1409.3215.pdf) method published by Google in 2014 to train a model for replying a sentence. The dataset I use is the [Movie Dialog Corpus provided by Cornell](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html).\n",
    "\n",
    "Are you ready to make your machine to talk to you? Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Note: The tutorial uses some code from two online resources: [How to build your first chatbot](https://tutorials.botsfloor.com/how-to-build-your-first-chatbot-c84495d4622d) and [Building a Chatbot](https://github.com/Currie32/Chatbot-from-Movie-Dialogue/blob/master/Chatbot_Attention.ipynb). I've shared the draft with and reviewed by the instructor, and it's confirmed that *it's okay to use these resources in this way*. Please check [the private Piazza post](https://piazza.com/class/jcizpany5u6522?cid=1595) for more information.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Sequence-to-sequence learning** is about training a model that converts one sequences domain to another. For example, it's used for transalation where it convert `Hello` to `你好`. It can also be used for building a chatbot that answers your techinical problem if it's trained on an IT helpdesk dataset ([A Neural Conversation Model, Google](https://arxiv.org/pdf/1506.05869v1.pdf)). In our case, we expect to train a chatbot that is capable of normal and casual conversations by using a movie subtitle dataset.\n",
    "\n",
    "    \"how are you?\" -> [Seq2Seq Model] -> \"good, you?\"\n",
    "    \n",
    "In a sequence to sequence model, there's a encoder layer, a decoder layer and an intermediate that connects these two layers.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*3lj8AGqfwEE5KCTJ-dXTvg.png) ([image source](https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d))\n",
    "\n",
    "Each word in the input sequence is first be embedded using the word embedding technique learned in our class, then it's be fed into the encoder. The output of a single decoder component is fed into the next decoder component since the first word may affect what the next word is. By having this model setup, we are able to train the model that knows how to convert one sequence to another.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*Ismhi-muID5ooWf3ZIQFFg.png)([image source](https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d))\n",
    "\n",
    "If you want to learn even more about this topic without reading difficult papers, the best way it to watch [this talk](https://www.youtube.com/watch?v=G5RY_SUJih4) held by Quoc Le, Google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent Neural Networks (RNN)** are the networks that contain loops in them, allowing information to persist. However, to make it looks like a traditional neural network, we can unroll the loop. This will simply the question since we can apply the old techniques in training. The chain-like architecture also makes it intimately related to sequences and lists.\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)([image source](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))\n",
    "\n",
    "However, there's problem of long-term dependencies in RNN. In RNN, it only look at recent information to perform the present task and it doesn't perform well when the gap between two dependents is large. For example, RNN may works well on predicting the next word of \"the clouds are in the ...\" is \"sky\", but it performs badly while trying to predict the next word of \"I grew up in Japan... I speak fluent ...(Japanese)\".\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png)([image source](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long Short Term Memory networks (LSTM)** are a special kind of RNN and t hey are capable of learning long-term dependencies. Inside the cell of LSTM, it contains different paths that enable the data be remembered for long-term or be forgotten. Explaining the details will be too much for this tutorial, please refer to [this resource](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) for detailed information.\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)([image source](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we are good to start with the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install some Python dependencies before starting following code. Please make sure you install exactly version 1.0.0 for Tensorflow since there's are some unsolved bug in the later versions while using deepcopy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    $ pip install tensorflow==1.0.0 wget==3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "import wget\n",
    "import zipfile\n",
    "import os\n",
    "import ntpath\n",
    "import shutil\n",
    "import string\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the corpus dataset from the Internet and it's cached under `DATA_ROOT_DIR`. We will onle need two files as input, `movie_lines.txt` and `movie_converstaions.txt`. `movie_lines.txt` stores the original sentence in the subtitle and each of the line has a corresponding ID. `movie_conversations.txt` contains lists of line IDs, and each list of line IDs represents a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RAW_DATA_URL = \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
    "DATA_ROOT_DIR = \"./data/\"\n",
    "DATA_DIR = DATA_ROOT_DIR + \"cornell movie-dialogs corpus/\"\n",
    "\n",
    "LINE_DATA = DATA_DIR + \"movie_lines.txt\"\n",
    "CONVERSATION_DATA = DATA_DIR + \"movie_conversations.txt\"\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    # Remove previous data\n",
    "    try:\n",
    "        shutil.rmtree(DATA_ROOT_DIR)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        os.remove(ntpath.basename(RAW_DATA_URL))\n",
    "    except OSError:\n",
    "        pass    \n",
    "    \n",
    "    # Download\n",
    "    raw_data_zip = wget.download(RAW_DATA_URL)\n",
    "    \n",
    "    # Unzip\n",
    "    with zipfile.ZipFile(raw_data_zip, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(DATA_ROOT_DIR)\n",
    "        \n",
    "    # Load data\n",
    "    with open(LINE_DATA, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        lines = f.read().split(\"\\n\")\n",
    "    \n",
    "    with open(CONVERSATION_DATA, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        conversations = f.read().split(\"\\n\")\n",
    "        \n",
    "    return lines, conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines, conversations = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines ['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!', 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!', 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.']\n",
      "conversations [\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\"]\n"
     ]
    }
   ],
   "source": [
    "# Prints out the heading lines\n",
    "# and see if you can get the same result as mine!\n",
    "print(\"lines\", lines[:3])\n",
    "print(\"conversations\", conversations[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding the data into the model, they should be processed in order to match the format we are expecting. The process data steps include:\n",
    "\n",
    "1. Extract question and answer pair\n",
    "2. Clean and filter data\n",
    "3. Tokenize\n",
    "4. Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Question and Answer Pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the raw data contains lots of characters that aren't helpful in our case. Now, we will process the raw data the try to retrieve a clean sequence of questions and answers. `questions` and `answers` are lists, and the entry of these two lists are maatched. That is, questions[i] is the question for answers[i]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we build the mapping object that maps line ID to the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id2line(lines):\n",
    "    id2line = {}\n",
    "    for line in lines:\n",
    "        fields = line.split(\" +++$+++ \")\n",
    "        if len(fields) != 5:\n",
    "            continue\n",
    "        _id, _content = fields[0], fields[4]\n",
    "        id2line[_id] = _content\n",
    "    return id2line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2line = get_id2line(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we retrieve the line IDs for each conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conversation_ids():\n",
    "    conversation_ids = []\n",
    "    for line in conversations:\n",
    "        fields = line.split(\" +++$+++ \")\n",
    "        if len(fields) != 4:\n",
    "            continue\n",
    "        # Remove commas and spaces\n",
    "        ids = fields[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\").split(\",\")\n",
    "        conversation_ids.append(ids)\n",
    "    return conversation_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conversation_ids = get_conversation_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_ids[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build the question/answer list pair by using the two objects just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_question_answer(id2line, conversation_ids):\n",
    "    \n",
    "    questions, answers = [], []\n",
    "    \n",
    "    # Visit all conversations\n",
    "    for ids in conversation_ids:\n",
    "        \n",
    "        # The answer is the next line of the question\n",
    "        for i in range(len(ids)-1):\n",
    "            questions.append(id2line[ids[i]])\n",
    "            answers.append(id2line[ids[i+1]])\n",
    "            \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions, answers = get_question_answer(id2line, conversation_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616 221616\n"
     ]
    }
   ],
   "source": [
    "print(len(questions), len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Filter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not a good idea to use the whole original data since there are many parts may confuse our model. Here, we will run through mutliple functions in cleaning and filtering the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we replace upper case letters with their lower case letter, remove common abbreviations, and remove unnecessary spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # Only lower case is used\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Avoid common abbreviations \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Remove mutiple spaces\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no no it is my fault we did not have a proper introduction\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(\"No, no, it's my fault -- we didn't have a proper introduction ---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_question_answer(questions, answers):\n",
    "    _questions = []\n",
    "    for question in questions:\n",
    "        _questions.append(clean_text(question))\n",
    "    _answers = []\n",
    "    for answer in answers:\n",
    "        _answers.append(clean_text(answer))\n",
    "    return _questions, _answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions, answers = clean_question_answer(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616 221616\n",
      "Q: can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again\n",
      "A: well i thought we would start with pronunciation if that is okay with you\n",
      "---\n",
      "Q: well i thought we would start with pronunciation if that is okay with you\n",
      "A: not the hacking and gagging and spitting part please\n",
      "---\n",
      "Q: not the hacking and gagging and spitting part please\n",
      "A: okay then how about we try out some french cuisine saturday night\n",
      "---\n",
      "Q: you are asking me out that is so cute that is your name again\n",
      "A: forget it\n",
      "---\n",
      "Q: no no it is my fault we did not have a proper introduction\n",
      "A: cameron\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(len(questions), len(answers))\n",
    "for i in range(5):\n",
    "    print(\"Q: %s\" % questions[i])\n",
    "    print(\"A: %s\" % answers[i])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we only keep the questions and the answers that has length between 2 and 20. Because, the RNN model we use later only works on a fixed length sequence and 20 is a reasonable size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_question_answer(questions, answers,\n",
    "                           min_line_length, max_line_length):\n",
    "    \n",
    "    _questions, _answers = [], []\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        question, answer = questions[i], answers[i]\n",
    "        q_size, a_size = len(question.split()), len(answer.split())\n",
    "        if ((q_size >= min_line_length and q_size <= max_line_length) and\n",
    "            (a_size >= min_line_length and a_size <= max_line_length)):\n",
    "            _questions.append(question)\n",
    "            _answers.append(answer)\n",
    "\n",
    "    return _questions, _answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_LINE_LENGTH, MAX_LINE_LENGTH = 2, 20\n",
    "questions, answers = filter_question_answer(\n",
    "    questions, answers, MIN_LINE_LENGTH, MAX_LINE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>276692.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.937989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.746700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              counts\n",
       "count  276692.000000\n",
       "mean        7.937989\n",
       "std         4.746700\n",
       "min         2.000000\n",
       "25%         4.000000\n",
       "50%         7.000000\n",
       "75%        11.000000\n",
       "max        20.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "for question in questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138346, 138346)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions), len(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculates the word frequence of both question and answer sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_freq(questions, answers):\n",
    "    word_freq = {}\n",
    "    for question in questions:\n",
    "        for word in question.split():\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    for answer in answers:\n",
    "        for word in answer.split():\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43438\n"
     ]
    }
   ],
   "source": [
    "word_freq = get_word_freq(questions, answers)\n",
    "print(len(word_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two maps the converts a word to its word ID (wid), one is for words in the question sentences and another one is for the words in answer sentences. These two maps are importance in later phases since we always need this information to convert a word to its wid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD_COUNT_THRESHOLD = 10\n",
    "def get_word_id(questions, answers, word_freq):\n",
    "    \n",
    "    question_wid_map, answer_wid_map = {}, {}\n",
    "    \n",
    "    # Questions\n",
    "    wid = 0\n",
    "    for word, count in word_freq.items():\n",
    "        if count < WORD_COUNT_THRESHOLD:\n",
    "            continue\n",
    "        question_wid_map[word] = wid\n",
    "        wid += 1\n",
    "    \n",
    "    # Answers\n",
    "    wid = 0\n",
    "    for word, count in word_freq.items():\n",
    "        if count < WORD_COUNT_THRESHOLD:\n",
    "            continue\n",
    "        answer_wid_map[word] = wid\n",
    "        wid += 1\n",
    "    \n",
    "    return question_wid_map, answer_wid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_wid_map, answer_wid_map = get_word_id(questions, answers, word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8072, 8072)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_wid_map), len(answer_wid_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign wid for the four special tokens. They are:\n",
    "\n",
    "- `<PAD>`: Padding words we used for the shorter input because that it's expected all inputs should have the same length\n",
    "- `<EOS>`: This token tells the decoder where a sentences ends.\n",
    "- `<UNK>`: Words that we ignore will be replaced by this token.\n",
    "- `<GO>`: This token tells the decoder when to start generating the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_special_tokens(question_wid_map, answer_wid_map):\n",
    "    tokens = [\"<PAD>\", \"<EOS>\", \"<UNK>\", \"<GO>\"]\n",
    "    for token in tokens:\n",
    "        question_wid_map[token] = len(question_wid_map) + 1\n",
    "    for token in tokens:\n",
    "        answer_wid_map[token] = len(answer_wid_map) + 1\n",
    "    return question_wid_map, answer_wid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_wid_map, answer_wid_map = add_special_tokens(question_wid_map, answer_wid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8076, 8076)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_wid_map), len(answer_wid_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the reverse maps of the above maps since it's helpful in reading the answer, which is the case that we need to convert an ID back to its original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reverse_wid_map(question_wid_map, answer_wid_map):\n",
    "    question_wid_map_r, answer_wid_map_r = {}, {}\n",
    "    for k, v in question_wid_map.items():\n",
    "        answer_wid_map_r[v] = k\n",
    "    for k, v in answer_wid_map.items():\n",
    "        question_wid_map_r[v] = k\n",
    "    return question_wid_map_r, answer_wid_map_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_wid_map_r, answer_wid_map_r = get_reverse_wid_map(question_wid_map, answer_wid_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `<EOS>` at the end of each answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(answers)):\n",
    "    answers[i] += \" <EOS>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build two objects, `question_wids` and `answer_wids`, to store the questions and answers represented in `wid`. Words that are ignore or unrecognized are replaced by `<UNK>` token. These two objects are important and will be used as the processed input dataset of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_wid(questions, answers, question_wid_map, answer_wid_map):\n",
    "    \n",
    "    question_wids = []\n",
    "    for question in questions:\n",
    "        wids = []\n",
    "        for word in question.split():\n",
    "            wids.append(question_wid_map[word]\n",
    "                        if word in question_wid_map\n",
    "                        else question_wid_map[\"<UNK>\"])\n",
    "        question_wids.append(wids)\n",
    "    \n",
    "    answer_wids = []\n",
    "    for answer in answers:\n",
    "        wids = []\n",
    "        for word in answer.split():\n",
    "            wids.append(answer_wid_map[word]\n",
    "                        if word in answer_wid_map\n",
    "                        else question_wid_map[\"<UNK>\"])\n",
    "        answer_wids.append(wids)\n",
    "            \n",
    "    return question_wids, answer_wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenized question and answer data\n",
    "question_wids, answer_wids = convert_to_wid(\n",
    "    questions, answers, question_wid_map, answer_wid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138346, 138346)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_wids), len(answer_wids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting the questions and answers by the length of questions will speed up the padding procedure while training. We apply bucket sort here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_question_answer_wid(question_wid, answer_wid):\n",
    "    # Bucket sort\n",
    "    sorted_q, sorted_a = [], []\n",
    "    for length in range(1, MAX_LINE_LENGTH + 1):\n",
    "        for k, v in enumerate(question_wid):\n",
    "            if len(v) == length:\n",
    "                sorted_q.append(question_wid[k])\n",
    "                sorted_a.append(answer_wid[k])\n",
    "    return sorted_q, sorted_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_q, sorted_a = sort_question_answer_wid(question_wids, answer_wids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138346 138346\n",
      "[[58, 48], [1, 84], [0, 85]]\n",
      "[[1, 59, 59, 59, 60, 61, 62, 1, 63, 12, 64, 36, 65, 66, 8074], [11, 193, 175, 55, 61, 21, 6, 20, 160, 11, 8074], [154, 8, 9, 166, 11, 467, 55, 272, 8074]]\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted_q), len(sorted_a))\n",
    "print(sorted_q[:3])\n",
    "print(sorted_a[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"**Tensorflow** is an open source software library for numberical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them.\"\n",
    "\n",
    "We are now starting to introduce tensorflow library into our task. We will first setup the tensors needed for later calcuation, build the seq2seq model, then train and test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initiailze a tensorflow session. Remember that, if you are running the later code multiple time, you should always run this `get_ft_session` again because it resets the graph. Otherwise, you will be given an error saying the variables are redefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tf_session():\n",
    "    tf.reset_default_graph()\n",
    "    return tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = get_tf_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Setup tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we setup the tensors for holding the input data and run-time parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_tensors():\n",
    "    ids = tf.placeholder(tf.int32, [None, None], name=\"ids\")\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name=\"targets\")\n",
    "    lr = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    return ids, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids, targets, lr, keep_prob = get_input_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ids:0\", shape=(?, ?), dtype=int32)\n",
      "Tensor(\"targets:0\", shape=(?, ?), dtype=int32)\n",
      "Tensor(\"learning_rate:0\", dtype=float32)\n",
      "Tensor(\"keep_prob:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(ids)\n",
    "print(targets)\n",
    "print(lr)\n",
    "print(keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parameter_tensors(ids):\n",
    "    return (\n",
    "        tf.placeholder_with_default(\n",
    "            MAX_LINE_LENGTH, None, name=\"sequence_length\"\n",
    "        ),\n",
    "        tf.shape(ids)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length, input_shape = get_parameter_tensors(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"sequence_length:0\", dtype=int32)\n",
      "Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(sequence_length)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few parameters needed to be manually set instead of being trained, which is called the hyperparameters. Feel free to change the value and see what changes in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "RNN_SIZE = 512\n",
    "NUM_LAYERS = 2\n",
    "ENCODING_EMBEDDING_SIZE = 512\n",
    "DECODING_EMBEDDING_SIZE = 512\n",
    "LEARNING_RATE = 0.005\n",
    "LEARNING_RATE_DECAY = 0.9\n",
    "MIN_LEARNING_RATE = 0.0001\n",
    "KEEP_PROBABILITY = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it comes to the most difficult part of this tutorial, building the seq2seq model. The `get_seq2seq_model` builds the seq2seq model using several helper functions. It returns the training and inference logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_seq2seq_model(\n",
    "    ids, targets, keep_prob,\n",
    "    batch_size, sequence_length,\n",
    "    answer_wid_size, question_wid_size,\n",
    "    enc_embedding_size, dec_embedding_size,\n",
    "    rnn_size, num_layers, question_wid_map):\n",
    "    \n",
    "    # Encode: embed sequence layer\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(\n",
    "        ids, answer_wid_size+1, enc_embedding_size,\n",
    "        initializer = tf.random_uniform_initializer(0, 1)\n",
    "    )\n",
    "    \n",
    "    # Encode: RNN layer of the embed input\n",
    "    enc_state = get_encoding_layer(\n",
    "        enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length\n",
    "    )\n",
    "    \n",
    "    # Decode: wrap the raw input\n",
    "    dec_input = get_encoding_input(targets, question_wid_map, batch_size)\n",
    "    \n",
    "    # Decode: \n",
    "    dec_embeddings = tf.Variable(\n",
    "        tf.random_uniform([question_wid_size+1, dec_embedding_size], 0, 1)\n",
    "    )\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    # Gets the decoding layer\n",
    "    train_logits, inference_logits = get_decoding_layer(\n",
    "        dec_embed_input, dec_embeddings, enc_state,\n",
    "        question_wid_size, sequence_length, rnn_size, num_layers,\n",
    "        question_wid_map, keep_prob, batch_size\n",
    "    )\n",
    "    \n",
    "    return train_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_encoding_layer` is a helper function that returns a bidirection dynamic rnn object by wrapping the input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_encoding_layer(rnn_inputs, rnn_size, num_layers,\n",
    "                       keep_prob, sequence_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    _, enc_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw = enc_cell,\n",
    "        cell_bw = enc_cell,\n",
    "        sequence_length = sequence_length,\n",
    "        inputs = rnn_inputs,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    return enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Remove the last word of each batch and concat with the <GO> token to the begining of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_encoding_input(targets, question_wid_map, batch_size):\n",
    "    ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat(\n",
    "        [tf.fill([batch_size, 1], question_wid_map[\"<GO>\"]), ending], 1)\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the decoding cells and returns the training and inference decoding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_decoding_layer(dec_embed_input, dec_embeddings,\n",
    "                       encoder_state, question_wid_size,\n",
    "                       sequence_length, rnn_size,\n",
    "                       num_layers, word_map, keep_prob, batch_size):\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "                \n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "            lstm, input_keep_prob=keep_prob)\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "        \n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        \n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(\n",
    "            x, question_wid_size, None, scope=decoding_scope,\n",
    "            weights_initializer=weights,\n",
    "            biases_initializer=biases\n",
    "        )\n",
    "        \n",
    "        train_logits = get_decoding_layer_train(\n",
    "            encoder_state, dec_cell, dec_embed_input,\n",
    "            sequence_length, decoding_scope, output_fn,\n",
    "            keep_prob, batch_size\n",
    "        )\n",
    "        \n",
    "        decoding_scope.reuse_variables()\n",
    "        inference_logits = get_decoding_layer_infer(\n",
    "            encoder_state, dec_cell, dec_embeddings,\n",
    "            word_map[\"<GO>\"], word_map[\"<EOS>\"],\n",
    "            sequence_length-1, question_wid_size,\n",
    "            decoding_scope, output_fn, keep_prob, batch_size\n",
    "        )\n",
    "        \n",
    "        return train_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the decode function of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_decoding_layer_train(\n",
    "    encoder_state, dec_cell, dec_embed_input,\n",
    "    sequence_length, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "    \n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(\n",
    "                attention_states,\n",
    "                attention_option=\"bahdanau\",\n",
    "                num_units=dec_cell.output_size\n",
    "            )\n",
    "    \n",
    "    train_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_train(\n",
    "        encoder_state[0],\n",
    "        att_keys,\n",
    "        att_vals,\n",
    "        att_score_fn,\n",
    "        att_construct_fn,\n",
    "        name = \"attn_dec_train\")\n",
    "    \n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "        dec_cell, \n",
    "        train_decoder_fn, \n",
    "        dec_embed_input, \n",
    "        sequence_length, \n",
    "        scope=decoding_scope)\n",
    "    \n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "    return output_fn(train_pred_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the decode logits of the prediction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_decoding_layer_infer(\n",
    "    encoder_state, dec_cell, dec_embeddings,\n",
    "    start_of_sequence_id, end_of_sequence_id,\n",
    "    maximum_length, vocab_size, decoding_scope,\n",
    "    output_fn, keep_prob, batch_size):\n",
    "\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(\n",
    "                attention_states,\n",
    "                attention_option=\"bahdanau\",\n",
    "                num_units=dec_cell.output_size)\n",
    "    \n",
    "    infer_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(\n",
    "        output_fn, \n",
    "        encoder_state[0], \n",
    "        att_keys, \n",
    "        att_vals, \n",
    "        att_score_fn, \n",
    "        att_construct_fn, \n",
    "        dec_embeddings,\n",
    "        start_of_sequence_id, \n",
    "        end_of_sequence_id, \n",
    "        maximum_length, \n",
    "        vocab_size, \n",
    "        name = \"attn_dec_inf\")\n",
    "    \n",
    "    inference_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "        dec_cell, \n",
    "        infer_decoder_fn, \n",
    "        scope=decoding_scope)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the top wrapper function to get a seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_logits, inference_logits = get_seq2seq_model(\n",
    "    tf.reverse(ids, [-1]), targets, keep_prob,\n",
    "    BATCH_SIZE, sequence_length,\n",
    "    len(answer_wids), len(question_wids),\n",
    "    ENCODING_EMBEDDING_SIZE, DECODING_EMBEDDING_SIZE,\n",
    "    RNN_SIZE, NUM_LAYERS, question_wid_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: be careful on reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Setup training optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having the seq2seq model ready, we setup an optimizer object that optimize the seq2seq model. In the same time, we also retrieve the cost tensor object which presents the cost of the current trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_optimizer(train_logits, targets,\n",
    "                        learning_rate, sequence_length):\n",
    "\n",
    "    tf.identity(inference_logits, \"logits\")\n",
    "    \n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        \n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            train_logits,\n",
    "            targets,\n",
    "            tf.ones([input_shape[0], sequence_length])\n",
    "        )\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [\n",
    "            (tf.clip_by_value(grad, -5., 5.), var)\n",
    "            for grad, var in gradients if grad is not None\n",
    "        ]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    \n",
    "    return train_op, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_optimizer, cost = get_train_optimizer(\n",
    "    train_logits, targets,\n",
    "    LEARNING_RATE, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"optimization/Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^optimization/Adam/update_EmbedSequence/embeddings/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_bidirectional_rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/weights/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_bidirectional_rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/biases/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_bidirectional_rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/weights/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_bidirectional_rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/biases/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_Variable/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_decoding/attention_keys/weights/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_decoding/attention_score/attnW/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_decoding/attention_score/attnV/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_decoding/multi_rnn_cell/cell_0/basic_lstm_cell/weights/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_decoding/multi_rnn_cell/cell_0/basic_lstm_cell/biases/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_decoding/multi_rnn_cell/cell_1/basic_lstm_cell/weights/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_decoding/multi_rnn_cell/cell_1/basic_lstm_cell/biases/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_decoding/attention_construct/weights/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_decoding/weights/ApplyAdam\"\n",
      "input: \"^optimization/Adam/update_decoding/biases/ApplyAdam\"\n",
      "input: \"^optimization/Adam/Assign\"\n",
      "input: \"^optimization/Adam/Assign_1\"\n",
      " Tensor(\"optimization/sequence_loss/truediv:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(train_optimizer, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take 85% of the original dataset as training data and 15% as validation data. This enables us to know when to \"stop\" training (we should stop training if the accuracy on test data starts to decrease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_valid_pivot = int(len(sorted_q) * 0.15)\n",
    "\n",
    "train_questions = sorted_q[train_valid_pivot:]\n",
    "train_answers = sorted_a[train_valid_pivot:]\n",
    "\n",
    "valid_questions = sorted_q[:train_valid_pivot]\n",
    "valid_answers = sorted_a[:train_valid_pivot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117595 20751\n"
     ]
    }
   ],
   "source": [
    "print(len(train_questions), len(valid_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start to train our defined model now. The training function here is designed for running on a normal laptop instead of a powerful machine that uses GPU or TPU. That is, we don't expect to come to a point that we see accuracy on the validation data starts to decrease because that will take forever if you run this training on a normal machine. Instead, I added a `loss_stop` variable, which is the threshold you set manually based on the capacity of your machine, the training will stop when it found the current loss value if lower than `lost_stop`. In my case, it took me around 1.5 hours to get a model with loss < 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    session, cost, epochs, train_questions, train_answers,\n",
    "    batch_size, learning_rate, keep_probability, loss_stop,\n",
    "    question_wid_map, answer_wid_map):\n",
    "    \n",
    "    # Epoch iteration\n",
    "    for epoch_i in range(1, epochs + 1):\n",
    "        \n",
    "        # Batch iteration\n",
    "        for batch_i, (questions_batch, answers_batch) in enumerate(\n",
    "            get_batch(train_questions, train_answers, batch_size,\n",
    "                      question_wid_map, answer_wid_map)):\n",
    "            \n",
    "            # Marks starting time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Runs train optimizer\n",
    "            _, loss = session.run(\n",
    "                [train_optimizer, cost],\n",
    "                {\n",
    "                    ids: questions_batch,\n",
    "                    targets: answers_batch,\n",
    "                    lr: learning_rate,\n",
    "                    sequence_length: answers_batch.shape[1],\n",
    "                    keep_prob: keep_probability\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Logs ending metrics\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Prints some logs\n",
    "            print(\"Epoch %d/%d Batch %d/%d Loss: %.3f Seconds: %.2f\" % (\n",
    "                epoch_i, epochs, batch_i,\n",
    "                len(train_questions) // batch_size,\n",
    "                loss, end_time - start_time\n",
    "            ))\n",
    "\n",
    "            if loss < loss_stop:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(questions, answers, batch_size,\n",
    "              question_wid_map, answer_wid_map):\n",
    "    \n",
    "    for batch_i in range(0, len(questions) // batch_size):\n",
    "        \n",
    "        # Finds starting index of this batch\n",
    "        start_i = batch_i * batch_size\n",
    "        \n",
    "        # Gets the batch of questions and answers\n",
    "        questions_batch = questions[start_i:start_i + batch_size]\n",
    "        answers_batch = answers[start_i:start_i + batch_size]\n",
    "        \n",
    "        # Pads and parses the questions, answers into numpy arrays\n",
    "        padded_questions_batch = np.array(\n",
    "            get_padded_batch(questions_batch, question_wid_map)\n",
    "        )\n",
    "        padded_answers_batch = np.array(\n",
    "            get_padded_batch(answers_batch, answer_wid_map)\n",
    "        )\n",
    "        \n",
    "        # Generates the batch into final output\n",
    "        # https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "        yield padded_questions_batch, padded_answers_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_padded_batch(sentence_batch, word_to_int):\n",
    "    max_sentence = max(\n",
    "        [len(sentence) for sentence in sentence_batch]\n",
    "    )\n",
    "    return [\n",
    "        sentence +\n",
    "        [word_to_int[\"<PAD>\"]] * (max_sentence - len(sentence))\n",
    "        for sentence in sentence_batch\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 Batch 0/918 Loss: 11.678 Seconds: 45.31\n",
      "Epoch 1/100 Batch 1/918 Loss: 17.817 Seconds: 46.16\n",
      "Epoch 1/100 Batch 2/918 Loss: 36.943 Seconds: 47.98\n",
      "Epoch 1/100 Batch 3/918 Loss: 7.723 Seconds: 42.30\n",
      "Epoch 1/100 Batch 4/918 Loss: 9.962 Seconds: 41.90\n",
      "Epoch 1/100 Batch 5/918 Loss: 5.850 Seconds: 43.69\n",
      "Epoch 1/100 Batch 6/918 Loss: 4.922 Seconds: 38.39\n",
      "Epoch 1/100 Batch 7/918 Loss: 5.748 Seconds: 40.18\n",
      "Epoch 1/100 Batch 8/918 Loss: 5.312 Seconds: 43.63\n",
      "Epoch 1/100 Batch 9/918 Loss: 4.563 Seconds: 39.41\n",
      "Epoch 1/100 Batch 10/918 Loss: 3.700 Seconds: 39.61\n",
      "Epoch 1/100 Batch 11/918 Loss: 4.479 Seconds: 40.46\n",
      "Epoch 1/100 Batch 12/918 Loss: 4.340 Seconds: 40.09\n",
      "Epoch 1/100 Batch 13/918 Loss: 3.600 Seconds: 40.22\n",
      "Epoch 1/100 Batch 14/918 Loss: 3.583 Seconds: 40.17\n",
      "Epoch 1/100 Batch 15/918 Loss: 3.290 Seconds: 41.80\n",
      "Epoch 1/100 Batch 16/918 Loss: 3.469 Seconds: 39.47\n",
      "Epoch 1/100 Batch 17/918 Loss: 3.365 Seconds: 40.70\n",
      "Epoch 1/100 Batch 18/918 Loss: 3.322 Seconds: 40.79\n",
      "Epoch 1/100 Batch 19/918 Loss: 2.997 Seconds: 39.52\n",
      "Epoch 1/100 Batch 20/918 Loss: 3.099 Seconds: 39.73\n",
      "Epoch 1/100 Batch 21/918 Loss: 3.500 Seconds: 39.50\n",
      "Epoch 1/100 Batch 22/918 Loss: 3.233 Seconds: 37.83\n",
      "Epoch 1/100 Batch 23/918 Loss: 2.729 Seconds: 40.79\n",
      "Epoch 1/100 Batch 24/918 Loss: 2.901 Seconds: 40.21\n",
      "Epoch 1/100 Batch 25/918 Loss: 2.827 Seconds: 40.39\n",
      "Epoch 1/100 Batch 26/918 Loss: 2.936 Seconds: 39.74\n",
      "Epoch 1/100 Batch 27/918 Loss: 3.070 Seconds: 39.76\n",
      "Epoch 1/100 Batch 28/918 Loss: 2.960 Seconds: 40.20\n",
      "Epoch 1/100 Batch 29/918 Loss: 2.711 Seconds: 37.76\n",
      "Epoch 1/100 Batch 30/918 Loss: 2.841 Seconds: 40.27\n",
      "Epoch 1/100 Batch 31/918 Loss: 2.630 Seconds: 39.88\n",
      "Epoch 1/100 Batch 32/918 Loss: 2.457 Seconds: 39.71\n",
      "Epoch 1/100 Batch 33/918 Loss: 2.280 Seconds: 38.45\n",
      "Epoch 1/100 Batch 34/918 Loss: 2.694 Seconds: 42.05\n",
      "Epoch 1/100 Batch 35/918 Loss: 2.605 Seconds: 48.34\n",
      "Epoch 1/100 Batch 36/918 Loss: 2.287 Seconds: 46.52\n",
      "Epoch 1/100 Batch 37/918 Loss: 2.743 Seconds: 43.27\n",
      "Epoch 1/100 Batch 38/918 Loss: 2.479 Seconds: 42.69\n",
      "Epoch 1/100 Batch 39/918 Loss: 2.708 Seconds: 40.92\n",
      "Epoch 1/100 Batch 40/918 Loss: 2.772 Seconds: 42.86\n",
      "Epoch 1/100 Batch 41/918 Loss: 2.440 Seconds: 39.57\n",
      "Epoch 1/100 Batch 42/918 Loss: 2.538 Seconds: 40.99\n",
      "Epoch 1/100 Batch 43/918 Loss: 2.343 Seconds: 43.96\n",
      "Epoch 1/100 Batch 44/918 Loss: 2.578 Seconds: 42.12\n",
      "Epoch 1/100 Batch 45/918 Loss: 2.571 Seconds: 41.66\n",
      "Epoch 1/100 Batch 46/918 Loss: 2.226 Seconds: 41.74\n",
      "Epoch 1/100 Batch 47/918 Loss: 2.497 Seconds: 39.03\n",
      "Epoch 1/100 Batch 48/918 Loss: 2.248 Seconds: 42.77\n",
      "Epoch 1/100 Batch 49/918 Loss: 2.559 Seconds: 41.90\n",
      "Epoch 1/100 Batch 50/918 Loss: 2.784 Seconds: 40.29\n",
      "Epoch 1/100 Batch 51/918 Loss: 2.482 Seconds: 42.10\n",
      "Epoch 1/100 Batch 52/918 Loss: 2.305 Seconds: 42.40\n",
      "Epoch 1/100 Batch 53/918 Loss: 2.507 Seconds: 41.39\n",
      "Epoch 1/100 Batch 54/918 Loss: 2.636 Seconds: 40.98\n",
      "Epoch 1/100 Batch 55/918 Loss: 2.016 Seconds: 40.89\n",
      "Epoch 1/100 Batch 56/918 Loss: 2.374 Seconds: 42.97\n",
      "Epoch 1/100 Batch 57/918 Loss: 2.636 Seconds: 42.55\n",
      "Epoch 1/100 Batch 58/918 Loss: 2.580 Seconds: 42.05\n",
      "Epoch 1/100 Batch 59/918 Loss: 2.519 Seconds: 42.22\n",
      "Epoch 1/100 Batch 60/918 Loss: 2.480 Seconds: 40.95\n",
      "Epoch 1/100 Batch 61/918 Loss: 2.399 Seconds: 41.00\n",
      "Epoch 1/100 Batch 62/918 Loss: 2.179 Seconds: 40.44\n",
      "Epoch 1/100 Batch 63/918 Loss: 2.408 Seconds: 41.48\n",
      "Epoch 1/100 Batch 64/918 Loss: 2.232 Seconds: 40.13\n",
      "Epoch 1/100 Batch 65/918 Loss: 2.455 Seconds: 40.30\n",
      "Epoch 1/100 Batch 66/918 Loss: 2.758 Seconds: 40.77\n",
      "Epoch 1/100 Batch 67/918 Loss: 2.490 Seconds: 38.22\n",
      "Epoch 1/100 Batch 68/918 Loss: 2.559 Seconds: 40.04\n",
      "Epoch 1/100 Batch 69/918 Loss: 2.231 Seconds: 40.55\n",
      "Epoch 1/100 Batch 70/918 Loss: 2.495 Seconds: 38.16\n",
      "Epoch 1/100 Batch 71/918 Loss: 2.547 Seconds: 36.09\n",
      "Epoch 1/100 Batch 72/918 Loss: 2.685 Seconds: 45.83\n",
      "Epoch 1/100 Batch 73/918 Loss: 2.252 Seconds: 43.05\n",
      "Epoch 1/100 Batch 74/918 Loss: 2.022 Seconds: 41.69\n",
      "Epoch 1/100 Batch 75/918 Loss: 2.915 Seconds: 41.16\n",
      "Epoch 1/100 Batch 76/918 Loss: 2.618 Seconds: 38.48\n",
      "Epoch 1/100 Batch 77/918 Loss: 2.850 Seconds: 57.84\n",
      "Epoch 1/100 Batch 78/918 Loss: 2.478 Seconds: 53.77\n",
      "Epoch 1/100 Batch 79/918 Loss: 2.142 Seconds: 46.31\n",
      "Epoch 1/100 Batch 80/918 Loss: 2.503 Seconds: 49.52\n",
      "Epoch 1/100 Batch 81/918 Loss: 2.409 Seconds: 41.11\n",
      "Epoch 1/100 Batch 82/918 Loss: 2.360 Seconds: 39.15\n",
      "Epoch 1/100 Batch 83/918 Loss: 2.281 Seconds: 41.48\n",
      "Epoch 1/100 Batch 84/918 Loss: 2.361 Seconds: 42.49\n",
      "Epoch 1/100 Batch 85/918 Loss: 2.468 Seconds: 41.92\n",
      "Epoch 1/100 Batch 86/918 Loss: 2.091 Seconds: 38.82\n",
      "Epoch 1/100 Batch 87/918 Loss: 2.217 Seconds: 41.22\n",
      "Epoch 1/100 Batch 88/918 Loss: 2.353 Seconds: 40.66\n",
      "Epoch 1/100 Batch 89/918 Loss: 2.360 Seconds: 41.75\n",
      "Epoch 1/100 Batch 90/918 Loss: 2.396 Seconds: 42.88\n",
      "Epoch 1/100 Batch 91/918 Loss: 2.546 Seconds: 42.41\n",
      "Epoch 1/100 Batch 92/918 Loss: 2.222 Seconds: 45.08\n",
      "Epoch 1/100 Batch 93/918 Loss: 2.406 Seconds: 46.96\n",
      "Epoch 1/100 Batch 94/918 Loss: 2.782 Seconds: 42.56\n",
      "Epoch 1/100 Batch 95/918 Loss: 2.113 Seconds: 44.30\n",
      "Epoch 1/100 Batch 96/918 Loss: 2.465 Seconds: 56.63\n",
      "Epoch 1/100 Batch 97/918 Loss: 2.426 Seconds: 63.73\n",
      "Epoch 1/100 Batch 98/918 Loss: 2.551 Seconds: 51.47\n",
      "Epoch 1/100 Batch 99/918 Loss: 2.270 Seconds: 46.74\n",
      "Epoch 1/100 Batch 100/918 Loss: 2.302 Seconds: 45.89\n",
      "Epoch 1/100 Batch 101/918 Loss: 2.228 Seconds: 40.94\n",
      "Epoch 1/100 Batch 102/918 Loss: 2.181 Seconds: 42.57\n",
      "Epoch 1/100 Batch 103/918 Loss: 2.254 Seconds: 41.16\n",
      "Epoch 1/100 Batch 104/918 Loss: 2.430 Seconds: 40.29\n",
      "Epoch 1/100 Batch 105/918 Loss: 2.551 Seconds: 39.55\n",
      "Epoch 1/100 Batch 106/918 Loss: 2.580 Seconds: 42.72\n",
      "Epoch 1/100 Batch 107/918 Loss: 2.259 Seconds: 42.44\n",
      "Epoch 1/100 Batch 108/918 Loss: 2.252 Seconds: 45.04\n",
      "Epoch 1/100 Batch 109/918 Loss: 2.201 Seconds: 54.81\n",
      "Epoch 1/100 Batch 110/918 Loss: 2.387 Seconds: 41.14\n",
      "Epoch 1/100 Batch 111/918 Loss: 2.387 Seconds: 40.90\n",
      "Epoch 1/100 Batch 112/918 Loss: 2.649 Seconds: 40.20\n",
      "Epoch 1/100 Batch 113/918 Loss: 2.373 Seconds: 41.44\n",
      "Epoch 1/100 Batch 114/918 Loss: 2.369 Seconds: 41.21\n",
      "Epoch 1/100 Batch 115/918 Loss: 1.981 Seconds: 40.85\n"
     ]
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "train(\n",
    "    session, cost, EPOCHS, train_questions, train_answers,\n",
    "    BATCH_SIZE, LEARNING_RATE, KEEP_PROBABILITY, 2.0,\n",
    "    question_wid_map, answer_wid_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Chat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the trained `answer_logits` to answer my questions. I built a function called `ask` which only takes one parameter, the question string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ask(question, session=session, question_wid_map=question_wid_map,\n",
    "        answer_wid_map_r=answer_wid_map_r,\n",
    "        inference_logits=inference_logits, batch_size=BATCH_SIZE):\n",
    "    \n",
    "    question_seq = question_to_seq(question, question_wid_map)\n",
    "    question_seq += [question_wid_map[\"<PAD>\"]] * (MAX_LINE_LENGTH - len(question_seq))\n",
    "    batch_shell = np.zeros((batch_size, MAX_LINE_LENGTH))\n",
    "    batch_shell[0] = question_seq\n",
    "    \n",
    "    # Run the model\n",
    "    answer_logits = session.run(\n",
    "        inference_logits, {\n",
    "            ids: batch_shell,\n",
    "            keep_prob: 1.0\n",
    "        }\n",
    "    )[0]\n",
    "\n",
    "    pad = answer_wid_map[\"<PAD>\"]\n",
    "    return \" \".join([answer_wid_map_r[i]\n",
    "                     for i in np.argmax(answer_logits, 1)\n",
    "                     if i != pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def question_to_seq(question, word_to_int):\n",
    "    question = clean_text(question)\n",
    "    return [\n",
    "        word_to_int.get(word, word_to_int[\"<UNK>\"])\n",
    "        for word in question.split()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am <UNK> <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(ask(\"how are you\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By increasing the size of the dataset set, and our computation power. We can definitely build a chatbot that appeared in [Black Mirror](https://en.wikipedia.org/wiki/Be_Right_Back) soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Cornell Movie Dialogs Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
    "- [A ten-minute introduction to sequence-to-sequence learning in Keras](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)\n",
    "- [Sequence to sequence model: Introduction and concepts](https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d)\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Tensorflow](https://www.tensorflow.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
