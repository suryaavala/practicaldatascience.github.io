{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "In this tutorial we're going to write a (very minimal) web crawl using Scrapy, which is a very hot and convenient Python scraping tool, here I will talk much more details than the documents shows, cause I would include some industrail tricks when using this smart scraping tool, talking about the whole scraping process, encoding issue, Xpath skills and some tricks dealing with IP forbidden.\n",
    "\n",
    "Here we need to note that it is not the first choice to run a Scrapy web crawler on Jupyter Notebook, so here I just write how to run it, if you want to start your own project, just start a local Scrapy project and copy my code in it.\n",
    "\n",
    "The example website is: http://www.imdb.com/chart/top?ref_=nv_mv_250_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My understanding of Scrpay Machenism\n",
    "\n",
    "Scrapy uses **Twisted** as a framework. **Twisted** is special because it is event-driven and is more suitable for asynchronous code. Operations that block threads include accessing files, databases, or the Web, generating new processes and processing the output of new processes (such as running shell commands), and executing system-level code (such as waiting for system queues). **Twisted** provides permission to execute above operation and does not block the code execution method.\n",
    "\n",
    "Here are the process sequence from what I understand:\n",
    "    1. The engine gets its initial request to start crawling.\n",
    "    2. The engine starts requesting the scheduler and prepares to crawl the next request.\n",
    "    The crawler scheduler returns the next request to the crawler engine.\n",
    "    4. The engine request is sent to the downloader and the web data is downloaded by downloading middleware.\n",
    "    5. Once the downloader completes the page download, the download result is returned to the engine.\n",
    "    6. The engine returns the downloader's response to the spider for processing through the middleware.\n",
    "    7. The spider responds and returns the processed items through the middleware and new requests to the engine.\n",
    "    8. The engine sends the processed items to the project pipeline, and then returns the processing result to the scheduler. The scheduler plans to process the next request.\n",
    "    9. Repeat the process (continue to step 1) until all url requests are crawled.\n",
    "    \n",
    "![alt text](scrapy.png \"Title\")\n",
    "*image citation: https://blog.csdn.net/yancey_blog/article/details/53888473*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Scrapy\n",
    "First of all, you need to set up Scrapy, just follow the official website, you can use native *pip* but I strongly recommend using *Anaconda* package, which is \n",
    "    > $ conda install -c anaconda scrapy\n",
    "    \n",
    "After installation, we can initial a project:\n",
    "    > $ scrapy startproject [tutorial] #[tutorial] is the name of your project.  \n",
    "    \n",
    "And then you will find a directory as following, and I attach the explanations:\n",
    "    tutorial/\n",
    "        scrapy.cfg            # deploy configuration file\n",
    "\n",
    "        tutorial/             # project's Python module, you'll import your code from here\n",
    "            __init__.py\n",
    "\n",
    "            items.py          # project items definition file\n",
    "\n",
    "            middlewares.py    # project middlewares file\n",
    "\n",
    "            pipelines.py      # project pipelines file\n",
    "\n",
    "            settings.py       # project settings file\n",
    "\n",
    "            spiders/          # a directory where you'll later put your spiders\n",
    "                __init__.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xpath introduction\n",
    "To write a web crawler, we need to navigate items in HTML page, here we use Xpath language, XPath is a language for selecting nodes in XML documents, which can also be used with HTML, in Xpath we may use CSS items, which define selectors to associate those styles with specific HTML elements. Scrapy has its own selector, Scrapy selectors are built over the lxml library, which means theyâ€™re very similar in speed and parsing accuracy.\n",
    "\n",
    "Here we will go through a Scrapy selector example to get familar with Xpath opertion in Scrapy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good']\n",
      "['Hello', 'World']\n",
      "good\n",
      "['HellogoodWorld']\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "body = '<html><body>Hello<span>good</span>World</body></html>'\n",
    "print (Selector(text=body).xpath('//span/text()').extract())\n",
    "print (Selector(text=body).xpath('//body/text()').extract())\n",
    "print (Selector(text=body).xpath('//span/text()').extract_first())\n",
    "print (Selector(text=body).xpath('string(//body)').extract())\n",
    "print (Selector(text=body).xpath('string(//span)').extract_first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above example, there are some tricks:\n",
    "    1. text() would retrive all texts in their seperated form, while string() would combine them together.\n",
    "    2. extract() returns a list, extract_first() returns the first element of all items which are searched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a spider\n",
    "Ok, after builing a Scrpy framework, we need to write a spider to scraping something:\n",
    "    > $ scrapy genspider [name] [domain]\n",
    "Here we generate a spider named **crawl_data.py**, the Python script is as following.\n",
    "We want to crawl all top rated movies' information including: title, url, year, rate points, director and introduction. Then get all these information into json files, one movie for one json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from urllib2 import urlopen\n",
    "import re\n",
    "\n",
    "# ImdbItem is a class of dictionary, its object is used to transmit data from spider to pipeline.\n",
    "from imdb.items import ImdbItem\n",
    "\n",
    "\n",
    "class CrawlDataSpider(scrapy.Spider):\n",
    "    name = 'crawl_data'\n",
    "    \n",
    "    # All scraping web cannot escape from these domains\n",
    "    allowed_domains = ['http://www.imdb.com/']\n",
    "    \n",
    "    # We can define initial starting urls here, it is a list, so we may have a list of entries.\n",
    "    start_urls = ['http://www.imdb.com/chart/top?ref_=nv_mv_250_6'\n",
    "                  ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for url in response.selector.xpath('//tbody[@class=\"lister-list\"]/tr/td/a/@href'):\n",
    "            # print url\n",
    "            url = response.urljoin(url.extract())\n",
    "            # Just go to next layer.\n",
    "            yield scrapy.Request(url, self.parse_d1)\n",
    "\n",
    "    def parse_next(self, response):\n",
    "        item = ImdbItem()\n",
    "        \n",
    "        url = response.url\n",
    "        \n",
    "        year = response.selector.xpath(\n",
    "            'string(//div[@class=\"titleBar\"]/div[@class=\"title_wrapper\"]/h1[@class=\"\"]/span/a)').extract_first()\n",
    "        \n",
    "        introduction = response.selector.xpath(\n",
    "            'string(//div[@class=\"plot_summary_wrapper\"]/div[@class=\"plot_summary \"]/div[@class=\"summary_text\"])').extract_first()\n",
    "        \n",
    "        title = response.selector.xpath(\n",
    "            '//div[@class=\"titleBar\"]/div[@class=\"title_wrapper\"]/h1[@class=\"\"]/text').extract_first()\n",
    "        \n",
    "        rate_points = response.selector.xpath(\n",
    "            'string(div[@class=\"ratingValue\"]/strong/span)').extract_first()\n",
    "        \n",
    "        director = response.selector.xpath(\n",
    "            'string(//div[@class=\"plot_summary \"]/div[@class=\"credit_summary_item\"]/span[@itemprop=\"director\"]/a/span)').extract_first()\n",
    "        \n",
    "        item['url'] = response.url\n",
    "        item['title'] = title\n",
    "        item['introduction'] = CrawlDataSpider.cleanup(introduction)\n",
    "        item['rate_points'] = rate_points\n",
    "        item['director'] = director\n",
    "        \n",
    "        return item\n",
    "\n",
    "\n",
    "    # This static method is used to clean the raw text, eliminating space or new_line_break.\n",
    "    @staticmethod\n",
    "    def cleanup(text):\n",
    "        if text is None:\n",
    "            return ''\n",
    "        tmp0 = re.sub(' +', ' ', text)\n",
    "        tmp1 = tmp0.strip().replace('\\t', '\\n').replace('\\r', '\\n')\n",
    "        tmp2 = re.sub('\\n +', '\\n', tmp1)\n",
    "        tmp3 = re.sub('\\n+', '\\n', tmp2)\n",
    "        return tmp3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## items.py\n",
    "Since we have defined the spider, we would define the data we want to send in pipeline, the item in **items.py** should be consestent with those in spider file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ImdbItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    introduction = scrapy.Field()\n",
    "    director = scrapy.Field()\n",
    "    rate_points = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipelines.py\n",
    "We also need to define our pipeline operation, after data is scraped and pre-processed in spider, we can do some storage operation in **piplelines.py**, either store them in database or just store them as local files. Here when output text into json files, we need to pay attention that the encoding type:\n",
    "    1. obviously utf-8 as encoding\n",
    "    2. decline ascii encoding, sometimes even you set utf-8 but the system still recognize ascii encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import codecs\n",
    "import json\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "class ImdbPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "        name = item['title']\n",
    "        path = 'json_file/' + name + '.json'\n",
    "        with codecs.open(path, 'w+', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(dict(item), ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings.py\n",
    "The Scrapy settings allows you to customize the behaviour of all Scrapy components, including the core, extensions, pipelines and spiders themselves. Here are some frequently useful settings:\n",
    "    1. ROBOTSTXT_OBEY: whether to obey web crawler robot rule, if you want to make it do as what you want, choose False.\n",
    "    2. DOWNLOAD_DELAY: the delay time between two requests, usually you need to set it arount 0.5s, if too freqent, your IP may be banned by server.\n",
    "    3. COOKIES_ENABLED, DEFAULT_REQUEST_HEADERS: whether to use self-defined cookies, which is key when avoiding ban from server, you can have mulytipul cookieses at one time and return one randomly for each request.\n",
    "    4. DOWNLOADER_MIDDLEWARES: whether to use self-defined download middleware.\n",
    "    5. ITEM_PIPELINES: whether to use pipeline, if you want some storage operations in \n",
    "    6. HTTPCACHE_ENABLED: whether to cache the original HTML files, since cached, each time scraping a html the system only need to process local HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Scrapy settings for imdb project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     http://doc.scrapy.org/en/latest/topics/settings.html\n",
    "#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html\n",
    "#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'imdb'\n",
    "\n",
    "SPIDER_MODULES = ['imdb.spiders']\n",
    "NEWSPIDER_MODULE = 'imdb.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'imdb (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = True\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "DEFAULT_REQUEST_HEADERS = {\n",
    "  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "  'Accept-Language': 'en',\n",
    "}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'imdb.middlewares.ImdbSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
    "    'imdb.middlewares.MyCustomDownloaderMiddleware': 543,\n",
    "}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "   'imdb.pipelines.ImdbPipeline': 300,\n",
    "}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See http://doc.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "HTTPCACHE_ENABLED = True\n",
    "HTTPCACHE_EXPIRATION_SECS = 0\n",
    "HTTPCACHE_DIR = 'httpcache'\n",
    "HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## middlewares.py\n",
    "Middlewares in Scrapy can be seen as two seperated parts: Spider Middlewares and Downloader Middlewares, one responsible customely define spider processing mechanism such as request or response; the other is to do customer modifity when download HTML files, most time we may not use these middlewares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user-agent\n",
    "User-agent is an important parameter for us to simulate the browser, mainly to prevent reptiles from being ban. In the previous chapters we learned that user-agent can be set in settings.py, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this situation still has a potential ban situation. In addition, we can increase the delay in crawling, which can also reduce the risk of being banned. Of course, these are relatively simple camouflage techniques. It is enough to use as a crawler.\n",
    "Then we set up more user-agent to simulate the browser to download web data, and randomly set a user-agent each time we download it, which is less likely to be banned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-\n",
    "\n",
    "\n",
    "import random\n",
    "from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware\n",
    "\n",
    "\n",
    "class RotateUserAgentMiddleware(UserAgentMiddleware):\n",
    "    def __init__(self, user_agent=''):\n",
    "        self.user_agent = user_agent\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        ua = random.choice(self.user_agent_list)\n",
    "        if ua:\n",
    "            # print(ua)\n",
    "            request.headers.setdefault('User-Agent', ua)\n",
    "\n",
    "    # the default user_agent_list composes chrome,I E,firefox,Mozilla,opera,netscape\n",
    "    # for more user agent strings,you can find it in http://www.useragentstring.com/pages/useragentstring.php\n",
    "    user_agent_list = [ \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\" \\\n",
    "        \"Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1\", \\\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to change middlewares setting in **settings.py**, just like following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
    "    'imdb.middlewares.MyCustomDownloaderMiddleware': 543,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Run\n",
    "At last, in terminal we type:\n",
    "    >scrapy crawl [spider_name]\n",
    "    \n",
    "So our spider is starting crawling web pages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run Scrapy on Jupyter Notebook?\n",
    "\n",
    "Create a new notebook and use CrawlerProcess or CrawlerRunner classes to run in a cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scrapy.crawler import CrawlerProcess\n",
    "# from scrapy.utils.project import get_project_settings\n",
    "\n",
    "# process = CrawlerProcess(get_project_settings())\n",
    "# \n",
    "# process.crawl('your-spider')\n",
    "# process.start() # the script will block here until the crawling is finished\n",
    "# # citation: https://stackoverflow.com/a/45341285/8299533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deploy your web crawler to cloud\n",
    "\n",
    "### Distributing crawlers: scrapy-redis-AWS\n",
    "\n",
    "The combination of scrapy and redis, with multiple hosts to build a distributed crawler development environment, if the reptile advanced development of distributed crawler is very necessary.\n",
    "\n",
    "### Scrapinghub\n",
    "\n",
    "To deploy your web crawler to cloud, most people would think AWS or GCP, however there is a professional scraping cloud service: Scrapinghub, https://scrapinghub.com/scrapy-cloud. Scrapy Cloud removes the need to setup and monitor servers and provides a nice UI to manage spiders and review scraped items, logs and stats. Of course you can use IP address agent to avoid IP ban from server.\n",
    "\n",
    "*citation: https://doc.scrapy.org/en/latest/topics/logging.html*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. How to avoid IP from being banned\n",
    "\n",
    "    1. Construct a Reasonable HTTP Request Header\n",
    "    2. set cookie learning\n",
    "    3. Normal time access path Delayed access\n",
    "    4. Note Implicit Input Field Values\n",
    "    5. How do reptiles generally avoid honeypots?\n",
    "    6. Use of Variable Remote IP Address Use of Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python3.5)",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
