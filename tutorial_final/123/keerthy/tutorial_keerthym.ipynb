{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Feature Selection Methods available in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will see select methods for feature selection using scikit-learn library. Feature Selection is the process of selecting the most relevant features based on various constraints that would contribute most to the output we are interested in. Having a good set of features can improve the accuracy of models manifold, especially when the dataset is large. Feature Selection is one of the important jobs while approaching a machine learning problem. The classifiers you write will be only as good as the features you provide. \n",
    "\n",
    "\n",
    "The process of feature selection enables one to think like a classifier. What data is useful to me to classify the input set? We can dump any unuseful data. Another point to keep in mind is to use independent features that give you different types of information about the data.Remove highly correlated data as they may lead to overfitting the model and hurt the accuracy.\n",
    "\n",
    "<img src =\"http://3.bp.blogspot.com/-Awzj2haNuko/UMkonIL1c5I/AAAAAAAAATw/6ayxi-F15d4/s1600/feature_selection.png\">\n",
    "\n",
    "\n",
    "Source : http://3.bp.blogspot.com/-Awzj2haNuko/UMkonIL1c5I/AAAAAAAAATw/6ayxi-F15d4/s1600/feature_selection.png\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics Covered:\n",
    "   In this tutorial, we will see a few methods to preform feature selection in Python, specifically using scikit-learn library. While Feature Extraction or Feature Engineering is a very intricate task in itself, we will not be focusing on how to engineer features in this tutorial. We will review few ways of selecting features in a given dataset and the ideology behind it. \n",
    "   \n",
    "We will cover the following topics in this tutorial:\n",
    "-  [Installing the libraries](#Installing-the-libraries)\n",
    "-  [Univariate Feature Selection](#Univariate-Feature-Selection)\n",
    "-  [Recursive Feature Elimination](#Recursive-Feature-Elimination)\n",
    "-  [Feature Selection Using SelectFromModel and LassoCV](#Feature-Selection-Using-SelectFromModel-and-LassoCV)\n",
    "-  [Removing Features with Low Variance using VarianceThreshold](#Removing-Features-with-Low-Variance-using-VarianceThreshold)\n",
    "-  [Example Demonstration](#Example-Demonstration)\n",
    "\n",
    "In all of my examples, I use a regression dataset but most of them are applicable on a classification dataset as well.For demonstration purposes, we will be using the Iris dataset or the Boston Housing Dataset. Finally, an example using the Vehicle Data Set with the Features we added as a part of HW3 will be used to demonstrate a few methods discussed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the libraries\n",
    "\n",
    "Scikit-learn requires:\n",
    "- Python (>= 2.7 or >= 3.3),\n",
    "- NumPy (>= 1.8.2),\n",
    "- SciPy (>= 0.13.3).\n",
    "\n",
    "Using pip: \n",
    "pip install -U scikit-learn\n",
    "\n",
    "or conda:\n",
    "\n",
    "conda install scikit-learn\n",
    "\n",
    "If you have Anaconda already installed, you do not need to perform the above step. A recent version of scikit-learn gets installed during the installation of Anaconda. After installation, please ensure the following code works for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.feature_selection import SelectPercentile,f_classif, f_regression,SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Univariate Feature Selection\n",
    "    \n",
    "This method is widely used for understanding the data better. This will not necessarily help better your generalization but will give you a fair idea of how the features are correlated to the target variable.\n",
    "    \n",
    "Scikit-learn provides f_regression method for computing the p-values.It takes one input variable and one output variable at a time and calculates the correlation in the form of p-values(also f-values).Lower p-value means that the correlation is strong, hence (generally) not a good feature to be included in your model and high p-value means that the correlation is purely due to chance.For classification,a good test is the Chi-square test.It is intended to test how likely it is that an observed distribution is due to chance. \n",
    "\n",
    "SciKit-learn provides feature selection classes whose objects has a transform method:\n",
    "- SelectKBest - keeps only those features that recevie the top k scores.\n",
    "- SelectPercentile -keeps only those features that score highest percentage of features\n",
    "- GenericUnivariateSelect -This is a configurable routine that takes in the mode,a number for modes that needs a parameter and a score function as parameters.\n",
    "\n",
    "These functions help with univariate statistics on each feature:\n",
    " \n",
    "- SelectFwe -used to select based on Family-wise Error\n",
    "- SelectFdr -used to select based on False Discovery Rate\n",
    "- SelectFpr -used to select based on False Positive Rate\n",
    "\n",
    "We will see an example of SelectPercentile using f_classif as the scoring function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature vector shape:(150, 4) \n",
      "After introducing noise data, shape:(150, 24) \n",
      "After performing feature selection, shape:(150, 3) \n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "#intiial size of the independent variables\n",
    "print(\"Original feature vector shape:%s \"  %str(iris.data.shape))\n",
    "\n",
    "# creating 20 columns of noise\n",
    "E = np.random.uniform(0, 0.1, size=(len(iris.data), 20))\n",
    "\n",
    "# Add the noisy data to the exisitng features\n",
    "X = np.hstack((iris.data, E))\n",
    "\n",
    "print(\"After introducing noise data, shape:%s \"  %str(X.shape))\n",
    "\n",
    "#response variable y\n",
    "y = iris.target \n",
    "\n",
    "\n",
    "# Univariate feature selection with F-test for feature scoring\n",
    "# We use the default selection function: the 10% most significant features\n",
    "selector = SelectPercentile(f_regression, percentile=10)\n",
    "fitted =selector.fit(X, y)\n",
    "new_set =selector.transform(X)\n",
    "print(\"After performing feature selection, shape:%s \"  %str(new_set.shape))\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above example demonstrates how the features have been removed with the SelectPercentile function with regression based on f distributon.We will see the scores of all the 24 features below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.33838996e+02   3.15975083e+01   1.34215919e+03   1.58955920e+03\n",
      "   1.76919703e+00   3.27990967e-02   5.58612793e+00   5.18552784e-01\n",
      "   8.24292783e-01   2.00030571e+00   3.38748966e-01   1.08472512e-01\n",
      "   1.81513098e+00   2.82386345e-01   2.11526376e-02   5.76839730e-01\n",
      "   8.05795666e-02   2.89174233e+00   1.83049454e-01   2.72182330e-01\n",
      "   2.79699763e-01   5.68928128e-02   5.25167697e-01   3.94660844e-01]\n",
      "[[ 5.1  1.4  0.2]\n",
      " [ 4.9  1.4  0.2]\n",
      " [ 4.7  1.3  0.2]\n",
      " [ 4.6  1.5  0.2]\n",
      " [ 5.   1.4  0.2]]\n"
     ]
    }
   ],
   "source": [
    "#prints the scores of each feature fed as X\n",
    "print(selector.scores_)\n",
    " \n",
    "#After fitting, the values of the input variable array\n",
    "print(new_set[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "The methodology is Recursive Feature Elimination(RFE) which, as the name suggests, recursively removes features based on the given set of rules and   a supervised learning estimator with fit method . The estimator is first trained on the dataset and weights are assigned to each feature. Recursively, those features that have the smallest weights are eliminated.We can specify the number of features to be removed at each iteration. The importance of each feature is obtained either through a coefficient of the features in the decision function or through a feature_importances_attribute.\n",
    " \n",
    "Recursive Feature Elimination with Cross Validation provides support for feature ranking and cross-validated selection of the best number of features.The fold can be specified as a parameter. It will recursively eliminate features untill it reaches the best cross validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris=datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "names=iris.feature_names\n",
    "names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the feature selection model:0.966666666667\n",
      "\n",
      "Optimal number of features 4\n",
      "\n",
      "Array indicating the features selected. True if the feature is retained and False if eliminated: \n",
      "[ True  True  True  True]\n",
      "\n",
      "Ranking:\n",
      " [1 1 1 1]\n",
      "\n",
      "The cross-validation scores such that grid_scores_[i] corresponds to the CV score of the i-th subset of features. \n",
      "[ 0.91421569  0.94689542  0.95383987  0.96691176]\n",
      "\n",
      "There are 3 classes and 4 features. Below are the weights assigned by the estimator for each feature against each class\n",
      "\n",
      "Iris Plant classes:\n",
      "Iris Setosa\n",
      "Iris Versicolour\n",
      "Iris Virginica\n",
      "Features: \n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      "[[ 0.18423394  0.45122265 -0.80794137 -0.45071277]\n",
      " [ 0.05318082 -0.8913984   0.40302358 -0.9365123 ]\n",
      " [-0.85076916 -0.9864955   1.38091588  1.86543944]]\n"
     ]
    }
   ],
   "source": [
    "estimator=LinearSVC()\n",
    "m = RFECV(estimator=estimator,cv=3)\n",
    "m.fit(X,y)\n",
    "print(\"Score of the feature selection model:%s\"%m.score(X,y))\n",
    "print(\"\\nOptimal number of features %s\"%m.n_features_)\n",
    "print(\"\\nArray indicating the features selected. True if the feature is retained and False if eliminated: \\n%s\"%m.support_)\n",
    "print(\"\\nRanking:\\n %s\"%m.ranking_)\n",
    "print(\"\\nThe cross-validation scores such that grid_scores_[i] corresponds to the CV score of the i-th subset of features. \\n%s\"%m.grid_scores_) \n",
    "# for x,y in zip(names, m.estimator_.coef_):\n",
    "#     print(x,y)\n",
    "print(\"\\nThere are 3 classes and 4 features. Below are the weights assigned by the estimator for each feature against each class\")\n",
    "print(\"\\nIris Plant classes:\"+\"\\nIris Setosa\\nIris Versicolour\\nIris Virginica\")\n",
    "print(\"Features: \\n\" +str(names)+ \"\\n\")\n",
    "print(m.estimator_.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature Selection Using SelectFromModel and LassoCV\n",
    "\n",
    "Meta-transformer for selecting features based on importance weights.An important parameter of SelectFromModel function is the \"threshold\" parameter. The threshold is either a string or a float value. Those features whose value is below the threshold are removed. The threshold  may be a string {\"median\",\"mean\"}. Float multiples can also be given, which can be used for scaling purposes, eg. \"1.25*mean\".\n",
    "\n",
    "If you are not giving a threshold,the mean is used. If threshold is not passed but the estimator has a penalty set to l1,the threshold used is 1e-5. For example, LassoCV is linear regression with l1 penalty. If used, without passing a threshold valie, the threshold is set to 1e-5.\n",
    "\n",
    "We will see an example with SelectFromModel and LassoCV\n",
    "\n",
    "Here is a quick look at the Boston House Prices dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_boston()\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  target  \n",
       "0     15.3  396.90   4.98    24.0  \n",
       "1     17.8  396.90   9.14    21.6  \n",
       "2     17.8  392.83   4.03    34.7  \n",
       "3     18.7  394.63   2.94    33.4  \n",
       "4     18.7  396.90   5.33    36.2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df['target'] = dataset.target\n",
    "df.head()\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before feature selection: 13\n",
      "\n",
      "No of features after fitting using selectFromModel: 10\n",
      "\n",
      "\n",
      "Reducing the number of features to 2 by increasing the threshold iteratively:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAEWCAYAAAA9232qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/P97ZogeVXzRxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27e8552fe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y=df.target\n",
    "X=df.drop('target',1)\n",
    "print(\"Number of features before feature selection: %s\"%X.shape[1])\n",
    "\n",
    "m = SelectFromModel(LassoCV())\n",
    "\n",
    "m.fit(X, y)\n",
    "\n",
    "n_features = m.transform(X).shape[1]\n",
    "\n",
    "print(\"\\nNo of features after fitting using selectFromModel: %d\" % n_features)\n",
    "\n",
    "#setting the threshold for plotting purposes\n",
    "m.threshold=10 ** -5\n",
    " \n",
    "# Reset the threshold till the number of features equals two.\n",
    "# Note that the attribute can be set directly instead of repeatedly\n",
    "# fitting the meta-transformer.\n",
    "\n",
    "print(\"\\n\\nReducing the number of features to 2 by increasing the threshold iteratively:\")\n",
    "while n_features > 2:\n",
    "    m.threshold += 0.1\n",
    "    X_transform = m.transform(X)\n",
    "    n_features = X_transform.shape[1]\n",
    "    \n",
    "    \n",
    "# Plot the selected two features from X.\n",
    "plt.title(\"Features selected from Boston using SelectFromModel with threshold %0.2f.\" % m.threshold)\n",
    "feature1 = X_transform[:, 0]\n",
    "feature2 = X_transform[:, 1]\n",
    "\n",
    "#Plotting feature 1 as the input against feature 2 output\n",
    "plt.plot(feature1, feature2, 'r.')\n",
    "plt.xlabel(\"Feature number 1\")\n",
    "plt.ylabel(\"Feature number 2\")\n",
    "plt.ylim([np.min(feature2), np.max(feature2)])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Features with Low Variance using VarianceThreshold\n",
    "\n",
    "\n",
    "A simple way of removing features from a dataset is to specify a variance threshold and removing everything that is below the threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are the variances of the original feature vector: \n",
      "\n",
      "[  7.37586143e+01   5.42861840e+02   4.69714297e+01   6.43854770e-02\n",
      "   1.34010989e-02   4.92695216e-01   7.90792473e+02   4.42525226e+00\n",
      "   7.56665313e+01   2.83486236e+04   4.67772630e+00   8.31828042e+03\n",
      "   5.08939794e+01]\n",
      "\n",
      " Printing the support array. True indicates the feature is included and False indicates the feature is not retained:\n",
      "\n",
      "[ True  True  True False False  True  True  True  True  True  True  True\n",
      "  True]\n"
     ]
    }
   ],
   "source": [
    "boston= datasets.load_boston() \n",
    "X=boston.data\n",
    "y=boston.target\n",
    "\n",
    "#Define the feature selector with threshold\n",
    "sel = VarianceThreshold(threshold=(.2 * (1 - .2)))\n",
    "\n",
    "#Fit and Transform\n",
    "sel.fit(X)\n",
    "sel.transform(X)\n",
    "print(\"The following are the variances of the original feature vector: \\n\")\n",
    "print(sel.variances_)\n",
    "\n",
    "#An index that selects the retained features from a feature vector. \n",
    "#If indices is False, this is a boolean array of shape [# input features],\n",
    "#in which an element is True iff its corresponding feature is selected for retention. If indices is True, \n",
    "#this is an integer array of shape [# output features] whose values are indices into the input feature vector.\n",
    "print(\"\\n Printing the support array. True indicates the feature is included and False indicates the \"\n",
    "      \"feature is not retained:\\n\")\n",
    "print(sel.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example Demonstration\n",
    "\n",
    "In this section we will look at some of the above methods implemented for the Vehicle Data Frame(vdf) produced in homework 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vdf = pd.read_csv(\"vdf12.csv\")\n",
    "vdf.columns\n",
    "vdf.head()\n",
    "y=vdf['eta']\n",
    "X=vdf.drop('eta',1) \n",
    "#Using Linear Support Vector Classification as the estimator \n",
    "m = SelectFromModel(LinearSVC(C=0.01, penalty='l1', dual=False))\n",
    "m.fit(X, y)\n",
    "new_X=m.transform(X)\n",
    "n_features =new_X.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True  True  True False  True False False False False False\n",
      " False  True False  True False False False False False  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(m.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are selected when the estimator is a classifier. We know there cannot be class memberships in this dataset. We use Linear Regression as a estimator in the following example and find out which features were eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features selected:10\n",
      "\n",
      "pdist has been eliminated\n",
      "\n",
      "spd has been eliminated\n",
      "\n",
      "lat has been eliminated\n",
      "\n",
      "lon has been eliminated\n",
      "\n",
      "bias has been eliminated\n",
      "\n",
      "sin_hdg has been eliminated\n",
      "\n",
      "cos_hdg has been eliminated\n",
      "\n",
      "sin_day_of_week has been eliminated\n",
      "\n",
      "cos_day_of_week has been eliminated\n",
      "\n",
      "sin_hour_of_day has been eliminated\n",
      "\n",
      "cos_hour_of_day has been eliminated\n",
      "\n",
      "sin_time_of_day has been eliminated\n",
      "\n",
      "cos_time_of_day has been eliminated\n",
      "\n",
      "weekday has been eliminated\n",
      "\n",
      "Braddock  has been selected\n",
      "\n",
      "Downtown has been selected\n",
      "\n",
      "Greenfield Only has been selected\n",
      "\n",
      "McKeesport  has been selected\n",
      "\n",
      "Murray-Waterfront has been selected\n",
      "\n",
      "Swissvale has been selected\n",
      "\n",
      "61A has been selected\n",
      "\n",
      "61B has been selected\n",
      "\n",
      "61C has been selected\n",
      "\n",
      "61D has been selected\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vdf = pd.read_csv(\"vdf12.csv\")\n",
    "vdf.columns\n",
    "vdf.head()\n",
    "y=vdf['eta']\n",
    "X=vdf.drop('eta',1)\n",
    "lr = LinearRegression(normalize=True)\n",
    "m = SelectFromModel(lr)\n",
    "m.fit(X, y)\n",
    "new_X=m.transform(X)\n",
    "n_features =new_X.shape[1]\n",
    "print(\"Number of features selected:%d\" %n_features + \"\\n\")\n",
    "new_X[:2]\n",
    "# print(m.get_support())\n",
    "# print(X.columns)\n",
    "for x,y in zip(list(X.columns),m.get_support()):\n",
    "   \n",
    "    if(y==False):\n",
    "        print(x + \" has been eliminated\\n\")\n",
    "    else:\n",
    "        print(x + \" has been selected\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The threshold of elimination is:11991409907391 \n",
      "Coeffecient of determination of the linear regression model for each feature:\n",
      "[ -7.97209554e-04  -3.08965235e-02   9.21567243e+01  -2.31550442e+01\n",
      "  -1.15746689e+12   4.82981759e-01  -5.87753431e-01   4.85866179e-01\n",
      "  -1.16607934e-01   2.56513171e+00  -3.11938922e-01  -2.65402556e+00\n",
      "  -7.87712798e-01   1.05836074e+00  -3.63033749e+13  -3.63033749e+13\n",
      "  -3.63033749e+13  -3.63033749e+13  -3.63033749e+13  -3.63033749e+13\n",
      "   1.72040304e+13   1.72040304e+13   1.72040304e+13   1.72040304e+13]\n"
     ]
    }
   ],
   "source": [
    "print(\"The threshold of elimination is:%d \" %m.threshold_)\n",
    "print(\"Coeffecient of determination of the linear regression model for each feature:\\n\" + str(m.estimator_.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, only those columns that are binary indicators of the route and destination were selected using the above model. R squared value of these columns where higher than the threshold. \n",
    "Let us examine the use of a Univariate Method using  f_regression. Univariate methods are most useful to regress each input against the output and get a better understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.05792308e+04,   2.72499367e+03,   1.97052988e+04,\n",
       "         1.97030195e+04,   1.97144615e+04,   1.18276301e+03,\n",
       "         7.90666359e+03,   2.05607309e+01,   1.25284031e+03,\n",
       "         1.51101663e+02,   4.02071321e+03,   1.96252312e+01,\n",
       "         4.07588700e+03,   1.55899087e+04,   6.07763141e+01,\n",
       "         2.27921169e+04,   1.05281735e+02,   7.17238874e+01,\n",
       "         6.00360629e+01,   3.80777881e+01,   1.22410896e+03,\n",
       "         4.85682997e+03,   5.60049393e+03,   4.32455805e+03])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "y=vdf['eta']\n",
    "X=vdf.drop('eta',1)\n",
    "X=normalize(X)\n",
    "#By Default uses 10 best features based on the f statistic\n",
    "m=SelectKBest(f_regression)\n",
    "m.fit(X,y)\n",
    "m.transform(X)\n",
    "m.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pdist', 'lat', 'lon', 'bias', 'cos_hdg', 'weekday', 'Downtown', '61B', '61C', '61D']\n"
     ]
    }
   ],
   "source": [
    "#Reference https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le\n",
    "vdf=vdf.drop('eta',axis=1)\n",
    "feature_names = list(vdf.columns.values)\n",
    "mask = m.get_support() #list of booleans\n",
    "new_features = [] # The list of your K best features\n",
    "\n",
    "for bool, feature in zip(mask, feature_names):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Summary and References:\n",
    "This tutorial demonstrates few combinations of the feature selection models and regressor classifiers and important attributes that these functions exposes. Following links and user guide were helpful in learning these methods:\n",
    "\n",
    "1. scikit-learn user guide : http://scikit-learn.org/stable/_downloads/scikit-learn-docs.pdf \n",
    "2. Selecting good features – Part IV : http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/ \n",
    "3. Feature Selection: https://github.com/knathanieltucker/bit-of-data-science-and-scikit-learn/blob/master/notebooks/FeatureSelection.ipynb\n",
    "4. Official Documentation:http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection \n",
    "5. Reference https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le\n",
    "\n",
    "\n",
    "Scikit-Learn citation:\n",
    "Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
