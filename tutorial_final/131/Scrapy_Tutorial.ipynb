{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Data collection is the first step of data science work, which is essential since it's the source for further manipulation and analysis on data. The efficiency and functionality of the ways to collect data is what a data scientist cares about when assembling data from web sources. Thus, it's very important to choose a proper method and tool for this part.\n",
    "\n",
    "This tutorial is going to introduce a data collection library, Scrapy. Most of us are already familiar with useful web requesting and parsing libraries like BeautifulSoup, etc. Compared to these libraries, Scrapy is a crawler with full framework, providing functions from requesting, parsing, storing, processing to exporting data. \n",
    "\n",
    "Such complete data collection progress realized by Scrapy is supported by its well-structured architecture. Take a look at the picture: https://www.dropbox.com/s/zmysnamlzqbd3rt/scrapy_architecture_02.png?dl=0. \n",
    "\n",
    "1. Engine: This is the core processing unit of Scrapy. It manages work flows between other components.\n",
    "2. Spiders: As its name states, this component send requests to crawl and parse reponses.\n",
    "3. Item Pipelines: When talking about Item Pipelines, Item should be explained first. Item is like a container, holding the data extracted. Items are then transported to Item pipelines, where data is processed and exported to some database.\n",
    "4. Downloader: It takes charge of fetching the webs and feed them to the engine, then to spiders.\n",
    "\n",
    "In this tutorial, an example to collect lyrics and other information of songs from http://www.metrolyrics.com/ will help understand how each component of Scrapy works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "\n",
    "[Install Scrapy](#install)\n",
    "\n",
    "[Write a Scrapy project](#write)\n",
    "   \n",
    "[Advantages of Scrapy](#advantages)\n",
    "\n",
    "[Practice experience](#experience)\n",
    "\n",
    "[Reference and further exploration](#more)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='install'></a>\n",
    "# Install Scrapy \n",
    "To get prepared to use scrapy, the first step is to install it. There are many ways for different platforms. \n",
    "With Anaconda installed on my PC, it's quite convenient to choose Scrapy from packages avaiable to install. It should be the same for those who use Miniconda. Without these environments, you can also simply use this command line, which is the typical method to install packages for python:\n",
    "\n",
    "$ pip install Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start to write a scrapy project.\n",
    "<a id='write'></a>\n",
    "# Write a Scrapy project\n",
    "\n",
    "## Step 1: Creating a project\n",
    "Under the directory you want to store your project, use this command line to create a new project:\n",
    "\n",
    "$ scrapy startproject  XXXX   (where XXXX refers to the project name)\n",
    "\n",
    "Here, I named my project as 'lyrics'. It will create a directory 'lyrics' for me with the following structure. (Note: By taking a look back to the main components of Scrapy I have mentioned above, you can understand more with these files automatically generated for us.)\n",
    "\n",
    "lyrics/   \n",
    "    \n",
    "    lyrics.cfg            # deploy configuration file\n",
    "\n",
    "    lyrics/               # Main code files are in this directory\n",
    "        __init__.py\n",
    "\n",
    "        items.py          # items definition file\n",
    "\n",
    "        middlewares.py    # middlewares file\n",
    "\n",
    "        pipelines.py      # pipelines file\n",
    "\n",
    "        settings.py       # settings file\n",
    "\n",
    "        spiders/          # You will later put your spiders here\n",
    "            __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define an Item\n",
    "Item in Scrapy is like a holder to deposit the data extrcted. So, the first job to do is to define an Item by modifying the items.py file. (Note: Here in this notebook, I just use the Jupyter Magic commands to load and rewrite those python files under 'lyrics' directory. If you want to run and test it, please follow the commands or just run outside of this notebook.)\n",
    "\n",
    "Spracy has provided an Item class. We need inherit it to define our Item calss, 'LyricsItem', to better structure data. The Field object is applied to allocate fields for Item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load lyrics/lyrics/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lyrics/lyrics/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lyrics/lyrics/items.py\n",
    "import scrapy\n",
    "\n",
    "class LyricsItem(scrapy.Item):\n",
    "    song = scrapy.Field()\n",
    "    artist = scrapy.Field()\n",
    "    url = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working as a data container, item in Scrapy is dictionary-like. We can build a simple instance of our Item to see. However, rather than passing dictionaries again and agian, it's more conveniente for other components of Scrapy to conduct further manipulation on data by accessing Item. So, later when writing spiders for a project, we just need to initalize and return Item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artist': 'Ed Sheeran', 'song': 'Perfect'}\n"
     ]
    }
   ],
   "source": [
    "from lyrics.lyrics.items import LyricsItem\n",
    "\n",
    "testLyricItem = LyricsItem(song=\"Perfect\", artist=\"Ed Sheeran\")\n",
    "print(testLyricItem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Write a spider\n",
    "As its name states, spider in scrapy is used to scrape the websites where we want to extract data from. It should be written under the 'spiders' directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lyrics/lyrics/spiders/lyricSpider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lyrics/lyrics/spiders/lyricSpider.py\n",
    "import scrapy\n",
    "from lyrics.items import LyricsItem\n",
    "\n",
    "class LyricSpider(scrapy.Spider):\n",
    "    name = 'lyricSpider'  #name for each spider should be unique\n",
    "    allowed_domains = [\"www.metrolyrics.com\"]\n",
    "    start_urls = [\"http://www.metrolyrics.com/top100.html\"] #where to start scraping\n",
    "    \n",
    "    \n",
    "    #the parse() method will be called by defualt\n",
    "    def parse(self, response):\n",
    "        #use Scrapy's Selector to select certain parts of the HTML source:\n",
    "        slc = scrapy.Selector(response)\n",
    "        sites = slc.xpath('//div[@id=\"top100\"]/div[@class=\"row\"]/div/div[@class=\"grid_8\"]/div/div/ul/li/span[3]')\n",
    "        \n",
    "        items = []        \n",
    "        for site in sites:\n",
    "            item = LyricsItem()\n",
    "            item['song'] = site.xpath('a/text()').extract_first().strip()[:-7] #get rid of the 'Lyric' word of the text.\n",
    "            item['artist'] = site.xpath('span/a/text()').extract()[0].strip() #extract()[0] and extract_first() does the same thing.\n",
    "            item['url'] = site.xpath('a/@href').extract()[0]\n",
    "            items.append(item)            \n",
    "        \n",
    "        return items   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selectors\n",
    "Here I already used Selector, which is provided by Scrapy to parse HTML/XML source. You can also use BeautifulSoup we have learned from the course. But it's slower than lxml. Scrapy's Selctor is right built based on lxml. With Selector, xpath(), css(), re() can be used to pass Xpath/CSS/regex expressions, helping parse the source.\n",
    "\n",
    "Xpath:\n",
    "\n",
    "In my LyricSpider above, I tried Xpath. Using Chrome's 'Inspect' tool, we can navigate to the 'top100' part of the webpage, and find the song's name is content of 'a' tag nested inside. So we can access it with this expression: \n",
    "\n",
    "//div[@id=\"top100\"]/div[@class=\"row\"]/div/div[@class=\"grid_8\"]/div/div/ul/li/span[3]/a/text()\n",
    "\n",
    "Inspect tool actually provides an automatically generated XPath expression, which is very convenient. All you need to do is nevigate to the element you want and right-click on it. Then select 'copy' --> 'copy Xpath'. More about the XPath rules can be found from https://www.w3.org/TR/xpath/.\n",
    "\n",
    "CSS:\n",
    "\n",
    "CSS is as explicit as XPath and easy to learn. Here's a little example to show how CSS selector works for parsing HTML. More about CSS can be found from https://www.w3.org/TR/selectors/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atheletic\n",
      "boots\n",
      "flats\n",
      "url:/set-up/asdh$%31988\n"
     ]
    }
   ],
   "source": [
    "from scrapy import Selector\n",
    "htmlTest = \"\"\"\n",
    "<div style=\"display: none;\" id=\"div-gpt-ad-skin\" x-dn=\"true\" data-google-query-id=\"CJK3_M_ElNoCFYoxaQod7UsBIw\">\n",
    "    <div id=\"google_ads_iframe_/8264/aw-metrolyrics/charts/top100_1__container__\" style=\"border: 0pt none;\">\n",
    "          <span>tea</span>\n",
    "          <li class = \"list_items\">atheletic<li>\n",
    "          <li class = \"list_items\">boots<li>\n",
    "          <li class = \"list_items\">flats<a href=\"/set-up/asdh$%31988\"></a><li>\n",
    "          <li class = \"grid_4\">frame in blue<li>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "slc1 = Selector(text = htmlTest)\n",
    "nodes = slc1.css('li.list_items')\n",
    "for node in nodes:\n",
    "    print(node.css('::text').extract_first())\n",
    "    if len(node.css('a::attr(href)').extract()) != 0:\n",
    "         print(\"url:\"+str(node.css('a::attr(href)').extract_first()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this Spider to scrape\n",
    "Now we can use this spider to extract the top ranked songs with their singers on the main page. Scrapy's Feed Exports supports several serialization formats like JSON, CSV and XML to store data. This is one of its advantages over other data collection methods since we require such a proper way to export and store the scraped data for further goals maybe by other systems or tools. Under the 'lyrics' directory, use this command line to invoke the spider and write data as json file:\n",
    "\n",
    "$ scrapy crawl lyricSpider -o topSongs_data.json\n",
    "\n",
    "Let's open this json file to see the data we have scraped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'song': 'Daru Badnaam', 'artist': 'Param Singh', 'url': 'http://www.metrolyrics.com/daru-badnaam-lyrics-param-singh.html'}, {'song': 'Beautiful In White', 'artist': 'Westlife', 'url': 'http://www.metrolyrics.com/beautiful-in-white-lyrics-westlife.html'}, {'song': 'Despacito', 'artist': 'Luis Fonsi', 'url': 'http://www.metrolyrics.com/despacito-lyrics-luis-fonsi.html'}, {'song': 'Torete', 'artist': 'Moonstar88', 'url': 'http://www.metrolyrics.com/torete-lyrics-moonstar88.html'}, {'song': 'Akin Ka Na Lang', 'artist': 'Morissette Amon', 'url': 'http://www.metrolyrics.com/akin-ka-na-lang-lyrics-morissette-amon.html'}, {'song': 'Summer Nights', 'artist': 'Grease', 'url': 'http://www.metrolyrics.com/summer-nights-lyrics-grease.html'}, {'song': 'Nadarang', 'artist': 'Shanti Dope', 'url': 'http://www.metrolyrics.com/nadarang-lyrics-shanti-dope.html'}, {'song': 'Pokémon Theme', 'artist': 'Pokémon', 'url': 'http://www.metrolyrics.com/pokemon-theme-lyrics-pokemon.html'}, {'song': 'A Thousand Years', 'artist': 'Christina Perri', 'url': 'http://www.metrolyrics.com/a-thousand-years-lyrics-christina-perri.html'}, {'song': 'Baa Baa Black Sheep', 'artist': 'Children Songs', 'url': 'http://www.metrolyrics.com/baa-baa-black-sheep-lyrics-children.html'}, {'song': 'What The Fuck', 'artist': '10 Years', 'url': 'http://www.metrolyrics.com/what-the-fuck-lyrics-10-years.html'}, {'song': 'High Rated Gabru', 'artist': 'Guru Randhawa', 'url': 'http://www.metrolyrics.com/high-rated-gabru-lyrics-guru-randhawa.html'}, {'song': \"You're The One That I Want\", 'artist': 'Grease', 'url': 'http://www.metrolyrics.com/youre-the-one-that-i-want-lyrics-grease.html'}, {'song': 'Seasons Of Love', 'artist': 'RENT', 'url': 'http://www.metrolyrics.com/seasons-of-love-lyrics-rent.html'}, {'song': 'Hayaan Mo Sila', 'artist': 'Ex Battalion', 'url': 'http://www.metrolyrics.com/hayaan-mo-sila-lyrics-ex-battalion.html'}, {'song': 'Do-Re-Mi', 'artist': 'The Sound Of Music', 'url': 'http://www.metrolyrics.com/doremi-maria-and-the-children-lyrics-the-sound-of-music.html'}, {'song': 'Phantom Of The Opera', 'artist': 'The Phantom of the Opera (Original London Cast)', 'url': 'http://www.metrolyrics.com/phantom-of-the-opera-lyrics-phantom-of-the-opera.html'}, {'song': \"Everybody's Free (to Wear Sunscreen)\", 'artist': 'Baz Luhrmann', 'url': 'http://www.metrolyrics.com/everybodys-free-to-wear-sunscreen-lyrics-baz-luhrmann.html'}, {'song': 'New York', 'artist': 'Alicia Keys', 'url': 'http://www.metrolyrics.com/new-york-lyrics-alicia-keys.html'}, {'song': 'Stand By Me', 'artist': 'Ben E. King', 'url': 'http://www.metrolyrics.com/stand-by-me-lyrics-ben-e-king.html'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"lyrics/topSongs_data.json\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An improved spider -- scraping multiple urls\n",
    "The previous spider is a simple example to illustrate what a basic spider looks like. It only scrapes the information from the 'start_urls'. What we really need to achieve is lyrics, which are on the page behind each song.  Next, a new spider is created to parse not only the information on the main page, but also those on chained urls. \n",
    "\n",
    "Note, the Item is also modified slightly, simply adding some fields. Only code for the new spider is shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lyrics/lyrics/spiders/lyricSpiderImproved.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lyrics/lyrics/spiders/lyricSpiderImproved.py\n",
    "import scrapy\n",
    "from lyrics.items import LyricsItem\n",
    "\n",
    "class LyricSpiderImproved(scrapy.Spider):\n",
    "    name = 'lyricSpiderImproved' \n",
    "    allowed_domains = [\"www.metrolyrics.com\"]\n",
    "    start_urls = [\"http://www.metrolyrics.com/top100.html\"]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        slc2 = scrapy.Selector(response)\n",
    "        sites = slc2.xpath('//*[@id=\"main-content\"]/div[1]/div/div/ul/li')\n",
    "    \n",
    "        for site in sites:\n",
    "            item = LyricsItem()\n",
    "            item['song'] = site.xpath('span[3]/a/text()').extrat_first().strip()[:-7] \n",
    "            item['artist'] = site.xpath('span[3]/span/a/text()').extrat_first().strip()\n",
    "            item['url'] = site.xpath('span[3]/a/@href').extrat_first()\n",
    "            \n",
    "            #The following fields are assigned for later use. \n",
    "            #Another field called ''lastWeekRank' is also declared in Item\n",
    "            item['thisWeekRank'] = site.xpath('span[1]/text()').extrat_first()\n",
    "            item['up'] = site.xpath('div[@class=\"last-week up\"]/text()').extrat_first()\n",
    "            item['same'] = site.xpath('div[@class=\"last-week same\"]/text()').extrat_first()\n",
    "            item['down'] = site.xpath('div[@class=\"last-week down\"]/text()').extrat_first()\n",
    "            \n",
    "            yield scrapy.Request(url = item['url'], meta = {'item':item}, callback = self.parse_lyric)\n",
    "\n",
    "    \n",
    "    def parse_lyric(self, response):\n",
    "        slc3 = scrapy.Selector(response)\n",
    "        textLines= slc3.xpath('//*[@id=\"lyrics-body-text\"]/p/text()').extract()\n",
    "        #print(textLines)\n",
    "        \n",
    "        item = response.meta['item']\n",
    "        item['lyric'] = \" \".join(textLines)\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core improvement is to pass the urls we want to follow and parse_lyric method as callbacks. It will then parse one step deeper. This way can also be applied to scrape linked pages. For example, if we want to get information of all shoes listed on Amazon, which may appear on multiple pages, we can use this method to go over from the first page to the last. Such circumstances are even easier, where no other parse methods need to be written. Find the next url and pass self.parse itself as callback. The spider will recursively scrape the pages until there's no next. You can have a try if you're interested. It's easier to implement compared to the Yelp reviews example in our homework :)\n",
    "\n",
    "Run this improved spider and export data into 'lyrics_data.json' file. See what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Beautiful In White\n",
      "Artist:  Westlife\n",
      "Lyric:\n",
      " Not sure if you know this \n",
      "But when we first met \n",
      "I got so nervous \n",
      "I couldn't speak \n",
      "In that very moment I found the one and \n",
      "My life had found its missing piece So as long as I live I'll love you \n",
      "Will have and hold you \n",
      "You look so beautiful in white \n",
      "And from now to my very last breath \n",
      "This day I'll cherish \n",
      "You look so beautiful in white \n",
      "Tonight What we have is timeless \n",
      "My love is endless \n",
      "And with this ring I say to the world \n",
      "You're my every reason \n",
      "You're all that I believe in \n",
      "With all my heart I mean every word So as long as I live I'll love you \n",
      "Will have and hold you \n",
      "You look so beautiful in white \n",
      "And from now to my very last breath \n",
      "This day I'll cherish \n",
      "You look so beautiful in white \n",
      "Tonight Oh, oh \n",
      "You look so beautiful in white \n",
      "Na na na na na \n",
      "So beautiful in white \n",
      "Tonight And if our daughter's what our future holds \n",
      "I hope she has your eyes \n",
      "Finds love like you and I did, yeah. \n",
      "When she falls in love we let her go \n",
      "I'll walk her down the aisle \n",
      "She'll look so beautiful in white... You look so beautiful in white So as long as I live I'll love you \n",
      "Will have and hold you \n",
      "You look so beautiful in white \n",
      "And from now to my very last breath \n",
      "This day I'll cherish \n",
      "You look so beautiful in white \n",
      "Tonight You look so beautiful in white tonight.\n"
     ]
    }
   ],
   "source": [
    "with open(\"lyrics/lyrics_data.json\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print(\"Name: \", data[3]['song'])\n",
    "print(\"Artist: \", data[3]['artist'])\n",
    "print(\"Lyric:\\n\", data[3]['lyric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Item Piplines\n",
    "According to the architecture of Scrapy, Items are finally transported to Item Pipelines. In such components, data is proccesed and decided whether to be dumped or kept. Common applications are cleaning data, validating fields and droping duplications. Besides, data can be stored into database through these pipelines.\n",
    "\n",
    "Now, I only want to keep those songs that rise in the ranking compared to last week. This is identified by whether the song has a 'div' tag with class \"last-week same/up/down\". Thus, in the 'process_item()' method which is called by default, I check their 'up' fields and dump those without value. And their last week's rankings then can be calculated. (Some modifications were also made on item.py file and the LyricSpiderImproved spider to add these fields.)\n",
    "\n",
    "Other than declaring json file in command line, here I write another pipeline class 'JsonWriterPipeline' to write json files. Then calling the spider will automatically write data as the format you want. Writing data into database like MongoDB is very similar to this, which can be refered from https://doc.scrapy.org/en/latest/topics/item-pipeline.html ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load lyrics/lyrics/pipelines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lyrics/lyrics/pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lyrics/lyrics/pipelines.py\n",
    "import json\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class LyricsPipeline(object):\n",
    "    \n",
    "    def process_item(self, item, spider):\n",
    "        if item['up']:\n",
    "            item['lastWeekRank'] = str(int(item['thisWeekRank'])+int(item['up'][1:]))\n",
    "            return item\n",
    "        else:\n",
    "            raise DropItem(\"Song %s does not rise in ranking\" % item)\n",
    "\n",
    "            \n",
    "class JsonWriterPipeline(object):\n",
    "    \n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('processed_data.json','w')\n",
    "        \n",
    "    def close_spider(self,spider):\n",
    "        self.file.close()\n",
    "        \n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only modifying the pipelines.py file is not enough though. It must be deployed in the setting.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load lyrics/lyrics/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lyrics/lyrics/settings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lyrics/lyrics/settings.py\n",
    "\n",
    "BOT_NAME = 'lyrics'\n",
    "\n",
    "SPIDER_MODULES = ['lyrics.spiders']\n",
    "NEWSPIDER_MODULE = 'lyrics.spiders'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = True\n",
    "\n",
    "\n",
    "# Configure item pipelines\n",
    "ITEM_PIPELINES = {\n",
    "   'lyrics.pipelines.LyricsPipeline': 300, #number decides the order to run. It should be between 0 and 1000.\n",
    "   'lyrics.pipelines.JsonWriterPipeline': 800,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the spider again and see how many records of songs we have filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "processed_data = []\n",
    "for line in open(\"lyrics/processed_data.json\", encoding='utf-8'):\n",
    "    processed_data.append(line)\n",
    "    \n",
    "print(len(data))\n",
    "print(len(processed_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even more: ImagesPipeline for scraping images\n",
    "Scrapy provides a special Pipeline, ImagesPipeline, which is very handy for downloading images. Now, let's use this amazing feature to scrape the albums' covers of songs.\n",
    "\n",
    "Note: The following code cells are not run here. Simply paste them into the current corresponding files for Item, ItemPipeline and settings. \n",
    "\n",
    "First, add some configurations in settings for the new Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Modify the pipelines' configuration\n",
    "ITEM_PIPELINES = {\n",
    "   'scrapy.pipelines.images.ImagesPipeline': 1,\n",
    "   'lyrics.pipelines.LyricsPipeline': 300,\n",
    "   'lyrics.pipelines.JsonWriterPipeline': 800,   \n",
    "}\n",
    "\n",
    "#Direct to the path where you want to store the downloaded images\n",
    "IMAGES_STORE = 'pics_scraped'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add fields for images and their urls in Item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Modify the class in items.py\n",
    "images_urls = scrapy.Field()\n",
    "images = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let the spider scrape the urls of images by impelementing this in parse() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Modify the LyricSpiderImproved class's parse() method in corresponding spider file\n",
    "item['images_urls']= site.xpath('span[2]/a/img/@src').extract_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, build this Pipeline class for images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Modify the pipelines.py\n",
    "from scrapy.pipelines.images import ImagesPipeline\n",
    "\n",
    "class MyImagesPipeline(ImagesPipeline): #Be careful about what to be inherited here!!!\n",
    "    \n",
    "    def get_media_requests(self,item,info):\n",
    "        for url in item['images_urls']:\n",
    "            yield scrapy.Request(url)\n",
    "    \n",
    "    def item_completed(self,results,item,info):\n",
    "        images_paths = [x['images'] for ok, x in results if ok]\n",
    "        if not image_paths:\n",
    "            raise DropItem(\"Song has no cover\")\n",
    "        item['images'] = image_paths\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='advantages'></a>\n",
    "# Advantages of Scrapy\n",
    "From the general practice of Scrapy above, we can feel and summarize the advantages of this web crawler.\n",
    "\n",
    "a) Scrapy has relatively complete framework. The whole progress of data collection, from requesting, storing, processing to storing data, has relative components for support. It is really well sturctrued.\n",
    "\n",
    "b) Ways to store data of Scrapy provide various choices. It makes more manipulations on data possible even on other systems or tools.\n",
    "\n",
    "c) Scrapy also have many amazing unique functions, like fast parsing by Selectors, scraping images by special pipelines.\n",
    "\n",
    "Scrapy's advantages are more than these illustreated here. For example, the spider can be adjusted automatically rather than manually so that requesting and parsing work with high efficiency. More can be found through futher practice on Scrapy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='experience'></a>\n",
    "# Practice experience\n",
    "When writing this tutorial at the very begining, I used to practice techniques about Scrapy on Amazon. The reason I changed this is that my IP was banned because at that time I was not aware of the frequncy of requests I send. To protect data, most of the companies and organizations are cautious about crawlers and other similar robot-like visitors. There are several ways to handle this.\n",
    "\n",
    "Some other tools can help rotate our IP address or help undirectly visit the website, which may be not proper or unpractical under some circumstances. So they are not recommended here. Only some nature methods related to Scrapy itself are listed here:\n",
    "\n",
    "a) Slow down requesting by setting download delays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add DOWNLOAD_DELAY variable in settings.py\n",
    "DOWNLOAD_DELAY = 3  # 2 or higher is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Disable cookies since they are used to keep track of us visitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set COOKIES_ENABLED to be false variable in settings.py\n",
    "COOKIES_ENABLED = false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, other than the techniques suggested above, the general rule for data collection is to BE POLITE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='more'></a>\n",
    "# References and further explorations\n",
    "This tutorial only provides guidlines for new users of Scrapy. The functions of Scrapy are more than what are introduced here. If you are interested, following links will be very useful:\n",
    "\n",
    "1.Official tutorial for Scrapy: https://doc.scrapy.org/en/latest/index.html\n",
    "\n",
    "2.Some useful topics about Scrapy:\n",
    "\n",
    "  Imitating logging: https://doc.scrapy.org/en/latest/topics/logging.html\n",
    "  \n",
    "  Debugging Spiders: https://doc.scrapy.org/en/latest/topics/debug.html\n",
    "  \n",
    "  Link Extractors: https://doc.scrapy.org/en/latest/topics/link-extractors.html\n",
    "  \n",
    "  Distributed crawlers based on Scrapy: https://github.com/rmax/scrapy-redis\n",
    "\n",
    "4.How to crawl the web politely with Scrapy: https://blog.scrapinghub.com/2016/08/25/how-to-crawl-the-web-politely-with-scrapy/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
