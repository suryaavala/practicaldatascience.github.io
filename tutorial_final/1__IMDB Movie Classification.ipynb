{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy\n",
    "import omdb\n",
    "import io, json\n",
    "import requests\n",
    "from sklearn.utils import shuffle\n",
    "from bs4 import BeautifulSoup\n",
    "import ast\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will be using a Logistic Regression Model to predict whether a user will like a movie based on the plot summary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For our problem, we intend on dividing the dataset of movies into 2 parts - one that will be liked by the user, and one that will not. To do so, we will be using a very simplistic model of analysis on an Logistic Regression Model - the movie genre. Our goal is to predict the genre of a movie, given the plot summary, and use this to identify whether the movie is suitable for our user. But movies can be of several genres. The following is a list of genres from IMDB.\n",
    "\n",
    "|                    Genres                   |\n",
    "|----------|----------|------------|----------|\n",
    "|Action    |Adventure |Animation   |Biography |\n",
    "|Comedy    |Crime\t  |Documentary |Drama     |\n",
    "|Family    |Fantasy   |Film Noir   |History\t  |\n",
    "|Horror    |Music     |Musical     |Mystery   | \n",
    "|Romance   |Sci-Fi\t  |Short       |Sport     |\n",
    "|Superhero |Thriller  |War         |Western   |\n",
    "\n",
    "\n",
    "So our first task is to list out the users favorite ones. For the sake of simplicity let us assume that the users genre preferences are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_prefs = ['Animation','Romance','Drama','Mystery']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict whether or not a movie will be iked by our user, we need to train our model. We will do so by collecting a large dataset of movies from IMDB, and gathering the info of these movies. This can be done using OMDb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OMDb\n",
    "\n",
    "We will be generating our dataset using the OMDb API. The OMDb API is a RESTful web service to obtain movie information, all content and images on the site are contributed and maintained by our users. You can find out more about the library on their website: http://www.omdbapi.com/\n",
    "\n",
    "### Authentication\n",
    "\n",
    "To use the OMDb API, however, we need to go through a few steps to set up an account with a private key and use that private key in this application. This can be done as follows:\n",
    "\n",
    "1. On the OMDb page, select the API Key option in the top menu\n",
    "2. Fill out the form to include a valid email ID. If you need more 1,000 requests (as for this program), you may need to become a patron first. A key will be sent to this email ID.\n",
    "3. Store this key in a file called 'api_key.txt' in the same directory as this file. We will read from this file in order to gain access to the key, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_key(filename):\n",
    "    \"\"\"\n",
    "    Return the private key used for authentication of OMDb.\n",
    "\n",
    "    Args:\n",
    "        filename (string): the name of the file that stores the key \n",
    "\n",
    "    Returns:\n",
    "        key (string): the private key\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read().replace('\\n','')\n",
    "\n",
    "api_key = get_key('api_key.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Movie Information\n",
    "\n",
    "Using the OMDb library we will now get the first 20,000 movies from IMDB. Each movie on IMDB has a unique IMDB serial ID. In order to gather the first 20,000 movies, we can use the seed Serial ID 'tt00000001' and incrementally gather the first 20,000 results. If the plot summary doesn't exist for that serial number or if that title hasn't been assigned Genre labels, we will skip that one and try again until we get a successful search.\n",
    "\n",
    "As we read in a result, we will build a Pandas DataFrame to keep track of the data. We just need the imdbID, the movie plot summary and the associated movie genres. We will then write this dataframe into a csv file for easy access later. This way you always have access to the original dataset information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(key, startID, numElems):\n",
    "    \"\"\"\n",
    "    Uses the OMDb API to build the dataset. Incrementally goes through ImdbIDs (starting at 'tt0000001') to find 1000 movies that have valid\n",
    "    Genre labels and Plot Summary.\n",
    "\n",
    "    Args:\n",
    "        key (string): Private Key required by the API.\n",
    "        startID (int): The ImdbID to start with.\n",
    "        numElems (int): The number of data points required.\n",
    "\n",
    "    Returns:\n",
    "        end_ID (int): The last imdbID that the search ended at.\n",
    "        df (pd.Dataframe): Final Dataframe with imdbID, plot and genres columns.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    imdbID = startID\n",
    "    df = pd.DataFrame()\n",
    "    while count<numElems:\n",
    "        imdbIDStr = str(imdbID)\n",
    "        response = requests.get('http://www.omdbapi.com/?apikey='+ str(key)+ '&plot=full&tomatoes=false&i=tt' + ('0'*\n",
    "                    (7-len(imdbIDStr))) + imdbIDStr)\n",
    "        try:\n",
    "            data = response.json()\n",
    "            if 'Genre' in data.keys():\n",
    "                genres = data['Genre'].split(',')\n",
    "                if data['Plot']!='N/A':\n",
    "                    df = df.append({'imdbID':'tt'+ (\"0\"*(7-len(imdbIDStr))) + imdbIDStr,'plot':data['Plot'],'genres':tuple(genres)},\n",
    "                                   ignore_index=True)\n",
    "                    count+=1\n",
    "        except:\n",
    "            print('imdbStr', imdbIDStr)\n",
    "            print('response', response)\n",
    "        imdbID +=1 \n",
    "    return imdbID, df\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_df_to_csv(filename, df):\n",
    "    \"\"\"\n",
    "    Writes the dataframe to a csv.\n",
    "\n",
    "    Args:\n",
    "        filename (string): The name of the csv file to be written\n",
    "        df (pd.Dataframe): The dataframe that is to be written to the csv file\n",
    "    \"\"\"\n",
    "    df.to_csv(filename, sep=',', encoding='utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When collecting our data, we need to try and collect as much data as possible, to train our model properly. In the cell below, we try to collect about 17000 data points. We split up the requests in 100-request batches with 10 second pauses in between to keep the request rate under than the maximum limit of the OMDb library. As each batch of requests is completed, we write the dataframe to end our csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_df = pd.DataFrame(columns=['imdbID','plot','genres'])\n",
    "startID = 1\n",
    "new_beginning = True\n",
    "for i in range(170):\n",
    "    api_key = get_key('api_key.txt')\n",
    "    time.sleep(10)\n",
    "    startID, new_df = get_data(api_key, startID, 100)\n",
    "    movie_df = movie_df.merge(new_df,how='outer')\n",
    "    new_df['genres'] = new_df['genres'].apply(lambda x: list(x))\n",
    "    new_df = new_df.set_index('imdbID')\n",
    "    with open('training_data.csv', 'a') as f:\n",
    "        if new_beginning:\n",
    "            new_df.to_csv(f, header=True)\n",
    "            new_beginning = False\n",
    "        else:\n",
    "            new_df.to_csv(f, header=False)\n",
    "movie_df = movie_df.set_index('imdbID')\n",
    "movie_df['genres'] = movie_df['genres'].apply(lambda x: list(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data\n",
    "\n",
    "The next step is to clean up our data in a way that we can use to train our model. In other words, we need to modify the string passed in as the plot summary to convert the string into tokens that satisfy some criteria. Each token must satisfy the following criteria.\n",
    "\n",
    "1. The tokens must all be in lower case.\n",
    "2. The tokens should appear in the same order as in the raw text.\n",
    "3. The tokens must be in their lemmatized form. If a word cannot be lemmatized (i.e, you get an exception), simply catch it and ignore it. These words will not appear in the token list.\n",
    "4. The tokens must not contain any punctuations. Punctuations should be handled as follows: (a) Apostrophe of the form 's must be ignored. e.g., She's becomes she. (b) Other apostrophes should be omitted. e.g, don't becomes dont. (c) Words must be broken at the hyphen and other punctuations.\n",
    "\n",
    "This is done to ensure that the pattern of tokens can map to a general meaning and thus a predictable result. But the above are the most common ways to make valid tokens. In addition to this, we will also add one more criterion. As the tokens are made from the plot summary of a movie, it is possible that the tokens contain some proper nouns. We do not want this to affect our prediction, so we will attempt to remove these.\n",
    "\n",
    "The best way to do so is to use the NLTK libary, we can tag each token with what type of word it is. NLTK tags all proper nouns with one of 2 tags: NNP and NNPS. So we can filter out each of these tags while taking care of each of the other token filters. You can find more information on the other types of tags [here](https://www.nltk.org/_modules/nltk/tag.html#pos_tag).\n",
    "\n",
    "Let us first begin with reading in the csv into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_in_data(filename):\n",
    "    \"\"\"\n",
    "    Reads in CSV file with data into a Pandas Dataframe\n",
    "\n",
    "    Args:\n",
    "        filename (string): The name of the csv file to be read in\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.Dataframe): The dataframe with the data that was read in\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep=',', encoding='utf-8')\n",
    "    df['genres'] = df['genres'].apply(lambda x: ast.literal_eval(x))\n",
    "    return df\n",
    "    \n",
    "movie_df = read_in_data('training_data.csv')\n",
    "print(movie_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be look as follows:\n",
    "    \n",
    "```python\n",
    ">>> print(movie_df.head())\n",
    "     imdbID                        genres  \\\n",
    "0  tt0000001         [Documentary,  Short]   \n",
    "1  tt0000002           [Animation,  Short]   \n",
    "2  tt0000003  [Animation,  Comedy,  Short]   \n",
    "3  tt0000005                       [Short]   \n",
    "4  tt0000007               [Short,  Sport]   \n",
    "\n",
    "                                                plot  \n",
    "0  Performing on what looks like a small wooden s...  \n",
    "1     Short film of 300 individually painted images.  \n",
    "2  One night, Arlequin come to see his lover Colo...  \n",
    "3  A stationary camera looks at a large anvil wit...  \n",
    "4  James J. Corbett and Peter Courtney meet in a ...  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's begin working with our data. Let's first write a function that tokenizes a string based on the required criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_string(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\"\n",
    "    Breaks the string down into its basic tokens by accounting for punctuation, capitalization, etc. and removing\n",
    "    any proper nouns.\n",
    "\n",
    "    Args:\n",
    "        text (string): String that needs to be tokenized\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "\n",
    "    Returns:\n",
    "        tokens (string list): The list of all the final tokens\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'s\",\"\")\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    for i in string.punctuation:\n",
    "        text = text.replace(i,\" \")\n",
    "    tokens = nltk.tag.pos_tag(nltk.word_tokenize(text))\n",
    "    result = []\n",
    "    for (token,token_type) in tokens:\n",
    "        if token_type!='NNS' and token_type!='NNPS':\n",
    "            try:\n",
    "                lemToken = lemmatizer.lemmatize(token)\n",
    "                result.append(str(lemToken))\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = tokenize_string(\"One night, Arlequin come to see his lover\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now apply all the above function to our dataframe in order to convert our plot summaries into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_tokenizer_to_df(df):\n",
    "    \"\"\"\n",
    "    Apply the tokenizer to each plot summary in the dataframe\n",
    "    \n",
    "    Args:\n",
    "        df (pd.Dataframe): movies dataframe that stores the plot summaries\n",
    "\n",
    "    Returns:\n",
    "        df (pd.Dataframe): The dataframe with the tokenized plot summaries\n",
    "    \"\"\"\n",
    "    df['plot_tokens'] = df['plot'].apply(tokenize_string)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_df = apply_tokenizer_to_df(movie_df)\n",
    "print(movie_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look as follows:\n",
    "\n",
    "```python \n",
    ">>> print(movie_df.head())\n",
    "   imdbID                        genres  \\\n",
    "0  tt0000001         [Documentary,  Short]   \n",
    "1  tt0000002           [Animation,  Short]   \n",
    "2  tt0000003  [Animation,  Comedy,  Short]   \n",
    "3  tt0000005                       [Short]   \n",
    "4  tt0000007               [Short,  Sport]   \n",
    "\n",
    "                                                plot  \\\n",
    "0  Performing on what looks like a small wooden s...   \n",
    "1     Short film of 300 individually painted images.   \n",
    "2  One night, Arlequin come to see his lover Colo...   \n",
    "3  A stationary camera looks at a large anvil wit...   \n",
    "4  James J. Corbett and Peter Courtney meet in a ...   \n",
    "\n",
    "                                         plot_tokens  \n",
    "0  [performing, on, what, look, like, a, small, w...  \n",
    "1      [short, film, of, 300, individually, painted]  \n",
    "2  [one, night, arlequin, come, to, see, his, lov...  \n",
    "3  [a, stationary, camera, look, at, a, large, an...  \n",
    "4  [j, corbett, and, peter, courtney, meet, in, a...  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Classifier\n",
    "\n",
    "The next thing that we will do is to train our Logistic Regression model to make predictions.\n",
    "Logistic Regression comes up with a probability function that can give the chance for an input to belong to one of the classes. You can read more about the Logistic Regression model [here](https://machinelearningmastery.com/logistic-regression-for-machine-learning/).\n",
    "\n",
    "### Create Features\n",
    "\n",
    "For the purpose of the task at hand, we will be constructing a bag-of-words TF-IDF feature vector. While we have taken care of proper nouns in each plot summary, we also need to remove very common words (i.e. stopwords) as they add almost no information regarding similarity of two pieces of text.\n",
    "\n",
    "Once this has been removed, we can create a sparse matrix of features for each tweet with the help of [sklearn.feature_extraction.text.TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\" \n",
    "    Creates the feature matrix using the processed movie plot summary text\n",
    "    \n",
    "    Inputs:\n",
    "        df (pd.DataFrame): The dataframe with the tokenized plot summaries\n",
    "        \n",
    "    Outputs:\n",
    "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "                                we need this to tranform test plot summaries in the same way as train plot summaries\n",
    "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "    \"\"\"\n",
    "    stopwords=nltk.corpus.stopwords.words('english')\n",
    "    tfidv = sklearn.feature_extraction.text.TfidfVectorizer(input = 'content', stop_words=stopwords, analyzer='word')\n",
    "    df['modified_plot'] = df['plot_tokens'].apply(lambda x: \" \".join(x))\n",
    "    smatrix = tfidv.fit_transform(list(df['modified_plot']))\n",
    "    return (tfidv, smatrix)\n",
    "\n",
    "\n",
    "(tfidf, X) = create_features(movie_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Labels\n",
    "\n",
    "The next thing we will do is add labels to our dataset. Each label represents whether or not the user will like the movie. As mentioned earlier, we do this in a very simplistic manner. Earlier in this tutorial, we had the user select a list a genres that he/she liked, and named this list `gen_prefs`. We will now go through our dataframe and assign each row a label of `1` if the genre labels of that entry contain atleast of the genres from `gen_prefs` and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_labels(df):\n",
    "    \"\"\" \n",
    "    Creates the class labels based genres\n",
    "    \n",
    "    Inputs:\n",
    "        df (pd.DataFrame): The dataframe with the tokenized plot summaries with a column 'genres'\n",
    "    \n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary numpy array of class labels\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for genres in df['genres']:\n",
    "        found = False\n",
    "        for genre in genres:\n",
    "            if genre in gen_prefs:\n",
    "                labels.append(1)\n",
    "                found = True\n",
    "                break\n",
    "        if found==False:\n",
    "            labels.append(0)\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = create_labels(movie_df)\n",
    "print(len(y))\n",
    "print(len(list(movie_df['genres'])))\n",
    "# Should both be 17000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the data\n",
    "\n",
    "Now that we have created our initial training data and labels, we need to feed this to our Logistic Regression model to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_classifier(X_train, y_train):\n",
    "    \"\"\" learns a classifier from the input features and labels using the kernel function supplied\n",
    "    Inputs:\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features, output of create_features_and_labels()\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels, output of create_features_and_labels()\n",
    "        \n",
    "    Outputs:\n",
    "        sklearn.linear_model.LogisticRegression: classifier learnt from data\n",
    "    \"\"\"\n",
    "    logreg = sklearn.linear_model.LogisticRegression()\n",
    "    return logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = learn_classifier(X, y)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look something like this:\n",
    "\n",
    " ```python\n",
    " >>> print(classifier)\n",
    " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Classifier\n",
    "\n",
    "Now that we have trained our classifier, the next thing we want to do is to test our classifier and find it's accuracy.\n",
    "Let us begin to do this by creating our test data. We can do so by using our earlier `get_data` function to find 3000 data points from the IMDbID of 57842 onwards (some arbitrary startID after 55,000 to avoid collision with training data). We will use this IMDbID as our starting point to avoid any collision with our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns=['imdbID','plot','genres'])\n",
    "startID = 57842\n",
    "new_beginning = True\n",
    "for i in range(30):\n",
    "    api_key = get_key('api_key.txt')\n",
    "    time.sleep(5)\n",
    "    startID, new_df = get_data(api_key, startID, 100)\n",
    "    test_df = test_df.merge(new_df,how='outer')\n",
    "test_df = test_df.set_index('imdbID')\n",
    "test_df['genres'] = test_df['genres'].apply(lambda x: list(x))\n",
    "\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look similar to this:\n",
    "\n",
    "```python\n",
    ">>> print(test_df.head())\n",
    "                                  genres  \\\n",
    "imdbID                                     \n",
    "tt0057842                        [Drama]   \n",
    "tt0057844                       [Comedy]   \n",
    "tt0057846  [Adventure,  Drama,  History]   \n",
    "tt0057851                      [Western]   \n",
    "tt0057852                [Crime,  Drama]   \n",
    "\n",
    "                                                        plot  \n",
    "imdbID                                                        \n",
    "tt0057842  THIS SPECIAL FRIENDSHIP tells of the tender re...  \n",
    "tt0057844  \"Doctor\" Jayne Mansfield is in Italy to show a...  \n",
    "tt0057846  In this first part of the Angélique cycle, set...  \n",
    "tt0057851  In the Arizona Territory in 1879, Captain Jeff...  \n",
    "tt0057852  The Creepiest, Crawliest and Deadliest Film Ev...  \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the Test Results\n",
    "\n",
    "Now let's classify our test data to predict whether or not the user will like the movie or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_movies(tfidf, classifier, test_data):\n",
    "    \"\"\" \n",
    "    predicts whether the user will like the movie or not from raw movie plot text\n",
    "    \n",
    "    Inputs:\n",
    "        tfidf (sklearn.feature_extraction.text.TfidfVectorizer): the TfidfVectorizer object used on training data\n",
    "        classifier (sklearn.linear_model.LogisticRegression): classifier learnt\n",
    "        test_data  (pd.DataFrame): tweets read from tweets_test.csv\n",
    "        \n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary vector of class labels for unlabeled tweets\n",
    "    \"\"\"\n",
    "    test_df = apply_tokenizer_to_df(test_data)\n",
    "    test_df['modified_plot'] = test_df['plot_tokens'].apply(lambda x: \" \".join(x))\n",
    "    smatrix = tfidf.transform(test_df['modified_plot'])\n",
    "    \n",
    "    return classifier.predict(smatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = classify_movies(tfidf, classifier, test_df)\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Accuracy\n",
    "Now we need to understand how well our classifier is performing. To do so, we will be using the accuracy measure. Accuracy is the ratio of the number of correct classifications to the total number of (correct or incorrect) classifications.\n",
    "As we have all the data from IMDb for our test data, we can add labels to this (exactly as we did for our training data), to get the validation data or expected results. Then we can compare our predicted results to our validation data to get the accuracy as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, y_pred, y_validation):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "    Inputs:\n",
    "        classifier (sklearn.linear_model.LogisticRegression): classifer to evaluate\n",
    "        y_pred (numpy.ndarray(int)): final result from predictions\n",
    "        y_validation (numpy.ndarray(int)): expected outputs\n",
    "    Outputs:\n",
    "        double: accuracy of classifier on the validation data\n",
    "    \"\"\"\n",
    "    cnt=0\n",
    "    for i in range(y_pred.size):\n",
    "        if y_pred[i]==y_validation[i]:\n",
    "            cnt=cnt+1 \n",
    "    return cnt/len(y_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_valid = create_labels(test_df)\n",
    "accuracy = evaluate_classifier(classifier, y_test_pred, y_test_valid)\n",
    "print(accuracy) #should get about 0.710666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ways to Improve Accuracy\n",
    "While the accuracy of 0.710666 is not great, it can improved by cleaning the data more accurately. One common tactic is to remove the least common words, or words that will not help to identify the true meaning of the text. Just like stopwords (words that occur too frequently), the least frequent words also tend to take away from the true meaning of the text as they may be names or other unnecessary information. \n",
    "It might also help to gather a larger dataset to improve training the model. With regards to collecting training data, one could also change the increment of imdbIDs to be random as well. This way our dataset will be more spreadout within the IMDb database instead of having 17,000 consecutive listings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
