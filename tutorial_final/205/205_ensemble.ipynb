{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When faced with low accuracy in a classification problem, a general approach is (decreasing order of ease of application):\n",
    "1. Try different classifiers \n",
    "2. Engineer existing features in a better way\n",
    "3. Extract and engineer obscure features \n",
    "\n",
    "At stage 1, when trying different classifiers, the instance when none turn out to be better than the rest. Before moving on to Stage 2 and 3, a reasonable choice is to try ensemble techniques, i.e. keep all the learners and integrate the results from each of them. This tutorial is aimed at converting a set of weak systems (classifiers/learners) into a single strong system. It will try to answer questions like - How can you combine the predictions? What are the different techniques? When to apply the different techniques? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are ensemble methods?\n",
    "\n",
    "Ensemble is a Machine Learning concept in which the idea is to train multiple models using the same learning algorithm. The ensembles take part in a bigger group of methods, called multiclassifiers, where a set of hundreds or thousands of learners with a common objective are fused together to solve the problem. There are mainly three different ways - \n",
    "1. Bagging (also, Bootstrap Aggregating) : An ensemble technique where a set of systems that combine, use the same learning technique. The data samples are picked with replacement to generate N learners whose combined results predict the test sample. Metaphorically, this system resembles a fair democracy where the results are based on the majority voting and each votes independent of the other. \n",
    "2. Boosting: Similar to Bagging, it is an ensemble technique where a set of systems that combine use the same learning technique. Unlike bagging, the data samples are picked not at random but every new subset contains the elements that were misclassified by previous models (sequential). For predicting the results, the averaging is done based on the majority voting. The higher the learner accuracy, the higher its say in predicting an unknown sample.\n",
    "3. Stacking: Stacking (or Stacked Generalization) is one of the main hybrid multiclassifiers similar to Boosting. Takes a set of different learners and combines them using new learning techniques.\n",
    "\n",
    "The idea behind all the above methods is to combine the predictions of several base estimators in order to improve generalizability or robustness over a single estimator. And the main focus of this tutorial is to analyze the performance of bagging and boosting. We will not be covering Stacking in this one.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When are these estimators used? \n",
    "As a general rule, it is a function of bias - variance tradeoff. \n",
    "\n",
    "Bagging is typically used when there is high variance in the model performance i.e. there is high training set accuracy but the validation set accuracy is poor. Assuming validation set is not ill-prepared, we try to beat the model inflexibility by resampling the data, picking non-flexible (overfitted), and averaging them together. This leads to same low bias but cancels out some of the variance. Most common bagging technique is Random forest (refer the code). \n",
    "\n",
    "On the other hand, boosting is employed when there is high bias in the model performance i.e. there is low training set performance. In other words, the model is too simple (i.e. biased) to catch the real relationship of the data. In such underfit cases, the model is boosted sequentially by increasing the weights of the misclassified ones emphasizing the most difficult cases. The results can be combined in various ways - weighted average of all predictions or selecting a few better performing learners. Gradient boosted trees sits at the core of the boosting algorithms. AdaBoost, LPBoost, XGBoost are some others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall start by looking at pre curated dataset in scikit learn called 20 newsgroup dataset.The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training and the other one for testing. The split between the train and test set is based upon a messages posted before and after a specific date. \n",
    "First, we will try to build the dataset by picking only 5 out of the 20 topics - namely, baseball, hockey, motorcycles, politics, and med."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.med',\n",
       " 'talk.politics.misc']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['rec.sport.baseball', 'rec.sport.hockey','rec.motorcycles', 'sci.med','talk.politics.misc']\n",
    "newsgroups_train = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'), categories=categories) \n",
    "list(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the recommendation on the Scikit-learn page, I have removed the metadata that has little to do with topic classification. The recommendation reads as - \"When evaluating text classifiers on the 20 Newsgroups data, you should strip newsgroup-related metadata. In scikit-learn, you can do this by setting remove=('headers', 'footers', 'quotes'). The F-score will be lower because it is more realistic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There is no data to show chromium is effective in promoting weight loss.  The\\n few studies that have been done using chromium have been very flawed and inher\\nently biased (the investigators were making money from marketing it).\\n  Theoretically it really doesnt make sense either. The claim is that chromium\\nwill increase muscle mass and decrease fat.  Of course, chromium is also used t\\no cure diabetes, high blood pressure and increase muscle mass in athletes(just\\nas well as anabolic steroids). Sounds like snake oil for the 1990's :-)\\n On the other hand, it really cant hurt you anywhere but your wallet, and place\\nbo effects of anything can be pretty dramatic...\\n\\n                                    -Paul\\n     ----------------------------------------------------------\\n    |  Paul Sovcik, Pharm.D. U of Illinois College of Pharmacy |\\n    |                                                          |\\n    |    Email- U18183@UICVM.UIC.EDU                           |\\n    |                                                          |\"]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "newsgroups_train.data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1900,)\n"
     ]
    }
   ],
   "source": [
    "#It has 1900 documents \n",
    "print (newsgroups_train.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we did in class, we are converting text to vectors using tfidf vectorizer. We wil not be spending much time on this as this tutorial is not focussed on putting across good feature engineering. The main focus is around teaching you the difference between the performances of bagging and boosting and when to use what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900, 23375)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use sparse functions for faster computations as the extracted TF-IDF vectors are very sparse, with an average of 89 non-zero components by sample in a more than 20000-dimensional space (less than .5% non-zero features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.2578947368421"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.nnz / float(vectors.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test',categories=categories)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification using Decision tree Classifier  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(vectors, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train accuracy - check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9782430918834631"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train = clf.predict(vectors)\n",
    "metrics.f1_score(newsgroups_train.target, pred_train, average='macro') #average = 'macro' is for multiclass targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to be fitting the data really well(high variance) as the train accuracy is really high and the test accuracy is low. This typically happens when the learner tries to fit the function on almost all the data points.\n",
    "\n",
    "You will notice that the scoring metric used is f1_score. By definition, this is the harmonic mean of precision and recall. It reaches its best value at 1 and worst score at 0. This is used here because the summing up AUC under the ROC for one class vs other classes is not very intuitive and there exists no direct implementation in python. Thus, in the multi-class and multi-label case, this accuracy score is just the weighted average of the F1 score of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7486636060460113"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A point to be noted here is that the system is highly unstable. If you keep retraining the model and test it against the same test cases, you will be surprised to see different f1 scores. There is a need for a stable, strong, and robust system. What do we do now? After going through the paragraphs above, you might have rightly guessed! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn.ensemble module includes an algorithm based on randomized decision trees: the RandomForest algorithm specifically designed for trees. This means keeping the underlying technique as decision tree classifier, a diverse set of classifiers is created by introducing randomness by picking samples with replacement. The prediction of the ensemble is given as the averaged prediction of the individual classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 score: 0.9730348084068229\n",
      "Test f1 score: 0.8776711399931504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "clf = clf.fit(vectors, newsgroups_train.target)\n",
    "pred_train = clf.predict(vectors)\n",
    "print(\"Train f1 score:\", metrics.f1_score(newsgroups_train.target, pred_train, average='macro')) #average = 'macro' is for multiclass targets\n",
    "pred = clf.predict(vectors_test)\n",
    "print(\"Test f1 score:\", metrics.f1_score(newsgroups_test.target, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for model selection - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Caution : This step might take time to finish\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "clf_rf = RandomForestClassifier()\n",
    "parameters = {'n_estimators':np.array([50,60,70,90,100,110]), 'criterion':('gini','entropy')}\n",
    "gridsearch = GridSearchCV(clf_rf, parameters)\n",
    "gridsearch_rf =gridsearch.fit(vectors, newsgroups_train.target)\n",
    "gridsearch_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 score: 0.9782430918834631\n",
      "Test f1 score: 0.9725533233915294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "clf = clf.fit(vectors, newsgroups_train.target)\n",
    "pred_train = clf.predict(vectors)\n",
    "print(\"Train f1 score:\", metrics.f1_score(newsgroups_train.target, pred_train, average='macro')) #average = 'macro' is for multiclass targets\n",
    "pred = clf.predict(vectors_test)\n",
    "print(\"Test f1 score:\", metrics.f1_score(newsgroups_test.target, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the bump in the accuracy just by choosing the right hyperparameters? For the best accuracy, I have tuned the hyperparameters using gridsearch which effectively tries all the combinations to get to the best results. In the above case, it runs the model 6*2 times to select the best model given the input parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification using Gradient Boosted Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 score: 0.9782249071021802\n",
      "Test f1 score: 0.9469486406408695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=1, loss='deviance', max_depth=3,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=1, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=150,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False) # gridsearch results of the model from below \n",
    "clf = clf.fit(vectors, newsgroups_train.target)\n",
    "pred_train = clf.predict(vectors)\n",
    "print(\"Train f1 score:\", metrics.f1_score(newsgroups_train.target, pred_train, average='macro')) #average = 'macro' is for multiclass targets\n",
    "pred = clf.predict(vectors_test)\n",
    "print(\"Test f1 score:\", metrics.f1_score(newsgroups_test.target, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, I had very less number of rows in my dataset .i.e only say 100. This is done to show you how low bias systems can be boosted by using gradient boosting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "indices=[]\n",
    "for x in range(100):\n",
    "    indices.append(random.randint(1,1900))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification using Decision tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 score: 0.9712363026530125\n",
      "Test f1 score: 0.4457757779168527\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(vectors[indices,], newsgroups_train.target[indices])\n",
    "pred_train = clf.predict(vectors[indices,])\n",
    "print(\"Train f1 score:\",metrics.f1_score(newsgroups_train.target[indices,], pred_train, average='macro')) #average = 'macro' is for multiclass targets\n",
    "pred = clf.predict(vectors_test)\n",
    "print(\"Test f1 score:\",metrics.f1_score(newsgroups_test.target, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification using Gradient Boosted Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 score: 0.9712363026530125\n",
      "Test f1 score: 0.5408965888836423\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=3, random_state=0)\n",
    "clf = clf.fit(vectors[indices,], newsgroups_train.target[indices])\n",
    "pred_train = clf.predict(vectors[indices,])\n",
    "print(\"Train f1 score:\",metrics.f1_score(newsgroups_train.target[indices,], pred_train, average='macro')) #average = 'macro' is for multiclass targets\n",
    "pred = clf.predict(vectors_test)\n",
    "print(\"Test f1 score:\",metrics.f1_score(newsgroups_test.target, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for model selection - GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=150,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Caution : This step will take time to finish\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "clf_gb = GradientBoostingClassifier()\n",
    "parameters = {'n_estimators':np.array([100,110,150]), 'max_depth':[1,3,4], 'learning_rate':[1,0.1]}\n",
    "gridsearch = GridSearchCV(clf_gb, parameters)\n",
    "gridsearch_gb =gridsearch.fit(vectors[indices,], newsgroups_train.target[indices])\n",
    "gridsearch_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 score: 0.9712363026530125\n",
      "Test f1 score: 0.5619697200050406\n"
     ]
    }
   ],
   "source": [
    "clf =GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=1, loss='deviance', max_depth=3,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=1, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=150,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "clf = clf.fit(vectors[indices,], newsgroups_train.target[indices])\n",
    "pred_train = clf.predict(vectors[indices,])\n",
    "print(\"Train f1 score:\",metrics.f1_score(newsgroups_train.target[indices,], pred_train, average='macro')) #average = 'macro' is for multiclass targets\n",
    "pred = clf.predict(vectors_test)\n",
    "print(\"Test f1 score:\",metrics.f1_score(newsgroups_test.target, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 score: 0.9712363026530125\n",
      "Test f1 score: 0.4930040078802798\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "clf = clf.fit(vectors[indices,], newsgroups_train.target[indices])\n",
    "pred_train = clf.predict(vectors[indices,])\n",
    "print(\"Train f1 score:\",metrics.f1_score(newsgroups_train.target[indices,], pred_train, average='macro')) #average = 'macro' is for multiclass targets\n",
    "pred = clf.predict(vectors_test)\n",
    "print(\"Test f1 score:\",metrics.f1_score(newsgroups_test.target, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, you can apply any of the two depending on the datasets you have. Like in the example above, if you have very less data which is not representative of the complete dataset, then boosting will give better results (in theory). There is no one rule for all, both the techniques will work towards stabilizing the system but you will have to employ one or the other depending on the train and the test accuracy. Compare the test accuracies to see the difference.\n",
    "\n",
    "PS - Not including a table to compare numbers as they might change if you re-run the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Scikit Learn: http://scikit-learn.org\n",
    "2. Dataset : http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "3. GridSearch : http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "4. F1 Score : http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "4. Miscellaneous : https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}