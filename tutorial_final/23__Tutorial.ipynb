{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This tutorial will introduce you to Hidden Markov Models and specifically the Viterbi Algorithm in the context of Part-of-Speech (POS) tagging for English sentences. I believe POS tagging fits in the data pre-processing part of the data science pipeline for text-based applications. After tokenization, stop-word removal and lemmatization, POS tagging is used to assign tags to the tokens. Building on this, chunking and building parse trees may be carried out for further feature generation. They may be included in the analysis/machine learning aspect of the pipeline to develop question answering systems, for sentiment analysis etc. \n",
    "\n",
    "The following is a flow chart of the intermediate stages in Sentiment Analysis:\n",
    "<img src=\"http://fotiad.is/assets/images/nlp/sentiwordnet-flowchart.png\">\n",
    "There are various approaches to Part-of-Speech tagging like Hidden Markov Models(HMM), Dynamic Programming, Unsupervised Learning. HMMs are a probabilistic approach to Part-of-Speech tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "This tutorial will provide a walkthrough of what Hidden Markov Models are and how they can be applied to POS tagging. It will guide you through the Viterbi algorithm and its manual as well as library-based implementation. Finally, it ends with some concluding results and thoughts on the topic.\n",
    "\n",
    "- [Data](#Data)\n",
    "- [Hidden Markov Models](#Hidden-Markov-Models)\n",
    "- [POS tagging and its relation to Hidden Markov Models](#POS-tagging-and-its-relation-to-Hidden-Markov-Models)\n",
    "- [Viterbi Algorithm](#Viterbi-Algorithm)\n",
    "- [Using the in-built hmm module in nltk](#Using-the-in-built-hmm-module-in-nltk)\n",
    "- [Shortcomings of HMM of nltk with no paramter specifications](#Shortcomings-of-above-HMM-of-nltk)\n",
    "- [Concluding notes](#Concluding-notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import hmm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial I am using the Brown Corpus available at:\n",
    "\n",
    "http://www.sls.hawaii.edu/bley-vroman/brown_corpus.html\n",
    "\n",
    "It has about a million words from 15 different categories of text samples. This text will be used in parameter estimation of the model.\n",
    "\n",
    "Natural Language Processing involves tagsets which define a mapping from different parts of speech to distinct symbols. The Penn Treebank tagset is the most widely used tagset.\n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "Thus to create training data, I tagged the sentences using the Penn tagset. The Brown Corpus has a tagged version too, with its own 87 tags based tagset. However, since the Penn tagset has now become a standard, I used the Stanford POS Tagger for tagging the file: brown_nolines.txt from the website.\n",
    "The tutorial, I followed:\n",
    "\n",
    "http://new.galalaly.me/index.php/2011/05/tagging-text-with-stanford-pos-tagger-in-java-applications/\n",
    "\n",
    "The tagged corpus looks like:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/apoorva-nitsure/PDS_Tutorial_Data/master/text_tagged.png\">\n",
    "\n",
    "All data files can be found at: https://github.com/apoorva-nitsure/PDS_Tutorial_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Models(HMMs) are a variant of Markov Models which are used to model sequential or temporal data. In probability theory, a Markov model is a stochastic model used to model randomly changing systems where future states depend only on the current state not on the events that occurred before it (Markov property). \n",
    "A HMM is a Markov chain for which the state is only partially observable. Observations are related to the state of the system, but are typically insufficient to precisely determine the state.\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Markov_model\n",
    "\n",
    "Basically a HMM, is a weighted finite automation with states, transitions between states and weighted paths, where each state can output observations belonging to a domain. \n",
    "It relies on two assumptions:\n",
    "\n",
    "- Naive Bayes Assumption\n",
    "\n",
    "Output of a particular state is dependent only on that state\n",
    "\n",
    "- Markov Assumption\n",
    "\n",
    "The next state depends only on the current state\n",
    "\n",
    "A typical definition of a HMM would include the following:\n",
    "\n",
    "1. Set of states \n",
    ": (The states in the model)\n",
    "\n",
    "2. Output Alphabet\n",
    ": (The allowed set of observables)\n",
    "\n",
    "3. Transition Probabilities\n",
    ": (The probability of moving from one state to another)\n",
    "\n",
    "4. Emission Probabilities\n",
    ":(The probabilitiy of getting an observable from a state)\n",
    "\n",
    "5. Initial State Probabilities\n",
    ":(The probability that the sequence will begin in a particular state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging and its relation to Hidden Markov Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is like time series or sequential data. Hence can be modelled using HMMs. In the real world we observe just a chunk of sentences. The words in the sentences can be mapped to observables and the Part of Speech as the hidden states. The algorithm to find the best state sequence is called the **Viterbi Algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Now we must have a HMM, with all its parameters estimated. It can be achieved using traning data i.e. the Brown Corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1 **: *Preprocessing data* \n",
    "\n",
    "This step involves getting all the words in the tagged file in the format of a tuple where each tuple has the word and it's tag.\n",
    "Here the words and tags are converted to lowercase to ensure that the same word/tag is not counted differently due to a difference in case. The Brown Corpus has all words separated by a space and is cleaned data, hence a split on space helps tokenize the text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('brown_tagged.txt') as ip_file:\n",
    "    for sent in ip_file:\n",
    "        sent = sent.strip('\\n')\n",
    "        sent = sent.lower()\n",
    "        # Forming a list of (word,tag) of all the sentences\n",
    "        temp = [nltk.tag.str2tuple(t) for t in sent.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: *Getting the set of states,the output alphabet,transition probabilities,emission probabilities*\n",
    "\n",
    "Now as we know that the states are tags and the observations are words, here we build dictionaries where the key represents a particular word/tag and its value is the frequency of occurence of that word/tag.\n",
    "\n",
    "Transition probabilities are probabilities of moving from one state to another which in terms of text can be modelled as the probabilitiy of the next part of speech occuring given a particular part of speech. For eg a noun is most likely to follow an adjective and hence will have the highest transition probability. Here the transistions dictionary stores as it's keys the next_tag|current_tag from the available data and frequency of a particular tag following another tag as its value.\n",
    "As the first tag is not going to have a previous tag we model that as 'INIT', a default initial state.\n",
    "\n",
    "The emission probabilities are the probabilities of getting particular observable values from different states. Now in case of POS estimation they would correspond to the probability of getting a particular word given a particular Part-of-Speech. For eg the P(amazing|adjective) would be higher than P(amazing|verb). Thus the word_plus_tag dictionary has as keys word|tag and values as the frequency of a word being tagged a particular Part-of-Speech. \n",
    "\n",
    "P(next_tag|current_tag) = Frequency(next_tag follows current_tag) / Frequency(current_tag)\n",
    "\n",
    "\n",
    "P(word|tag) = Frequency(word associated with that tag) / Frequency(tag)\n",
    "\n",
    "Tag list is a list of all tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_frequency = {}\n",
    "tag_frequency = {}\n",
    "previous = 'INIT'\n",
    "transitions = {}\n",
    "word_plus_tag = {}\n",
    "word_tag_list = []\n",
    "\n",
    "for (word,tag) in temp:\n",
    "    if word is not None and tag is not None:\n",
    "        word_tag_list.append((word,tag))\n",
    "        if word not in word_frequency.keys():\n",
    "            word_frequency[word] = 0\n",
    "        if tag not in tag_frequency.keys():\n",
    "            tag_frequency[tag] = 0\n",
    "        word_given_tag = word + '#|#' + tag\n",
    "        if word_given_tag not in word_plus_tag.keys():\n",
    "            word_plus_tag[word_given_tag] = 0\n",
    "        t = tag + '#|#' + previous\n",
    "        if t not in transitions.keys():\n",
    "            transitions[t] = 0\n",
    "        tag_frequency[tag] = tag_frequency[tag] + 1\n",
    "        word_frequency[word] = word_frequency[word] + 1\n",
    "        word_plus_tag[word_given_tag] = word_plus_tag[word_given_tag] + 1\n",
    "        transitions[t] = transitions[t] + 1\n",
    "        previous = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To find transition and emission probabilities\n",
    "emission_probabilities = {}\n",
    "transition_probabilities = {}\n",
    "for key,value in transitions.items():\n",
    "    individual = key.split('#|#')\n",
    "    if individual[1] != 'INIT':\n",
    "        transition_probabilities[key] = transitions[key]/tag_frequency[individual[1]] \n",
    "for key,value in word_plus_tag.items():\n",
    "    individual = key.split('#|#')\n",
    "    emission_probabilities[key] = word_plus_tag[key]/tag_frequency[individual[1]] \n",
    "    \n",
    "tag_list = list(tag_frequency.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: *Getting the initial probabilities*\n",
    "\n",
    "The initial probabilities of starting in a particular state are indicative of the sentence starting with a particular part of speech. Most commonly they are are articles or nouns.\n",
    "\n",
    "Thus the procedure I have followed here is to again create a new file from the tagged corpus with each sentence on a new line and store the tags of the first word of every sentence along with the number of times that tag occurs as the first word among all the sentences. \n",
    "\n",
    "P(tag|'INIT') = Frequency(Tag being the first tag in a sentence) / Total number of sentences\n",
    "\n",
    "For tags that don't occur as the first tag in a sentence are assigned a default probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_prob = 0.0001\n",
    "initial = {}\n",
    "sent_count = 0\n",
    "c = 0\n",
    "with open('brown_tagged_new.txt') as ip_file:\n",
    "    for sent in ip_file:\n",
    "        sent = sent.strip('\\n')\n",
    "        tokens = sent.split()\n",
    "        if len(tokens) >= 1:\n",
    "            sent_count = sent_count + 1\n",
    "            tokens[0] = tokens[0].lower()\n",
    "            if '/' in tokens[0]:\n",
    "                tt = tokens[0].split('/')\n",
    "                if tt[1] != '':\n",
    "                    if tt[1] not in initial.keys():\n",
    "                        initial[tt[1]] = 0\n",
    "                    initial[tt[1]] = initial[tt[1]]  + 1\n",
    "ip_file.close()\n",
    "\n",
    "\n",
    "for key,val in initial.items():\n",
    "    initial[key] = initial[key] / sent_count\n",
    "for tag in tag_frequency.keys():\n",
    "    if tag not in initial.keys():\n",
    "        initial[tag] = default_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm\n",
    "\n",
    "The Viterbi Algorithm is a dynamic programming based algorithm to find the most likely hidden state sequence having seen the observable data.\n",
    "\n",
    "This algorithm generates a path ${\\displaystyle X=(x_{1},x_{2},\\ldots ,x_{T})}$, which is a sequence of states ${\\displaystyle x_{n}\\in S=\\{s_{1},s_{2},\\dots ,s_{K}\\}}$ that most likely generates the observations \n",
    "${\\displaystyle Y=(y_{1},y_{2},\\ldots ,y_{T})\\in \\{1,2,\\dots ,N\\}^{T}}$ \n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Viterbi_algorithm\n",
    "\n",
    "The algorithm style that I have referred to is: https://www.cse.iitb.ac.in/~cs626-460-2012/, https://github.com/phirwe/CSCI5832-Natural-Language-Processing/blob/master/PA2/hirve-poorwa-assgn2.py\n",
    "\n",
    "Here T = length of the observation sequence and N = the number of possible states.\n",
    "\n",
    "Data Structures used:\n",
    "\n",
    "Sequence_score array: To maintain the probability score of all the possible sequences. Dimensions: (N * T)\n",
    "Sequence_score array (i,t) represents the product of the probability of t'th word to be emitted from the state i and the maximum  probabilistic path for the t-1 words where the next tag will be i.\n",
    "\n",
    "Back_pointer array: To recover the path with highest probability. Dimensions: (N * T)\n",
    "\n",
    "The Viterbi Algorithm is comprised of 3 steps:\n",
    "\n",
    "***Initialization***\n",
    "\n",
    "This step involves determining the starting state of the sequence. The best tag for the first word is found out by finding the tag which gives the maximum value for P(word|tag) * P(tag|'INIT') \n",
    "\n",
    "***Iteration***\n",
    "\n",
    "In this step for each word and tag combination we get the maximum sequence score possible. That is for each word and tag we run a loop through all tags and find the tag tprev which gives the maximum product of P(tag|tagprev) X sequencescore[tagprev,word-1]. This tagprev is stored in the backpointer array. The final value for sequence score[tag,word] is given by the product of P(word|tag) and the maximum product from the previous P(tag|tagprev) X sequencescore[tagprev,word-1]. \n",
    "\n",
    "This step basically finds out the the best sequence at every word and every possibility of tag. Thus for every word and tag we find the best possible sequence leading upto that tag and store that previous-tag as a reference in the backpointer array.\n",
    "\n",
    "***Sequence Identification***\n",
    "\n",
    "Now after all the sequence scores are estimated, we must get the path that gives the maximum score. So for the last word, the sequence scores for each tag and word combination represent the paths ending with a particular tag. Thus for the last word, we choose the tag which has the maximum sequence score. Now the backpointer array for each word and tag represents the previous tag which gives the best previous tag leading to a high score. Therefore when we check the backpointer of the previous tag, it will point to the index of its previous tag which gave the maximum sequence score. In that way we can reach to the best tag for the first word. \n",
    "\n",
    "Now we are ready with our tag sequence! But wait, its in the reverse order, to get the final answer reverse it!\n",
    "\n",
    "***Smoothing***\n",
    "\n",
    "Not all words/transistions in the test data may be present in the training data. Thus to handle unseen words/transitions a default probability can be assigned. Here it is a parameter to my function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(sent,smoothing_parameter):\n",
    "\n",
    "    sent = sent.strip('\\n').lower()\n",
    "    words = word_tokenize(sent)\n",
    "    T = len(words)\n",
    "    N = len(tag_frequency)\n",
    "    sequence_score = np.zeros((N, T))\n",
    "    back_pointer = np.zeros((N, T)) - np.ones((N, T))\n",
    "    previous = 'INIT'\n",
    "    #initialization step\n",
    "    first = words[0]\n",
    "    for i in range(N):\n",
    "        tag = tag_list[i]\n",
    "        word_cond = first + \"#|#\" + tag\n",
    "        transition = tag + \"#|#\" + previous\n",
    "        word_given_tag = smoothing_parameter\n",
    "        tag_state = smoothing_parameter\n",
    "        if word_cond in emission_probabilities.keys():\n",
    "            word_given_tag = emission_probabilities[word_cond]\n",
    "        if transition in initial.keys():\n",
    "            tag_state = initial[transistion]\n",
    "        sequence_score[i, 0] = word_given_tag * tag_state\n",
    "    best_tag = np.nanargmax(sequence_score[:, 0])\n",
    "    back_pointer[best_tag,0] = -2\n",
    "    #iteration step\n",
    "    for t in range(1, T):\n",
    "        for i in range(0, N):\n",
    "            word = words[t]\n",
    "            tag = tag_list[i]\n",
    "            word_given_tag = word + \"#|#\" + tag\n",
    "            if word_given_tag not in emission_probabilities:\n",
    "                conditional_prob = smoothing_parameter\n",
    "            else:\n",
    "                conditional_prob = emission_probabilities[word_given_tag]\n",
    "            max_score = -np.inf\n",
    "            max_index = -np.inf\n",
    "            for j in range(0, N):\n",
    "                transition = tag_list[i] + \"#|#\" + tag_list[j]\n",
    "                if transition not in transition_probabilities:\n",
    "                    value = smoothing_parameter\n",
    "                else:\n",
    "                    value = transition_probabilities[transition]\n",
    "\n",
    "                if max_score <= sequence_score[j, t - 1] * value:\n",
    "                    max_score = sequence_score[j, t - 1] * value\n",
    "                    max_index = j\n",
    "            sequence_score[i, t] = max_score * conditional_prob\n",
    "            back_pointer[i,t] = max_index\n",
    "    #sequence identification\n",
    "    all_tags = []\n",
    "    last_tag_index = np.nanargmax(sequence_score[:, T-1])\n",
    "    all_tags.append(last_tag_index)\n",
    "    for i in range(T-2,0,-1):\n",
    "        temp = all_tags[-1]\n",
    "        k = i + 1\n",
    "        all_tags.append(back_pointer[int(temp),k])\n",
    "    all_tags.append(best_tag)\n",
    "    final_answer = []\n",
    "    k = 0\n",
    "    for i in range(len(all_tags), 0 , -1):\n",
    "        final_answer.append((words[k],tag_list[int(all_tags[i-1])]))\n",
    "        k = k + 1\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing the algorithm with sentences\n",
    "\n",
    "Let's test the algorithm with a particular sentence which involves most of the words from the training data, i.e. the Brown Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('fulton', 'NNP'), ('county', 'NNP'), ('grand', 'NNP'), ('jury', 'NNP'), ('said', 'VBD'), ('an', 'DT'), ('investigation', 'NN'), ('of', 'IN'), ('atlanta', 'NNP'), (\"'s\", 'POS'), ('recent', 'JJ'), ('primary', 'JJ'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'DT'), ('evidence', 'NN'), (\"''\", \"''\"), ('of', 'IN'), ('any', 'DT'), ('irregularity', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "ans = viterbi(\"The Fulton County Grand Jury said an investigation of Atlanta 's recent primary election produced `` no evidence '' of any irregularity .\",0.0001)\n",
    "print(ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the in-built hmm module in nltk\n",
    "\n",
    "The hmm model in nltk can be run in both supervised and unsupervised fashion.\n",
    "The supervised version assumes you have training data to train the model.\n",
    "In this case we do and this is fed to the trainer.\n",
    "\n",
    "The train data is in the format of (word,tag) for each sentence which is a list of tuples. Thus train_data is a list of lists.\n",
    "\n",
    "Here I am training the HiddenMarkovModelTrainer with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HiddenMarkovModelTagger 93 states and 54814 output symbols>\n"
     ]
    }
   ],
   "source": [
    "#Training data\n",
    "train = temp\n",
    "train = [train]\n",
    "train_data = train\n",
    "#Training to get parameter estimates\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "tagger = trainer.train_supervised(train_data)\n",
    "#HMM Properties\n",
    "print(tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('fulton', 'NNP'), ('county', 'NNP'), ('grand', 'NNP'), ('jury', 'NNP'), ('said', 'VBD'), ('an', 'DT'), ('investigation', 'NN'), ('of', 'IN'), ('atlanta', 'NNP'), (\"'s\", 'POS'), ('recent', 'JJ'), ('primary', 'JJ'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'DT'), ('evidence', 'NN'), (\"''\", \"''\"), ('of', 'IN'), ('any', 'DT'), ('irregularity', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"The Fulton County Grand Jury said an investigation of Atlanta 's recent primary election produced `` no evidence '' of any irregularity .\"\n",
    "text = text.lower()\n",
    "print(tagger.tag(text.split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Evaluation\n",
    "\n",
    "Comparing the outcomes from both the hmm module and manual implementation of the Viterbi algorithm we can see that both are same. So while testing on data similar to training data both perform equally well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings of above HMM of nltk\n",
    "\n",
    "Now consider the following sentence.\n",
    "\n",
    "\"Hello World\"\n",
    "\n",
    "This sentence has words which occured in the training data only a few times (hello ocurred 10 times out of the million words). \n",
    "\n",
    "### Using the trained tagger from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'DT'), ('world', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "t = \"Hello World\"\n",
    "t = t.lower()\n",
    "\n",
    "print(tagger.tag(t.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the self-implemented Viterbi algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'UH'), ('world', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(viterbi(\"Hello World\",0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that hello is assigned the tag 'DT' by nltk's module and my implementation assigns it the tag 'UH'.\n",
    "DT means determiner which is generally used for articles whereas UH stands for interjection.\n",
    "Hello is an interjection which is correctly classified by the manual implementation.\n",
    "\n",
    "Also the module tags world as 'DT'. However the manual implementation with smoothing produces NN as the tag for it, which stands for singular/mass nouns.\n",
    "\n",
    "Here we can see that nltk's hmm tagger performs poorly because the sentence consists of unseen transitions and rare words.\n",
    "Since no smoothing is applied in the source code as seen from: https://www.nltk.org/_modules/nltk/tag/hmm.html\n",
    "\n",
    "it performs poorly. We can also see that it's case-sensitive. \n",
    "\n",
    "However the manual implementation has smoothing to take care of unseen data giving a better result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding notes\n",
    "\n",
    "This is an introductory tutorial to HMM for POS tagging. It will help you get started in understanding the basic concepts and the easiest method for it's implementation.\n",
    "\n",
    "The above implementation of Viterbi algorithm is crud. Specifically tokenization is carried out by splitting on spaces, which is a very simplistic methodology. Also log-transforms of the probabilities can be made. There are many other ways to implement this algorithm. Instead of giving a numerical parameter for smoothing, assigning a default tag such as NNP is a common way to handle unseen words or unseen Part-of-Speech transitions.\n",
    "\n",
    "For more information you could visit:\n",
    "\n",
    "- http://www.nltk.org/book/ch05.html\n",
    "- https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/\n",
    "- https://web.stanford.edu/~jurafsky/slp3/10.pdf\n",
    "- https://cs.nyu.edu/courses/spring12/CSCI-GA.2590-001/lecture4.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
