{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Neural networks are very useful and worth exploring to solve machine learning problems. It is an algorithm mathematical model that simulates the behavioral characteristics of animal neural networks and performs distributed parallel information processing. This type of network relies on the complexity of the system to achieve the purpose of processing information by adjusting the interconnected relationships among a large number of internal nodes.\n",
    "\n",
    "This tutorial is to introduce the PyBrain library, which aims at building neural networks to solve data processing problems. Though the PyBrain is a small library comparing to other machine learning libraries, it's quite convenient and has a good performance. This documentation will first introduce the encapsulation of data and the quick building of a network. Then we will learn how to build a network to solve a classification problem and a linear regression problem.\n",
    "\n",
    "PyBrain module is a relatively easy-to-use neural network modeling toolkit. It has open source processing modules for supervised learning data, and the establishment of data sets and network training is also very convenient. Of course it does not only apply to supervised learning data. All in all, pybrain's ability to quickly build a variety of neural networks can still bring convenience to future work.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## installation and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the numpy, scipy and matplotlib library except the PyBrain during the learning period. So make sure you have all of these in your environment first. Then the following is what we should do to install the Pybrain library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ git clone git://github.com/pybrain/pybrain.git\n",
    "\n",
    "$ python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detailed installation tutorial, please refer to http://wiki.github.com/pybrain/pybrain/installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Form a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should form datasets including inputs and targets before training our network. PyBrain has the pybrain.dataset for us to form our datasets. We'll use SupervisedDataSet class to illustrate more as following, which is used for standard supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.datasets import SupervisedDataSet\n",
    "ds = SupervisedDataSet(2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell creates a dataset which contains two dimensional inputs and one dimensional output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could add datas to the dataset.The following cell adds the inputs and output to train a XOR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds.addSample((0, 0), (0,))\n",
    "ds.addSample((0, 1), (1,))\n",
    "ds.addSample((1, 0), (1,))\n",
    "ds.addSample((1, 1), (0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could build a network with the buildNetwork as follow, the following net has 2 inputs, threee hidden neurons and an output neuron. These layer has already been connected to Fullconnection Objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "net = buildNetwork(2, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network adopts sigmoid squashing function by default for the hidden layer. We could flexibly change this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.structure import TanhLayer\n",
    "from pybrain.structure import SoftmaxLayer\n",
    "net = buildNetwork(2, 3, 1, hiddenclass=TanhLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell use the Tanh function for the hidden layer for the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the buildNetwork method, we could build our network from a more concrete and flexible way. Let's illustrate this with the feedforward network as an example. First we build a FeedForwardNetwork object without any parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.structure import FeedForwardNetwork\n",
    "n = FeedForwardNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could set the layers separately. The parameters in the following methods determine how many neurons does each layer have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.structure import LinearLayer, SigmoidLayer\n",
    "inLayer = LinearLayer(2)\n",
    "hiddenLayer = SigmoidLayer(3)\n",
    "outLayer = LinearLayer(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add the layers into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n.addInputModule(inLayer)\n",
    "n.addModule(hiddenLayer)\n",
    "n.addOutputModule(outLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It still needs to be explicitly determined how they should be connected. For this we use the most common connection type, which produces a full connectivity between layers, by connecting each neuron of one layer with each neuron of the other. This is implemented by the FullConnection class. Then we should add the connection into the network too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.structure import FullConnection\n",
    "in_to_hidden = FullConnection(inLayer, hiddenLayer)\n",
    "hidden_to_out = FullConnection(hiddenLayer, outLayer)\n",
    "n.addConnection(in_to_hidden)\n",
    "n.addConnection(hidden_to_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the follwing call to do some internal initialization. for example, the modules are sorted topologically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n.sortModules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the above network and dataset for training our network with the help of the backpropagation algrithm. This call trains the net for one full epoch and returns a double proportional to the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80193786038326775"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "trainer = BackpropTrainer(net, ds)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.22517474811751079,\n",
       "  0.21682182900060601,\n",
       "  0.20896542698232856,\n",
       "  0.20157189191421851,\n",
       "  0.19459549825564756,\n",
       "  0.18799025534319461,\n",
       "  0.1817039400639302,\n",
       "  0.17577988482480278,\n",
       "  0.17016881843526074,\n",
       "  0.16483740461433805,\n",
       "  0.15980104861581981,\n",
       "  0.15499447248222473,\n",
       "  0.15040090938108719,\n",
       "  0.14606361516617325,\n",
       "  0.14191375949697779,\n",
       "  0.13797135106928551,\n",
       "  0.13417538513450242,\n",
       "  0.13058148776043735,\n",
       "  0.12713740461215328,\n",
       "  0.12388077124976764,\n",
       "  0.12076737413695732,\n",
       "  0.11778343778028533,\n",
       "  0.11493042765086887,\n",
       "  0.11221203206121594,\n",
       "  0.10958837923690613,\n",
       "  0.10707770909084359,\n",
       "  0.1046565142721082,\n",
       "  0.10236149728766843,\n",
       "  0.10017268173955879,\n",
       "  0.098048930762344097,\n",
       "  0.096005921417982823,\n",
       "  0.094051056246807832,\n",
       "  0.092162706188148424,\n",
       "  0.090348342109507304,\n",
       "  0.088604310221977081,\n",
       "  0.086921382117587251,\n",
       "  0.085334579033544092,\n",
       "  0.08378773908050513,\n",
       "  0.08230396864892163,\n",
       "  0.080875185813377423,\n",
       "  0.079488319217214748,\n",
       "  0.078155823902063604,\n",
       "  0.076860431475922392,\n",
       "  0.075608031978423043,\n",
       "  0.074424717804630641,\n",
       "  0.073270028924201244,\n",
       "  0.072150885106173832,\n",
       "  0.071069975573964994,\n",
       "  0.070025809897021715,\n",
       "  0.069004489421535795,\n",
       "  0.068027107720301697,\n",
       "  0.067085779095678921,\n",
       "  0.066161089107076521,\n",
       "  0.065276873026305846,\n",
       "  0.064419299081513143,\n",
       "  0.063582549483454445,\n",
       "  0.062774435926470584,\n",
       "  0.061984184292675992,\n",
       "  0.061221739685418546,\n",
       "  0.060476510611758537,\n",
       "  0.059747584050065437,\n",
       "  0.059047400786187504,\n",
       "  0.05836188665707806,\n",
       "  0.057704710337669475,\n",
       "  0.057052753707866666,\n",
       "  0.056424574678098048,\n",
       "  0.055820501938873494,\n",
       "  0.055227413465593622,\n",
       "  0.054640280758642558,\n",
       "  0.054077076877593604,\n",
       "  0.053529070118502255,\n",
       "  0.052988851817433812,\n",
       "  0.052460915696250728,\n",
       "  0.05195015918624557,\n",
       "  0.051446723958719263,\n",
       "  0.050956593138373939,\n",
       "  0.050484150107752222,\n",
       "  0.050012385237780739,\n",
       "  0.049559949881470104,\n",
       "  0.049110035607321412,\n",
       "  0.048669939033861326,\n",
       "  0.048239294182407938,\n",
       "  0.047818799022769372,\n",
       "  0.047402878291830715,\n",
       "  0.046998567633787208,\n",
       "  0.046608538008889831,\n",
       "  0.046218806960144056,\n",
       "  0.045833439047652164,\n",
       "  0.045460270430514198,\n",
       "  0.045100721244999087,\n",
       "  0.044739590402420093,\n",
       "  0.04438402953208246,\n",
       "  0.044036973848447174,\n",
       "  0.04369180101514495,\n",
       "  0.043362631752460468,\n",
       "  0.043033395726137742,\n",
       "  0.0427073882393934,\n",
       "  0.042393548331102739,\n",
       "  0.042081477461178497,\n",
       "  0.041772743569064762,\n",
       "  0.041470321116841576,\n",
       "  0.041169089886501177,\n",
       "  0.040877821928047425,\n",
       "  0.040588359050584923,\n",
       "  0.040309227433597987,\n",
       "  0.040025256239930383,\n",
       "  0.039754075387179968,\n",
       "  0.039477956612803532,\n",
       "  0.039212527160422506,\n",
       "  0.038948780287369859,\n",
       "  0.038683678850050932,\n",
       "  0.038429398610707773,\n",
       "  0.038171460064132738,\n",
       "  0.03792298587212211,\n",
       "  0.037675095853376465,\n",
       "  0.037425120691714504,\n",
       "  0.037185476895254997,\n",
       "  0.036947689107961229,\n",
       "  0.036710192198944037,\n",
       "  0.03647397443945273,\n",
       "  0.036241155640253049,\n",
       "  0.036012556816198242,\n",
       "  0.035790634407008241,\n",
       "  0.035568246870995177,\n",
       "  0.035345159643666725,\n",
       "  0.035126917922600635,\n",
       "  0.03490905019707944,\n",
       "  0.034699454572299539,\n",
       "  0.034486973598815661,\n",
       "  0.034275541459428266,\n",
       "  0.034071274298554652,\n",
       "  0.033863575546582723,\n",
       "  0.033661014180891298,\n",
       "  0.033462336708135136,\n",
       "  0.033263116470596064,\n",
       "  0.033063849746988526,\n",
       "  0.03287118067885824,\n",
       "  0.032675018907140051,\n",
       "  0.032485519707086997,\n",
       "  0.032295352796827208,\n",
       "  0.032108339550803243,\n",
       "  0.031919958143092293,\n",
       "  0.031732395028307568,\n",
       "  0.031551178905304962,\n",
       "  0.031367248358474979,\n",
       "  0.031190495151711583,\n",
       "  0.031007149604194328,\n",
       "  0.030833872102842139,\n",
       "  0.030657420667173519,\n",
       "  0.030481951449776456,\n",
       "  0.030306550555513883,\n",
       "  0.03013692353047423,\n",
       "  0.029966369403154582,\n",
       "  0.029794736487332846,\n",
       "  0.029627101988952079,\n",
       "  0.029463402167534593,\n",
       "  0.029295747566045766,\n",
       "  0.02913507986666471,\n",
       "  0.028968570392460858,\n",
       "  0.028808901140164284,\n",
       "  0.02864592749974294,\n",
       "  0.028488967560286942,\n",
       "  0.028331327131457043,\n",
       "  0.028173984857354371,\n",
       "  0.028015227078782873,\n",
       "  0.027862574575127377,\n",
       "  0.027706632049194296,\n",
       "  0.027553454404444771,\n",
       "  0.027402083224131558,\n",
       "  0.027250943228319242,\n",
       "  0.02710328403404505,\n",
       "  0.026952356524273097,\n",
       "  0.026804143770450883,\n",
       "  0.026659575675230422,\n",
       "  0.026513714244796701,\n",
       "  0.026369282328184668,\n",
       "  0.026223636721094557,\n",
       "  0.026080064572005263,\n",
       "  0.025937142020723183],\n",
       " [2.2855149740650718,\n",
       "  2.23236057149626,\n",
       "  2.1816847719163386,\n",
       "  2.134235042695348,\n",
       "  2.0878475268029861,\n",
       "  2.0444250972475135,\n",
       "  2.001981996257455,\n",
       "  1.9613467004786187,\n",
       "  1.9231910121348272,\n",
       "  1.8859102821154521,\n",
       "  1.8508435954955735,\n",
       "  1.8172522850355941,\n",
       "  1.7842119735222886,\n",
       "  1.7532925683871159,\n",
       "  1.7227805365487641,\n",
       "  1.6942189586199874,\n",
       "  1.6660737603090787,\n",
       "  1.6389259838979489,\n",
       "  1.6129126977206938,\n",
       "  1.5885458233229799,\n",
       "  1.5650684098081882,\n",
       "  1.5424415774278548,\n",
       "  1.5199140120286148,\n",
       "  1.4989178976469639,\n",
       "  1.4785962577286391,\n",
       "  1.4583984640308056,\n",
       "  1.4390185431467675,\n",
       "  1.4209135770582157,\n",
       "  1.4034295223928692,\n",
       "  1.3864575071468614,\n",
       "  1.3695124613666705,\n",
       "  1.3531657629601341,\n",
       "  1.3374731812777976,\n",
       "  1.322318530761198,\n",
       "  1.3076801505534541,\n",
       "  1.2935374326938376,\n",
       "  1.2803866691621273,\n",
       "  1.2675771954435695,\n",
       "  1.2552694951246557,\n",
       "  1.243354489974366,\n",
       "  1.2312467708659944,\n",
       "  1.2195451325386155,\n",
       "  1.2083137375408621,\n",
       "  1.1973571327054144,\n",
       "  1.1873174489828842,\n",
       "  1.1775047459401569,\n",
       "  1.167532476127515,\n",
       "  1.1583459741607283,\n",
       "  1.1489995458091338,\n",
       "  1.1400353603298286,\n",
       "  1.1312738549849011,\n",
       "  1.12332228425289,\n",
       "  1.1151643747892612,\n",
       "  1.1077018971191073,\n",
       "  1.100462137827074,\n",
       "  1.0929063665041876,\n",
       "  1.0860171015307789,\n",
       "  1.0790064157109187,\n",
       "  1.0725354961548081,\n",
       "  1.065838242434999,\n",
       "  1.0594474192714021,\n",
       "  1.0531717956854083,\n",
       "  1.0470715973549714,\n",
       "  1.0416674801551327,\n",
       "  1.0360118320147067,\n",
       "  1.0304475046773567,\n",
       "  1.0255345679335064,\n",
       "  1.0206588294381131,\n",
       "  1.0156320509437822,\n",
       "  1.0111477485380258,\n",
       "  1.0067898129443997,\n",
       "  1.0020649026651152,\n",
       "  0.99758607045148129,\n",
       "  0.99351962707816865,\n",
       "  0.98929359899009683,\n",
       "  0.98546731859063696,\n",
       "  0.98185546150189107,\n",
       "  0.97823986663290019,\n",
       "  0.97480835985117376,\n",
       "  0.97102229476124891,\n",
       "  0.96735209624764507,\n",
       "  0.96379406026288295,\n",
       "  0.96034462328290149,\n",
       "  0.95710335464985175,\n",
       "  0.95388092486881904,\n",
       "  0.95119258557598019,\n",
       "  0.94822088720838538,\n",
       "  0.94533642099184445,\n",
       "  0.94288949821787371,\n",
       "  0.94048244347952836,\n",
       "  0.93781717120238195,\n",
       "  0.93523083361978554,\n",
       "  0.93261770026914992,\n",
       "  0.93008579404379121,\n",
       "  0.92805133292609632,\n",
       "  0.92565380600145108,\n",
       "  0.92333130414021036,\n",
       "  0.92152284658094485,\n",
       "  0.91932356118989023,\n",
       "  0.91729661560800313,\n",
       "  0.91555983736815705,\n",
       "  0.91357215422185345,\n",
       "  0.91202544300807098,\n",
       "  0.91054793403829992,\n",
       "  0.90908253925106752,\n",
       "  0.90758097413426841,\n",
       "  0.90620031647665811,\n",
       "  0.9047818950207549,\n",
       "  0.90348217506721085,\n",
       "  0.90185218545313073,\n",
       "  0.90056767393793735,\n",
       "  0.89939780320326224,\n",
       "  0.89786873652270505,\n",
       "  0.8968107921407964,\n",
       "  0.89536543191564122,\n",
       "  0.89407134470255123,\n",
       "  0.89313287945866493,\n",
       "  0.89183790806247998,\n",
       "  0.89066044233420105,\n",
       "  0.88952166143201117,\n",
       "  0.88835003703934456,\n",
       "  0.88760012361738505,\n",
       "  0.88684341574897763,\n",
       "  0.88573143151822453,\n",
       "  0.88476033834646695,\n",
       "  0.88375307330007835,\n",
       "  0.88278459220749905,\n",
       "  0.88222499163714863,\n",
       "  0.88138157736954736,\n",
       "  0.8807683911450972,\n",
       "  0.88027800186986171,\n",
       "  0.87940669350467116,\n",
       "  0.87896585425023366,\n",
       "  0.87853998278592815,\n",
       "  0.87776536083155732,\n",
       "  0.87702378437698825,\n",
       "  0.87667546011232667,\n",
       "  0.87594954819124182,\n",
       "  0.87564379824093752,\n",
       "  0.87499172891479249,\n",
       "  0.87436940731622992,\n",
       "  0.87384202537160116,\n",
       "  0.87324074339075775,\n",
       "  0.87305062795723509,\n",
       "  0.87251647788041931,\n",
       "  0.87236050789033626,\n",
       "  0.87192689148331348,\n",
       "  0.87176905614854483,\n",
       "  0.87136383211464008,\n",
       "  0.87097798103104129,\n",
       "  0.87079825448597947,\n",
       "  0.87072630807272822,\n",
       "  0.87031600678238019,\n",
       "  0.86992955490062929,\n",
       "  0.8698804049045159,\n",
       "  0.86983668457956587,\n",
       "  0.86945647579740759,\n",
       "  0.8694685829855866,\n",
       "  0.86920891484900376,\n",
       "  0.86924127955193653,\n",
       "  0.86894074967968771,\n",
       "  0.86896957312948342,\n",
       "  0.86869544209114857,\n",
       "  0.8686847245357201,\n",
       "  0.86841191225244074,\n",
       "  0.86849285183327096,\n",
       "  0.86827302723460908,\n",
       "  0.86840343930714381,\n",
       "  0.86850796508007488,\n",
       "  0.86864324095656353,\n",
       "  0.86875298215207875,\n",
       "  0.8685660843435915,\n",
       "  0.86872489542392006,\n",
       "  0.86885795597380633,\n",
       "  0.86866767129641764,\n",
       "  0.86876024165797006,\n",
       "  0.86868434800635741,\n",
       "  0.86853371582697958,\n",
       "  0.86840074181072124,\n",
       "  0.86831326898892314])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.trainUntilConvergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above call make the training end until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the .activate() method, which expects a list, tuple or an array as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34661302])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.activate([2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice One: Classification with PyBrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will process a classification problem, from the precess of setting up a dataset to visualize the results with PyBrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The followings are PyBrain libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.datasets            import ClassificationDataSet\n",
    "from pybrain.utilities           import percentError\n",
    "from pybrain.tools.shortcuts     import buildNetwork\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from pybrain.structure.modules   import SoftmaxLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings are for graphical output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import ion, ioff, figure, draw, contourf, clf, show, hold, plot\n",
    "from scipy import diag, arange, meshgrid, where\n",
    "from numpy.random import multivariate_normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to generate three classed, which all obey normal distribution. First we initialize the means and the variance. We use 2-dimensional points as inputs for clearly visulization. So the initialization are as following. For each class, we have 400 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "means = [(-1,0),(2,4),(3,1)]\n",
    "cov = [diag([1,1]), diag([0.5,1.2]), diag([1.5,0.7])]\n",
    "alldata = ClassificationDataSet(2, 1, nb_classes=3)\n",
    "for n in range(400):\n",
    "    for klass in range(3):\n",
    "        input = multivariate_normal(means[klass],cov[klass])\n",
    "        alldata.addSample(input, [klass])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could divide our dataset into two parts, one is the training set, the other is the test set. For neural network classification, it is highly advisable to encode classes with one output neuron per class. Note that this operation duplicates the original targets and stores them in an (integer) field named ‘class', with the _convertToOneOfMany method. Since only the ClassificationDataSet has the method _convertToOneOfMany. So we must change the training set and the test set from SupervisedDataSet to ClassificationDataSet first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training patterns:  900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tstdata_temp, trndata_temp = alldata.splitWithProportion(0.25)\n",
    "\n",
    "tstdata = ClassificationDataSet(2, 1, nb_classes=3)\n",
    "for n in range(0, tstdata_temp.getLength()):\n",
    "    tstdata.addSample( tstdata_temp.getSample(n)[0], tstdata_temp.getSample(n)[1] )\n",
    "\n",
    "trndata = ClassificationDataSet(2, 1, nb_classes=3)\n",
    "for n in range(0, trndata_temp.getLength()):\n",
    "    trndata.addSample( trndata_temp.getSample(n)[0], trndata_temp.getSample(n)[1] )\n",
    "print(\"Number of training patterns: \", len(trndata))\n",
    "\n",
    "trndata._convertToOneOfMany()\n",
    "tstdata._convertToOneOfMany()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25% of the raw dataset has been divided as test set, and 75% for the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could build and train our network with the dataset above. What we should pay attention to is that the input layer and output layer must correspond to the dataset. Since we do the classification, the output layer uses a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fnn = buildNetwork( trndata.indim, 5, trndata.outdim, outclass=SoftmaxLayer)\n",
    "trainer = BackpropTrainer( fnn, dataset=trndata, momentum=0.1, verbose=True, weightdecay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate a square grid of data points and put it into a dataset, which we can then classify to obtain a nice contour field for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = arange(-3.5, 6.5, 0.1)\n",
    "X, Y = meshgrid(ticks, ticks) # here we generate a 2-d plane,with the ticks as the range value\n",
    "griddata = ClassificationDataSet(2,1, nb_classes=3)\n",
    "for i in range(X.size):\n",
    "    griddata.addSample([X.ravel()[i],Y.ravel()[i]], [0]) # add the inputs into griddata.\n",
    "griddata._convertToOneOfMany()  # this is still needed to make the fnn feel comfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could train our network.Here we train 20 times with 1 epoch in one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error:  0.0736821208972\n",
      "Total error:  0.035530374686\n",
      "Total error:  0.024584318864\n",
      "Total error:  0.0200973242223\n",
      "Total error:  0.0177248410822\n",
      "Total error:  0.0161721010931\n",
      "Total error:  0.0152092384641\n",
      "Total error:  0.0145223712535\n",
      "Total error:  0.0140938491402\n",
      "Total error:  0.0137798340821\n",
      "Total error:  0.0136043612494\n",
      "Total error:  0.0134528978352\n",
      "Total error:  0.0133990996835\n",
      "Total error:  0.0132661065907\n",
      "Total error:  0.0132965118485\n",
      "Total error:  0.0131383826134\n",
      "Total error:  0.0132667399516\n",
      "Total error:  0.0130700316935\n",
      "Total error:  0.0132162389365\n",
      "Total error:  0.0131611474466\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    trainer.trainEpochs( 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use testOnClassData() method and the percentError function to calculate the percent error to the output on the training set and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20   train error: 4.22%   test error: 6.00%\n"
     ]
    }
   ],
   "source": [
    "trnresult = percentError( trainer.testOnClassData(),\n",
    "                              trndata['class'] )\n",
    "tstresult = percentError( trainer.testOnClassData(\n",
    "           dataset=tstdata ), tstdata['class'] )\n",
    "\n",
    "print(\"epoch: %d\" % trainer.totalepochs, \"  train error: %.2f%%\" % trnresult, \"  test error: %.2f%%\" % tstresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our grid data through the FNN, get the most likely class and shape it into a square array again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = fnn.activateOnDataset(griddata)\n",
    "out = out.argmax(axis=1)  # the highest output activation gives the class\n",
    "out = out.reshape(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could show the result with the plot as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwzANBgs7wzBMg/H/AUkJ2ICkVhQLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1799bac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure(1)\n",
    "for c in [0,1,2]:\n",
    "    here, _ = where(tstdata['class']==c)\n",
    "    plot(tstdata['input'][here,0],tstdata['input'][here,1],'o')\n",
    "contourf(X, Y, out)   # plot the contour\n",
    "ioff()\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Two: Linear Regression with PyBrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For preparation, we have two datasets in /data, one for training, the other for testing. We'll first train our network on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the data from train file and store to x_train and y_train seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from pybrain.datasets.supervised import SupervisedDataSet\n",
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "\n",
    "train_file = 'data/train.csv'\n",
    "hidden_size = 100\n",
    "epochs = 600 \n",
    "\n",
    "# load data\n",
    "train = np.loadtxt(train_file, delimiter = ',')\n",
    "\n",
    "x_train = train[:,0:-1]\n",
    "y_train = train[:,-1]\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "input_size = x_train.shape[1]\n",
    "target_size = y_train.shape[1]\n",
    "\n",
    "# prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we form the SupervisedDataSet object as the input of the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = SupervisedDataSet(input_size, target_size)\n",
    "ds.setField('input', x_train)\n",
    "ds.setField('target', y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train our network and print the mse of each epoch during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 mse: 0.22296643373\n",
      "epoch : 2 mse: 0.0561892831443\n",
      "epoch : 3 mse: 0.0344892424813\n",
      "epoch : 4 mse: 0.0260595222727\n",
      "epoch : 5 mse: 0.020794811536\n",
      "epoch : 6 mse: 0.0181257939061\n",
      "epoch : 7 mse: 0.0158878162451\n",
      "epoch : 8 mse: 0.0143722862715\n",
      "epoch : 9 mse: 0.0132074069985\n",
      "epoch : 10 mse: 0.0124227916853\n",
      "epoch : 11 mse: 0.0116658970605\n",
      "epoch : 12 mse: 0.0111162519901\n",
      "epoch : 13 mse: 0.010564477661\n",
      "epoch : 14 mse: 0.010006170306\n",
      "epoch : 15 mse: 0.00966808333664\n",
      "epoch : 16 mse: 0.00935857659081\n",
      "epoch : 17 mse: 0.00912477118066\n",
      "epoch : 18 mse: 0.00887478079187\n",
      "epoch : 19 mse: 0.00851130611597\n",
      "epoch : 20 mse: 0.00834269016198\n",
      "epoch : 21 mse: 0.00800895032429\n",
      "epoch : 22 mse: 0.00796012641417\n",
      "epoch : 23 mse: 0.00779838449905\n",
      "epoch : 24 mse: 0.00764792541302\n",
      "epoch : 25 mse: 0.00743000364209\n",
      "epoch : 26 mse: 0.00736049669858\n",
      "epoch : 27 mse: 0.00720774813148\n",
      "epoch : 28 mse: 0.00716394186147\n",
      "epoch : 29 mse: 0.00703192409051\n",
      "epoch : 30 mse: 0.0068801292674\n",
      "epoch : 31 mse: 0.00679593220546\n",
      "epoch : 32 mse: 0.00673909986262\n",
      "epoch : 33 mse: 0.0066411492617\n",
      "epoch : 34 mse: 0.00649967502239\n",
      "epoch : 35 mse: 0.00645667503748\n",
      "epoch : 36 mse: 0.00627341910251\n",
      "epoch : 37 mse: 0.0063067833931\n",
      "epoch : 38 mse: 0.00618821556714\n",
      "epoch : 39 mse: 0.00615645432693\n",
      "epoch : 40 mse: 0.00608211270038\n",
      "epoch : 41 mse: 0.00598522439628\n",
      "epoch : 42 mse: 0.00589599223047\n",
      "epoch : 43 mse: 0.00585907433689\n",
      "epoch : 44 mse: 0.00586507686326\n",
      "epoch : 45 mse: 0.00578717749131\n",
      "epoch : 46 mse: 0.00574308037162\n",
      "epoch : 47 mse: 0.00565769171739\n",
      "epoch : 48 mse: 0.00559406187808\n",
      "epoch : 49 mse: 0.00554031315792\n",
      "epoch : 50 mse: 0.00555414114408\n",
      "epoch : 51 mse: 0.00549320978685\n",
      "epoch : 52 mse: 0.00554367657275\n",
      "epoch : 53 mse: 0.00540213848981\n",
      "epoch : 54 mse: 0.00532033677493\n",
      "epoch : 55 mse: 0.00527974317754\n",
      "epoch : 56 mse: 0.00528589446763\n",
      "epoch : 57 mse: 0.00529131620028\n",
      "epoch : 58 mse: 0.00521870884204\n",
      "epoch : 59 mse: 0.00521060979718\n",
      "epoch : 60 mse: 0.00516503509659\n",
      "epoch : 61 mse: 0.00516126762311\n",
      "epoch : 62 mse: 0.0050968987687\n",
      "epoch : 63 mse: 0.00504442890661\n",
      "epoch : 64 mse: 0.00501109864195\n",
      "epoch : 65 mse: 0.0050433364368\n",
      "epoch : 66 mse: 0.00496207783808\n",
      "epoch : 67 mse: 0.00496523623917\n",
      "epoch : 68 mse: 0.00497170460211\n",
      "epoch : 69 mse: 0.00489299629807\n",
      "epoch : 70 mse: 0.00487103796487\n",
      "epoch : 71 mse: 0.00487872724403\n",
      "epoch : 72 mse: 0.00489150184938\n",
      "epoch : 73 mse: 0.00485477098554\n",
      "epoch : 74 mse: 0.00480887851055\n",
      "epoch : 75 mse: 0.00477713005927\n",
      "epoch : 76 mse: 0.0047941524029\n",
      "epoch : 77 mse: 0.00472776429396\n",
      "epoch : 78 mse: 0.0047397774548\n",
      "epoch : 79 mse: 0.00472415686473\n",
      "epoch : 80 mse: 0.0046686965769\n",
      "epoch : 81 mse: 0.00463982311463\n",
      "epoch : 82 mse: 0.00462947983125\n",
      "epoch : 83 mse: 0.00464880226909\n",
      "epoch : 84 mse: 0.00458127532747\n",
      "epoch : 85 mse: 0.00456503922735\n",
      "epoch : 86 mse: 0.004565309195\n",
      "epoch : 87 mse: 0.00453829517366\n",
      "epoch : 88 mse: 0.00455038108066\n",
      "epoch : 89 mse: 0.00449615535432\n",
      "epoch : 90 mse: 0.00450118563297\n",
      "epoch : 91 mse: 0.00448486485094\n",
      "epoch : 92 mse: 0.00443788727068\n",
      "epoch : 93 mse: 0.00446710280418\n",
      "epoch : 94 mse: 0.00440604540784\n",
      "epoch : 95 mse: 0.00440547584778\n",
      "epoch : 96 mse: 0.0043885965918\n",
      "epoch : 97 mse: 0.00436994232409\n",
      "epoch : 98 mse: 0.00432422898841\n",
      "epoch : 99 mse: 0.0043654759756\n",
      "epoch : 100 mse: 0.00434926969808\n",
      "epoch : 101 mse: 0.00434274933024\n",
      "epoch : 102 mse: 0.00429183276451\n",
      "epoch : 103 mse: 0.00435449808289\n",
      "epoch : 104 mse: 0.00429097329552\n",
      "epoch : 105 mse: 0.00429791413943\n",
      "epoch : 106 mse: 0.00424491374526\n",
      "epoch : 107 mse: 0.00422928913335\n",
      "epoch : 108 mse: 0.00422495922179\n",
      "epoch : 109 mse: 0.0042361646342\n",
      "epoch : 110 mse: 0.00422438973983\n",
      "epoch : 111 mse: 0.00420005138078\n",
      "epoch : 112 mse: 0.00416764740141\n",
      "epoch : 113 mse: 0.0041382326286\n",
      "epoch : 114 mse: 0.00411825068141\n",
      "epoch : 115 mse: 0.00409896567747\n",
      "epoch : 116 mse: 0.0041300101609\n",
      "epoch : 117 mse: 0.00410937996604\n",
      "epoch : 118 mse: 0.00409544970622\n",
      "epoch : 119 mse: 0.00413812829449\n",
      "epoch : 120 mse: 0.00410246784712\n",
      "epoch : 121 mse: 0.00404717397535\n",
      "epoch : 122 mse: 0.00406916822311\n",
      "epoch : 123 mse: 0.00403303157494\n",
      "epoch : 124 mse: 0.00400948139763\n",
      "epoch : 125 mse: 0.00401779488217\n",
      "epoch : 126 mse: 0.00401782150345\n",
      "epoch : 127 mse: 0.00396732318969\n",
      "epoch : 128 mse: 0.00398990657598\n",
      "epoch : 129 mse: 0.00398555888253\n",
      "epoch : 130 mse: 0.00399063845765\n",
      "epoch : 131 mse: 0.00397620686794\n",
      "epoch : 132 mse: 0.00394124099463\n",
      "epoch : 133 mse: 0.00394318468744\n",
      "epoch : 134 mse: 0.00394429631158\n",
      "epoch : 135 mse: 0.0039217966198\n",
      "epoch : 136 mse: 0.00394539097979\n",
      "epoch : 137 mse: 0.00394800309977\n",
      "epoch : 138 mse: 0.0038996750869\n",
      "epoch : 139 mse: 0.00387427798717\n",
      "epoch : 140 mse: 0.0038555940704\n",
      "epoch : 141 mse: 0.0038578615606\n",
      "epoch : 142 mse: 0.00385693231055\n",
      "epoch : 143 mse: 0.00385447054121\n",
      "epoch : 144 mse: 0.00382371014531\n",
      "epoch : 145 mse: 0.00379530445065\n",
      "epoch : 146 mse: 0.00378687574171\n",
      "epoch : 147 mse: 0.00382254699949\n",
      "epoch : 148 mse: 0.00377266015769\n",
      "epoch : 149 mse: 0.00376276511641\n",
      "epoch : 150 mse: 0.00378783043689\n",
      "epoch : 151 mse: 0.00375719181978\n",
      "epoch : 152 mse: 0.00379541282386\n",
      "epoch : 153 mse: 0.0037600486018\n",
      "epoch : 154 mse: 0.00375091941972\n",
      "epoch : 155 mse: 0.00371725979128\n",
      "epoch : 156 mse: 0.0037509854162\n",
      "epoch : 157 mse: 0.0037198651277\n",
      "epoch : 158 mse: 0.00367615170363\n",
      "epoch : 159 mse: 0.0037055800656\n",
      "epoch : 160 mse: 0.00374170459533\n",
      "epoch : 161 mse: 0.00367061813454\n",
      "epoch : 162 mse: 0.0037329421453\n",
      "epoch : 163 mse: 0.00370072979758\n",
      "epoch : 164 mse: 0.00366914515317\n",
      "epoch : 165 mse: 0.00367881545702\n",
      "epoch : 166 mse: 0.00364382395393\n",
      "epoch : 167 mse: 0.00364407487503\n",
      "epoch : 168 mse: 0.00367311456145\n",
      "epoch : 169 mse: 0.00361881715756\n",
      "epoch : 170 mse: 0.00362573636866\n",
      "epoch : 171 mse: 0.00362358070123\n",
      "epoch : 172 mse: 0.00360365368104\n",
      "epoch : 173 mse: 0.00360759430612\n",
      "epoch : 174 mse: 0.00367083705306\n",
      "epoch : 175 mse: 0.00364827613324\n",
      "epoch : 176 mse: 0.00359405824704\n",
      "epoch : 177 mse: 0.00360706917396\n",
      "epoch : 178 mse: 0.00360599707473\n",
      "epoch : 179 mse: 0.00359929480645\n",
      "epoch : 180 mse: 0.00357228554267\n",
      "epoch : 181 mse: 0.00357511059599\n",
      "epoch : 182 mse: 0.00359608626888\n",
      "epoch : 183 mse: 0.00357239503626\n",
      "epoch : 184 mse: 0.00357021000747\n",
      "epoch : 185 mse: 0.00354831217638\n",
      "epoch : 186 mse: 0.00358087729564\n",
      "epoch : 187 mse: 0.00352825661959\n",
      "epoch : 188 mse: 0.00352184461916\n",
      "epoch : 189 mse: 0.0035198370455\n",
      "epoch : 190 mse: 0.00352306316488\n",
      "epoch : 191 mse: 0.00353831460866\n",
      "epoch : 192 mse: 0.00354404767878\n",
      "epoch : 193 mse: 0.00352068584321\n",
      "epoch : 194 mse: 0.00347360481917\n",
      "epoch : 195 mse: 0.00348896675941\n",
      "epoch : 196 mse: 0.00347326983658\n",
      "epoch : 197 mse: 0.00349977191387\n",
      "epoch : 198 mse: 0.00348840199341\n",
      "epoch : 199 mse: 0.00349161022592\n",
      "epoch : 200 mse: 0.00353212104233\n",
      "epoch : 201 mse: 0.00344397797133\n",
      "epoch : 202 mse: 0.0034440815851\n",
      "epoch : 203 mse: 0.00348177420563\n",
      "epoch : 204 mse: 0.00345627223863\n",
      "epoch : 205 mse: 0.00343350465951\n",
      "epoch : 206 mse: 0.00344419842254\n",
      "epoch : 207 mse: 0.00341419496124\n",
      "epoch : 208 mse: 0.0034153444097\n",
      "epoch : 209 mse: 0.00344375357804\n",
      "epoch : 210 mse: 0.00342369530684\n",
      "epoch : 211 mse: 0.00343903382332\n",
      "epoch : 212 mse: 0.00339439066334\n",
      "epoch : 213 mse: 0.00340564442872\n",
      "epoch : 214 mse: 0.00343162399681\n",
      "epoch : 215 mse: 0.003433471495\n",
      "epoch : 216 mse: 0.00343254118461\n",
      "epoch : 217 mse: 0.00342095384797\n",
      "epoch : 218 mse: 0.00340771626819\n",
      "epoch : 219 mse: 0.00340254537184\n",
      "epoch : 220 mse: 0.00339270513207\n",
      "epoch : 221 mse: 0.00338607094871\n",
      "epoch : 222 mse: 0.00339335458856\n",
      "epoch : 223 mse: 0.00340819215633\n",
      "epoch : 224 mse: 0.00334952146625\n",
      "epoch : 225 mse: 0.00338711117413\n",
      "epoch : 226 mse: 0.00336798576364\n",
      "epoch : 227 mse: 0.00336244113378\n",
      "epoch : 228 mse: 0.00335321500392\n",
      "epoch : 229 mse: 0.00336011988054\n",
      "epoch : 230 mse: 0.00339394309\n",
      "epoch : 231 mse: 0.00338690927318\n",
      "epoch : 232 mse: 0.00335125822457\n",
      "epoch : 233 mse: 0.0033855589073\n",
      "epoch : 234 mse: 0.00334086388107\n",
      "epoch : 235 mse: 0.0033376881386\n",
      "epoch : 236 mse: 0.0033229281099\n",
      "epoch : 237 mse: 0.00333218231043\n",
      "epoch : 238 mse: 0.00334632644446\n",
      "epoch : 239 mse: 0.0033463373765\n",
      "epoch : 240 mse: 0.003322398556\n",
      "epoch : 241 mse: 0.00330585477894\n",
      "epoch : 242 mse: 0.00334161260095\n",
      "epoch : 243 mse: 0.0033024918265\n",
      "epoch : 244 mse: 0.00332919849494\n",
      "epoch : 245 mse: 0.00333009598651\n",
      "epoch : 246 mse: 0.00331798591718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 247 mse: 0.00332662176832\n",
      "epoch : 248 mse: 0.00330596442853\n",
      "epoch : 249 mse: 0.00328988394377\n",
      "epoch : 250 mse: 0.00327525724193\n",
      "epoch : 251 mse: 0.00327425444969\n",
      "epoch : 252 mse: 0.00331232034249\n",
      "epoch : 253 mse: 0.0033017202812\n",
      "epoch : 254 mse: 0.00328166243843\n",
      "epoch : 255 mse: 0.00331097883756\n",
      "epoch : 256 mse: 0.00326832741457\n",
      "epoch : 257 mse: 0.00329371600751\n",
      "epoch : 258 mse: 0.00329242950346\n",
      "epoch : 259 mse: 0.00325851724722\n",
      "epoch : 260 mse: 0.00323053775569\n",
      "epoch : 261 mse: 0.00327441789629\n",
      "epoch : 262 mse: 0.00328746329269\n",
      "epoch : 263 mse: 0.00329131345619\n",
      "epoch : 264 mse: 0.00325225345958\n",
      "epoch : 265 mse: 0.00325032396234\n",
      "epoch : 266 mse: 0.00326234832021\n",
      "epoch : 267 mse: 0.00326300839848\n",
      "epoch : 268 mse: 0.00326680296973\n",
      "epoch : 269 mse: 0.00325594590879\n",
      "epoch : 270 mse: 0.00326350118388\n",
      "epoch : 271 mse: 0.0032188768413\n",
      "epoch : 272 mse: 0.00323447966395\n",
      "epoch : 273 mse: 0.00319495446291\n",
      "epoch : 274 mse: 0.00325819376725\n",
      "epoch : 275 mse: 0.0032462382583\n",
      "epoch : 276 mse: 0.00324200717606\n",
      "epoch : 277 mse: 0.00321767968084\n",
      "epoch : 278 mse: 0.00322748133947\n",
      "epoch : 279 mse: 0.0032504867626\n",
      "epoch : 280 mse: 0.00321187379628\n",
      "epoch : 281 mse: 0.00326721544788\n",
      "epoch : 282 mse: 0.00320460587016\n",
      "epoch : 283 mse: 0.0032238873811\n",
      "epoch : 284 mse: 0.00325728113313\n",
      "epoch : 285 mse: 0.00320215125656\n",
      "epoch : 286 mse: 0.00326484006438\n",
      "epoch : 287 mse: 0.00320439914031\n",
      "epoch : 288 mse: 0.00320070066776\n",
      "epoch : 289 mse: 0.00317925211871\n",
      "epoch : 290 mse: 0.00317728734829\n",
      "epoch : 291 mse: 0.00318714157722\n",
      "epoch : 292 mse: 0.00317727447667\n",
      "epoch : 293 mse: 0.00321015925383\n",
      "epoch : 294 mse: 0.0031678307521\n",
      "epoch : 295 mse: 0.00321406724001\n",
      "epoch : 296 mse: 0.00319177632322\n",
      "epoch : 297 mse: 0.00319242708097\n",
      "epoch : 298 mse: 0.00314936050085\n",
      "epoch : 299 mse: 0.00314661892457\n",
      "epoch : 300 mse: 0.00318158205402\n",
      "epoch : 301 mse: 0.00316037897893\n",
      "epoch : 302 mse: 0.00314092000818\n",
      "epoch : 303 mse: 0.00315475851386\n",
      "epoch : 304 mse: 0.00321014008103\n",
      "epoch : 305 mse: 0.00318556624436\n",
      "epoch : 306 mse: 0.00316302086515\n",
      "epoch : 307 mse: 0.00315910433496\n",
      "epoch : 308 mse: 0.00317790580095\n",
      "epoch : 309 mse: 0.00314385843894\n",
      "epoch : 310 mse: 0.00317788671732\n",
      "epoch : 311 mse: 0.00311173252305\n",
      "epoch : 312 mse: 0.00313797344193\n",
      "epoch : 313 mse: 0.00317289675265\n",
      "epoch : 314 mse: 0.00311790357518\n",
      "epoch : 315 mse: 0.00310452524533\n",
      "epoch : 316 mse: 0.00318360183563\n",
      "epoch : 317 mse: 0.00316524263575\n",
      "epoch : 318 mse: 0.00313256946414\n",
      "epoch : 319 mse: 0.00315640353703\n",
      "epoch : 320 mse: 0.00313589371279\n",
      "epoch : 321 mse: 0.0031231987064\n",
      "epoch : 322 mse: 0.00309189775618\n",
      "epoch : 323 mse: 0.00311039168522\n",
      "epoch : 324 mse: 0.00313294086471\n",
      "epoch : 325 mse: 0.00311245408392\n",
      "epoch : 326 mse: 0.00314001800318\n",
      "epoch : 327 mse: 0.00313683342438\n",
      "epoch : 328 mse: 0.00311443906875\n",
      "epoch : 329 mse: 0.00310353577455\n",
      "epoch : 330 mse: 0.00312449238031\n",
      "epoch : 331 mse: 0.00311750218033\n",
      "epoch : 332 mse: 0.00313300032269\n",
      "epoch : 333 mse: 0.00310325543569\n",
      "epoch : 334 mse: 0.00313243079942\n",
      "epoch : 335 mse: 0.0030673755962\n",
      "epoch : 336 mse: 0.00308413085505\n",
      "epoch : 337 mse: 0.00306945451151\n",
      "epoch : 338 mse: 0.00308505888704\n",
      "epoch : 339 mse: 0.00311913562736\n",
      "epoch : 340 mse: 0.00306098982522\n",
      "epoch : 341 mse: 0.00310236526768\n",
      "epoch : 342 mse: 0.00309489249044\n",
      "epoch : 343 mse: 0.00307217861741\n",
      "epoch : 344 mse: 0.0030790099097\n",
      "epoch : 345 mse: 0.00311683363469\n",
      "epoch : 346 mse: 0.00312924829737\n",
      "epoch : 347 mse: 0.00310350922356\n",
      "epoch : 348 mse: 0.00308015053249\n",
      "epoch : 349 mse: 0.00306958516932\n",
      "epoch : 350 mse: 0.00308245277763\n",
      "epoch : 351 mse: 0.00309583643429\n",
      "epoch : 352 mse: 0.00310660938636\n",
      "epoch : 353 mse: 0.00310588678939\n",
      "epoch : 354 mse: 0.00307594711511\n",
      "epoch : 355 mse: 0.00307595906567\n",
      "epoch : 356 mse: 0.00307465027565\n",
      "epoch : 357 mse: 0.00308168609613\n",
      "epoch : 358 mse: 0.00307603608166\n",
      "epoch : 359 mse: 0.00306753087638\n",
      "epoch : 360 mse: 0.003065825385\n",
      "epoch : 361 mse: 0.00306702029969\n",
      "epoch : 362 mse: 0.00305329956728\n",
      "epoch : 363 mse: 0.00303639075068\n",
      "epoch : 364 mse: 0.00304178673937\n",
      "epoch : 365 mse: 0.00302580067818\n",
      "epoch : 366 mse: 0.00301612685822\n",
      "epoch : 367 mse: 0.00307467391254\n",
      "epoch : 368 mse: 0.00305060907847\n",
      "epoch : 369 mse: 0.00304530260865\n",
      "epoch : 370 mse: 0.00305781836684\n",
      "epoch : 371 mse: 0.00303081161174\n",
      "epoch : 372 mse: 0.00305320391544\n",
      "epoch : 373 mse: 0.00305679685554\n",
      "epoch : 374 mse: 0.00304467739667\n",
      "epoch : 375 mse: 0.00304992398488\n",
      "epoch : 376 mse: 0.00307667788389\n",
      "epoch : 377 mse: 0.00302758434367\n",
      "epoch : 378 mse: 0.00302948318675\n",
      "epoch : 379 mse: 0.0030279912608\n",
      "epoch : 380 mse: 0.00303645613094\n",
      "epoch : 381 mse: 0.00304480735559\n",
      "epoch : 382 mse: 0.00303077540821\n",
      "epoch : 383 mse: 0.00301289958107\n",
      "epoch : 384 mse: 0.0030124805867\n",
      "epoch : 385 mse: 0.00300078397623\n",
      "epoch : 386 mse: 0.00302653955525\n",
      "epoch : 387 mse: 0.00302523974415\n",
      "epoch : 388 mse: 0.00304209765937\n",
      "epoch : 389 mse: 0.0030301714153\n",
      "epoch : 390 mse: 0.00301104756434\n",
      "epoch : 391 mse: 0.00301412309948\n",
      "epoch : 392 mse: 0.00303437419962\n",
      "epoch : 393 mse: 0.00297389068742\n",
      "epoch : 394 mse: 0.00300300440438\n",
      "epoch : 395 mse: 0.0030280875653\n",
      "epoch : 396 mse: 0.00298969247002\n",
      "epoch : 397 mse: 0.00301879391299\n",
      "epoch : 398 mse: 0.00301438899749\n",
      "epoch : 399 mse: 0.00299936842053\n",
      "epoch : 400 mse: 0.00299912118979\n",
      "epoch : 401 mse: 0.00299760067567\n",
      "epoch : 402 mse: 0.00303681365214\n",
      "epoch : 403 mse: 0.00302254937617\n",
      "epoch : 404 mse: 0.00297054146683\n",
      "epoch : 405 mse: 0.00301150844\n",
      "epoch : 406 mse: 0.00297909910482\n",
      "epoch : 407 mse: 0.0030164745517\n",
      "epoch : 408 mse: 0.00301440934001\n",
      "epoch : 409 mse: 0.00299151189899\n",
      "epoch : 410 mse: 0.00296033299561\n",
      "epoch : 411 mse: 0.00301599082319\n",
      "epoch : 412 mse: 0.00301427203146\n",
      "epoch : 413 mse: 0.00295859055007\n",
      "epoch : 414 mse: 0.00299588415725\n",
      "epoch : 415 mse: 0.00293643696047\n",
      "epoch : 416 mse: 0.0029687142902\n",
      "epoch : 417 mse: 0.00300235455394\n",
      "epoch : 418 mse: 0.00302114626631\n",
      "epoch : 419 mse: 0.00299216048732\n",
      "epoch : 420 mse: 0.00296316316748\n",
      "epoch : 421 mse: 0.00295014073287\n",
      "epoch : 422 mse: 0.00297615286455\n",
      "epoch : 423 mse: 0.00297317549375\n",
      "epoch : 424 mse: 0.00297611752572\n",
      "epoch : 425 mse: 0.00301396723514\n",
      "epoch : 426 mse: 0.00296260165927\n",
      "epoch : 427 mse: 0.00298313492655\n",
      "epoch : 428 mse: 0.00298523450446\n",
      "epoch : 429 mse: 0.0029661375857\n",
      "epoch : 430 mse: 0.00297479870637\n",
      "epoch : 431 mse: 0.00297489771395\n",
      "epoch : 432 mse: 0.00301430603283\n",
      "epoch : 433 mse: 0.00293770416242\n",
      "epoch : 434 mse: 0.00297234300888\n",
      "epoch : 435 mse: 0.00296088382233\n",
      "epoch : 436 mse: 0.00295793949023\n",
      "epoch : 437 mse: 0.00296319179281\n",
      "epoch : 438 mse: 0.00296624124058\n",
      "epoch : 439 mse: 0.00296093058587\n",
      "epoch : 440 mse: 0.00293790188739\n",
      "epoch : 441 mse: 0.00298394355543\n",
      "epoch : 442 mse: 0.00296356581536\n",
      "epoch : 443 mse: 0.00297338419598\n",
      "epoch : 444 mse: 0.0029600810458\n",
      "epoch : 445 mse: 0.00299032836679\n",
      "epoch : 446 mse: 0.00298080263301\n",
      "epoch : 447 mse: 0.00294076878339\n",
      "epoch : 448 mse: 0.0029434058199\n",
      "epoch : 449 mse: 0.00292096493621\n",
      "epoch : 450 mse: 0.00296396271872\n",
      "epoch : 451 mse: 0.0029496187457\n",
      "epoch : 452 mse: 0.00294423724856\n",
      "epoch : 453 mse: 0.0029497085823\n",
      "epoch : 454 mse: 0.00295538289876\n",
      "epoch : 455 mse: 0.00295619928926\n",
      "epoch : 456 mse: 0.00293841975343\n",
      "epoch : 457 mse: 0.00295305381585\n",
      "epoch : 458 mse: 0.00292498489688\n",
      "epoch : 459 mse: 0.00295525907724\n",
      "epoch : 460 mse: 0.00294404034116\n",
      "epoch : 461 mse: 0.00295105829842\n",
      "epoch : 462 mse: 0.00294175282458\n",
      "epoch : 463 mse: 0.00290021767773\n",
      "epoch : 464 mse: 0.00291136810272\n",
      "epoch : 465 mse: 0.00290918459488\n",
      "epoch : 466 mse: 0.00289361409934\n",
      "epoch : 467 mse: 0.00289823144771\n",
      "epoch : 468 mse: 0.00291297363685\n",
      "epoch : 469 mse: 0.00293295371925\n",
      "epoch : 470 mse: 0.00292334952531\n",
      "epoch : 471 mse: 0.00292829673415\n",
      "epoch : 472 mse: 0.00295349045369\n",
      "epoch : 473 mse: 0.00295374060408\n",
      "epoch : 474 mse: 0.00291862371205\n",
      "epoch : 475 mse: 0.00291012496112\n",
      "epoch : 476 mse: 0.00292799942522\n",
      "epoch : 477 mse: 0.00291874669545\n",
      "epoch : 478 mse: 0.00288665631641\n",
      "epoch : 479 mse: 0.00293179545511\n",
      "epoch : 480 mse: 0.00292100364274\n",
      "epoch : 481 mse: 0.00292116787362\n",
      "epoch : 482 mse: 0.00291399507696\n",
      "epoch : 483 mse: 0.0029227869772\n",
      "epoch : 484 mse: 0.00290789092449\n",
      "epoch : 485 mse: 0.00291043331711\n",
      "epoch : 486 mse: 0.0029086272463\n",
      "epoch : 487 mse: 0.00292359021578\n",
      "epoch : 488 mse: 0.00292134418957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 489 mse: 0.00288333819445\n",
      "epoch : 490 mse: 0.00292787383281\n",
      "epoch : 491 mse: 0.0028758458536\n",
      "epoch : 492 mse: 0.00292652565553\n",
      "epoch : 493 mse: 0.00288974178341\n",
      "epoch : 494 mse: 0.0028947829905\n",
      "epoch : 495 mse: 0.00286429597847\n",
      "epoch : 496 mse: 0.0029025303703\n",
      "epoch : 497 mse: 0.00290718200685\n",
      "epoch : 498 mse: 0.00292160565514\n",
      "epoch : 499 mse: 0.00290967636263\n",
      "epoch : 500 mse: 0.0029111100732\n",
      "epoch : 501 mse: 0.00293310914759\n",
      "epoch : 502 mse: 0.00286150629932\n",
      "epoch : 503 mse: 0.00285650233266\n",
      "epoch : 504 mse: 0.00290021884426\n",
      "epoch : 505 mse: 0.00289852948933\n",
      "epoch : 506 mse: 0.00287917435959\n",
      "epoch : 507 mse: 0.00289431489073\n",
      "epoch : 508 mse: 0.00290841050832\n",
      "epoch : 509 mse: 0.00287069459107\n",
      "epoch : 510 mse: 0.00287564264352\n",
      "epoch : 511 mse: 0.00289624736793\n",
      "epoch : 512 mse: 0.00290820612108\n",
      "epoch : 513 mse: 0.00288446665077\n",
      "epoch : 514 mse: 0.00286742998785\n",
      "epoch : 515 mse: 0.00288161852154\n",
      "epoch : 516 mse: 0.00287238633085\n",
      "epoch : 517 mse: 0.00287648126904\n",
      "epoch : 518 mse: 0.00286275553475\n",
      "epoch : 519 mse: 0.00289016086345\n",
      "epoch : 520 mse: 0.00288580687587\n",
      "epoch : 521 mse: 0.00288241162277\n",
      "epoch : 522 mse: 0.00290666751166\n",
      "epoch : 523 mse: 0.00287820105555\n",
      "epoch : 524 mse: 0.00287189492347\n",
      "epoch : 525 mse: 0.00284904393138\n",
      "epoch : 526 mse: 0.00284076863975\n",
      "epoch : 527 mse: 0.00286954249131\n",
      "epoch : 528 mse: 0.00289445362912\n",
      "epoch : 529 mse: 0.00285159195214\n",
      "epoch : 530 mse: 0.0028760723546\n",
      "epoch : 531 mse: 0.00285351399865\n",
      "epoch : 532 mse: 0.00286139328684\n",
      "epoch : 533 mse: 0.00284802722826\n",
      "epoch : 534 mse: 0.00284137146725\n",
      "epoch : 535 mse: 0.00286677575471\n",
      "epoch : 536 mse: 0.00287517145441\n",
      "epoch : 537 mse: 0.00283519070954\n",
      "epoch : 538 mse: 0.00283345861352\n",
      "epoch : 539 mse: 0.00285574572418\n",
      "epoch : 540 mse: 0.0028765506979\n",
      "epoch : 541 mse: 0.00284634024493\n",
      "epoch : 542 mse: 0.00285799275316\n",
      "epoch : 543 mse: 0.00284919750611\n",
      "epoch : 544 mse: 0.00281919069118\n",
      "epoch : 545 mse: 0.00286249908294\n",
      "epoch : 546 mse: 0.00284630170717\n",
      "epoch : 547 mse: 0.00287353369576\n",
      "epoch : 548 mse: 0.00283119024454\n",
      "epoch : 549 mse: 0.00283967323622\n",
      "epoch : 550 mse: 0.00286337388387\n",
      "epoch : 551 mse: 0.00283019202523\n",
      "epoch : 552 mse: 0.00284520379927\n",
      "epoch : 553 mse: 0.00284600282874\n",
      "epoch : 554 mse: 0.00285944813703\n",
      "epoch : 555 mse: 0.00281063795734\n",
      "epoch : 556 mse: 0.00283987933927\n",
      "epoch : 557 mse: 0.00284701482723\n",
      "epoch : 558 mse: 0.0028672647219\n",
      "epoch : 559 mse: 0.00281277873572\n",
      "epoch : 560 mse: 0.00284648431566\n",
      "epoch : 561 mse: 0.00284583330399\n",
      "epoch : 562 mse: 0.00282071926786\n",
      "epoch : 563 mse: 0.0028275832611\n",
      "epoch : 564 mse: 0.00285311182978\n",
      "epoch : 565 mse: 0.0028258857972\n",
      "epoch : 566 mse: 0.00283442010222\n",
      "epoch : 567 mse: 0.0028644344256\n",
      "epoch : 568 mse: 0.00280896657949\n",
      "epoch : 569 mse: 0.00283689549576\n",
      "epoch : 570 mse: 0.00282898019153\n",
      "epoch : 571 mse: 0.00284265285038\n",
      "epoch : 572 mse: 0.00285283518829\n",
      "epoch : 573 mse: 0.00282002116169\n",
      "epoch : 574 mse: 0.00282620825026\n",
      "epoch : 575 mse: 0.00284529760353\n",
      "epoch : 576 mse: 0.00283329420564\n",
      "epoch : 577 mse: 0.00281593454378\n",
      "epoch : 578 mse: 0.00283556448604\n",
      "epoch : 579 mse: 0.00281951242168\n",
      "epoch : 580 mse: 0.002817398278\n",
      "epoch : 581 mse: 0.00285316839303\n",
      "epoch : 582 mse: 0.00281547682817\n",
      "epoch : 583 mse: 0.00282982959599\n",
      "epoch : 584 mse: 0.00283539621515\n",
      "epoch : 585 mse: 0.00280516129663\n",
      "epoch : 586 mse: 0.00283391122785\n",
      "epoch : 587 mse: 0.00282800271985\n",
      "epoch : 588 mse: 0.00285464269902\n",
      "epoch : 589 mse: 0.00284400764773\n",
      "epoch : 590 mse: 0.00284175127541\n",
      "epoch : 591 mse: 0.0028168229763\n",
      "epoch : 592 mse: 0.00282519618226\n",
      "epoch : 593 mse: 0.00283026579381\n",
      "epoch : 594 mse: 0.00280032945613\n",
      "epoch : 595 mse: 0.00281906897045\n",
      "epoch : 596 mse: 0.00282015122792\n",
      "epoch : 597 mse: 0.0028349589625\n",
      "epoch : 598 mse: 0.00284055181923\n",
      "epoch : 599 mse: 0.00284316922809\n",
      "epoch : 600 mse: 0.0028059704572\n"
     ]
    }
   ],
   "source": [
    "net = buildNetwork(input_size, hidden_size, target_size, bias = True)\n",
    "trainer = BackpropTrainer(net,ds)\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    mse = trainer.train()\n",
    "    print(\"epoch : \" + str(i + 1) + \" mse: \" + str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test on the test set. We'll calculate the MSE between the output and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MSE: 0.00667683936378\n",
      "[[ 0.68259456]\n",
      " [ 0.95473997]\n",
      " [ 0.42577135]\n",
      " [ 1.17111249]\n",
      " [ 0.93799493]\n",
      " [ 0.31712054]\n",
      " [ 0.543584  ]\n",
      " [ 0.3343799 ]\n",
      " [ 1.03439539]\n",
      " [ 0.60700109]\n",
      " [ 0.6401014 ]\n",
      " [ 0.71679339]\n",
      " [ 0.65422633]\n",
      " [ 0.58307116]\n",
      " [ 0.74081322]\n",
      " [ 0.35336052]\n",
      " [ 0.58785506]\n",
      " [ 1.0723286 ]\n",
      " [ 0.92049425]\n",
      " [ 0.65192294]\n",
      " [ 0.46035888]\n",
      " [ 1.06972049]\n",
      " [ 1.39554651]\n",
      " [ 0.65399696]\n",
      " [ 0.40271817]\n",
      " [ 0.82902727]\n",
      " [ 0.80963328]\n",
      " [ 0.97119978]\n",
      " [ 0.63134251]\n",
      " [ 0.85917391]\n",
      " [ 0.83905378]\n",
      " [ 0.54131148]\n",
      " [ 1.07282725]\n",
      " [ 0.72914132]\n",
      " [ 0.69350733]\n",
      " [ 0.47977387]\n",
      " [ 0.87626082]\n",
      " [ 1.38685584]\n",
      " [ 0.41260356]\n",
      " [ 0.82408331]\n",
      " [ 0.40766507]\n",
      " [ 0.749688  ]\n",
      " [ 0.73612446]\n",
      " [ 0.60145099]\n",
      " [ 0.45376053]\n",
      " [ 0.85265094]\n",
      " [ 0.61614174]\n",
      " [ 0.67920498]\n",
      " [ 0.31432014]\n",
      " [ 0.89946952]\n",
      " [ 0.95419218]\n",
      " [ 0.81446818]\n",
      " [ 1.03853221]\n",
      " [ 0.96924468]\n",
      " [ 0.43886468]\n",
      " [ 0.38963251]\n",
      " [ 0.58893579]\n",
      " [ 0.39514249]\n",
      " [ 0.26956619]\n",
      " [ 0.64043927]\n",
      " [ 0.98424354]\n",
      " [ 0.56806864]\n",
      " [ 1.1716757 ]\n",
      " [ 0.49106454]\n",
      " [ 0.97301438]\n",
      " [ 0.81725947]\n",
      " [ 0.59014212]\n",
      " [ 0.9139349 ]\n",
      " [ 1.06154642]\n",
      " [ 0.70568018]\n",
      " [ 0.50749399]\n",
      " [ 0.57565017]\n",
      " [ 0.76999209]\n",
      " [ 0.79723617]\n",
      " [ 0.8631966 ]\n",
      " [ 0.72485521]\n",
      " [ 0.4901501 ]\n",
      " [ 0.45902644]\n",
      " [ 1.35246805]\n",
      " [ 0.44671756]\n",
      " [ 0.72404555]\n",
      " [ 0.2891894 ]\n",
      " [ 0.33155025]\n",
      " [ 0.58570184]\n",
      " [ 1.05293905]\n",
      " [ 0.59085697]\n",
      " [ 0.38722109]\n",
      " [ 1.16179732]\n",
      " [ 0.98679908]\n",
      " [ 0.49449736]\n",
      " [ 0.95692076]\n",
      " [ 0.51636545]\n",
      " [ 0.63523753]\n",
      " [ 0.73423304]\n",
      " [ 0.8942584 ]\n",
      " [ 0.59305317]\n",
      " [ 1.09568146]\n",
      " [ 1.34292597]\n",
      " [ 0.58555791]\n",
      " [ 0.35572703]\n",
      " [ 0.41359475]\n",
      " [ 1.07739069]\n",
      " [ 0.58431969]\n",
      " [ 0.64992259]\n",
      " [ 0.4214211 ]\n",
      " [ 1.16472458]\n",
      " [ 0.49741208]\n",
      " [ 0.65837663]\n",
      " [ 0.61281146]\n",
      " [ 0.72419673]\n",
      " [ 0.51766409]\n",
      " [ 0.81809214]\n",
      " [ 0.75763615]\n",
      " [ 0.58335303]\n",
      " [ 0.32881732]\n",
      " [ 0.39835695]\n",
      " [ 0.62124059]\n",
      " [ 0.65550278]\n",
      " [ 0.94653755]\n",
      " [ 1.16879834]\n",
      " [ 1.02305642]\n",
      " [ 0.50640115]\n",
      " [ 0.51433282]\n",
      " [ 0.96270771]\n",
      " [ 0.80879756]\n",
      " [ 0.62416221]\n",
      " [ 1.00035652]\n",
      " [ 0.71353735]\n",
      " [ 0.63175355]\n",
      " [ 0.66207347]\n",
      " [ 0.63172767]\n",
      " [ 0.45098595]\n",
      " [ 0.97675297]\n",
      " [ 1.07363728]\n",
      " [ 0.5354934 ]\n",
      " [ 0.88270036]\n",
      " [ 0.78824649]\n",
      " [ 0.99476599]\n",
      " [ 1.20540237]\n",
      " [ 0.9430714 ]\n",
      " [ 0.99081295]\n",
      " [ 0.5962896 ]\n",
      " [ 0.7207048 ]\n",
      " [ 0.59394991]\n",
      " [ 0.90963118]\n",
      " [ 0.35891907]\n",
      " [ 0.76643533]\n",
      " [ 0.6351193 ]\n",
      " [ 0.38322778]\n",
      " [ 1.12840625]\n",
      " [ 1.0769334 ]\n",
      " [ 1.10949779]\n",
      " [ 1.14295869]\n",
      " [ 0.69266898]\n",
      " [ 0.54272631]\n",
      " [ 0.65994112]\n",
      " [ 0.4112593 ]\n",
      " [ 0.42856143]\n",
      " [ 0.69809952]\n",
      " [ 0.87810547]\n",
      " [ 1.10056046]\n",
      " [ 0.44320993]\n",
      " [ 0.62310402]\n",
      " [ 0.58821396]\n",
      " [ 0.46618345]\n",
      " [ 0.60033419]\n",
      " [ 0.59851956]\n",
      " [ 1.13476872]\n",
      " [ 0.85098155]\n",
      " [ 0.98527935]\n",
      " [ 1.10499816]\n",
      " [ 0.66307701]\n",
      " [ 0.59046482]\n",
      " [ 0.80721449]\n",
      " [ 0.70806244]\n",
      " [ 0.78895954]\n",
      " [ 0.75492228]\n",
      " [ 0.78151385]\n",
      " [ 0.3229294 ]\n",
      " [ 0.38729081]\n",
      " [ 0.88816749]\n",
      " [ 0.91684288]\n",
      " [ 0.42987178]\n",
      " [ 1.24411214]\n",
      " [ 0.96390844]\n",
      " [ 0.73308399]\n",
      " [ 0.57287002]\n",
      " [ 0.68847332]\n",
      " [ 0.79390884]\n",
      " [ 0.40675129]\n",
      " [ 0.56126526]\n",
      " [ 1.07890366]\n",
      " [ 1.1235222 ]\n",
      " [ 0.34359062]\n",
      " [ 0.95567993]\n",
      " [ 0.68032266]\n",
      " [ 1.00644211]\n",
      " [ 0.77221699]\n",
      " [ 0.28397809]\n",
      " [ 0.90926591]\n",
      " [ 0.366697  ]\n",
      " [ 0.36696854]\n",
      " [ 0.86475289]\n",
      " [ 0.56103536]\n",
      " [ 1.1688265 ]\n",
      " [ 1.22896531]\n",
      " [ 0.87395687]\n",
      " [ 0.78889492]\n",
      " [ 0.80558037]\n",
      " [ 0.41662646]\n",
      " [ 1.07826946]\n",
      " [ 0.46807135]\n",
      " [ 0.6727962 ]\n",
      " [ 1.2777173 ]\n",
      " [ 0.5086455 ]\n",
      " [ 0.87626589]\n",
      " [ 1.25765307]\n",
      " [ 0.65502258]\n",
      " [ 0.84908544]\n",
      " [ 0.65458279]\n",
      " [ 0.60052047]\n",
      " [ 0.77091831]\n",
      " [ 0.51906382]\n",
      " [ 0.9129867 ]\n",
      " [ 0.96356132]\n",
      " [ 0.83205551]\n",
      " [ 0.34433471]\n",
      " [ 0.45072765]\n",
      " [ 1.18962601]\n",
      " [ 0.25874992]\n",
      " [ 1.20676747]\n",
      " [ 0.29436252]\n",
      " [ 0.77862495]\n",
      " [ 0.69390927]\n",
      " [ 0.85343844]\n",
      " [ 0.42638716]\n",
      " [ 0.43164181]\n",
      " [ 0.99418543]\n",
      " [ 0.8924162 ]\n",
      " [ 0.4905257 ]\n",
      " [ 0.50961178]\n",
      " [ 0.45150083]\n",
      " [ 0.75854663]\n",
      " [ 0.97681349]\n",
      " [ 0.24378352]\n",
      " [ 0.80985423]\n",
      " [ 0.78473436]\n",
      " [ 0.92467743]\n",
      " [ 0.56931377]\n",
      " [ 0.64501737]\n",
      " [ 1.10619327]\n",
      " [ 0.77521065]\n",
      " [ 0.31870762]\n",
      " [ 0.7478617 ]\n",
      " [ 0.56952821]\n",
      " [ 0.94042024]\n",
      " [ 0.93326609]\n",
      " [ 1.07902967]\n",
      " [ 1.03092995]\n",
      " [ 1.24332558]\n",
      " [ 0.85938666]\n",
      " [ 0.68415068]\n",
      " [ 0.62290792]\n",
      " [ 0.8567085 ]\n",
      " [ 1.03961593]\n",
      " [ 0.47838023]\n",
      " [ 0.77262256]\n",
      " [ 0.85041358]\n",
      " [ 0.7827256 ]\n",
      " [ 0.87558918]\n",
      " [ 1.23899392]\n",
      " [ 0.59304822]\n",
      " [ 0.66404455]\n",
      " [ 0.87697748]\n",
      " [ 0.57832976]\n",
      " [ 0.92648865]\n",
      " [ 0.59702127]\n",
      " [ 0.29706641]\n",
      " [ 0.64173752]\n",
      " [ 0.86762374]\n",
      " [ 0.92484087]\n",
      " [ 0.52729218]\n",
      " [ 0.79233469]\n",
      " [ 0.80260568]\n",
      " [ 1.03276359]\n",
      " [ 0.76899509]\n",
      " [ 0.94906124]\n",
      " [ 0.2894184 ]\n",
      " [ 0.44745069]\n",
      " [ 0.56361219]\n",
      " [ 0.8703329 ]\n",
      " [ 0.32251275]\n",
      " [ 0.90505323]\n",
      " [ 0.66822765]\n",
      " [ 1.04388358]\n",
      " [ 0.41731853]\n",
      " [ 0.57895331]\n",
      " [ 0.32201235]\n",
      " [ 0.75461131]\n",
      " [ 0.93673204]\n",
      " [ 0.45382029]\n",
      " [ 1.26960998]\n",
      " [ 0.3144612 ]\n",
      " [ 1.1907793 ]\n",
      " [ 1.18860283]\n",
      " [ 0.183907  ]\n",
      " [ 0.82986582]\n",
      " [ 0.72736333]\n",
      " [ 0.34413431]\n",
      " [ 0.504173  ]\n",
      " [ 0.60547719]\n",
      " [ 0.59500357]\n",
      " [ 0.49384759]\n",
      " [ 0.37329042]\n",
      " [ 0.33338197]\n",
      " [ 0.39711177]\n",
      " [ 0.94457237]\n",
      " [ 0.44094726]\n",
      " [ 0.53574884]\n",
      " [ 1.03756626]\n",
      " [ 0.43842596]\n",
      " [ 0.75350669]\n",
      " [ 0.93375104]\n",
      " [ 1.02389044]\n",
      " [ 0.65233221]\n",
      " [ 0.66071321]\n",
      " [ 0.60344149]\n",
      " [ 0.69819054]\n",
      " [ 0.47848323]\n",
      " [ 0.80155584]\n",
      " [ 0.61730025]\n",
      " [ 0.57812813]\n",
      " [ 0.77993419]\n",
      " [ 0.54731526]\n",
      " [ 0.83345431]\n",
      " [ 0.72184117]\n",
      " [ 0.55170338]\n",
      " [ 0.71365533]\n",
      " [ 1.1224962 ]\n",
      " [ 0.553054  ]\n",
      " [ 0.33701719]\n",
      " [ 0.41937775]\n",
      " [ 0.82028104]\n",
      " [ 1.08844403]\n",
      " [ 0.24717786]\n",
      " [ 0.66802532]\n",
      " [ 0.50801454]\n",
      " [ 0.81398239]\n",
      " [ 0.9903057 ]\n",
      " [ 0.44181247]\n",
      " [ 0.42904202]\n",
      " [ 1.22190494]\n",
      " [ 0.88216577]\n",
      " [ 0.54462404]\n",
      " [ 0.33857959]\n",
      " [ 0.6188752 ]\n",
      " [ 0.83665597]\n",
      " [ 0.49306284]\n",
      " [ 0.58215662]\n",
      " [ 0.4325515 ]\n",
      " [ 0.918988  ]\n",
      " [ 0.7440416 ]\n",
      " [ 0.76267701]\n",
      " [ 0.69589607]\n",
      " [ 0.98922037]\n",
      " [ 0.6877073 ]\n",
      " [ 0.91219545]\n",
      " [ 1.18200435]\n",
      " [ 1.11988249]\n",
      " [ 0.38890002]\n",
      " [ 1.10373714]\n",
      " [ 0.89337521]\n",
      " [ 0.79056235]\n",
      " [ 0.47910707]\n",
      " [ 1.0498082 ]\n",
      " [ 0.73385062]\n",
      " [ 0.60084752]\n",
      " [ 0.56258449]\n",
      " [ 0.5273965 ]\n",
      " [ 0.26086504]\n",
      " [ 0.40547018]\n",
      " [ 0.9291783 ]\n",
      " [ 1.02091745]\n",
      " [ 0.95140497]\n",
      " [ 0.62001008]\n",
      " [ 1.01792559]\n",
      " [ 0.34075471]\n",
      " [ 0.90071737]\n",
      " [ 0.86766312]\n",
      " [ 0.85426042]\n",
      " [ 0.92029559]\n",
      " [ 0.39498852]\n",
      " [ 0.42119386]\n",
      " [ 1.15922711]\n",
      " [ 0.7268657 ]\n",
      " [ 1.11187379]\n",
      " [ 0.80207961]\n",
      " [ 0.76470629]\n",
      " [ 0.55298484]\n",
      " [ 0.64458205]\n",
      " [ 0.40166183]\n",
      " [ 0.82369258]\n",
      " [ 1.20791958]\n",
      " [ 0.96039236]\n",
      " [ 0.87862645]\n",
      " [ 0.75288734]\n",
      " [ 0.43087659]\n",
      " [ 0.9643445 ]\n",
      " [ 0.93641189]\n",
      " [ 0.8455693 ]\n",
      " [ 0.80398815]\n",
      " [ 0.83163666]\n",
      " [ 0.80352249]\n",
      " [ 1.19999615]\n",
      " [ 0.63902766]\n",
      " [ 0.65501257]\n",
      " [ 0.44391945]\n",
      " [ 0.62342314]\n",
      " [ 0.60779504]\n",
      " [ 1.09154282]\n",
      " [ 0.33252495]\n",
      " [ 0.97562934]\n",
      " [ 0.87411454]\n",
      " [ 0.67956014]\n",
      " [ 0.74466865]\n",
      " [ 0.90277731]\n",
      " [ 0.52783922]\n",
      " [ 0.42922211]\n",
      " [ 0.70618802]\n",
      " [ 0.32803783]\n",
      " [ 0.65575565]\n",
      " [ 0.94437057]\n",
      " [ 0.84566281]\n",
      " [ 0.32438362]\n",
      " [ 0.510036  ]\n",
      " [ 0.63399459]\n",
      " [ 0.68574469]\n",
      " [ 0.77138216]\n",
      " [ 0.93651885]\n",
      " [ 0.92052294]\n",
      " [ 0.28112719]\n",
      " [ 0.62699566]\n",
      " [ 0.56038019]\n",
      " [ 0.47497816]\n",
      " [ 0.96227542]\n",
      " [ 0.61897956]\n",
      " [ 0.7905388 ]\n",
      " [ 0.65536992]\n",
      " [ 0.83395526]\n",
      " [ 0.46010476]\n",
      " [ 0.77845779]\n",
      " [ 1.15306041]\n",
      " [ 1.1052901 ]\n",
      " [ 0.49280418]\n",
      " [ 0.55050876]\n",
      " [ 0.5011398 ]\n",
      " [ 1.2545775 ]\n",
      " [ 0.92652379]\n",
      " [ 1.13268829]\n",
      " [ 1.16423975]\n",
      " [ 0.6418128 ]\n",
      " [ 0.87792497]\n",
      " [ 0.54543364]\n",
      " [ 0.91988306]\n",
      " [ 0.7581484 ]\n",
      " [ 0.70345005]\n",
      " [ 0.68337019]\n",
      " [ 0.58787813]\n",
      " [ 0.61627088]\n",
      " [ 1.08793422]\n",
      " [ 0.6985456 ]\n",
      " [ 0.37933211]\n",
      " [ 0.35775541]\n",
      " [ 1.32456091]\n",
      " [ 0.90561459]\n",
      " [ 0.412978  ]\n",
      " [ 0.89169087]\n",
      " [ 0.64565835]\n",
      " [ 1.01453849]\n",
      " [ 0.71779561]\n",
      " [ 0.90925031]\n",
      " [ 0.40118234]\n",
      " [ 0.66468824]\n",
      " [ 0.82323663]\n",
      " [ 0.2584268 ]\n",
      " [ 1.36068781]\n",
      " [ 0.63398428]\n",
      " [ 0.37852497]\n",
      " [ 0.64965179]\n",
      " [ 0.53738468]\n",
      " [ 0.80995201]\n",
      " [ 0.80635472]\n",
      " [ 0.63514293]\n",
      " [ 0.45876587]\n",
      " [ 0.46806005]\n",
      " [ 0.96623271]\n",
      " [ 0.90021506]\n",
      " [ 0.76245315]\n",
      " [ 0.7495306 ]\n",
      " [ 0.45085826]\n",
      " [ 0.33587891]\n",
      " [ 0.68513402]\n",
      " [ 0.27821335]\n",
      " [ 0.84146558]\n",
      " [ 1.09624329]\n",
      " [ 1.02952981]\n",
      " [ 0.70379504]\n",
      " [ 0.7299111 ]\n",
      " [ 0.55162817]\n",
      " [ 0.83589516]\n",
      " [ 0.65093661]\n",
      " [ 0.88491129]\n",
      " [ 0.8969216 ]\n",
      " [ 0.83927735]\n",
      " [ 0.51634754]\n",
      " [ 1.18640805]\n",
      " [ 0.65566634]\n",
      " [ 0.30905686]\n",
      " [ 0.86894665]\n",
      " [ 0.81155471]\n",
      " [ 0.95767586]\n",
      " [ 0.74011989]\n",
      " [ 0.87503764]\n",
      " [ 0.54760937]\n",
      " [ 0.79066789]\n",
      " [ 0.77992147]\n",
      " [ 0.67521245]\n",
      " [ 0.41097501]\n",
      " [ 0.61755484]\n",
      " [ 0.5922238 ]\n",
      " [ 0.81955236]\n",
      " [ 0.84905783]\n",
      " [ 0.95415162]\n",
      " [ 0.52584736]\n",
      " [ 0.81665553]\n",
      " [ 0.79004204]\n",
      " [ 0.67686048]\n",
      " [ 0.36186022]\n",
      " [ 1.29592763]\n",
      " [ 1.00348317]\n",
      " [ 0.85321227]\n",
      " [ 0.71958821]\n",
      " [ 0.6211023 ]\n",
      " [ 0.71476086]\n",
      " [ 0.65715452]\n",
      " [ 0.310815  ]\n",
      " [ 0.38671431]\n",
      " [ 0.56835948]\n",
      " [ 1.14699563]\n",
      " [ 0.64527738]\n",
      " [ 0.79959263]\n",
      " [ 0.94911232]\n",
      " [ 0.32859049]\n",
      " [ 0.38760448]\n",
      " [ 0.89064838]\n",
      " [ 1.04844553]\n",
      " [ 0.6081931 ]\n",
      " [ 0.53405593]\n",
      " [ 0.732939  ]\n",
      " [ 0.93626706]\n",
      " [ 0.52473604]\n",
      " [ 0.5902449 ]\n",
      " [ 0.76404047]\n",
      " [ 0.60412222]\n",
      " [ 1.1608587 ]\n",
      " [ 1.02433844]\n",
      " [ 0.74022101]\n",
      " [ 0.77032541]\n",
      " [ 0.96828619]\n",
      " [ 0.82340533]\n",
      " [ 1.08355323]\n",
      " [ 0.81288247]\n",
      " [ 0.44553568]\n",
      " [ 0.83856833]\n",
      " [ 1.16654298]\n",
      " [ 0.4637726 ]\n",
      " [ 0.84337659]\n",
      " [ 0.92222137]\n",
      " [ 0.37448949]\n",
      " [ 0.73940534]\n",
      " [ 0.62839328]\n",
      " [ 0.88747664]\n",
      " [ 0.51753685]\n",
      " [ 0.87789384]\n",
      " [ 1.0672809 ]\n",
      " [ 0.86603938]\n",
      " [ 1.12785154]\n",
      " [ 0.72902849]\n",
      " [ 0.40599713]\n",
      " [ 0.63018778]\n",
      " [ 0.85822107]\n",
      " [ 0.64809288]\n",
      " [ 0.39013223]\n",
      " [ 0.92338469]\n",
      " [ 0.88799123]\n",
      " [ 0.58104857]\n",
      " [ 0.72960838]\n",
      " [ 0.89766617]\n",
      " [ 0.8811384 ]\n",
      " [ 0.91584094]\n",
      " [ 1.05939134]\n",
      " [ 0.8851869 ]\n",
      " [ 0.59827473]\n",
      " [ 0.73951863]\n",
      " [ 0.86770484]\n",
      " [ 0.77842378]\n",
      " [ 0.77802474]\n",
      " [ 1.17083822]\n",
      " [ 0.68218046]\n",
      " [ 0.7491553 ]\n",
      " [ 0.63995195]\n",
      " [ 0.38329297]\n",
      " [ 0.71108497]\n",
      " [ 0.45292094]\n",
      " [ 0.87480776]\n",
      " [ 0.7713114 ]\n",
      " [ 0.42617252]\n",
      " [ 0.56122013]\n",
      " [ 1.17287492]\n",
      " [ 0.86913823]\n",
      " [ 0.51392452]\n",
      " [ 0.88927034]\n",
      " [ 0.82977404]\n",
      " [ 0.43461994]\n",
      " [ 0.529831  ]\n",
      " [ 0.82014172]\n",
      " [ 0.87259929]\n",
      " [ 0.7854255 ]\n",
      " [ 1.22130981]\n",
      " [ 0.62587572]\n",
      " [ 0.30302004]\n",
      " [ 0.60894886]\n",
      " [ 1.01335104]\n",
      " [ 0.54935405]\n",
      " [ 0.6166396 ]\n",
      " [ 1.02411926]\n",
      " [ 0.53502109]\n",
      " [ 1.06378464]\n",
      " [ 0.83494427]\n",
      " [ 0.78882278]\n",
      " [ 0.5330495 ]\n",
      " [ 0.76469353]\n",
      " [ 0.96391005]\n",
      " [ 0.45856898]\n",
      " [ 0.65851511]\n",
      " [ 0.61010308]\n",
      " [ 0.91163184]\n",
      " [ 0.75448405]\n",
      " [ 0.29738096]\n",
      " [ 0.69518594]\n",
      " [ 0.86683913]\n",
      " [ 0.80843714]\n",
      " [ 0.81420496]\n",
      " [ 0.35466408]\n",
      " [ 0.48257529]\n",
      " [ 0.55637953]\n",
      " [ 0.86232798]\n",
      " [ 0.46666874]\n",
      " [ 0.47591809]\n",
      " [ 0.49476993]\n",
      " [ 0.5713714 ]\n",
      " [ 0.92976943]\n",
      " [ 0.36076044]\n",
      " [ 0.64771029]\n",
      " [ 0.40905482]\n",
      " [ 0.87718954]\n",
      " [ 0.75687013]\n",
      " [ 0.74924857]\n",
      " [ 0.80503282]\n",
      " [ 0.64807951]\n",
      " [ 0.33840636]\n",
      " [ 0.63218622]\n",
      " [ 0.97008788]\n",
      " [ 0.62089943]\n",
      " [ 0.33366693]\n",
      " [ 0.66311571]\n",
      " [ 0.86917654]\n",
      " [ 0.64436035]\n",
      " [ 0.95725184]\n",
      " [ 0.90591768]\n",
      " [ 0.5474096 ]\n",
      " [ 0.63872028]\n",
      " [ 0.60370705]\n",
      " [ 0.68515838]\n",
      " [ 0.30553439]\n",
      " [ 0.97757268]\n",
      " [ 0.67808871]\n",
      " [ 0.40465519]\n",
      " [ 0.9412806 ]\n",
      " [ 0.24784591]\n",
      " [ 0.40658259]\n",
      " [ 0.29872122]\n",
      " [ 1.00964015]\n",
      " [ 0.6584014 ]\n",
      " [ 1.15463271]\n",
      " [ 0.60212841]\n",
      " [ 0.73655474]\n",
      " [ 1.24053917]\n",
      " [ 1.01463774]\n",
      " [ 1.0493275 ]\n",
      " [ 0.88679159]\n",
      " [ 0.89404545]\n",
      " [ 0.80937245]\n",
      " [ 0.62050793]\n",
      " [ 0.92521098]\n",
      " [ 0.8226854 ]\n",
      " [ 0.41661857]\n",
      " [ 0.61601407]\n",
      " [ 0.65749691]\n",
      " [ 0.45051749]\n",
      " [ 0.7590033 ]\n",
      " [ 1.33906026]\n",
      " [ 0.632762  ]\n",
      " [ 0.96166688]\n",
      " [ 0.54000037]\n",
      " [ 0.76726899]\n",
      " [ 0.80907584]\n",
      " [ 0.55268014]\n",
      " [ 0.58114245]\n",
      " [ 1.03985536]\n",
      " [ 0.61692138]\n",
      " [ 0.31405975]\n",
      " [ 0.95471763]\n",
      " [ 1.08470383]\n",
      " [ 0.49634918]\n",
      " [ 0.92020219]\n",
      " [ 0.67157952]\n",
      " [ 0.21150562]\n",
      " [ 0.69328058]\n",
      " [ 0.50523708]\n",
      " [ 0.96230298]\n",
      " [ 0.51830498]\n",
      " [ 0.85636849]\n",
      " [ 0.61900475]\n",
      " [ 0.51700047]\n",
      " [ 1.190499  ]\n",
      " [ 1.17064195]\n",
      " [ 0.62424491]\n",
      " [ 0.80359444]\n",
      " [ 1.04645175]\n",
      " [ 0.91400852]\n",
      " [ 0.45287319]\n",
      " [ 0.53391773]\n",
      " [ 0.48718273]\n",
      " [ 1.2406761 ]\n",
      " [ 0.64027146]\n",
      " [ 0.26485428]\n",
      " [ 0.98973308]\n",
      " [ 0.50815459]\n",
      " [ 0.79059712]\n",
      " [ 0.6364503 ]\n",
      " [ 0.64474966]\n",
      " [ 0.32918342]\n",
      " [ 1.03643236]\n",
      " [ 0.75444221]\n",
      " [ 1.01346206]\n",
      " [ 1.00127852]\n",
      " [ 0.88761802]\n",
      " [ 0.93944382]\n",
      " [ 0.82765752]\n",
      " [ 0.5186809 ]\n",
      " [ 0.24097187]\n",
      " [ 0.73419789]\n",
      " [ 0.74455909]\n",
      " [ 0.48505211]\n",
      " [ 0.75376547]\n",
      " [ 1.02106524]\n",
      " [ 0.81294827]\n",
      " [ 0.50702812]\n",
      " [ 0.61043213]\n",
      " [ 0.43307447]\n",
      " [ 1.13181104]\n",
      " [ 1.25150035]\n",
      " [ 0.59233115]\n",
      " [ 0.41175946]\n",
      " [ 1.04592147]\n",
      " [ 0.5576597 ]\n",
      " [ 0.51644247]\n",
      " [ 1.12409181]\n",
      " [ 0.44706676]\n",
      " [ 0.42251911]\n",
      " [ 0.88069025]\n",
      " [ 1.36438702]\n",
      " [ 1.4202563 ]\n",
      " [ 0.84223642]\n",
      " [ 1.01406178]\n",
      " [ 0.39199665]\n",
      " [ 0.76217064]\n",
      " [ 0.90570073]\n",
      " [ 0.36683946]\n",
      " [ 1.27994066]\n",
      " [ 0.63703099]\n",
      " [ 0.32783145]\n",
      " [ 0.77549091]\n",
      " [ 1.13076711]\n",
      " [ 0.35782519]\n",
      " [ 0.26667978]\n",
      " [ 0.27867567]\n",
      " [ 0.95954551]\n",
      " [ 0.17131618]\n",
      " [ 0.63465679]\n",
      " [ 0.62951444]\n",
      " [ 0.73111746]\n",
      " [ 0.51369118]\n",
      " [ 0.60336135]\n",
      " [ 0.80782045]\n",
      " [ 0.79308193]\n",
      " [ 0.74760129]\n",
      " [ 0.26462654]\n",
      " [ 0.89734598]\n",
      " [ 0.94860387]\n",
      " [ 0.3645076 ]\n",
      " [ 0.56644522]\n",
      " [ 0.64668765]\n",
      " [ 0.88164855]\n",
      " [ 0.70608916]\n",
      " [ 0.56059551]\n",
      " [ 0.75557747]\n",
      " [ 1.12907941]\n",
      " [ 1.08915616]\n",
      " [ 1.12589065]\n",
      " [ 0.45796067]\n",
      " [ 1.10281916]\n",
      " [ 0.707704  ]\n",
      " [ 0.64534961]\n",
      " [ 0.58553395]\n",
      " [ 0.84612069]\n",
      " [ 0.93278542]\n",
      " [ 0.69470307]\n",
      " [ 0.81201409]\n",
      " [ 1.16686783]\n",
      " [ 0.35673032]\n",
      " [ 0.71792346]\n",
      " [ 0.35383727]\n",
      " [ 0.7320146 ]\n",
      " [ 0.77161325]\n",
      " [ 0.79609037]\n",
      " [ 0.29003578]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "test_file = \"data/test.csv\"\n",
    "test = np.loadtxt(test_file, delimiter = ',')\n",
    "x_test = test[:,0:-1]\n",
    "y_test = test[:,-1]\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "#output column\n",
    "y_test_output = np.zeros(y_test.shape)\n",
    "\n",
    "input_size = x_test.shape[1]\n",
    "target_size = y_test.shape[1]\n",
    "# prepare dataset\n",
    "\n",
    "ds = SupervisedDataSet( input_size, target_size )\n",
    "ds.setField('input', x_test)\n",
    "ds.setField('target', y_test_output)\n",
    "\n",
    "# predict\n",
    "\n",
    "p = net.activateOnDataset(ds)\n",
    "mse = mean_squared_error(y_test, p)\n",
    "print(\"Testing MSE:\", mse)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the classification and regression practice, we could know the ease of use for the PyBrain library. Actually, the PyBrain library has many details for network design. If you are interested in this, Much more detail about the libraries and questions on PyBrain are available from the following links.\n",
    "\n",
    "[PyBrain introduction](http://pybrain.org/): The brief introduction to PyBrain.\n",
    "\n",
    "[Quick start for PyBrain](http://pybrain.org/pages/features): A video for you to learn the library fast.\n",
    "\n",
    "[PyBrain docs](http://pybrain.org/docs/): The tutorial for PyBrain.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
