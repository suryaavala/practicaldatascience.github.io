{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting News Popularity using FeedForward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Author_: Utkarsha Khadke\n",
    "\n",
    "_Date_  : March 30, 2018 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In today's world, there is profusion of user generated content due to the rise of various social media platforms. Contributors and opinionated readers cause a lot of stir as a reaction to the news articles that are published online. This has led to a surge in the research studies related to the popularity of news article based on its topic, content and the sentiments it contains.\n",
    "\n",
    "In this tutorial, we will learn how we can use deep neural networks to train a large dataset of news articles and predict whether the new news item will become popular on social media or not. These news items are collected from well-known aggregators such as Google News and Yahoo! News and their respective social feedback on multiple platforms such as Facebook, LinkedIn and Google+. The data collected is of a duration of 8 months ranging between November 2015 and July 2016. It consists of around 100,000 news items on topics such as economy, Obama, palestine and microsoft.\n",
    "\n",
    "The data is obtained from  https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms# . You may visit this link for more information on how the data is collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import string\n",
    "import sklearn\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the keras libraries\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting a random seed\n",
    "random.seed(42) #cause it is the answer to the Ultimate Question of Life, the Universe, and Everything\n",
    "                #Reference: The Hitchhiker's Guide to the Galaxy by Douglas Adams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we import the dataset from a CSV file, store it in a pandas dataframe and convert the columns into proper data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the file path \n",
    "news_filepath = \"./News_Final.csv\"\n",
    "\n",
    "#Loading the contents of CSV in memory          \n",
    "news_dataset = pd.read_csv(news_filepath)\n",
    "\n",
    "#Resetting the index\n",
    "news_dataset = news_dataset.reset_index(drop = True)\n",
    "\n",
    "#Changing data types\n",
    "news_dataset[['IDLink']] = news_dataset[['IDLink']].astype('int64')\n",
    "\n",
    "#Subsetting the news dataset\n",
    "news_dataset = news_dataset.sample(n=1500) #news_dataset.iloc[random.sample(news_dataset.index, 1000)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: As a result of limitation of my laptop configuration, I have limited the input data to contain 1500 news articles. If required, you may directly pass the processed_ds dataset to the create_feature_matrix function in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDLink</th>\n",
       "      <th>Title</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic</th>\n",
       "      <th>PublishDate</th>\n",
       "      <th>SentimentTitle</th>\n",
       "      <th>SentimentHeadline</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>GooglePlus</th>\n",
       "      <th>LinkedIn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61679</th>\n",
       "      <td>41616</td>\n",
       "      <td>Malaysia Economy Is Sustainable At The Core - ...</td>\n",
       "      <td>KUALA LUMPUR -- Prime Minister Datuk Seri Naji...</td>\n",
       "      <td>malaysiandigest.com</td>\n",
       "      <td>economy</td>\n",
       "      <td>2016-04-12 10:09:18</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.090924</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37435</th>\n",
       "      <td>77102</td>\n",
       "      <td>What Obama's Cybersecurity Strategy Leaves Out</td>\n",
       "      <td>What Obama's Cybersecurity Strategy Leaves Out...</td>\n",
       "      <td>MIT Technology Review</td>\n",
       "      <td>obama</td>\n",
       "      <td>2016-02-09 22:45:32</td>\n",
       "      <td>-0.051031</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>73</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66084</th>\n",
       "      <td>44530</td>\n",
       "      <td>Mitsubishi Motors admits manipulating fuel eco...</td>\n",
       "      <td>Mitsubishi Motors Corp admitted to manipulatin...</td>\n",
       "      <td>Channel NewsAsia</td>\n",
       "      <td>economy</td>\n",
       "      <td>2016-04-23 03:33:00</td>\n",
       "      <td>-0.034722</td>\n",
       "      <td>0.164685</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55119</th>\n",
       "      <td>36327</td>\n",
       "      <td>WonderCon expected to do wonders for Los Angel...</td>\n",
       "      <td>A hulking $36 million is expected to flow thro...</td>\n",
       "      <td>LA Daily News</td>\n",
       "      <td>economy</td>\n",
       "      <td>2016-03-26 03:28:51</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>-0.026064</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15026</th>\n",
       "      <td>9730</td>\n",
       "      <td>Azerbaijan's economy increases</td>\n",
       "      <td>The non-oil sector of the economy increased by...</td>\n",
       "      <td>Trend News Agency</td>\n",
       "      <td>economy</td>\n",
       "      <td>2015-12-16 15:58:18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.102062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       IDLink                                              Title  \\\n",
       "61679   41616  Malaysia Economy Is Sustainable At The Core - ...   \n",
       "37435   77102     What Obama's Cybersecurity Strategy Leaves Out   \n",
       "66084   44530  Mitsubishi Motors admits manipulating fuel eco...   \n",
       "55119   36327  WonderCon expected to do wonders for Los Angel...   \n",
       "15026    9730                     Azerbaijan's economy increases   \n",
       "\n",
       "                                                Headline  \\\n",
       "61679  KUALA LUMPUR -- Prime Minister Datuk Seri Naji...   \n",
       "37435  What Obama's Cybersecurity Strategy Leaves Out...   \n",
       "66084  Mitsubishi Motors Corp admitted to manipulatin...   \n",
       "55119  A hulking $36 million is expected to flow thro...   \n",
       "15026  The non-oil sector of the economy increased by...   \n",
       "\n",
       "                      Source    Topic          PublishDate  SentimentTitle  \\\n",
       "61679    malaysiandigest.com  economy  2016-04-12 10:09:18        0.041667   \n",
       "37435  MIT Technology Review    obama  2016-02-09 22:45:32       -0.051031   \n",
       "66084       Channel NewsAsia  economy  2016-04-23 03:33:00       -0.034722   \n",
       "55119          LA Daily News  economy  2016-03-26 03:28:51        0.041667   \n",
       "15026      Trend News Agency  economy  2015-12-16 15:58:18        0.000000   \n",
       "\n",
       "       SentimentHeadline  Facebook  GooglePlus  LinkedIn  \n",
       "61679           0.090924         0           2         0  \n",
       "37435           0.022097        73           3        48  \n",
       "66084           0.164685        -1           0         0  \n",
       "55119          -0.026064        12           2         2  \n",
       "15026          -0.102062         0           0         0  "
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Dictionary of this dataset is as follows:\n",
    "1. IDLink (integer): Unique identifier of news items\n",
    "2. Title (string): Title of the news item according to the official media sources\n",
    "3. Headline (string): Headline of the news item according to the official media sources\n",
    "4. Source (string): Original news outlet that published the news item\n",
    "5. Topic (string): Query topic used to obtain the items in the official media sources\n",
    "6. PublishDate (timestamp): Date and time of the news items' publication\n",
    "7. SentimentTitle (numeric): Sentiment score of the text in the news items' title\n",
    "8. SentimentHeadline (numeric): Sentiment score of the text in the news items' headline\n",
    "9. Facebook (numeric): News items' popularity: number of times article is shared on Facebook\n",
    "10. GooglePlus (numeric): News items' popularity: number of times article is shared on Facebook Google+\n",
    "11. LinkedIn (numeric): News items' popularity: number of times article is shared on Facebook LinkedIn\n",
    "\n",
    "For the last 3 columns, value -1 indicates data for that news item for the corresponding social media platform could not be collected, value 0 indicates no shares and a non-zero positive value indicates number of shares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Popularity\n",
    "\n",
    "In our dataset, we will define the definition of popularity to be a summation of total number of shares on all the three social media platforms: Facebook, LinkedIn and Google+. After that, we will plot a density plot of popularity distribution to determine an arbitrary and learned point of threshold which will help us get a binary value for whether a news item becomes popular or not.\n",
    "\n",
    "Let us define a function to perform the summation and apply it on each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to count Total number of times a news article is shared\n",
    "def total_popularity(row):\n",
    "    total = 0\n",
    "    if row['Facebook'] > 0:\n",
    "        total += row['Facebook'] \n",
    "    if row['LinkedIn'] > 0:\n",
    "        total += row['LinkedIn']         \n",
    "    if row['GooglePlus'] > 0:\n",
    "        total += row['GooglePlus']  \n",
    "    return total\n",
    "\n",
    "#Calling the function on every row to determine Total Popularity\n",
    "news_dataset['TotalPopularity'] = news_dataset.apply(lambda row: total_popularity(row), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot a graph of popularity density to identify the perfect arbitary breakpoint for classifying binary popularity for an article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c0db86358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reference: https://stackoverflow.com/questions/4150171/how-to-create-a-density-plot-in-matplotlib \n",
    "density = gaussian_kde(news_dataset['TotalPopularity'])\n",
    "xs = np.linspace(0,50000,400)\n",
    "density.covariance_factor = lambda : 1\n",
    "density._compute_covariance()\n",
    "plt.plot(xs,density(xs))\n",
    "plt.axvline(news_dataset['TotalPopularity'].mean(), color='r', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(1400, color='b', linestyle='dashed', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can see that the mean of total popularity is almost zero. Also, a majority of the news articles have total popularity less than 1400. News articles with popularity higher than that are very less in number. Therefore, we can infer that a lot of the articles have not been shared on either of Facebook, LinkedIn or GooglePlus. \n",
    "\n",
    "Let us consider the measure of popularity to be zero if it is not shared at all and one if shared at least once on Facebook, LinkedIn and GooglePlus.\n",
    "\n",
    "We will write a function for binary classification based on the learned classifying valuee and apply it on every row to create the target variable 'Popular' (1 if popular, 0 otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADy1JREFUeJzt3H+MZWddx/H3hy4F+SFbulNSd1en\nhEVpSAzNpC6SILIEaTHd/tGaErFLs3ETrIiUKKv+UQP/tP4qNiHFlVa2BqG1EruBKmm2JahxN0wp\nlv6QdC11d2xlB9uuPxqEytc/7rMybmd3LnNn7u3wvF/J5J7znOec8312Zucz5zn3nlQVkqT+PG/S\nBUiSJsMAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHVq3aQLOJUNGzbU9PT0pMuQ\npDXlnnvu+UZVTS3V7zkdANPT08zOzk66DElaU5L88zD9nAKSpE4ZAJLUKQNAkjplAEhSpwwASeqU\nASBJnTIAJKlTBoAkdcoAkKROPac/CSxp9U3v/uykS9AiHr3m7at+Dq8AJKlTBoAkdcoAkKROGQCS\n1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmd\nMgAkqVNLBkCSm5IcTXL/graXJ7kzycPt9YzWniTXJzmU5L4k5y3YZ0fr/3CSHaszHEnSsIa5Avg4\n8LYT2nYD+6tqC7C/rQNcAGxpX7uAG2AQGMDVwE8A5wNXHw8NSdJkLBkAVfUF4IkTmrcDe9vyXuDi\nBe0318ABYH2Ss4GfAe6sqieq6kngTp4dKpKkMVruPYBXVNXjAO31rNa+ETiyoN9caztZuyRpQlb6\nJnAWaatTtD/7AMmuJLNJZufn51e0OEnSdy03AL7epnZor0db+xyweUG/TcBjp2h/lqraU1UzVTUz\nNTW1zPIkSUtZbgDsA46/k2cHcPuC9svbu4G2AsfaFNHngLcmOaPd/H1ra5MkTci6pTok+STwJmBD\nkjkG7+a5Brg1yU7gMHBp634HcCFwCHgauAKgqp5I8iHgi63fB6vqxBvLkqQxWjIAquodJ9m0bZG+\nBVx5kuPcBNz0PVUnSVo1fhJYkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6\nZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMG\ngCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnRopAJK8L8kDSe5P8skkL0xyTpKDSR5OckuS\n01vfF7T1Q2379EoMQJK0PMsOgCQbgV8BZqrqtcBpwGXAtcB1VbUFeBLY2XbZCTxZVa8Crmv9JEkT\nMuoU0DrgB5KsA14EPA68Gbitbd8LXNyWt7d12vZtSTLi+SVJy7TsAKiqfwF+DzjM4Bf/MeAe4Kmq\neqZ1mwM2tuWNwJG27zOt/5nLPb8kaTSjTAGdweCv+nOAHwJeDFywSNc6vsspti087q4ks0lm5+fn\nl1ueJGkJo0wBvQX4WlXNV9W3gU8DPwmsb1NCAJuAx9ryHLAZoG1/GfDEiQetqj1VNVNVM1NTUyOU\nJ0k6lVEC4DCwNcmL2lz+NuBB4G7gktZnB3B7W97X1mnb76qqZ10BSJLGY5R7AAcZ3Mz9EvCVdqw9\nwAeAq5IcYjDHf2Pb5UbgzNZ+FbB7hLolSSNat3SXk6uqq4GrT2h+BDh/kb7fBC4d5XySpJXjJ4El\nqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6\nZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMG\ngCR1ygCQpE4ZAJLUqZECIMn6JLcl+cckDyV5fZKXJ7kzycPt9YzWN0muT3IoyX1JzluZIUiSlmPU\nK4A/BP66qn4M+HHgIWA3sL+qtgD72zrABcCW9rULuGHEc0uSRrDsAEjyg8AbgRsBqupbVfUUsB3Y\n27rtBS5uy9uBm2vgALA+ydnLrlySNJJRrgBeCcwDf5Lk3iQfS/Ji4BVV9ThAez2r9d8IHFmw/1xr\nkyRNwCgBsA44D7ihql4H/Bffne5ZTBZpq2d1SnYlmU0yOz8/P0J5kqRTGSUA5oC5qjrY1m9jEAhf\nPz61016PLui/ecH+m4DHTjxoVe2pqpmqmpmamhqhPEnSqSw7AKrqX4EjSX60NW0DHgT2ATta2w7g\n9ra8D7i8vRtoK3Ds+FSRJGn81o24/3uATyQ5HXgEuIJBqNyaZCdwGLi09b0DuBA4BDzd+kqSJmSk\nAKiqLwMzi2zatkjfAq4c5XySpJXjJ4ElqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJ\nnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQp\nA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUqZEDIMlpSe5N8pm2fk6Sg0keTnJL\nktNb+wva+qG2fXrUc0uSlm8lrgDeCzy0YP1a4Lqq2gI8Cexs7TuBJ6vqVcB1rZ8kaUJGCoAkm4C3\nAx9r6wHeDNzWuuwFLm7L29s6bfu21l+SNAGjXgF8GPh14Dtt/Uzgqap6pq3PARvb8kbgCEDbfqz1\nlyRNwLIDIMnPAker6p6FzYt0rSG2LTzuriSzSWbn5+eXW54kaQmjXAG8AbgoyaPApxhM/XwYWJ9k\nXeuzCXisLc8BmwHa9pcBT5x40KraU1UzVTUzNTU1QnmSpFNZdgBU1W9U1aaqmgYuA+6qqp8H7gYu\nad12ALe35X1tnbb9rqp61hWAJGk8VuNzAB8ArkpyiMEc/42t/UbgzNZ+FbB7Fc4tSRrSuqW7LK2q\nPg98vi0/Apy/SJ9vApeuxPmGNb37s+M8nYbw6DVvn3QJkho/CSxJnTIAJKlTBoAkdcoAkKROGQCS\n1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmd\nMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROLTsAkmxO\ncneSh5I8kOS9rf3lSe5M8nB7PaO1J8n1SQ4luS/JeSs1CEnS926UK4BngPdX1WuArcCVSc4FdgP7\nq2oLsL+tA1wAbGlfu4AbRji3JGlEyw6Aqnq8qr7Ulv8DeAjYCGwH9rZue4GL2/J24OYaOACsT3L2\nsiuXJI1kRe4BJJkGXgccBF5RVY/DICSAs1q3jcCRBbvNtTZJ0gSMHABJXgL8BfCrVfXvp+q6SFst\ncrxdSWaTzM7Pz49aniTpJEYKgCTPZ/DL/xNV9enW/PXjUzvt9WhrnwM2L9h9E/DYicesqj1VNVNV\nM1NTU6OUJ0k6hVHeBRTgRuChqvqDBZv2ATva8g7g9gXtl7d3A20Fjh2fKpIkjd+6EfZ9A/ALwFeS\nfLm1/SZwDXBrkp3AYeDStu0O4ELgEPA0cMUI55YkjWjZAVBVf8vi8/oA2xbpX8CVyz2fJGll+Ulg\nSeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCk\nThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqU\nASBJnTIAJKlTBoAkdWrsAZDkbUm+muRQkt3jPr8kaWCsAZDkNOAjwAXAucA7kpw7zhokSQPjvgI4\nHzhUVY9U1beATwHbx1yDJInxB8BG4MiC9bnWJkkas3VjPl8Waav/1yHZBexqq/+Z5KsjnG8D8I0R\n9l9rnvPjzbUrfsjn/JhXgWPuQK4dacw/MkyncQfAHLB5wfom4LGFHapqD7BnJU6WZLaqZlbiWGtB\nb+MFx9wLx7w6xj0F9EVgS5JzkpwOXAbsG3MNkiTGfAVQVc8k+WXgc8BpwE1V9cA4a5AkDYx7Coiq\nugO4Y0ynW5GppDWkt/GCY+6FY14Fqaqle0mSvu/4KAhJ6tSaD4ClHi2R5AVJbmnbDyaZHn+VK2uI\nMV+V5MEk9yXZn2Sot4Q9lw37CJEklySpJGv+HSPDjDnJz7Xv9QNJ/mzcNa60IX62fzjJ3UnubT/f\nF06izpWS5KYkR5Pcf5LtSXJ9+/e4L8l5K1pAVa3ZLwY3kv8JeCVwOvAPwLkn9Pkl4KNt+TLglknX\nPYYx/zTworb87h7G3Pq9FPgCcACYmXTdY/g+bwHuBc5o62dNuu4xjHkP8O62fC7w6KTrHnHMbwTO\nA+4/yfYLgb9i8BmqrcDBlTz/Wr8CGObREtuBvW35NmBbksU+kLZWLDnmqrq7qp5uqwcYfN5iLRv2\nESIfAn4H+OY4i1slw4z5F4GPVNWTAFV1dMw1rrRhxlzAD7bll3HC54jWmqr6AvDEKbpsB26ugQPA\n+iRnr9T513oADPNoif/rU1XPAMeAM8dS3er4Xh+nsZPBXxBr2ZJjTvI6YHNVfWacha2iYb7PrwZe\nneTvkhxI8raxVbc6hhnzbwPvTDLH4N2E7xlPaROzqo/PGfvbQFfYko+WGLLPWjL0eJK8E5gBfmpV\nK1p9pxxzkucB1wHvGldBYzDM93kdg2mgNzG4yvubJK+tqqdWubbVMsyY3wF8vKp+P8nrgT9tY/7O\n6pc3Eav6+2utXwEs+WiJhX2SrGNw2XiqS67numHGTJK3AL8FXFRV/z2m2lbLUmN+KfBa4PNJHmUw\nV7pvjd8IHvZn+/aq+nZVfQ34KoNAWKuGGfNO4FaAqvp74IUMnhP0/Wqo/+/LtdYDYJhHS+wDdrTl\nS4C7qt1dWaOWHHObDvkjBr/81/q8MCwx5qo6VlUbqmq6qqYZ3Pe4qKpmJ1PuihjmZ/svGdzwJ8kG\nBlNCj4y1ypU1zJgPA9sAkryGQQDMj7XK8doHXN7eDbQVOFZVj6/Uwdf0FFCd5NESST4IzFbVPuBG\nBpeJhxj85X/Z5Coe3ZBj/l3gJcCft/vdh6vqookVPaIhx/x9Zcgxfw54a5IHgf8Bfq2q/m1yVY9m\nyDG/H/jjJO9jMBXyrrX8B12STzKYwtvQ7mtcDTwfoKo+yuA+x4XAIeBp4IoVPf8a/reTJI1grU8B\nSZKWyQCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlT/wuaSO5bnhqunQAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c0d99a6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating a function to add column 'Popularity' based on values from column 'TotalPopularity'\n",
    "def add_popularity(row):\n",
    "    if row['TotalPopularity'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "news_dataset['Popular'] = news_dataset.apply(lambda row: add_popularity(row), axis = 1)\n",
    "print(len(news_dataset.loc[news_dataset['Popular'] > 0]))\n",
    "\n",
    "# Plotting a histogram of Popularity\n",
    "plt.hist(news_dataset['Popular'], bins = 3 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this histogram we see that almost 75% of the news articles are popular. Therefore, if we were to classify all the news items to be popular, we would get an accuracy score of around 75% which means, we would need a model that gives us more accuracy than this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Document Term Matrix\n",
    "\n",
    "Document Term Matrix consists of all the unique words in the corpus, in this case, the news headline and the count or frequency of that word in each document. It will result in a sparse matrix with rows equal to those of original data and columns with unique words.\n",
    "\n",
    "We will first write a function to process every news headline to create a list of valuable words in the headline by removing twitter handles, URLs, tagged words, strage characters, punctuations, stop words and lemmatizing the remaining words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    \n",
    "    #Converting to lower case\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    #Removing handles of form '@trump'\n",
    "    text = re.sub(r'@([A-Za-z0-9_]+)', '', text)\n",
    "    \n",
    "    #Removing URL of form 'http...'\n",
    "    text = re.sub(r'http(.[^ ]+)', '', text)\n",
    "    \n",
    "    #Removing tagged words\n",
    "    text = re.sub(r'(<.+>)+', '', text)\n",
    "    \n",
    "    #Removing strange characters like â€™\n",
    "    include = set(string.printable)\n",
    "    text = list(filter(lambda x: x in include, text))\n",
    "    text = ''.join(text)\n",
    "    \n",
    "    #Removing punctuations    \n",
    "    exclude = set(string.punctuation)\n",
    "    exclude = exclude - set(\"\\'\")\n",
    "    text = text.replace(\"\\'s\",\"\")\n",
    "    text = text.replace(\"\\'\",\"\")    \n",
    "    for e in exclude:\n",
    "        text = text.replace(e, \" \")\n",
    "    \n",
    "    #Remove words less than 3 in length\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', '', text)\n",
    "    \n",
    "    #Initializing the result\n",
    "    result = []\n",
    "    \n",
    "    ##lammetizing\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    \n",
    "    #Reference: Homework 3: text_classification.ipynb\n",
    "    for token in tokenized:\n",
    "        string_token = str(token)\n",
    "        try:\n",
    "            result.append(lemmatizer.lemmatize(string_token))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    #Getting NLTK stop words:\n",
    "    stopwords=nltk.corpus.stopwords.words('english')\n",
    "    modified_stopwords = [i.replace(\"\\'\", \"\") for i in stopwords]\n",
    "    \n",
    "    #Appending modified stop words list with some more words of no significance \n",
    "    total_stopwords = modified_stopwords + ['next', 'break', 'else', 'terms', 'while']\n",
    "    \n",
    "    #Removing the stop words\n",
    "    result = [item for item in result if item not in total_stopwords] #result - total_stopwords\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of calling the above function is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bond', 'market', 'booming', 'sign', 'investor', 'faith', 'resilience', 'economy', 'bond', 'sale', 'company', 'good', 'credit', 'rating', 'billion', 'october']\n"
     ]
    }
   ],
   "source": [
    "text = 'The bond market is booming again, @ohholybutt a sign of investorsâ€™ faith in the resilience of the U.S. economy. while http://www.imemc.org, U.S. bond sales by companies with good credit ratings hit $103 billion in October, '\n",
    "print(process(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write a function to process the entire dataset's headline column to be replaced by lists of relevant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_headline(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" process all text in the dataframe using process_text() function.\n",
    "        #Reference: Homework 3: text_classification.ipynb\n",
    "    Inputs\n",
    "        df: pd.DataFrame: dataframe containing a column 'text' loaded from the CSV file\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs\n",
    "        pd.DataFrame: dataframe in which the values of headline column have been changed from str to list(str),\n",
    "                        the output from process() function. Other columns remain unaffected.\n",
    "    \"\"\"\n",
    "    for i, row in df.iterrows():\n",
    "        df.set_value(i,'Headline', process(row['Headline']))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now run the above function on our dataset to update the News Headline column and store it in a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_ds = process_headline(news_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creating the feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a function to create the feature matrix by segregating all the unique words from the Headline column into their own respective columns and storing the frequency value for those terms in the corresponding rows. We also add other input features from the original matrix such as the Topic, SentimentTitle, SentimentHeadline and the target variable 'Popular'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_matrix(processed_ds): \n",
    "    \"\"\" creates the feature matrix using the processed headline text\n",
    "    Inputs:\n",
    "        news: pd.DataFrame: processed news dataset, containing the column 'Headline'\n",
    "        #rare_words: list(str): one of the outputs of get_feature_and_rare_words() function\n",
    "    Outputs:\n",
    "        feature_matrix: matrix of unique words and their frequency for each row\n",
    "                                we need this to tranform test news in the same way as train news\n",
    "    \"\"\"\n",
    "    \n",
    "    #Getting unique words from the Headline column\n",
    "    #Reference: https://stackoverflow.com/questions/3724551/python-uniqueness-for-list-of-lists\n",
    "    total_words = [x for y in processed_ds['Headline'] for x in y]\n",
    "    total_unique_words = set(total_words)\n",
    "    \n",
    "    #Adding additional feature column names\n",
    "    total_features = list(total_unique_words) + ['Topic',\n",
    "        'SentimentTitle', 'SentimentHeadline', 'Popular']\n",
    "    \n",
    "    #Initializing the feature matrix\n",
    "    feature_matrix = pd.DataFrame(columns = total_features)\n",
    "    \n",
    "    print(\"Final Columns: \", feature_matrix.columns)\n",
    "    \n",
    "    #Reference: Practical Data science: Homework 3: text classification\n",
    "    #Creating string corpus\n",
    "    for i, row in processed_ds.iterrows():\n",
    "        \n",
    "        #counting the frequency of words in the headline\n",
    "        word_freq = Counter(row['Headline'])\n",
    "              \n",
    "        #Storing the corresponding frequencies for each row\n",
    "        row_list = []\n",
    "        row_list = [0]*len(total_unique_words)\n",
    "        new_row_df = pd.DataFrame([row_list],columns=total_unique_words)                \n",
    "        for word in word_freq:\n",
    "            new_row_df.loc[0,word] = word_freq[word]\n",
    "                \n",
    "        #Adding other feature columns to feature matrix:\n",
    "        #Topic\n",
    "        new_row_df.loc[0,'Topic'] = row['Topic']\n",
    "        \n",
    "        #SentimentTitle\n",
    "        new_row_df.loc[0,'SentimentTitle'] = row['SentimentTitle']\n",
    "\n",
    "        #SentimentHeadline\n",
    "        new_row_df.loc[0,'SentimentHeadline'] = row['SentimentHeadline']\n",
    "        \n",
    "        #Popularity = target variable\n",
    "        new_row_df.loc[0,'Popular'] = row['Popular']\n",
    "        \n",
    "        #Appending the row to the final matrix\n",
    "        feature_matrix = pd.concat([feature_matrix, new_row_df], ignore_index= True )    \n",
    "    \n",
    "    #Returning values\n",
    "    return feature_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call the above function on our input data and store the result in a new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Columns:  Index(['luxury', 'indonesia', 'employee', '2025', 'larijani', 'urging',\n",
      "       'encouraged', 'brash', 'ieee', 'fifty',\n",
      "       ...\n",
      "       'vietnamese', 'amosun', 'desperate', 'error', 'hulking', 'every',\n",
      "       'Topic', 'SentimentTitle', 'SentimentHeadline', 'Popular'],\n",
      "      dtype='object', length=5920)\n",
      "Dimensions of input matrix 2\n"
     ]
    }
   ],
   "source": [
    "#Subsetting the dataframe and passing it to create the final feature matrix\n",
    "input_matrix = create_feature_matrix(processed_ds)\n",
    "print(\"Dimensions of input matrix\", input_matrix.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input matrix preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>luxury</th>\n",
       "      <th>indonesia</th>\n",
       "      <th>employee</th>\n",
       "      <th>2025</th>\n",
       "      <th>larijani</th>\n",
       "      <th>urging</th>\n",
       "      <th>encouraged</th>\n",
       "      <th>brash</th>\n",
       "      <th>ieee</th>\n",
       "      <th>fifty</th>\n",
       "      <th>...</th>\n",
       "      <th>vietnamese</th>\n",
       "      <th>amosun</th>\n",
       "      <th>desperate</th>\n",
       "      <th>error</th>\n",
       "      <th>hulking</th>\n",
       "      <th>every</th>\n",
       "      <th>Topic</th>\n",
       "      <th>SentimentTitle</th>\n",
       "      <th>SentimentHeadline</th>\n",
       "      <th>Popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>economy</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.090924</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>obama</td>\n",
       "      <td>-0.051031</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>economy</td>\n",
       "      <td>-0.034722</td>\n",
       "      <td>0.164685</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>economy</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>-0.026064</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>economy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.102062</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5920 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  luxury indonesia employee 2025 larijani urging encouraged brash ieee fifty  \\\n",
       "0      0         0        0    0        0      0          0     0    0     0   \n",
       "1      0         0        0    0        0      0          0     0    0     0   \n",
       "2      0         0        0    0        0      0          0     0    0     0   \n",
       "3      0         0        0    0        0      0          0     0    0     0   \n",
       "4      0         0        0    0        0      0          0     0    0     0   \n",
       "\n",
       "    ...   vietnamese amosun desperate error hulking every    Topic  \\\n",
       "0   ...            0      0         0     0       0     0  economy   \n",
       "1   ...            0      0         0     0       0     0    obama   \n",
       "2   ...            0      0         0     0       0     0  economy   \n",
       "3   ...            0      0         0     0       1     0  economy   \n",
       "4   ...            0      0         0     0       0     0  economy   \n",
       "\n",
       "  SentimentTitle SentimentHeadline Popular  \n",
       "0       0.041667          0.090924     1.0  \n",
       "1      -0.051031          0.022097     1.0  \n",
       "2      -0.034722          0.164685     0.0  \n",
       "3       0.041667         -0.026064     1.0  \n",
       "4       0.000000         -0.102062     0.0  \n",
       "\n",
       "[5 rows x 5920 columns]"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset, we have a vast amount of input variables and a single output variable. Therefore, we need a model with a high computational predictive power for our prediction. \n",
    "\n",
    "Enter **Neural Networks**!\n",
    "\n",
    "The simplest definition of a neural network, more properly referred to as an 'artificial' neural network (ANN), is provided by the inventor of one of the first neurocomputers, Dr. Robert Hecht-Nielsen. He defines a neural network as:\n",
    "    _\"...a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs.\"_ [1]\n",
    "\n",
    "It is a computer system modeled on our human brain and nervous system. \n",
    "    \n",
    "There are different types of neural networks such as Recurrent Neural networks, convolutional neural networks,etc. In our case, we will use a **Feedforward neural network**. Feedforward neural networks are great for learning a pattern between a set of inputs and outputs with high computational power. According to Wiki, a feedforward neural network is an artificial neural network wherein connections between the units do not form a cycle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model, we have the input layer, one hidden layer and an output as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ffnn.png\" alt=\"NewsDataset Neural Network\" title=\"FeedForward Neural Network\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, at every step, our model takes a weighted sum of the inputs at that step, applies an activation function on it and gives a specified number of outputs that are equivalent to the number of inputs the next layer takes. We also define a loss function that helps to optimize our model by decreasing the loss. In our case it is binary_crossentropy as our output variable is a binary classifier output. For activation, we use the sigmoid function in order to capture the linear as well as non-linear property of our functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform random sampling of the data to define 70% of it to be training set and the remaining 30% to be the test set. We also separate the target variable 'Popular' in a 1D array from the rest of the input variables for both, the train and test set. The input variable Topic and the target variable 'Popular' needs to be converted into categorial types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the topic variable to categorical type\n",
    "#Reference:https://stackoverflow.com/questions/32011359/convert-categorical-data-in-pandas-dataframe\n",
    "input_matrix['Topic'] = input_matrix['Topic'].astype('category')\n",
    "cat_columns = input_matrix.select_dtypes(['category']).columns\n",
    "\n",
    "#Converting the string category to numeric code for each\n",
    "input_matrix[cat_columns] = input_matrix[cat_columns].apply(lambda x : x.cat.codes)\n",
    "\n",
    "#Converting the target variable to categorical type\n",
    "input_matrix['Popular'] = to_categorical(input_matrix['Popular'])\n",
    "\n",
    "#Separating the input variables from the target variable\n",
    "X = input_matrix.drop(['Popular'], axis = 1)\n",
    "Y = input_matrix['Popular']\n",
    "\n",
    "#Randomly splitting the data into train(70%) and test(30%) sets\n",
    "training_set_x, test_set_x, training_set_y, test_set_y = train_test_split(X, Y, test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize the model as Sequential which means it is a linear stack of layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the Neural Network\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add the input layer specifying the shape of the input data same as our input matrix, give the ouputs to be 10 and specify the activation function to be a sigmoid to learn both linear as well as non-linear functions in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the input layer\n",
    "model.add(Dense(units = 15, input_shape=(training_set_x.shape[1],), activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a hidden layer after the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding a hidden layer\n",
    "model.add(Dense(units = 10, input_dim=15, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we add the output layer by defining only 1 output and sigmoid to be the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding the output layer\n",
    "model.add(Dense(units = 1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the optimizer by specifying the learning rate which is the ability of the neural network to abandon old beliefs for new ones, that is, how quickly it learns. The rest of the parameters are recommended to be kept the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating an optimizer\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compile our model using the above optimizer, define the loss function that is adapted for binary output and define the metric for measurement to be accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling the RNN\n",
    "model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fit the model on our training dataset by mentioning the number of epochs to train the model and verbosity mode to be silent (0). The batch size which is the number of samples per gradient update is kept at a default of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c0a2c9f28>"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to training set\n",
    "model.fit(training_set_x, training_set_y, epochs=40, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting outcome using the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call the predict function on our trained model on the train as well as test data. It returns a numpy array of predictions between 0 and 1. We will define our threshold to be 0.5, meaning that if the value is less 0.5, the popularity is zero, one otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the outcome on the training set and alloting the appropriate class\n",
    "y_train_pred = model.predict(training_set_x)\n",
    "y_train_class_pred = y_train_pred > 0.5\n",
    "y_train_class_pred = y_train_class_pred.astype(int)\n",
    "\n",
    "#Predicting the outcome on the test set and alloting the appropriate class\n",
    "y_test_pred = model.predict(test_set_x)\n",
    "y_test_class_pred = y_test_pred > 0.5\n",
    "y_test_class_pred = y_test_class_pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c0db8f080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a histogram distribution of Predicted Popularity on test set\n",
    "plt.hist(y_test_pred, bins = 30 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to wiki, accuracy of a classifier measures the fraction of all data points that are correctly classified by it; it is the ratio of the number of correct classifications to the total number of (correct or incorrect) classifications. We will now calculate the same for our predictions and guage how accurate they are, using sklearn library's accuracy_score method for our train as well as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy score is 0.990\n",
      "The testing accuracy score is 0.727\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://github.com/neelabhpant/Deep-Learning-in-Python/blob/master/Logistic_Regression_Keras.ipynb\n",
    "print(\"The training accuracy score is {:0.3f}\".format(accuracy_score(training_set_y, y_train_class_pred)))\n",
    "print(\"The testing accuracy score is {:0.3f}\".format(accuracy_score(test_set_y, y_test_class_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our training accuracy is pretty high while our test accuracy is just okay. This is because, as we saw earlier, if we were to classify all the news items to be popular, we would still get around 75% accuracy. In that sense, our model is not really doing a good job and we would need to find ways to either tune it more, find more appropriate features or use another model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Confusion Matrix :\n",
      "\n",
      "              Predicted Not Popular  Predicted Popular\n",
      "Not Popular                    789                  0\n",
      "Popular                         10                251\n",
      "------------------------------------------------\n",
      "Testing Data Confusion Matrix :\n",
      "\n",
      "              Predicted Not Popular  Predicted Popular\n",
      "Not Popular                    307                 24\n",
      "Popular                         99                 20\n"
     ]
    }
   ],
   "source": [
    "#Function to create the confusion matrix \n",
    "def create_confusion_matrix(true_y, y_pred, categories=[0, 1]):\n",
    "    \n",
    "    #Converting to 1D lists\n",
    "    true_y = list(true_y)    \n",
    "    y_pred = list(y_pred.flatten())\n",
    "\n",
    "    #Confusion matrix\n",
    "    conmat = confusion_matrix(true_y, y_pred)\n",
    "    \n",
    "    #Defining the categories\n",
    "    pred_categories = [\"Predicted \"+ c for c in categories]\n",
    "    \n",
    "    #Creating the CM dataframe\n",
    "    cm_df = pd.DataFrame(conmat, index=categories, columns=pred_categories)\n",
    "    \n",
    "    return cm_df\n",
    "\n",
    "#Printing the confusion matrix\n",
    "print(\"Training Data Confusion Matrix :\\n\\n\",create_confusion_matrix(training_set_y, y_train_class_pred, [\"Not Popular\", \"Popular\"]))\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Testing Data Confusion Matrix :\\n\\n\",create_confusion_matrix(test_set_y, y_test_class_pred, [\"Not Popular\", \"Popular\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the model: K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can probably improve this accuracy by tuning our model using K-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross validation accuracy is 0.7257 ± 0.0149\n"
     ]
    }
   ],
   "source": [
    "#Function to build the neural network so that it can be run over and over again\n",
    "def build_neuralnet_model():\n",
    "    nnmodel = Sequential()\n",
    "    nnmodel.add(Dense(15, input_shape=(training_set_x.shape[1],), activation='sigmoid'))\n",
    "    nnmodel.add(Dense(units = 10, input_dim=15, activation='sigmoid'))\n",
    "    nnmodel.add(Dense(1, activation='sigmoid'))\n",
    "    nnmodel.compile(optimizer=rmsprop,\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "    return nnmodel\n",
    "\n",
    "#Reference: https://github.com/neelabhpant/Deep-Learning-in-Python/blob/master/Logistic_Regression_Keras.ipynb\n",
    "nnmodel = KerasClassifier(build_fn=build_neuralnet_model,\n",
    "                        epochs=40,\n",
    "                        verbose=0)\n",
    "\n",
    "#Running the Kfold cross validation on the neural net model created\n",
    "cross_val = KFold(5, shuffle=True)\n",
    "scores = cross_val_score(nnmodel, training_set_x, training_set_y, cv=cross_val)\n",
    "\n",
    "print(\"The cross validation accuracy is {:0.4f} ± {:0.4f}\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the cross validation accuracy is more or less the same as the Accuracy we got previously, we can say that our model is sufficiently tuned even after shuffling our data through various folds of cross-valdiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible further research areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to increase our model performance, we can expand our research in the following ways:\n",
    "1. Instead of using a Document Term Matrix, we can possibly use a TFIDF matrix as input features.\n",
    "2. Instead of converting Popularity to a binary variable, we can predict the actual measure of popularity, that is, the total number of shares. \n",
    "\n",
    "But wait... what if the ordering of data mattered? If we order our news items according to their Publish Dates, the popularity of a previously published news item may affect the popularity of the newly published news article on the same topic, that is, economy and having similar kind of words along with their frequency.\n",
    "\n",
    "As a result, by ordering the input data by Publish Date and making it **sequential**, we can possibly use Recurrent Neural Networks to use previous hidden state for a **particular topic** and predict the popularity based on similar words and sentiments, to predict the popularity for subsequent news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html\n",
    "2. https://keras.io/optimizers/\n",
    "2. https://keras.io/getting-started/sequential-model-guide/\n",
    "3. https://arxiv.org/pdf/1801.07055.pdf\n",
    "4. https://github.com/neelabhpant/Deep-Learning-in-Python/blob/master/Logistic_Regression_Keras.ipynb\n",
    "5. https://github.com/kimanalytics/Recurrent-Neural-Network-to-Predict-Stock-Prices/blob/master/Recurrent%20Neural%20Network%20to%20Predict%20Tesla%20Stock%20Prices.ipynb\n",
    "6. https://github.com/Kulbear/stock-prediction/blob/master/data-preparation.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
