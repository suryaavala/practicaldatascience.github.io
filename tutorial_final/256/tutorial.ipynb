{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning with PyTorch\n",
    "## Introduction\n",
    "This tutorial will introduce PyTorch, a deep learning framework for python initiated by Facebook.  \n",
    "Compared to Tensorflow, PyTorch has some features that benefit the development and make it worth learning:\n",
    "- In TensorFlow, the structure of the neural network must be defined and compiled statically before a model can run, while in PyTorch the structure can change dynamically in the run time, which can be illustrated especially in [sequences modeling](#Example:-Tweets-sentiment-prediction). \n",
    "- Users with sufficient knowledge for deep learning can start to code in PyTorch quickly, while TensorFlow's static mechanism leads to more tedious concepts and boilerplate code for users to learn.\n",
    "- The dynamical mechanism also makes it eaiser to debug in PyTorch. Common Python IDEs like [PyCharm](https://www.jetbrains.com/pycharm/) can directly debug PyTorch but not TensorFlow. \n",
    "\n",
    "\n",
    "## Content\n",
    "We will cover some basic and advanced usages of PyTorch and finally use an application example to illustrate how to use PyTorch to solve a real data problem.   \n",
    "Following topics are covered:\n",
    "- [Basic I: Tensor](#Basic-I:-Tensor)\n",
    "- [Basic II: Autograd: automatic differentiation](#Basic-II:-Autograd:-automatic-differentiation)\n",
    "- [Basic III: Constructing neural networks](#Basic-III:-Constructing-neural-networks)\n",
    "- [Advanced I: Module and Function Customization](#Advanced-I:-Module-and-Function-Customization)\n",
    "- [Advanced II: Loss Function Customization](#Advanced-II:-Loss-Function-Customization)\n",
    "- [Example: Tweets sentiment prediction](#Example:-Tweets-sentiment-prediction)\n",
    "- [Further resources](#Further-resources)\n",
    "\n",
    "**Note**:  \n",
    "PyTorch, Numpy and Pandas should be installed for running the code.  \n",
    "As memtion in the [official website](http://pytorch.org/), PyTorch can be installed as below:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s1.ax1x.com/2018/03/17/95zSX9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic I: Tensor\n",
    "Implementing a neural netowork often involves matrixes, or more generally, [tensors](https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c) computations. In numpy, we may use ndarrays for such computations. In PyTorch, Tensors serve as similar data objects compared to ndarrays in numpy, while it is easy for Tensor to use the power of GPUs.\n",
    "### Operations\n",
    "The below code snippets show some most common operations of Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation example\n",
      "\n",
      " 4.2549e-37  0.0000e+00  2.4027e-01\n",
      " 4.5901e-41  2.4027e-01  4.5901e-41\n",
      " 1.1884e-37  0.0000e+00  1.1884e-37\n",
      " 0.0000e+00  7.7008e-16  4.5901e-41\n",
      " 1.5342e-19  1.1707e-19  0.0000e+00\n",
      " 0.0000e+00  1.3354e+09  4.5901e-41\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n",
      "\n",
      " 0.6403  0.2933  0.7005\n",
      " 0.5391  0.7813  0.7213\n",
      " 0.7448  0.8847  0.7505\n",
      " 0.4780  0.7247  0.4475\n",
      " 0.3711  0.4441  0.6590\n",
      " 0.5147  0.4491  0.4959\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n",
      "Size example\n",
      "torch.Size([6, 3])\n",
      "Addition example\n",
      "\n",
      " 1.3746  1.2270  1.0780\n",
      " 0.8528  1.6781  0.8199\n",
      " 1.3832  1.7739  1.0324\n",
      " 0.8843  1.7162  1.0872\n",
      " 1.1871  0.4506  1.4266\n",
      " 1.1611  1.0453  1.4595\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n",
      "\n",
      " 1.3746  1.2270  1.0780\n",
      " 0.8528  1.6781  0.8199\n",
      " 1.3832  1.7739  1.0324\n",
      " 0.8843  1.7162  1.0872\n",
      " 1.1871  0.4506  1.4266\n",
      " 1.1611  1.0453  1.4595\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n",
      "\n",
      " 1.3746  1.2270  1.0780\n",
      " 0.8528  1.6781  0.8199\n",
      " 1.3832  1.7739  1.0324\n",
      " 0.8843  1.7162  1.0872\n",
      " 1.1871  0.4506  1.4266\n",
      " 1.1611  1.0453  1.4595\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n",
      "Matrix multiplication example\n",
      "\n",
      " 3.6781  4.8028  3.5943\n",
      " 2.0505  2.4719  1.9216\n",
      " 2.8420  3.2647  2.8794\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "Slicing example\n",
      "\n",
      " 0.2933\n",
      " 0.7813\n",
      " 0.8847\n",
      " 0.7247\n",
      " 0.4441\n",
      " 0.4491\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      "\n",
      " 0.6403\n",
      " 0.2933\n",
      " 0.7005\n",
      " 0.5391\n",
      " 0.7813\n",
      " 0.7213\n",
      " 0.7448\n",
      " 0.8847\n",
      " 0.7505\n",
      " 0.4780\n",
      " 0.7247\n",
      " 0.4475\n",
      " 0.3711\n",
      " 0.4441\n",
      " 0.6590\n",
      " 0.5147\n",
      " 0.4491\n",
      " 0.4959\n",
      "[torch.FloatTensor of size 18]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Creation example\")\n",
    "# Construct a 6x3 matrix, uninitialized\n",
    "x = torch.Tensor(6, 3)\n",
    "print(x)\n",
    "# Construct a randomly initialized matrix from a uniform distribution on the interval [0,1)\n",
    "x = torch.rand(6, 3)\n",
    "print(x)\n",
    "\n",
    "print(\"Size example\")\n",
    "# Get the size of the tensor\n",
    "print(x.size())\n",
    "\n",
    "print(\"Addition example\")\n",
    "# Addition: syntax 1\n",
    "y = torch.rand(6, 3)\n",
    "print(x + y)\n",
    "\n",
    "# Addition: syntax 2\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# Addition: in-place. And all operations that mutates a tensor in-place \n",
    "#is post-fixed with an _ character\n",
    "y.add_(x)\n",
    "print(y)\n",
    "\n",
    "# Matric multiplication\n",
    "print(\"Matrix multiplication example\")\n",
    "z = torch.rand(3,6)\n",
    "print(torch.mm(z,y))\n",
    "\n",
    "print(\"Slicing example\")\n",
    "# Slicing, standard NumPy-like indexing syntax is supported,\n",
    "# including indexing using boolean array\n",
    "print(x[:, 1])\n",
    "print(x[x>0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More operations on Tensor can be found [here](http://pytorch.org/docs/master/torch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Bridge\n",
    "Tensor can be converted to a NumPy array and vice versa.  \n",
    "And we need to note that \n",
    "the Tensor and NumPy array will share their underlying memory locations, and changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert tensor to numpy array\n",
    "tensor = torch.ones(6)\n",
    "print(tensor)\n",
    "array = tensor.numpy()\n",
    "print(array)\n",
    "\n",
    "# Convert numpy array to tensor\n",
    "import numpy as np\n",
    "array = np.ones(5)\n",
    "tensor = torch.from_numpy(array)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 1. 1. 1.]\n",
      "\n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the numpy array will also change the tensor converted from it\n",
    "array[0]=2\n",
    "print(array)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPU\n",
    "Tensors can be moved onto GPU to utilize its power for speeding up the computation using the **.cuda** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code will get the on-GPU tensor if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic II: Autograd: automatic differentiation\n",
    "Computing gradients is the key step of conducting backprop in a neural network. Without any other supports, the computation of gradient requiring conducting differentiation will involves much mannual effort to determine how to compute. In order to mitigate this effort, the **autograd** package in PyTorch is designed to provide automatic differentiation for all **defined operations** ([later](#Advanced-I:-Customized-Module-and-Function) we will see the exception) on Tensors. Unlike frameworks like Tensorflow which provide automatic differentiation in a static way, PyTorch designs it as a define-by-run framework, meaning that the backprop depends on how the code is run, and every single iteration can be different instead of being fixed once compiled.\n",
    "\n",
    "### Variable\n",
    "It is easy to get the gradents in PyTorch. **autograd.Variable** class wraps a Tensor, and supports nearly all of operations defined on it. Once finishing the computation, calling **.backward()** will make all the gradients computed automatically.  \n",
    "We can access the raw tensor through its **.data** attribute, and the gradients w.r.t this Variable is accumulated into **.grad**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://pytorch.org/tutorials/_images/Variable.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function\n",
    "**Function** is another important class for autograd implementation. Each Variable has a **.grad_fn** attribute that references a Function who created this Variable (but grad_fn is None for Variables created by the user). Function links the Variable it creates and other Variables it use to create this Variable. Therefore, by interconnecting Variable and Function, we can represent a acyclic graph to encode a complete history of the computation.   \n",
    "**Note**: the .grad_fn attribute will be created automatically when any operation is applied on Variable(s) to create to new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1  1\n",
      " 1  1  1\n",
      " 1  1  1\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Create a variable, and requires_grad will mark the Variable whose gradient is needed to\n",
    "# be computed during backprop\n",
    "x = Variable(torch.ones(3, 3), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4  4  4\n",
      " 4  4  4\n",
      " 4  4  4\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "<AddBackward0 object at 0x7ff4265e3750>\n"
     ]
    }
   ],
   "source": [
    "# Do an operation of variable\n",
    "y = x + 3\n",
    "print(y)\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn.\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct differentiation, we can call **.backward()** on a Variable, by which gradients w.r.t it will be computed for **all its ancestors in the Variable acyclic graph**:\n",
    "- If Variable is a scalar (i.e. it holds a one element data), we don’t need to specify any arguments to backward()\n",
    "- If it has more elements, we need to specify a gradient argument, which is a tensor with matching shape.  \n",
    "  \n",
    "Note, once a Variable is created, its backward method can only be called once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.7778  1.7778  1.7778\n",
      " 1.7778  1.7778  1.7778\n",
      " 1.7778  1.7778  1.7778\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 2\n",
    "o = z.mean()\n",
    "\n",
    "\n",
    "# o.backward() is equivalent to doing out.backward(torch.Tensor([1.0])), since out is a scalar\n",
    "o.backward()\n",
    "# After calling o.backward(), we can access the computed gradients of x by x.grad\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should print a matrix with element of value 1.7778.  \n",
    "Its mannual differentiation process is:  \n",
    "$o=\\frac{1}{9}\\sum_{i}z_i$  \n",
    "$z_i=2(x_i+3)^2$, and $z_i | _{x_i}=32$  \n",
    "Hence, $\\frac{∂o}{∂x_i}=\\frac{4}{9}(x_i+3)$  \n",
    "$\\frac{∂o}{∂x_i}|_{x_i=1}=\\frac{16}{9}≈1.7778$  \n",
    "But now, with Autograd, we can save the effor for these differentiation works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic III: Constructing neural networks\n",
    "We can use the **torch.nn** package to construct neural networks.   \n",
    "### Define the network\n",
    "Let's take a look at the classic [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) model for classifying digit images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://pytorch.org/tutorials/_images/mnist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a feed-forward network, it takes the input, feeds it through several layers one by one, and finally generates the output. This kind of networks can be taken as a kind of acyclic graph where each layer in it can be taken as one node in the graph, and the direction of the data flow indicates the edge direction between nodes. Hence, we can see that the acyclic graph representation mechanism described in [Autograd](#Basic-II:-Autograd:-automatic-differentiation) section is quite suitable.  \n",
    "  \n",
    "Each neural network model in PyTorch is defined as a subclass of the **nn.Module** class. Every nn.Module (or its subclass) builds on top of other Modules or Functions, and contains a method **forward(input)** that returns the **output**. The input to the forward is an autograd.Variable. So is the output.  \n",
    "The only things needed to be done is to define the forward method, where we can use any Tensor operations. And the backward method for backprop will be automatically defined using autograd.  \n",
    "\n",
    "The model above can be implemented as below (for better reference, [official example code](https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/neural_networks_tutorial.py) is used here):  \n",
    "**Note**: several pre-defined building blocks of neural networks are under the [nn package](http://pytorch.org/docs/nn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All learnable parameters of a model are returned by **net.parameters()**, which is a generator that returns parameters in an order that corresponds to the order of layers in the network starting from the input.  \n",
    "  \n",
    "**Note**: The entire torch.nn package only supports inputs in the form of a mini-batch of samples, instead a single sample.  \n",
    "For example, nn.Conv2d taks a 4D Tensor of nSamples x nChannels x Height x Width as input.  \n",
    "Therefore, if using a single sample, input.unsqueeze(0) is needed to add a fake batch dimension, where the parameter indicates the index of the dimension to be added.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "A loss function takes the (output, target) pairs as inputs, and computes a value for estimating the difference between the output and the target. Several [loss functions](http://pytorch.org/docs/nn.html#loss-functions) are pre-defined under the nn package.  \n",
    "  \n",
    "For example, **nn.MSELoss** computes the mean-squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 38.9208\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_data = Variable(torch.randn(1, 1, 32, 32))\n",
    "# Note that the object of nn.Module will forward the \n",
    "# calling of __call__ method to the forward method, so net(input_data)\n",
    "# actually equals to calling net.forward(input_data)\n",
    "output = net(input_data)\n",
    "target = Variable(torch.arange(1, 11))  # a dummy target just for demo\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling **loss.backward()**, the whole graph will be differentiated w.r.t. the loss and gradients of all Variables in the graph will be accumulated in their .grad attributes.\n",
    "**Note**: before accumulating gradients through backprop, we should use **net.zero_grad()** to zero the gradient buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc3.bias.grad before backward\n",
      "None\n",
      "fc3.bias.grad after backward\n",
      "Variable containing:\n",
      "-0.2015\n",
      "-0.3807\n",
      "-0.5985\n",
      "-0.7914\n",
      "-1.0168\n",
      "-1.1685\n",
      "-1.3838\n",
      "-1.6255\n",
      "-1.8084\n",
      "-2.0424\n",
      "[torch.FloatTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print('fc3.bias.grad before backward')\n",
    "print(net.fc3.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('fc3.bias.grad after backward')\n",
    "print(net.fc3.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the parameters\n",
    "Finally, we need to update the parameters for the network during the training. But we don't need to implement the optimization methods to compute the updates by ourself. Instead, PyTorch provides several optimization methods for updating the weights of the network under [torch.optim](http://pytorch.org/docs/0.3.1/optim.html).  \n",
    "  \n",
    "Each optimization method take the parameters of the network as input with possible specific arguments for the method itself to generate the optimizer. By calling **.step()** of the optimizer after the backprop, the parameters of the network will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create the optimizer, 'lr' is the learning rate parameter\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# Below is what needed to be done in one iteration of a loop :\n",
    "optimizer.zero_grad()   # Zero the gradient buffers\n",
    "output = net(input_data)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Conduct the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can summarize a typical process for training a network in PyTorch as follows:\n",
    "1. Use optimization methods under **torch.optim** to generate an optimizer for the constructed network\n",
    "2. Call **zero_grad** of the network or of the optimizer to zero the gradient buffers\n",
    "3. Give the input(a mini-batch of samples) to the network (object **net** here)\n",
    "4. Use the output to compute the **loss** with the target using a specific loss function\n",
    "5. Conduct backprop through calling the **backward** method on the loss\n",
    "6. Call **step** of the optimizer to update the parameters\n",
    "7. Back to step 2 to start a new mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced I: Module and Function Customization\n",
    "### Comparison\n",
    "In [Basic III: Constructing neural networks](#Basic-III:-Constructing-neural-networks), we built a customized CNN-based network in the form of a new **Module** using existed **Modules** and **Functions** provided under [torch.nn](http://pytorch.org/docs/0.3.1/nn.html) and [torch.nn.functional](http://pytorch.org/docs/0.3.1/nn.html#torch-nn-functional). Now let's take a look on the comparison between Module and Function.  \n",
    "- In general, Module can be taken as a layer of the network or a complete network built on top of other Modules or Functions as seen before. Module defines the hyperparameters of the layer(s) along with how to conduct forward operations to transform the input **Variable** into the output **Variable**, and also keeps the parameters of the layer(s) during training. Since Module is built on top of other Modules (hence on top of Functions finally) or Functions, its backward method can be implemented by simply calling backward methods of Functions'. Therefore PyTorch automatically do it for us and we don't need to define them.\n",
    "- However, Function **cannot** keep the parameters of the layer, so it would be more suitable for defining operations like activation function and pooling only. Similarly, we needs to define hyperparameters (if needed) and forward method for Function. Additionally, we need to define the backward method for it, which means we need to conduct the differentiation process ourselves.\n",
    "\n",
    "### Customization\n",
    "We've seen how to write a customized Module [before](#Define-the-network), now let's look at how to write a customized Function. Here we use [LeakyReLU](https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29#Leaky_ReLUs) as an example.  \n",
    "The definition of LeakyReLU is:  \n",
    "  \n",
    "$f(x)=max(0,x)+a×min(0,x)$  \n",
    "  \n",
    "where a is a small non-zero hyperparameter. And its differentiation is:  \n",
    "  \n",
    "$\\begin{equation}  \n",
    "\\frac{∂f(x)}{∂x}=\\left\\{  \n",
    "             \\begin{array}{lr}  \n",
    "             a, if\\ x≤0 &  \\\\\n",
    "             1, if\\ x>0 &    \n",
    "             \\end{array}  \n",
    "\\right.  \n",
    "\\end{equation}  $  \n",
    "  \n",
    "Now we can implement the LeakyReLU Function as below: ([clamp](http://pytorch.org/docs/0.3.1/torch.html?highlight=torch%20clamp#torch.clamp) document may be helpful)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLeakyReLU(torch.autograd.Function):\n",
    "\n",
    "    \n",
    "    def __init__(self,a):\n",
    "        self.a=a # Keep the hyperparameter a\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        # Save the input (whose type is Tensor) for later use in backward\n",
    "        self.save_for_backward(input_)         \n",
    "        # LeakyReLU is a combination of minimum and scaled maximum clamps\n",
    "        output = input_.clamp(min=0) + self.a * input_.clamp(max=0) \n",
    "        # For LeakyReLU, one input Tensor only results in one output Tensor, \n",
    "        # but you can output a tuple containing multiple Tensors when needed\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_input):  \n",
    "        # According to the chain rule, dloss / dx = (dloss / doutput) * (doutput / dx)\n",
    "        # and dloss / doutput is the grad_input argument here, whose type is Tensor\n",
    "        # The number of input Tensors here is equals to the number of output Tensors of the \n",
    "        # forward method. And each input Tensor here represents the gradient w.r.t. that output.\n",
    "        # For LeakyReLU, there is only one output Tensor for the forward method, so we \n",
    "        # receive only one input Tensor here correspondingly.\n",
    "        input_, = self.saved_tensors\n",
    "        grad_input_copy = grad_input.clone()\n",
    "        # Now, we need to multiply the dloss / doutput by doutput/dx according to the chain \n",
    "        # rule shown above.\n",
    "        # According to the differentiation, LeakyReLU only scale dloss / doutput of those elements \n",
    "        # whose input element<0 by a, and others are kept as original\n",
    "        # Note that as mentioned before, Tensor is quite like the numpy.narray, therefore\n",
    "        # retrieving elements using multi-dimensional boolean array is also supported just\n",
    "        # like numpy.narray\n",
    "        grad_input_copy[input_ <= 0] = self.a*grad_input_copy[input_ <= 0]\n",
    "        return grad_input_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0500\n",
      " 0.5000\n",
      " 0.3000\n",
      "[torch.DoubleTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "# Now we can validate the implementation of MyLeakyReLU\n",
    "array = np.array([-0.5,0.5,0.3])\n",
    "tensor = torch.from_numpy(array)\n",
    "input_ = Variable(tensor,requires_grad=True)\n",
    "leaky_relu = MyLeakyReLU(a=0.1)\n",
    "output_ = leaky_relu(input_)\n",
    "\n",
    "# We can see the element with value of -0.5 will be scaled by a=0.1\n",
    "# and becomes -0.05\n",
    "print(output_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0333\n",
      " 0.3333\n",
      " 0.3333\n",
      "[torch.DoubleTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "o=output_.mean()\n",
    "o.backward()\n",
    "\n",
    "# Note that since o=1/3(∑LeakyLeRU(xi)), do/dxi = 1/3*a if xi<=0, or 1/3 if xi>0\n",
    "# So gradient for x0 which is less than 0 will be 1/3*a ≈ 0.0333\n",
    "print(input_.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced II: Loss Function Customization\n",
    "Loss functions can be seen as a special kind of Module, which takes one or more Tensors as inputs and return a scalar as output. Indeed, all pre-defined loss functions in PyTorch are implemented as subclasses of Module.  \n",
    "  \n",
    "One example of defining a L1 loss function is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Function(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyL1Function, self).__init__()\n",
    "    \n",
    "    def forward(self,input_,target):\n",
    "        abs_result=torch.abs(input_-target)\n",
    "        return torch.sum(abs_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# We can validate our implementation as below\n",
    "l1loss=MyL1Function()\n",
    "t1=torch.from_numpy(np.array([1,1,1]))\n",
    "t2=torch.from_numpy(np.array([0,-1,2]))\n",
    "loss=l1loss(t1,t2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Tweets sentiment prediction\n",
    "**Note**: This tutorial focuses on the usage of PyTorch, so due to the limit of words, knowledges about [word embedding](https://en.wikipedia.org/wiki/Word_embedding), [RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) will not be covered here, but you can reference the links attached.  \n",
    "  \n",
    "The goal here is to build a LSTM-based model for tweets sentiment prediction.  \n",
    "Assume the input tweet consists of words sequence $w_1,…,w_M$, where $w_i∈V$, the vocabulary, and let $L=\\{0,1\\}$ be the sentiment labels set. Our model need to generate the prediction sentiment label $\\widehat{l}$ of the tweet , where $\\widehat{l}∈L$.  \n",
    "  \n",
    "This section will introduce a full pipeline of building, training and testing the model as well as illustrate the technique to feed varing length sequences input to RNN-family model in PyTorch.  \n",
    "  \n",
    "Let's first take a look at the model. As illustrated below, tweets will first be transferred into padded word id sequences, and then each word id will be replaced by its corresponding word embedding vector, which then serve as the input for the LSTM unit at each time step. Output of each LSTM unit will be transferred to the units of next time step and next layer of corresponding time step, except the unit of the last time step. The output of the unit of the last time step in the last layer will serve as the input to the last linear layer, whose output will pass through a log softmax unit to generate the final output of the model.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s1.ax1x.com/2018/03/22/9Hi7rV.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above can be implemented as followings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import (pack_padded_sequence, pad_packed_sequence)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, label_size, embedding_dim, \n",
    "                 lstm_hidden_size, lstm_num_layers, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=False)\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=lstm_hidden_size, out_features=label_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, word_ids, seq_lens):\n",
    "        # word_ids is assumed to be in the form of nSamples x max_seq_len, \n",
    "        # each sample is of varing length substantially but should all be\n",
    "        # padded to be of same length, the maximum sequence length\n",
    "        #\n",
    "        # seq_lens is assumed to be in the form of nSamples x 1, where each\n",
    "        # element is an integer that indicates the length of the sample before\n",
    "        # padding\n",
    "        #\n",
    "        # nSamples equals to the size of a mini-batch, hence below all use \n",
    "        # batch_first=True since nSamples is the first dimension\n",
    "        embedding = self.embedding(word_ids)\n",
    "        # Since each sample (word sequence here representing a tweet) is \n",
    "        # substantially of varing length, those padded values (word ids here) \n",
    "        # should not be used to compute gradient and so on.\n",
    "        # PyTorch provides a class called PackedSequence to store such type of\n",
    "        # varing length samples, which can be taken as the input of rnn-family\n",
    "        # layer such as LSTM, GRU, etc. And pack_padded_sequence function is the \n",
    "        # key function to transform the same-length padded sequences to PackedSequence,\n",
    "        # provided the sequences and the original sequence lengths.\n",
    "        # However, it requires sequences input to be sorted by length in a decreasing\n",
    "        # order, which should be paid attention to during input preparation.\n",
    "        embedding = pack_padded_sequence(embedding, seq_lens, batch_first=True)\n",
    "        # Note that although for each mini-batch, the input sequences length seems to be the same,\n",
    "        # but different batch may have different maximum sequence length. However, unlike Tensorflow\n",
    "        # which compiles the graph statically, PyTorch allow dinamically and automatically changing\n",
    "        # the structure parameters of the network in the run time, therefore we don't need to do any\n",
    "        # special action to deal with these differences, while Tensorflow, instead, has to at least\n",
    "        # estimate the maximum sequence length for all possible sequences at the very beginning \n",
    "        # to avoid re-compile the model.\n",
    "        out_lstm, _ = self.lstm(embedding)\n",
    "        # pad_packed_sequence just does the reverse thing of pack_padded_sequence's,\n",
    "        # which transforms the PackedSequence back to padded sequences.\n",
    "        out_lstm, lengths = pad_packed_sequence(out_lstm, batch_first=True)\n",
    "        # For classification, we only need the last time step\n",
    "        # output from LSTM to feed the final linear layer\n",
    "        lengths = [l - 1 for l in lengths]\n",
    "        last_output = out_lstm[range(len(lengths)),lengths]\n",
    "        out_linear = self.dense(last_output)\n",
    "        output = self.softmax(out_linear)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the data preprocessing code. Note that the tweet csv file can be downloaded [here](https://drive.google.com/file/d/1qkFbgdg_gs85sqUhvToQ8R33R3jvDPwK/view?usp=sharing), which is indeed the first 5000 tweets from a [bigger dataset](http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Clean the text by rules\n",
    "def cleanup_text(texts):\n",
    "    cleaned_text = []\n",
    "    for text in texts:\n",
    "        # remove &quot and &amp\n",
    "        text = re.sub(r'&quot;(.*?)&quot;', \"\\g<1>\", text)\n",
    "        text = re.sub(r'&amp;', \"\", text)\n",
    "\n",
    "        # replace emoticon\n",
    "        text = re.sub(r'(^| )(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)', \"\\g<1>TOKEMOTICON\", text)\n",
    "\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"tokemoticon\", \"TOKEMOTICON\")\n",
    "\n",
    "        # replace url\n",
    "        text = re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?',\n",
    "                    \"TOKURL\", text)\n",
    "\n",
    "        # replace mention\n",
    "        text = re.sub(r'@[\\w]+', \"TOKMENTION\", text)\n",
    "\n",
    "        # replace hashtag\n",
    "        text = re.sub(r'#[\\w]+', \"TOKHASHTAG\", text)\n",
    "\n",
    "        # replace dollar\n",
    "        text = re.sub(r'\\$\\d+', \"TOKDOLLAR\", text)\n",
    "\n",
    "        # remove punctuation\n",
    "        text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "\n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "\n",
    "        # remove newline\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "        cleaned_text.append(text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Transform the words sequence into padded word ids sequence\n",
    "def prepare_sequence(seq, to_ix,max_len):\n",
    "    unseen = len(to_ix)\n",
    "    idxs = [to_ix[w] if w in to_ix else unseen for w in seq]\n",
    "    return np.pad(np.array(idxs),(0,max_len-len(idxs)),\"constant\")\n",
    "\n",
    "# Transform the text data into padded word ids sequence and \n",
    "# sort them by their length in a descending order\n",
    "# Return transformed text data, original sequence lengths list and label list\n",
    "# in corresponding sorted order\n",
    "def transform(data,word_to_ix):\n",
    "    sorted_data = data.sort_values(by=\"seq_len\",ascending=False)\n",
    "    text_x = sorted_data[\"text\"].values\n",
    "    seq_lens = list(map(lambda x: len(x), text_x))\n",
    "    max_len = max(seq_lens)\n",
    "    text_x = np.array(map(lambda seq: prepare_sequence(seq, word_to_ix, max_len), text_x))\n",
    "    text_x = torch.autograd.Variable(torch.from_numpy(text_x))\n",
    "    return text_x, seq_lens, sorted_data[\"label\"].values\n",
    "\n",
    "dataset_name = \"sentiment-analysis-dataset5000.csv\"\n",
    "train_frac = 0.7\n",
    "\n",
    "data = pd.read_csv(dataset_name, names=[\"text\", \"label\"])\n",
    "data[\"text\"] = cleanup_text(data[\"text\"].values)\n",
    "data[\"text\"] = data[\"text\"].apply(lambda x: x.split())\n",
    "data[\"seq_len\"] = data[\"text\"].apply(len)\n",
    "data = data.sample(frac=1, random_state=2018).reset_index(drop=True)\n",
    "data_size = len(data)\n",
    "train_data = data[:(int)(train_frac * data_size)]\n",
    "val_data = data[(int)(train_frac * data_size):]\n",
    "\n",
    "word_to_ix = {}\n",
    "for tweet in train_data[\"text\"].values:\n",
    "    for word in tweet:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "\n",
    "vocab_size = len(word_to_ix)+1 # One extra slot for all unseen words\n",
    "label_size = len(data.label.unique())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time, only 10 epoch is trained below, but it is enough to illustrate the process of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "batch_range:[0,100),loss:0.71413857\n",
      "batch_range:[100,200),loss:0.6888852\n",
      "batch_range:[200,300),loss:0.6732738\n",
      "batch_range:[300,400),loss:0.66105306\n",
      "batch_range:[400,500),loss:0.5905137\n",
      "batch_range:[500,600),loss:0.54367226\n",
      "batch_range:[600,700),loss:0.7153888\n",
      "batch_range:[700,800),loss:0.6606488\n",
      "batch_range:[800,900),loss:0.6947618\n",
      "batch_range:[900,1000),loss:0.57920736\n",
      "batch_range:[1000,1100),loss:0.618479\n",
      "batch_range:[1100,1200),loss:0.59570414\n",
      "batch_range:[1200,1300),loss:0.60670584\n",
      "batch_range:[1300,1400),loss:0.6103523\n",
      "batch_range:[1400,1500),loss:0.60352015\n",
      "batch_range:[1500,1600),loss:0.6328545\n",
      "batch_range:[1600,1700),loss:0.5811124\n",
      "batch_range:[1700,1800),loss:0.6144712\n",
      "batch_range:[1800,1900),loss:0.59883475\n",
      "batch_range:[1900,2000),loss:0.6295128\n",
      "batch_range:[2000,2100),loss:0.66279894\n",
      "batch_range:[2100,2200),loss:0.5876168\n",
      "batch_range:[2200,2300),loss:0.52975965\n",
      "batch_range:[2300,2400),loss:0.5235077\n",
      "batch_range:[2400,2500),loss:0.56950676\n",
      "batch_range:[2500,2600),loss:0.567173\n",
      "batch_range:[2600,2700),loss:0.6232381\n",
      "batch_range:[2700,2800),loss:0.5260246\n",
      "batch_range:[2800,2900),loss:0.5074758\n",
      "batch_range:[2900,3000),loss:0.49873847\n",
      "batch_range:[3000,3100),loss:0.57968\n",
      "batch_range:[3100,3200),loss:0.529693\n",
      "batch_range:[3200,3300),loss:0.532749\n",
      "batch_range:[3300,3400),loss:0.59339476\n",
      "batch_range:[3400,3500),loss:0.49484384\n",
      "epoch:1\n",
      "batch_range:[0,100),loss:0.5167083\n",
      "batch_range:[100,200),loss:0.50651217\n",
      "batch_range:[200,300),loss:0.60640156\n",
      "batch_range:[300,400),loss:0.65138006\n",
      "batch_range:[400,500),loss:0.43487468\n",
      "batch_range:[500,600),loss:0.48387894\n",
      "batch_range:[600,700),loss:0.42228967\n",
      "batch_range:[700,800),loss:0.4805618\n",
      "batch_range:[800,900),loss:0.598099\n",
      "batch_range:[900,1000),loss:0.5125799\n",
      "batch_range:[1000,1100),loss:0.4973307\n",
      "batch_range:[1100,1200),loss:0.45311773\n",
      "batch_range:[1200,1300),loss:0.483442\n",
      "batch_range:[1300,1400),loss:0.5428786\n",
      "batch_range:[1400,1500),loss:0.43381488\n",
      "batch_range:[1500,1600),loss:0.5334266\n",
      "batch_range:[1600,1700),loss:0.5037317\n",
      "batch_range:[1700,1800),loss:0.5242858\n",
      "batch_range:[1800,1900),loss:0.5165519\n",
      "batch_range:[1900,2000),loss:0.49052277\n",
      "batch_range:[2000,2100),loss:0.55941194\n",
      "batch_range:[2100,2200),loss:0.49656284\n",
      "batch_range:[2200,2300),loss:0.431197\n",
      "batch_range:[2300,2400),loss:0.43343636\n",
      "batch_range:[2400,2500),loss:0.43956646\n",
      "batch_range:[2500,2600),loss:0.45114765\n",
      "batch_range:[2600,2700),loss:0.45732194\n",
      "batch_range:[2700,2800),loss:0.38266417\n",
      "batch_range:[2800,2900),loss:0.41424257\n",
      "batch_range:[2900,3000),loss:0.37598377\n",
      "batch_range:[3000,3100),loss:0.47805715\n",
      "batch_range:[3100,3200),loss:0.44085875\n",
      "batch_range:[3200,3300),loss:0.42167786\n",
      "batch_range:[3300,3400),loss:0.44572765\n",
      "batch_range:[3400,3500),loss:0.3112123\n",
      "epoch:2\n",
      "batch_range:[0,100),loss:0.40104267\n",
      "batch_range:[100,200),loss:0.404972\n",
      "batch_range:[200,300),loss:0.40317953\n",
      "batch_range:[300,400),loss:0.49698693\n",
      "batch_range:[400,500),loss:0.31046832\n",
      "batch_range:[500,600),loss:0.41773972\n",
      "batch_range:[600,700),loss:0.25439262\n",
      "batch_range:[700,800),loss:0.32532293\n",
      "batch_range:[800,900),loss:0.4198908\n",
      "batch_range:[900,1000),loss:0.32345298\n",
      "batch_range:[1000,1100),loss:0.3246232\n",
      "batch_range:[1100,1200),loss:0.2636728\n",
      "batch_range:[1200,1300),loss:0.3765198\n",
      "batch_range:[1300,1400),loss:0.35468948\n",
      "batch_range:[1400,1500),loss:0.21444409\n",
      "batch_range:[1500,1600),loss:0.41721618\n",
      "batch_range:[1600,1700),loss:0.40171513\n",
      "batch_range:[1700,1800),loss:0.44671613\n",
      "batch_range:[1800,1900),loss:0.4009192\n",
      "batch_range:[1900,2000),loss:0.28797796\n",
      "batch_range:[2000,2100),loss:0.34931052\n",
      "batch_range:[2100,2200),loss:0.3885919\n",
      "batch_range:[2200,2300),loss:0.31983125\n",
      "batch_range:[2300,2400),loss:0.33550087\n",
      "batch_range:[2400,2500),loss:0.30783194\n",
      "batch_range:[2500,2600),loss:0.2960944\n",
      "batch_range:[2600,2700),loss:0.29397988\n",
      "batch_range:[2700,2800),loss:0.18379298\n",
      "batch_range:[2800,2900),loss:0.21587166\n",
      "batch_range:[2900,3000),loss:0.2183861\n",
      "batch_range:[3000,3100),loss:0.33457854\n",
      "batch_range:[3100,3200),loss:0.21766022\n",
      "batch_range:[3200,3300),loss:0.2195436\n",
      "batch_range:[3300,3400),loss:0.23093931\n",
      "batch_range:[3400,3500),loss:0.1752686\n",
      "epoch:3\n",
      "batch_range:[0,100),loss:0.2581824\n",
      "batch_range:[100,200),loss:0.21634969\n",
      "batch_range:[200,300),loss:0.23544198\n",
      "batch_range:[300,400),loss:0.34105584\n",
      "batch_range:[400,500),loss:0.17150831\n",
      "batch_range:[500,600),loss:0.27419332\n",
      "batch_range:[600,700),loss:0.15626445\n",
      "batch_range:[700,800),loss:0.171829\n",
      "batch_range:[800,900),loss:0.2833344\n",
      "batch_range:[900,1000),loss:0.1722816\n",
      "batch_range:[1000,1100),loss:0.19908355\n",
      "batch_range:[1100,1200),loss:0.15610407\n",
      "batch_range:[1200,1300),loss:0.20942008\n",
      "batch_range:[1300,1400),loss:0.19471584\n",
      "batch_range:[1400,1500),loss:0.11175543\n",
      "batch_range:[1500,1600),loss:0.1899341\n",
      "batch_range:[1600,1700),loss:0.17724234\n",
      "batch_range:[1700,1800),loss:0.28807035\n",
      "batch_range:[1800,1900),loss:0.25973773\n",
      "batch_range:[1900,2000),loss:0.124459416\n",
      "batch_range:[2000,2100),loss:0.2679035\n",
      "batch_range:[2100,2200),loss:0.20687857\n",
      "batch_range:[2200,2300),loss:0.15790088\n",
      "batch_range:[2300,2400),loss:0.20105657\n",
      "batch_range:[2400,2500),loss:0.23290846\n",
      "batch_range:[2500,2600),loss:0.14618132\n",
      "batch_range:[2600,2700),loss:0.18864807\n",
      "batch_range:[2700,2800),loss:0.09031494\n",
      "batch_range:[2800,2900),loss:0.11600113\n",
      "batch_range:[2900,3000),loss:0.13243759\n",
      "batch_range:[3000,3100),loss:0.21435985\n",
      "batch_range:[3100,3200),loss:0.1068398\n",
      "batch_range:[3200,3300),loss:0.24495246\n",
      "batch_range:[3300,3400),loss:0.123230495\n",
      "batch_range:[3400,3500),loss:0.09052865\n",
      "epoch:4\n",
      "batch_range:[0,100),loss:0.082834214\n",
      "batch_range:[100,200),loss:0.091489896\n",
      "batch_range:[200,300),loss:0.20082073\n",
      "batch_range:[300,400),loss:0.35884035\n",
      "batch_range:[400,500),loss:0.20916305\n",
      "batch_range:[500,600),loss:0.20255223\n",
      "batch_range:[600,700),loss:0.14330587\n",
      "batch_range:[700,800),loss:0.1126502\n",
      "batch_range:[800,900),loss:0.15099892\n",
      "batch_range:[900,1000),loss:0.11716853\n",
      "batch_range:[1000,1100),loss:0.15363958\n",
      "batch_range:[1100,1200),loss:0.096831694\n",
      "batch_range:[1200,1300),loss:0.14888453\n",
      "batch_range:[1300,1400),loss:0.19848953\n",
      "batch_range:[1400,1500),loss:0.16086236\n",
      "batch_range:[1500,1600),loss:0.21675764\n",
      "batch_range:[1600,1700),loss:0.10804524\n",
      "batch_range:[1700,1800),loss:0.1786839\n",
      "batch_range:[1800,1900),loss:0.15611753\n",
      "batch_range:[1900,2000),loss:0.060880028\n",
      "batch_range:[2000,2100),loss:0.096344516\n",
      "batch_range:[2100,2200),loss:0.13826981\n",
      "batch_range:[2200,2300),loss:0.08274944\n",
      "batch_range:[2300,2400),loss:0.09943524\n",
      "batch_range:[2400,2500),loss:0.1736731\n",
      "batch_range:[2500,2600),loss:0.12589048\n",
      "batch_range:[2600,2700),loss:0.19226192\n",
      "batch_range:[2700,2800),loss:0.08823454\n",
      "batch_range:[2800,2900),loss:0.15175116\n",
      "batch_range:[2900,3000),loss:0.13423884\n",
      "batch_range:[3000,3100),loss:0.13249943\n",
      "batch_range:[3100,3200),loss:0.109827116\n",
      "batch_range:[3200,3300),loss:0.07437593\n",
      "batch_range:[3300,3400),loss:0.096241385\n",
      "batch_range:[3400,3500),loss:0.07553768\n",
      "epoch:5\n",
      "batch_range:[0,100),loss:0.18973073\n",
      "batch_range:[100,200),loss:0.09360603\n",
      "batch_range:[200,300),loss:0.114769265\n",
      "batch_range:[300,400),loss:0.10751359\n",
      "batch_range:[400,500),loss:0.14053793\n",
      "batch_range:[500,600),loss:0.18461683\n",
      "batch_range:[600,700),loss:0.1866522\n",
      "batch_range:[700,800),loss:0.14638959\n",
      "batch_range:[800,900),loss:0.18647486\n",
      "batch_range:[900,1000),loss:0.13324873\n",
      "batch_range:[1000,1100),loss:0.12260287\n",
      "batch_range:[1100,1200),loss:0.11077421\n",
      "batch_range:[1200,1300),loss:0.09915804\n",
      "batch_range:[1300,1400),loss:0.098450154\n",
      "batch_range:[1400,1500),loss:0.045569867\n",
      "batch_range:[1500,1600),loss:0.1797788\n",
      "batch_range:[1600,1700),loss:0.13449441\n",
      "batch_range:[1700,1800),loss:0.16877957\n",
      "batch_range:[1800,1900),loss:0.16501951\n",
      "batch_range:[1900,2000),loss:0.14581075\n",
      "batch_range:[2000,2100),loss:0.12772588\n",
      "batch_range:[2100,2200),loss:0.1264837\n",
      "batch_range:[2200,2300),loss:0.111017436\n",
      "batch_range:[2300,2400),loss:0.06579509\n",
      "batch_range:[2400,2500),loss:0.19958006\n",
      "batch_range:[2500,2600),loss:0.11295117\n",
      "batch_range:[2600,2700),loss:0.1380266\n",
      "batch_range:[2700,2800),loss:0.14989768\n",
      "batch_range:[2800,2900),loss:0.21390963\n",
      "batch_range:[2900,3000),loss:0.1311479\n",
      "batch_range:[3000,3100),loss:0.07112041\n",
      "batch_range:[3100,3200),loss:0.03646463\n",
      "batch_range:[3200,3300),loss:0.038373396\n",
      "batch_range:[3300,3400),loss:0.048189048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_range:[3400,3500),loss:0.07493071\n",
      "epoch:6\n",
      "batch_range:[0,100),loss:0.118619315\n",
      "batch_range:[100,200),loss:0.25951198\n",
      "batch_range:[200,300),loss:0.1966622\n",
      "batch_range:[300,400),loss:0.39412773\n",
      "batch_range:[400,500),loss:0.10707372\n",
      "batch_range:[500,600),loss:0.101805285\n",
      "batch_range:[600,700),loss:0.047954027\n",
      "batch_range:[700,800),loss:0.071542256\n",
      "batch_range:[800,900),loss:0.08736799\n",
      "batch_range:[900,1000),loss:0.13768645\n",
      "batch_range:[1000,1100),loss:0.13045196\n",
      "batch_range:[1100,1200),loss:0.13703293\n",
      "batch_range:[1200,1300),loss:0.18783821\n",
      "batch_range:[1300,1400),loss:0.13811305\n",
      "batch_range:[1400,1500),loss:0.12632355\n",
      "batch_range:[1500,1600),loss:0.06476125\n",
      "batch_range:[1600,1700),loss:0.13325772\n",
      "batch_range:[1700,1800),loss:0.08644192\n",
      "batch_range:[1800,1900),loss:0.08522044\n",
      "batch_range:[1900,2000),loss:0.03621664\n",
      "batch_range:[2000,2100),loss:0.104220115\n",
      "batch_range:[2100,2200),loss:0.07371977\n",
      "batch_range:[2200,2300),loss:0.07876557\n",
      "batch_range:[2300,2400),loss:0.14184876\n",
      "batch_range:[2400,2500),loss:0.10451463\n",
      "batch_range:[2500,2600),loss:0.103263654\n",
      "batch_range:[2600,2700),loss:0.06039832\n",
      "batch_range:[2700,2800),loss:0.029282477\n",
      "batch_range:[2800,2900),loss:0.040079642\n",
      "batch_range:[2900,3000),loss:0.043937925\n",
      "batch_range:[3000,3100),loss:0.053387213\n",
      "batch_range:[3100,3200),loss:0.10749113\n",
      "batch_range:[3200,3300),loss:0.014059801\n",
      "batch_range:[3300,3400),loss:0.09133634\n",
      "batch_range:[3400,3500),loss:0.087396145\n",
      "epoch:7\n",
      "batch_range:[0,100),loss:0.03998662\n",
      "batch_range:[100,200),loss:0.014773768\n",
      "batch_range:[200,300),loss:0.049474057\n",
      "batch_range:[300,400),loss:0.032098014\n",
      "batch_range:[400,500),loss:0.0398015\n",
      "batch_range:[500,600),loss:0.061406817\n",
      "batch_range:[600,700),loss:0.022308152\n",
      "batch_range:[700,800),loss:0.066654414\n",
      "batch_range:[800,900),loss:0.07013673\n",
      "batch_range:[900,1000),loss:0.068331964\n",
      "batch_range:[1000,1100),loss:0.036117293\n",
      "batch_range:[1100,1200),loss:0.023663044\n",
      "batch_range:[1200,1300),loss:0.01220236\n",
      "batch_range:[1300,1400),loss:0.03027084\n",
      "batch_range:[1400,1500),loss:0.045673426\n",
      "batch_range:[1500,1600),loss:0.026825972\n",
      "batch_range:[1600,1700),loss:0.07756248\n",
      "batch_range:[1700,1800),loss:0.010685168\n",
      "batch_range:[1800,1900),loss:0.032889694\n",
      "batch_range:[1900,2000),loss:0.0072355447\n",
      "batch_range:[2000,2100),loss:0.07356458\n",
      "batch_range:[2100,2200),loss:0.021974595\n",
      "batch_range:[2200,2300),loss:0.019043462\n",
      "batch_range:[2300,2400),loss:0.049951572\n",
      "batch_range:[2400,2500),loss:0.07763179\n",
      "batch_range:[2500,2600),loss:0.033687346\n",
      "batch_range:[2600,2700),loss:0.018331217\n",
      "batch_range:[2700,2800),loss:0.0025856283\n",
      "batch_range:[2800,2900),loss:0.013822721\n",
      "batch_range:[2900,3000),loss:0.020637123\n",
      "batch_range:[3000,3100),loss:0.010395845\n",
      "batch_range:[3100,3200),loss:0.013618805\n",
      "batch_range:[3200,3300),loss:0.036532216\n",
      "batch_range:[3300,3400),loss:0.012762181\n",
      "batch_range:[3400,3500),loss:0.0028630393\n",
      "epoch:8\n",
      "batch_range:[0,100),loss:0.0037116217\n",
      "batch_range:[100,200),loss:0.004382899\n",
      "batch_range:[200,300),loss:0.06872538\n",
      "batch_range:[300,400),loss:0.029036576\n",
      "batch_range:[400,500),loss:0.02340672\n",
      "batch_range:[500,600),loss:0.010149172\n",
      "batch_range:[600,700),loss:0.0051558698\n",
      "batch_range:[700,800),loss:0.03847043\n",
      "batch_range:[800,900),loss:0.06428835\n",
      "batch_range:[900,1000),loss:0.016655365\n",
      "batch_range:[1000,1100),loss:0.013059135\n",
      "batch_range:[1100,1200),loss:0.01949679\n",
      "batch_range:[1200,1300),loss:0.003974748\n",
      "batch_range:[1300,1400),loss:0.018820187\n",
      "batch_range:[1400,1500),loss:0.00355307\n",
      "batch_range:[1500,1600),loss:0.013981492\n",
      "batch_range:[1600,1700),loss:0.08807738\n",
      "batch_range:[1700,1800),loss:0.0089643635\n",
      "batch_range:[1800,1900),loss:0.015099554\n",
      "batch_range:[1900,2000),loss:0.0044698208\n",
      "batch_range:[2000,2100),loss:0.0047608297\n",
      "batch_range:[2100,2200),loss:0.008958645\n",
      "batch_range:[2200,2300),loss:0.0012484918\n",
      "batch_range:[2300,2400),loss:0.030921748\n",
      "batch_range:[2400,2500),loss:0.03734209\n",
      "batch_range:[2500,2600),loss:0.0071592657\n",
      "batch_range:[2600,2700),loss:0.036889378\n",
      "batch_range:[2700,2800),loss:0.0045305854\n",
      "batch_range:[2800,2900),loss:0.0054266513\n",
      "batch_range:[2900,3000),loss:0.007224356\n",
      "batch_range:[3000,3100),loss:0.0029791405\n",
      "batch_range:[3100,3200),loss:0.0023445273\n",
      "batch_range:[3200,3300),loss:0.0026769661\n",
      "batch_range:[3300,3400),loss:0.0038726465\n",
      "batch_range:[3400,3500),loss:0.0018400045\n",
      "epoch:9\n",
      "batch_range:[0,100),loss:0.004690688\n",
      "batch_range:[100,200),loss:0.0012110946\n",
      "batch_range:[200,300),loss:0.043386985\n",
      "batch_range:[300,400),loss:0.0050856913\n",
      "batch_range:[400,500),loss:0.010319438\n",
      "batch_range:[500,600),loss:0.017879294\n",
      "batch_range:[600,700),loss:0.004969809\n",
      "batch_range:[700,800),loss:0.02213736\n",
      "batch_range:[800,900),loss:0.0027916594\n",
      "batch_range:[900,1000),loss:0.0013286914\n",
      "batch_range:[1000,1100),loss:0.0025152585\n",
      "batch_range:[1100,1200),loss:0.0022202919\n",
      "batch_range:[1200,1300),loss:0.0018584952\n",
      "batch_range:[1300,1400),loss:0.003167274\n",
      "batch_range:[1400,1500),loss:0.001731565\n",
      "batch_range:[1500,1600),loss:0.0045786053\n",
      "batch_range:[1600,1700),loss:0.071572095\n",
      "batch_range:[1700,1800),loss:0.004699833\n",
      "batch_range:[1800,1900),loss:0.01714961\n",
      "batch_range:[1900,2000),loss:0.0016148336\n",
      "batch_range:[2000,2100),loss:0.005551461\n",
      "batch_range:[2100,2200),loss:0.022892766\n",
      "batch_range:[2200,2300),loss:0.061817855\n",
      "batch_range:[2300,2400),loss:0.027815228\n",
      "batch_range:[2400,2500),loss:0.0046071196\n",
      "batch_range:[2500,2600),loss:0.0012264765\n",
      "batch_range:[2600,2700),loss:0.0018342909\n",
      "batch_range:[2700,2800),loss:0.00044970796\n",
      "batch_range:[2800,2900),loss:0.008123679\n",
      "batch_range:[2900,3000),loss:0.0047046687\n",
      "batch_range:[3000,3100),loss:0.0019613907\n",
      "batch_range:[3100,3200),loss:0.003675481\n",
      "batch_range:[3200,3300),loss:0.0630378\n",
      "batch_range:[3300,3400),loss:0.0046215975\n",
      "batch_range:[3400,3500),loss:0.0010859222\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "embedding_dim=300\n",
    "lstm_hidden_size=256\n",
    "lstm_num_layers=3\n",
    "dropout=0.2\n",
    "\n",
    "\"\"\"\n",
    "As summarized before, the typical steps are:\n",
    " 1.   Use optimization methods under torch.optim to generate an optimizer for the constructed network\n",
    " 2.   Call zero_grad of the network or of the optimizer to zero the gradient buffers\n",
    " 3.   Give the input(a mini-batch of samples) to the network\n",
    " 4.   Use the output to compute the loss with the target using a specific loss function\n",
    " 5.   Conduct backprop through calling the backward method on the loss\n",
    " 6.   Call step of the optimizer to update the parameters\n",
    " 7.   Back to step 2 to start a new mini-batch\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "model = LSTMClassifier(vocab_size, label_size, embedding_dim,\n",
    "                       lstm_hidden_size, lstm_num_layers, dropout)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "torch.manual_seed(2018)\n",
    "for epoch in range(10):\n",
    "    print(\"epoch:\"+str(epoch))\n",
    "    for batch_index in range(0, len(train_data), batch_size):\n",
    "        batch_data = train_data[batch_index: batch_index+batch_size]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        train_x, seq_lens, train_y = transform(batch_data,word_to_ix)\n",
    "        \n",
    "        y_pred = model(train_x, seq_lens)\n",
    "        \n",
    "        train_y = torch.autograd.Variable(torch.from_numpy(train_y))\n",
    "        \n",
    "        \n",
    "        loss = loss_fn(y_pred, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"batch_range:[\"+str(batch_index)+\",\"+str(batch_index+batch_size)+\"),loss:\"+str(loss.data.numpy()[0]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.748667\n"
     ]
    }
   ],
   "source": [
    "val_x, val_seq_lens, val_y = transform(val_data,word_to_ix)\n",
    "y_pred = model(val_x,val_seq_lens)\n",
    "_, label_pred = torch.max(y_pred,1)\n",
    "print(\"accuracy:%f\" % (sum(label_pred.data.numpy() == val_y)/float(len(val_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further resources\n",
    "- [Official examples](https://github.com/pytorch/examples)\n",
    "- [30 lines code for most models](https://github.com/yunjey/pytorch-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
