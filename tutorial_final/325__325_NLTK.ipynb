{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This tutorial will introduce how to use NLTK package to analyze grammatical features of text data, including __POS tagging__ and __Parsing__. \n",
    "\n",
    "__POS tagging__ is marking the __part-of-speech__ of words in the text. A part-of-speech is a category of words that play similar roles in the grammatical structure of a sentence. In English, the basic parts of speech include _noun_, _adjective_, _adverb_, _verb_, _preposition_, etc. In NLTK, POS tagging has several tag sets so that it can give us more grammatical information about words. This tutorial will not just stop at POS tagging, but will introduce some fun tasks based on that, along with which some frequently-used NLTK resources and functions will also be introduced.\n",
    "\n",
    "__Parsing__ is the syntax analysis of a sentence. Basically, it breaks down the sentence and marks each component its grammatical role. Different from POS tagging, parsing can show the grammar role of several words as a phrase and also their relationship to each other. In this way, parsing can show the grammatical hierarchy structure of a sentence, which is a tree. Based on parsing, like POS tagging, we can also do a lot of tasks.\n",
    "\n",
    "The theory of POS tagging and parsing used in NLTK is beyond this tutorial. You just need to know that they are not based on solid theories but just probability analysis on large corpus, so sometimes they may make mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using NLTK package, it is common to meet LookupError that shows some resources are not found. As NLTK contains a lot of resources, it doesn't download everything in installation, so sometimes you need to download them manually. To do that, open a terminal and open a python shell:\n",
    "\n",
    "    $ python3\n",
    "\n",
    "In the python shell, import nltk and download the specific resource you need by its name:\n",
    "\n",
    "    >>> import nltk\n",
    "    >>> nltk.download(\"resource_name\")\n",
    "\n",
    "We will cover following contents in this tutorial.\n",
    "- [1 POS tagging](#1-POS-tagging)\n",
    "    - [1.1 Introduction](#1.1-Introduction)\n",
    "    - [1.2 Application: Topic Analysis by Extracting Nouns](#1.2-Application:-Topic-Analysis-by-Extracting-Nouns)\n",
    "- [2 Parsing](#2-Parsing)\n",
    "    - [2.1 Introduction](#2.1-Introduction)\n",
    "    - [2.2 Stanford Parser](#2.2-Stanford-Parser)\n",
    "    - [2.3 Application: Question Generation](#2.3-Application:-Question-Generation)\n",
    "- [Reference](#Reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 POS tagging\n",
    "### 1.1 Introduction\n",
    "First, let's see a basic example of POS tagging in NLTK. We need to use nltk.__word_tokenize()__ to split text into tokens first, and then use nltk.__pos_tag()__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Practical', 'NNP'), ('Data', 'NNP'), ('Science', 'NNP'), ('is', 'VBZ'), ('awesome', 'JJ'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Practical Data Science is awesome!\"\n",
    "tokens = word_tokenize(test_text)\n",
    "tags = pos_tag(tokens)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above you can see that nltk.pos_tag() receives tokens as argument and returns list of tuples including each word's POS tag. The default tag set of NLTK is __Penn Treebank Tag Set__, which is widely used in NLP community.\n",
    "\n",
    "Part of POS tags in Penn Treebank Tag Set is listed below.\n",
    "\n",
    "| Tag | Description            | Tag  | Description                    |\n",
    "| :-- | :--------------------- | :--- | :----------------------------- |\n",
    "| VB  | verb, base form        | VBD  | verb, past sense               |\n",
    "| VBG | verb, present particle | VBZ  | verb, 3rd person sing. present |\n",
    "| NN  | noun, singular         | NNS  | noun, plural                   |\n",
    "| NNP | proper noun, singular  | NNPS | proper noun, plural            |\n",
    "| JJ  | adjective              | PRP  | personal pronoun               |\n",
    "| RB  | adverb                 | DT   | determiner                     |\n",
    "\n",
    "With POS tagging, we can do some tasks based on that. \n",
    "\n",
    "### 1.2 Application: Topic Analysis by Extracting Nouns\n",
    "We can extract all the nouns from the text, which is useful because nouns usually show the topic. We can see that tags of nouns, i.e. _NN_, _NNS_, _NNP_, _NNPS_, all start with \"NN\", and that is designed deliberately for us to use. The example text below is from https://en.wikipedia.org/wiki/Data_science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Data', 'NNP'), ('science', 'NN'), ('science', 'NN'), ('field', 'NN'), ('methods', 'NNS'), ('processes', 'NNS'), ('algorithms', 'NN'), ('systems', 'NNS'), ('knowledge', 'NN'), ('insights', 'NNS'), ('data', 'NNS'), ('forms', 'NNS'), ('mining', 'NN'), ('Data', 'NNP'), ('science', 'NN'), ('concept', 'NN'), ('statistics', 'NNS'), ('data', 'NNS'), ('analysis', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('methods', 'NNS'), ('order', 'NN'), ('phenomena', 'NNS'), ('data', 'NNS'), ('techniques', 'NNS'), ('theories', 'NNS'), ('fields', 'NNS'), ('areas', 'NNS'), ('mathematics', 'NNS'), ('statistics', 'NNS'), ('information', 'NN'), ('science', 'NN'), ('computer', 'NN'), ('science', 'NN'), ('subdomains', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('classification', 'NN'), ('cluster', 'NN'), ('analysis', 'NN'), ('uncertainty', 'NN'), ('quantification', 'NN'), ('science', 'NN'), ('data', 'NN'), ('mining', 'NN'), ('databases', 'NNS'), ('visualization', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = '''Data science, also known as data-driven science, is an interdisciplinary field of scientific methods, \n",
    "          processes, algorithms and systems to extract knowledge or insights from data in various forms, \n",
    "          either structured or unstructured, similar to data mining. Data science is a \"concept to unify \n",
    "          statistics, data analysis, machine learning and their related methods\" in order to \"understand and \n",
    "          analyze actual phenomena\" with data. It employs techniques and theories drawn from many fields \n",
    "          within the broad areas of mathematics, statistics, information science, and computer science, \n",
    "          in particular from the subdomains of machine learning, classification, cluster analysis, \n",
    "          uncertainty quantification, computational science, data mining, databases, and visualization.'''\n",
    "tokens = word_tokenize(text)\n",
    "tags = pos_tag(tokens)\n",
    "nouns = [tag for tag in tags if tag[1].startswith(\"NN\")]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use other tag sets, just declare it in the pos_tag(). For example, if we use __\"universal\"__ tagset, in which all nouns are just tagged with 'NOUN', we can extract all nouns more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Data', 'NOUN'), ('science', 'NOUN'), ('science', 'NOUN'), ('field', 'NOUN'), ('methods', 'NOUN'), ('processes', 'NOUN'), ('algorithms', 'NOUN'), ('systems', 'NOUN'), ('knowledge', 'NOUN'), ('insights', 'NOUN'), ('data', 'NOUN'), ('forms', 'NOUN'), ('mining', 'NOUN'), ('Data', 'NOUN'), ('science', 'NOUN'), ('concept', 'NOUN'), ('statistics', 'NOUN'), ('data', 'NOUN'), ('analysis', 'NOUN'), ('machine', 'NOUN'), ('learning', 'NOUN'), ('methods', 'NOUN'), ('order', 'NOUN'), ('phenomena', 'NOUN'), ('data', 'NOUN'), ('techniques', 'NOUN'), ('theories', 'NOUN'), ('fields', 'NOUN'), ('areas', 'NOUN'), ('mathematics', 'NOUN'), ('statistics', 'NOUN'), ('information', 'NOUN'), ('science', 'NOUN'), ('computer', 'NOUN'), ('science', 'NOUN'), ('subdomains', 'NOUN'), ('machine', 'NOUN'), ('learning', 'NOUN'), ('classification', 'NOUN'), ('cluster', 'NOUN'), ('analysis', 'NOUN'), ('uncertainty', 'NOUN'), ('quantification', 'NOUN'), ('science', 'NOUN'), ('data', 'NOUN'), ('mining', 'NOUN'), ('databases', 'NOUN'), ('visualization', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "tags_universal = pos_tag(tokens, tagset='universal')\n",
    "nouns_universal = [tag for tag in tags_universal if tag[1]==\"NOUN\"]\n",
    "print(nouns_universal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just list all the tags tuple may not be that useful. We can analyze its frequency using nltk.__FreqDist()__. It will return a FreqDist object and using its __most_common(N)__ method will return frequencies of the most frequent _N_ (tag, token) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 34 samples and 48 outcomes>\n",
      "[(('science', 'NN'), 6), (('data', 'NNS'), 3), (('learning', 'NN'), 2), (('machine', 'NN'), 2), (('mining', 'NN'), 2)]\n",
      "['science', 'data', 'learning', 'machine', 'mining']\n"
     ]
    }
   ],
   "source": [
    "nouns_fd = nltk.FreqDist(nouns)\n",
    "print(nouns_fd)\n",
    "nouns_freq = [noun for noun in nouns_fd.most_common(5)]\n",
    "freq_nouns = [noun[0] for (noun, _) in nouns_fd.most_common(5)]\n",
    "print(nouns_freq)\n",
    "print(freq_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'science', 'data', 'mining', 'analysis', 'Data' are quite good words for describing the topic of our example text. \n",
    "\n",
    "## 2 Parsing\n",
    "### 2.1 Introduction\n",
    "Parsing is more complicated than POS tagging. To begin with, we start with a simple example to get some intuition. First, we need to define a set of context-free grammars (CFG) as our rules for parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.grammar.CFG'>\n",
      "Grammar with 9 productions (start state = S)\n",
      "    S -> NP VP\n",
      "    NP -> N\n",
      "    NP -> Det JJ N\n",
      "    VP -> V NP\n",
      "    Det -> 'a'\n",
      "    JJ -> 'good'\n",
      "    N -> 'CMU'\n",
      "    N -> 'reputation'\n",
      "    V -> 'has'\n"
     ]
    }
   ],
   "source": [
    "from nltk import CFG\n",
    "sentence = \"CMU has a good reputation\"\n",
    "grammar = CFG.fromstring('''\n",
    "    S -> NP VP\n",
    "    NP -> N\n",
    "    NP -> Det JJ N\n",
    "    VP -> V NP\n",
    "    Det -> \"a\"\n",
    "    JJ -> \"good\"\n",
    "    N -> \"CMU\" | \"reputation\"\n",
    "    V -> \"has\"\n",
    "    ''')\n",
    "print(type(grammar))\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here \"NP\" refers to Noun Phrase, \"VP\" refers to Verb Phrase. A lot of sentences are in the structure of NP VP, including our example sentence. \n",
    "\n",
    "As you can see, defining grammar by ourselves requires a lot of work if you want your parser to be general, but it is still useful for some professionals. We can build a parser based on our grammars. A basic kind of parser is __Recursive Descent Parser__, which builds our parsing tree from top to down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.parse.recursivedescent.RecursiveDescentParser'>\n",
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "from nltk import RecursiveDescentParser\n",
    "parser = RecursiveDescentParser(grammar)\n",
    "print(type(parser))\n",
    "tokens = word_tokenize(sentence)\n",
    "parse_trees = parser.parse(tokens)\n",
    "print(type(parse_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the parsing result is a generator. We need a for loop to get the actual parsing tree. IPython can visualize the parsing tree quite clear as an image, and the print function can show a string version of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tree.Tree'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('NP', [Tree('N', ['CMU'])]), Tree('VP', [Tree('V', ['has']), Tree('NP', [Tree('Det', ['a']), Tree('JJ', ['good']), Tree('N', ['reputation'])])])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (N CMU)) (VP (V has) (NP (Det a) (JJ good) (N reputation))))\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "for tree in parse_trees:\n",
    "    print(type(tree))\n",
    "    IPython.core.display.display(tree)\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Stanford Parser\n",
    "To save effort, we can use some predefined parser model developed by NLP community. One common choice is __Stanford Parser__. \n",
    "\n",
    "NLTK has an interface for Stanford Parser but it doesn't contain the actual model. We need to download the model from https://nlp.stanford.edu/software/lex-parser.shtml (the latest version right now is 3.9.1 released at 2018-02-07) and extract it. Use the following code to install Stanford Parser (englishPCFG) to NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.parse.stanford.StanfordParser'>\n",
      "<class 'nltk.tree.Tree'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAADLCAIAAAAUdDp8AAAJNmlDQ1BkZWZhdWx0X3JnYi5pY2MAAHiclZFnUJSHFobP933bCwvssnRYepMqZQHpvUmvogJL7yxLEbEhYgQiiog0RZCggAGjUiRWRLEQFBSxoFkkCCgxGEVUUPLDOxPn3vHHfX49884755yZA0ARBQBARQFSUgV8Pxd7TkhoGAe+IZKXmW7n4+MJ3+X9KCAAAPdWfb/zXSjRMZk8AFgGgHxeOl8AgOQCgGaOIF0AgBwFAFZUUroAADkLACx+SGgYAHIDAFhxX30cAFhRX30eAFj8AD8HABQHQKLFfeNR3/h/9gIAKNvxBQmxMbkc/7RYQU4kP4aT6ediz3FzcOD48NNiE5Jjvjn4/yp/B0FMrgAAwCEtfRM/IS5ewPmfoUYGhobw7y/e+gICAAh78L//AwDf9NIaAbgLANi+f7OoaoDuXQBSj//NVI8CMAoBuu7wsvjZXzMcAAAeKMAAFkiDAqiAJuiCEZiBJdiCE7iDNwRAKGwAHsRDCvAhB/JhBxRBCeyDg1AD9dAELdAOp6EbzsMVuA634S6MwhMQwhS8gnl4D0sIghAROsJEpBFFRA3RQYwQLmKNOCGeiB8SikQgcUgqkoXkIzuREqQcqUEakBbkF+QccgW5iQwjj5AJZBb5G/mEYigNZaHyqDqqj3JRO9QDDUDXo3FoBpqHFqJ70Sq0ET2JdqFX0NvoKCpEX6ELGGBUjI0pYboYF3PAvLEwLBbjY1uxYqwSa8TasV5sALuHCbE57COOgGPiODhdnCXOFReI4+EycFtxpbga3AlcF64fdw83gZvHfcHT8XJ4HbwF3g0fgo/D5+CL8JX4Znwn/hp+FD+Ff08gENgEDYIZwZUQSkgkbCaUEg4TOgiXCcOEScICkUiUJuoQrYjexEiigFhErCaeJF4ijhCniB9IVJIiyYjkTAojpZIKSJWkVtJF0ghpmrREFiWrkS3I3uRo8iZyGbmJ3Eu+Q54iL1HEKBoUK0oAJZGyg1JFaadco4xT3lKpVGWqOdWXmkDdTq2inqLeoE5QP9LEado0B1o4LYu2l3acdpn2iPaWTqer023pYXQBfS+9hX6V/oz+QYQpoifiJhItsk2kVqRLZETkNYPMUGPYMTYw8hiVjDOMO4w5UbKouqiDaKToVtFa0XOiY6ILYkwxQzFvsRSxUrFWsZtiM+JEcXVxJ/Fo8ULxY+JXxSeZGFOF6cDkMXcym5jXmFMsAkuD5cZKZJWwfmYNseYlxCWMJYIkciVqJS5ICNkYW53txk5ml7FPsx+wP0nKS9pJxkjukWyXHJFclJKVspWKkSqW6pAalfokzZF2kk6S3i/dLf1UBiejLeMrkyNzROaazJwsS9ZSlidbLHta9rEcKqct5ye3We6Y3KDcgryCvIt8uny1/FX5OQW2gq1CokKFwkWFWUWmorVigmKF4iXFlxwJjh0nmVPF6efMK8kpuSplKTUoDSktKWsoByoXKHcoP1WhqHBVYlUqVPpU5lUVVb1U81XbVB+rkdW4avFqh9QG1BbVNdSD1Xerd6vPaEhpuGnkabRpjGvSNW00MzQbNe9rEbS4Wklah7XuaqPaJtrx2rXad3RQHVOdBJ3DOsOr8KvMV6Wualw1pkvTtdPN1m3TndBj63nqFeh1673WV9UP09+vP6D/xcDEINmgyeCJobihu2GBYa/h30baRjyjWqP7q+mrnVdvW92z+o2xjnGM8RHjhyZMEy+T3SZ9Jp9NzUz5pu2ms2aqZhFmdWZjXBbXh1vKvWGON7c332Z+3vyjhamFwOK0xV+WupZJlq2WM2s01sSsaVozaaVsFWnVYCW05lhHWB+1Ftoo2UTaNNo8t1WxjbZttp2207JLtDtp99rewJ5v32m/6GDhsMXhsiPm6OJY7DjkJO4U6FTj9MxZ2TnOuc153sXEZbPLZVe8q4frftcxN3k3nluL27y7mfsW934Pmoe/R43Hc09tT75nrxfq5e51wGt8rdra1LXd3uDt5n3A+6mPhk+Gz6++BF8f31rfF36Gfvl+A/5M/43+rf7vA+wDygKeBGoGZgX2BTGCwoNaghaDHYPLg4Uh+iFbQm6HyoQmhPaEEcOCwprDFtY5rTu4bircJLwo/MF6jfW5629ukNmQvOHCRsbGyI1nIvARwRGtEcuR3pGNkQtRblF1UfM8B94h3qto2+iK6NkYq5jymOlYq9jy2Jk4q7gDcbPxNvGV8XMJDgk1CW8SXRPrExeTvJOOJ60kByd3pJBSIlLOpYqnJqX2pymk5aYNp+ukF6ULMywyDmbM8z34zZlI5vrMHgFLkC4YzNLM2pU1kW2dXZv9ISco50yuWG5q7uAm7U17Nk3nOef9tBm3mbe5L18pf0f+xBa7LQ1bka1RW/u2qWwr3Da13WX7iR2UHUk7fiswKCgveLczeGdvoXzh9sLJXS672opEivhFY7std9f/gPsh4YehPav3VO/5UhxdfKvEoKSyZLmUV3rrR8Mfq35c2Ru7d6jMtOzIPsK+1H0P9tvsP1EuVp5XPnnA60BXBaeiuOLdwY0Hb1YaV9YfohzKOiSs8qzqqVat3le9XBNfM1prX9tRJ1e3p27xcPThkSO2R9rr5etL6j8dTTj6sMGloatRvbHyGOFY9rEXTUFNAz9xf2pplmkuaf58PPW48ITfif4Ws5aWVrnWsja0Latt9mT4ybs/O/7c067b3tDB7ig5BaeyTr38JeKXB6c9Tved4Z5pP6t2tq6T2VnchXRt6prvju8W9oT2DJ9zP9fXa9nb+aver8fPK52vvSBxoewi5WLhxZVLeZcWLqdfnrsSd2Wyb2Pfk6shV+/3+/YPXfO4duO68/WrA3YDl25Y3Th/0+LmuVvcW923TW93DZoMdv5m8lvnkOlQ1x2zOz13ze/2Dq8ZvjhiM3LlnuO96/fd7t8eXTs6/CDwwcOx8DHhw+iHM4+SH715nP146cn2cfx48VPRp5XP5J41/q71e4fQVHhhwnFi8Ln/8yeTvMlXf2T+sTxV+IL+onJacbplxmjm/Kzz7N2X615OvUp/tTRX9KfYn3WvNV+f/cv2r8H5kPmpN/w3K3+XvpV+e/yd8bu+BZ+FZ+9T3i8tFn+Q/nDiI/fjwKfgT9NLOcvE5arPWp97v3h8GV9JWVn5By6ikLxSF1/9AAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAHXRFWHRTb2Z0d2FyZQBHUEwgR2hvc3RzY3JpcHQgOS4xOJQFEHMAAA8fSURBVHic7d2xb+NGFgbwyeGqVUUgayCVBV4T2EUKrqoUdkE3u+0ybTYNDWyRahOyS0oS2X+AbJLa3OKaVaMBzq61LOVUHtjAFXcWYiIBrPJ8xbudYySLI1GkSI6+H4JAorXWiNKnNxTleZ88PDwwAFjuL00PAKDtEBIABYQEQAEhAVBASAAUEBIABYQEQOGvTQ+giO/7aZpalsUYsyzLcRzGWJZlYRjSDeTGJEnSNKWNnucZhuH7Pl2VvyEIgu0/BNDBQ7t5nrd44e7uji4HQXB1dXV1dRUEAW25u7uTt5z7hwDltLqSMMbSNPV9P8uy/EbDMOiC67pUVTzPm/sRQFXaHhLLsoIg4Jybptn0WGBHdePA3bbtJEnkVVlY4jh2Xdd13TiO534EUJVPHlr8BUc6cPc8z7btLMt834+iaMUDd8YY55xznqap4ziu6zb1KKDrWh0SgDboxnQLoEEICYACQgKggJAAKOgQkpO3b9Obm6ZHAdrSISR8MhG3t02PArSlQ0gYY6gkUB9NQgJQH4QEQAEhAVDQISRWv9/0EEBnOoTE6PWaHgLoTIeQANRKk5DgPAnUR5OQZLNZ00MAbWkSEoD6ICQACjqExHjypOkhgM50CIm5t9f0EEBnOoQEoFaahCS7v296CKAtTUKSXl83PQTQliYhAagP1t0CUEAlAVBASAAUEBIABYQEQAEhAVBoexOfYrINQ5ZlWZadnZ01PSLQULdDEoZhvhtJ08MBPXX7PIkQIkkSy7JM00S/OKhJt0MipWnKOZftRQEq1O0D93xfOHRLhJp0+5hECOH7Pl3GdAtqosl0C6A+3Z5uAWwBQgKggJAAKOgQkuvffmt6CKCzDn+6lc1m8fl5fHHxn4eH32cz9/jYPToynz5telygm05+usUvL5PxOD4/Z4w5g8Hnn3327z/+oKv24aEzGLhHR82OEHTSpZDI0iFub829PQqDLB35nxq9HgoLVKUbIUnGY355KUsH/bfsxvk6g8ICm2t1SMR0Gl9cJOPxo6WjGAoLVKWlIUnGY/qPMeYeHzuDgX1wUO5XobDAhtoVkrnS4R4ducfHlayHjcICpbUlJPHFBZ9MKikdxVBYYF0Nh4RKR3x+nt3fW/2+MxhUVTqKobDA6hoLCU2r+GRi9HqUDWt/f/vDQGEBpW2HZK500Myq8S48KCxQYEshyWYzesNOr6+bLR3FUFhgUe0hSW9u4vPzZDzO7u/pZdeG0lEMhQXy6grJXOno6OsMhQVYHSFZLB1df2GhsOy4ykKyC68kFJbdVEFI5r64XvztQw3swtsB5JUPSfEX13cBCsuOKBMSMZ2Gw+HulI5ic4XFe/7ce/686UFBlUpWkpO3b61+f9dKRzEqLEavF7x82fRYoEpt+YIjQGvpsFoKQK0QEgAFhARAASEBUFhpcTrf94UQZ2dnQog4jtM0HY1Gvu+naWpZFmPMNE3XdWseauukaer7vm3b1DyI9lIQBKenp7Rb2K7uGd08rMbzvNFoJC8vu7Broii6urqSl+/u7h7+vDd2ds/oZI3plmEYaZoubhdCVJfZjnFdN45juiyEoBan0i7vGZ2ssRawZVlxHMuJBPs43zAMY5ebFRqGkWVZmqaO48iN1H/LNM0gCJobGlRjvQWzHceRb5yMMcuy8CJwHCdJEjoakRuxW3Sy0nSLDtY554ZhyBkFHbjT0WqdI2w70zSFELJjoxCCdots5ghdh6+lACjgPAmAAkICoICQACggJAAKZXomZrNZen1t9fstXz5ry2hxyvT6mjGGP+XVSZlPt/jl5clPP42++66mhd87J99N5cUXX/zr9987vdoYzOlw993GzXVTCRxHLolPi4/F5+fh+/dYI6LrUEnKWLERlx7LWAIqyRoKSsejjCdP3KMj9+gIhaXTSp5x/+Sbb3aqklTSwxGFpaMQkiI19XDUb7lkvZUPydnr1xqvSVdh+99lUFi6onxIvBcv9FuFrb72vwVQWFoOIfmfLZSOYigsrbXrIWmkdBRDYWmb3Q1J46WjGApLe5QMyd88zxkMuhiSFpaOYigsjdtoVfluhaTlpaMYCkuD9A9J50pHMRSW7SsfEuPJk7PXrysfUIU6XTqKobBsU/mQMMZGb95UPZ4KaFY6iqGwbIFWIdG4dBRDYamVDiHJZrNwONyR0lEMhaUOJUMSX1wwxlryBIjp9G/ff79TpaOYLCyMsQ8//ND0cDpPk8XpstlsN0tHMeyWSmgSEoD6YEkhAAWEBEABIQFQWLoQxGKfxCiKTk9PHcdxXZdzHoah4zi2bW+hRWCWZaenp6Zpep5nGEYcx0mSRFFEA5vr25jvebBTDYbiOI7jeDQaGYZBjTGiKOKcJ0lCuyjLsiAI5tpxgVpBq7jFPom0hboEPtowsb4WgVdXV1EUyctnZ2dz9ygvyJsFQUBNDHeH53ny4S/umbu7uyAImhlZlymmW4t9Em3bTpLk0RvX2s2HeuXQ5TiOZe+1xY45VE+SJLFtewffNS3L4pw/+qO5bn6wIkVILMtabCZKDdDyW+hlyjmvtQ2abduc8yzLZFsp9rElXRAEpmnKodIFy7J2sAvX4lMm30doetzUwLpLvTjdXJ9ExphpmnMh2U6LQNu2aar96GGP4zhhGFqWlWUZ55wOReI43sH2ha7rhmEor6K15YaWhkT2ScxPWug9yXVdz/OePXvGci0C2VaiQq1uF/s20lUKBr0+aOOjPbV1xTlP0zSOY9d1ZV2l59H3fdu2UUbKwRl3AAWcJwFQQEgAFBASAAWEBEAB/Un0RH939Y9ff93/9FNnMLD295seUYeV73T18PPPdQyohFb9LXHjkvGYX17SX/DKjebenn1wYB8caNwIoD6oJJqgYFA2zL29r7/88t2HD59/9tnozRuKDb+8pD/odQYD+/DQPjjAShErQki6Lb25oWyI21taJ4UmV/67d/+8u/v7t98yxpzBgAoI3ZhPJqe//MIYs/p9WiwCk7FiCEkn5dcWM3o9ZzCwX76UU6lsNovPz93j47lXv7W/b+3vs5cvxXTKLy/5ZBK+fx++f4/JWDEdQmL1++n1ddOj2AYxnVLdoMfrHh/nsyGFw2F2f+89f77s95hPn1LHU/bxGAaTsQI6hER7+bXnGGPOYEDTqkdXQhHTKZWRFV/lmIwpISTtRdngkwmtSWn1+9GrV8uyIYXDIWMs+Pj3NqvDZGwZTULCJ5Omh1Cl/HqtVr8fOI4zGKxSGaiMeC9ebLLcFiZjczQJiR7ypzjMvT3vxYt1l/QNh0Oj1ys4GlkXJmMMIWkDev3F5+eUDfkx7rq/h97vNywjy+zyZAwhaczcKQ56z95kLeNwODT39ururLSDkzEdQtKt5W7pY9z44uLRUxyl0bt79OpVFWNc1Y5MxnQIidXvNz0EtblTHM5g4D1/XuGy/FRGmlrnX+/JmA4habO5j3GLT3GUFl9c8MmkDd35tJyM6RMSMZ22Z+8vnuJY/WPcEsLhkKY3dfzy0rSZjCEkFct/jFt3Nggd3kRff13fXWyo65MxfULSrLlvqpf+GHdd1AqPpjF139fmOjoZQ0g2suyb6lsbQDgcitvbNhyNrKtDk7EyITF6PfvwsPKhlEbjMXq9Ld9venPz7McfK/wYtwRxe7v4lfhuWTYZi169aklTTixOt5FkPG75fLqjkvHYPjxsyRkwhARAAUsKASggJAAKCAmAQjdCIoQ4OTlpehSwo4pCEscxdUg6OTkJw5BeqbLVWBzHJycntJGagdCWZb3INmGaZhv6mNEjlfIb5f+3MxL5vMj+SkIIOYw6noK6+b7/1VdfsdwDoZcWPUDOef7BbtuyZoqj0Ug2oZQNKWWrUWrzKbuNyn9VX2NRumvP81zXpc6mNEjvo/wtieu6sjFqVWOQl0ejkWxu+lDnA19xPA0OoyorNrLdvqWVRHZUY39u9EyNC6md0jZCnGOaJrVHlO8otm3Tlrlb0kbXdetrLGrb9k610dqOtRrZbk2ZYxIhRCNdbamfaP6u5YQw30CUOgZSr/d8C1I9PDrlaGweUrUVG9lu2dKQzDWnzA+deibKq4ZhNNXkVgghW+/KLYwxmmvNPYRqhWG4/VrKcn3A829VOjUZfrSRbbMPcOl3t0zTNE1THp46jiN7iNL0hpp6CiE8z5M3q6l1Zf6uqX0mdTylYWRZlmUZ/VQIEccxZSbLsmp7neabmNL+ocuyvyl19KzwHhcZhjHXSDW/sbuVc8VGto3A11IAFLpxngSgQQgJgAJCAqCAkAAoICTlpTc31K6xQfHFhf/uXbNjqMnJ27fpzU3To2AMIdlEdn/f+Gr2YjrVtYERn0zyvVEbhJBsqiXvdlpqSf4Rkk215N1OS9ls1vQQGENIAJQQEgAFhARAASEprxMri3ZXexZAREgAFBCSTeHTLe0hJJvCeZL64DwJQDcgJAAKCAmAAkKyEXNvr+khaKs9La8Qko2054nUz/a7Mi2DkAAoICSbEre3TQ9BW/gIWBMt+Tq3llpyohbrbgEooJIAKCAkAAoICYACQgKggJDoL9+8TmP19eJDSPRXbf+J+qyY5GU3G41GlQ7n/5b2J4EC1AWFMZZlmed5NXUFSdN0rsNTEAT5jY7jUL/VRzeGYZhlGaunaQznPAzDIAiSJJE7IY5j2W3H87x8K5X82GgjtfVL09RxHPq3sv2LTDXnXDZJpY35m9m2LR8atYjJ52RxnwghTk9PXddN03S9J66pZo3aqLWXqrxs2/biRnl5ceNoNJJNOs/OzuoYpOd5URTJq1dXV/JeZCfa/MgfHXB+nAWDXKV57dz2ZTvqw4cPNMLV9wkqSRnyLXNrvSPX7dDNOZfvx47j1NQDNd8QTwhB/cboKhUxtv7I8+R+rrAdHI1nrScOISmDejUyxrIsq68tY966r3Jqkkyzke20HaX5zGIfyeKRp2lakCK5n5v94AFfSykj/5ylaep5Xh3z/rljEnq5lDgmMU0zSZIoiio8dqLmxnRHslNkfhimaVJg8vtKjm2u4ERRJDcahmGapuM4cgv1xKQG5fl7MQyDupDToYvMG41n2TGJ4ziu69L4VzzWR0i6QfZz7ZzujlzCdKvV5Mc7m8zsG0QfOsVx3Eg776qgkgAo4GQigAJCAqCAkAAoICQACggJgMJ/AaIm4kuNUb/RAAAAAElFTkSuQmCC",
      "text/plain": [
       "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NNP', ['CMU'])]), Tree('VP', [Tree('VBZ', ['has']), Tree('NP', [Tree('DT', ['a']), Tree('JJ', ['good']), Tree('NN', ['reputation'])])])])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (NNP CMU))\n",
      "    (VP (VBZ has) (NP (DT a) (JJ good) (NN reputation)))))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.parse import stanford\n",
    "os.environ[\"CLASSPATH\"] = \"stanford-parser-full-2018-02-27/\" # path to the extracted folder\n",
    "parser = stanford.StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "print(type(parser))\n",
    "parse_trees = parser.parse(tokens)\n",
    "for tree in parse_trees:\n",
    "    print(type(tree))\n",
    "    IPython.core.display.display(tree)\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Application: Question Generation\n",
    "With a general parser, we can do much more things based on it, for example, a simple question generation system!\n",
    "\n",
    "The text is from https://simple.wikipedia.org/wiki/Google. First, we need to split the long text into sentences, NLTK has a nltk.tokenize.__sent_tokenize()__ function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Google's search engine can find pictures, videos, news, Usenet newsgroups, and things to buy online.\n",
      "1 By June 2004, Google had 4.28 billion web pages on its database, 880 million picturesand 845 million Usenet messages — six billion things.\n",
      "2 Google's American website has an Alexa rank of 1, meaning it is the most widely visited website in the world.\n",
      "3 It is so widelyknown that people are using the word \"google\" as a verb that means \"to search for something on Google\"; but because more than half of people on the web use it, \"google\" has been used to mean \"to search the web\".\n",
      "4 Parent company of google is ALPHABET.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = (\"\"\"Google's search engine can find pictures, videos, news, Usenet newsgroups, and things to buy online. \"\"\" \n",
    "        \"\"\"By June 2004, Google had 4.28 billion web pages on its database, 880 million pictures\"\"\" \n",
    "        \"\"\"and 845 million Usenet messages — six billion things. Google's American website \"\"\"\n",
    "        \"\"\"has an Alexa rank of 1, meaning it is the most widely visited website in the world. It is so widely\"\"\" \n",
    "        \"\"\"known that people are using the word \"google\" as a verb that means \"to search for something on Google\"; \"\"\" \n",
    "        \"\"\"but because more than half of people on the web use it, \"google\" has been used to mean \"to search the web\". \"\"\"\n",
    "        \"\"\"Parent company of google is ALPHABET.\"\"\")\n",
    "sentences = sent_tokenize(text)\n",
    "for i,sent in enumerate(sentences):\n",
    "    print(str(i)+\" \"+sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting sentences, we can use parsing to extract sentences in structure of NP VP, such as \"John likes data science.\", because this kind of sentences are easy to be transformed to questions, such as, \"Who likes data science?\", \"Does John like data science?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Google's search engine can find pictures, videos, news, Usenet newsgroups, and things to buy online.\n",
      "1 Google's American website has an Alexa rank of 1, meaning it is the most widely visited website in the world.\n",
      "2 Parent company of google is ALPHABET.\n"
     ]
    }
   ],
   "source": [
    "def selectNPVP(sentences):\n",
    "    res = []\n",
    "    for sentence in sentences:\n",
    "        parse_trees = parser.raw_parse(sentence)\n",
    "        for tree in parse_trees:\n",
    "            if tree[0][0].label() == \"NP\" and tree[0][1].label() == \"VP\": # select NP VP sentences\n",
    "                res.append((sentence, tree))\n",
    "    return res\n",
    "\n",
    "NPVP_sents = selectNPVP(sentences)\n",
    "for i,sent in enumerate(NPVP_sents):\n",
    "    print(str(i)+\" \"+sent[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting NP VP sentences, we can manipulate the structure of sentences and transforme them into questions. There are two kinds of questions, WH questions and yes/no questions. \n",
    "\n",
    "Let's try to generate WH questions from the sentences above, to do that, simply replace the NP part with WH words, such as \"what\", \"who\", etc. Here, we just use \"what\" for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'can', 'find', 'pictures', ',', 'videos', ',', 'news', ',', 'Usenet', 'newsgroups', ',', 'and', 'things', 'to', 'buy', 'online', '.']\n",
      "['what', 'has', 'an', 'Alexa', 'rank', 'of', '1', ',', 'meaning', 'it', 'is', 'the', 'most', 'widely', 'visited', 'website', 'in', 'the', 'world', '.']\n",
      "['what', 'is', 'ALPHABET', '.']\n"
     ]
    }
   ],
   "source": [
    "def generateWHQuestion(sentence, tree):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    np = tree[0][0].leaves() # NP part of the sentence\n",
    "    wh_word = \"what\"\n",
    "    res = tokens[len(np):] # discard NP part\n",
    "    res.insert(0, wh_word) # insert WH word\n",
    "    return res\n",
    "\n",
    "questions_tokens = []\n",
    "questions_tokens.extend([generateWHQuestion(sent, tree) for sent, tree in NPVP_sents])\n",
    "for q in questions_tokens:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate yes/no questions, it is more tricky because you need to check if the verb is \"be words\" like _am_, _is_ , _are_, etc. or modal words like _can_, _could_, etc. because they have a different transformation to question. \n",
    "\n",
    "For other normal words, we need to find the corresponding form of \"do\" to put at the beginning. In this way, get the tense of verb is necessary and __POS tagging__ can help us to do that. Additionally, we need to restore the verb to its base form, NLTK provides __lemmatizer__ for us to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'can', 'find', 'pictures', ',', 'videos', ',', 'news', ',', 'Usenet', 'newsgroups', ',', 'and', 'things', 'to', 'buy', 'online', '.']\n",
      "['what', 'has', 'an', 'Alexa', 'rank', 'of', '1', ',', 'meaning', 'it', 'is', 'the', 'most', 'widely', 'visited', 'website', 'in', 'the', 'world', '.']\n",
      "['what', 'is', 'ALPHABET', '.']\n",
      "['can', 'google', \"'s\", 'search', 'engine', 'find', 'pictures', ',', 'videos', ',', 'news', ',', 'Usenet', 'newsgroups', ',', 'and', 'things', 'to', 'buy', 'online', '.']\n",
      "['does', 'google', \"'s\", 'American', 'website', 'have', 'an', 'Alexa', 'rank', 'of', '1', ',', 'meaning', 'it', 'is', 'the', 'most', 'widely', 'visited', 'website', 'in', 'the', 'world', '.']\n",
      "['is', 'parent', 'company', 'of', 'google', 'ALPHABET', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def generateYNQuestion(sentence, tree):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    res = tokens[:]\n",
    "    special_words = {\"am\", \"is\", \"are\", \"was\", \"were\", \"can\", \"could\"}\n",
    "    np = tree[0][0].leaves() # NP part of sentence\n",
    "    vp = tree[0][1].leaves() # VP part of sentence\n",
    "    if vp[0] in special_words: # assume VP[0] is the verb for simplicity\n",
    "        res.remove(vp[0])\n",
    "        res.insert(0, vp[0]) # move VP[0] to the beginning of sentence\n",
    "    else:\n",
    "        do_words_map = {\"VBZ\": \"does\", \"VB\": \"do\", \"VBD\": \"did\"}\n",
    "        tags = pos_tag(tokens)\n",
    "        do_word = do_words_map[tags[len(np)][1]] # len(np) is the index of the verb\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        res[len(np)] = lemmatizer.lemmatize(res[len(np)], 'v') # change the verb to base form\n",
    "        res.insert(0, do_word) # insert do word to the beginning\n",
    "    res[1] = res[1].lower() # make the original start letter of sentence into lower case\n",
    "    return res\n",
    "\n",
    "questions_tokens.extend([generateYNQuestion(sent, tree) for sent, tree in NPVP_sents])\n",
    "for q in questions_tokens:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we join our tokens together into a sentence. We need to make the first letter into upper case and replace the final period with question mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 What can find pictures, videos, news, Usenet newsgroups, and things to buy online?\n",
      "1 What has an Alexa rank of 1, meaning it is the most widely visited website in the world?\n",
      "2 What is ALPHABET?\n",
      "3 Can google's search engine find pictures, videos, news, Usenet newsgroups, and things to buy online?\n",
      "4 Does google's American website have an Alexa rank of 1, meaning it is the most widely visited website in the world?\n",
      "5 Is parent company of google ALPHABET?\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def tokens2Question(tokens):\n",
    "    tokens[0] = tokens[0][0].upper()+tokens[0][1:]\n",
    "    tokens[-1] = '?'\n",
    "    res = \"\"\n",
    "    punctuations = set(string.punctuation)\n",
    "    for i,token in enumerate(tokens):\n",
    "        # the final question mark\n",
    "        if i == len(tokens)-1:\n",
    "            res += token\n",
    "        # if the next word is a punctuation, there should be no whitespace between them\n",
    "        elif tokens[i+1][0] in punctuations:\n",
    "            res += token\n",
    "        else:\n",
    "            res += token + ' '\n",
    "    return res\n",
    "\n",
    "questions = [tokens2Question(tokens) for tokens in questions_tokens]\n",
    "for i,q in enumerate(questions):\n",
    "    print(str(i)+\" \"+q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get our questions! You can see that our question generation system is far from perfect because we have to make some assumptions of our input text, but it is enough as an simple application to show how to use parsing in NLTK.\n",
    "\n",
    "# Reference\n",
    "If you want to know more about NLTK and grammatical analysis, you can refer to the following links.\n",
    "1. NLTK Documentation: https://www.nltk.org/\n",
    "2. NLTK Book: http://www.nltk.org/book/ (Full of NLTK examples! Very useful)\n",
    "3. Stanford Parsing: https://nlp.stanford.edu/software/lex-parser.shtml\n",
    "\n",
    "\n",
    "Thank you for reading this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}