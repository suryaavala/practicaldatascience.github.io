{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will introduce you to some basic techniques for performing sentiment analysis. We will concentrate on the tools provided by [Natural Language Toolkit (NLTK)](https://www.nltk.org/), a Python package used for natural language tasks. Sentiment analysis is the process of extracting subjective information about a piece of text, particularly focusing on whether the writer's attitude toward the subject is positive or negative. This topic is useful in a variety of areas. It can allow businesses to understand how consumers are reacting to new initiatives, and adjust their marketing strategy accordingly. Or, it can allow policymakers to gain insight on which groups or demographics to target in their campaigns. These are just a couple examples of the practical applications of sentiment analysis.\n",
    " \n",
    "### Tutorial content\n",
    "\n",
    "In this tutorial, we will first show how to perform sentiment analysis using VADER, a tool contained in NLTK, with an example of applying this to text from social media. Then, we will introduce ways to train a new classifier with NLTK.\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Installing the libraries](#Installing-the-libraries)\n",
    "- [VADER sentiment analysis tools](#VADER-sentiment-analysis-tools)\n",
    "- [Example application: sentiment analysis on tweets](#Example-application:-sentiment-analysis-on-tweets)\n",
    "- [Training a Naive Bayes Classifier with NLTK](#Training-a-Naive-Bayes-Classifier-with-NLTK)\n",
    "- [Saving and reloading classifiers](#Saving-and-reloading-classifiers)\n",
    "- [Incorporating classifiers from scikit-learn](#Incorporating-classifiers-from-scikit-learn)\n",
    "- [Summary and references](#Summary-and-references)\n",
    "\n",
    "## Installing the libraries\n",
    "\n",
    "We will use `nltk`, which is already included in Anaconda. There is an extra lexicon and a corpus that need to be downloaded separately:\n",
    "\n",
    "  ```python\n",
    "  >>>nltk.download('vader-lexicon')\n",
    "  >>>nltk.download('movie_reviews')\n",
    "  ```\n",
    " \n",
    "Another library we will be using is `tweepy`, to collect tweets from Twitter. You can install this using `pip`:\n",
    "\n",
    "    $ pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER sentiment analysis tools\n",
    "\n",
    "Now that we've installed the libraries, let's get started with VADER. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a module that contains tools for sentiment analysis, including a sentiment analyzer that is ready to be applied to any string of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my machine, the imports raise a warning that the `twython` library has not been installed. Do not worry about this, since we will not be using any functions that require that library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a `SentimentIntensityAnalyzer`object, which will be used to analyze strings of text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SentimentIntensityAnalyzer` has a lexicon of words that are commonly used in subjective text. Often, these are words that convey feeling or emotion. Each word has been assigned a score representing how positive or negative it is. This lexicon can be accessed with the `make_lex_dict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1 0.9 -2.5\n"
     ]
    }
   ],
   "source": [
    "lexicon_dict = analyzer.make_lex_dict()\n",
    "print(lexicon_dict['great'], lexicon_dict['okay'], lexicon_dict['horrible'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the word 'great' has a positive score, and 'horrible' has a negative score. This means that they have a positive and negative sentiment, respectively. The word 'okay' also has a positive score, but the value is smaller than that of 'great', showing that 'great' has a stronger positive sentiment than 'okay'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key method of `SentimentIntensityAnalyzer` is `polarity_scores`. It is called on a string of text, and returns four float values: 'compound', 'negative', 'neutral', and 'positive'. The 'positive', 'negative', and 'neutral' values give scores of how positive, negative, and neutral the sentiment of the text is, respectively. The analyzer uses the lexicon to take into account the scores of the individual words in the text. The 'compound' value is a sum of these three scores, normalized to be between -1 and 1. Values closer to -1 represent more negative, and values closer to 1 represent more positive, sentiment. A value close to 0 means that the text is neutral, so it does not contain subjective information. In this way, the compound score can identify whether the overall sentiment of a piece of text is positive, negative, or neutral, and how strong that sentiment is.\n",
    "\n",
    "Let's see some simple examples using `polarity_scores`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.3612, 'neg': 0.0, 'neu': 0.286, 'pos': 0.714}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores('I like chocolate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the call to `polarity_scores` returns a dictionary containing all four values. The 'compound' value for the string \"I like chocolate\" is 0.3612, which represents a positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.4199, 'neg': 0.0, 'neu': 0.264, 'pos': 0.736}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores('I like chocolate!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER also takes into account punctuation marks that may affect the sentiment. When we add an exclamation mark to the same sentence, the intensity of the sentiment goes up. So, since the sentence was already positive, it becomes more positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.7798, 'neg': 0.0, 'neu': 0.128, 'pos': 0.872}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores('I like chocolate! :D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting property of VADER is that it is particularly suited for social media use. So, the `SentimentIntensityAnalyzer` knows how to take emoticons into account, as seen here. The intensity of the above sentence with a happy emoticon at the end went up to 0.7798, showing a highly positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example application: sentiment analysis on tweets\n",
    "\n",
    "We will now show an application of analyzing the sentiment of tweets containing a particular word or phrase. The first step is to load `tweepy`, the Python module for retrieving tweets, and set up the authentication. You will also need to register an application on Twitter and get the keys. I have placed my consumer key, consumer secret, access token, and access token secret, all separated by newlines, in a file called \"api_keys.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and set up tweepy API\n",
    "import tweepy\n",
    "with open('api_keys.txt', 'r') as f:\n",
    "    creds = f.read().split('\\n')\n",
    "    consumer_key = creds[0]\n",
    "    consumer_secret = creds[1]\n",
    "    access_token = creds[2]\n",
    "    access_token_secret = creds[3]\n",
    "    \n",
    "# Set up the authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_tweets_by_query` is our main function to retrieve tweets that contain the given query string. The default maximum number of tweets that will be returned by `tweepy` is 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Use the api object to retrieve recent tweets containing the query string\n",
    "# Return a list containing the distinct matching tweets\n",
    "def get_tweets_by_query(query):\n",
    "    tweets_text = set()\n",
    "    tweets = api.search(q = query, lang = 'en')\n",
    "    for tweet in tweets:\n",
    "        match = re.match(r\"RT @\\w*: \", tweet.text)\n",
    "        text = tweet.text\n",
    "        if (match):\n",
    "            text = text[match.end():]\n",
    "        tweets_text.add(text)\n",
    "    return list(tweets_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function to retrieve tweets, we can use VADER to return the fractions of positive, negative, and neutral tweets in the retrieved tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use VADER to classify the given text as positive, negative, or neutral \n",
    "def analyze_text(text):\n",
    "    sentiments = analyzer.polarity_scores(text)\n",
    "    if (sentiments['compound'] > 0):\n",
    "        return \"positive\"\n",
    "    elif (sentiments['compound'] < 0):\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Returns the percentage of positive, negative, and neutral tweets\n",
    "def analyze_tweets(tweets):\n",
    "    (num_positive, num_neutral, num_negative) = (0, 0, 0)\n",
    "    for tweet in tweets:\n",
    "        sentiment = analyze_text(tweet)\n",
    "        if (sentiment == \"positive\"):\n",
    "            num_positive += 1\n",
    "        elif (sentiment == \"negative\"):\n",
    "            num_negative += 1\n",
    "        else:\n",
    "            num_neutral += 1\n",
    "    total = len(tweets)\n",
    "    return ((num_positive/total), (num_negative/total), (num_neutral/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these functions to understand the sentiment towards a particular word or phrase, like \"chocolate\". We start by retrieving the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chocolate_tweets = get_tweets_by_query('chocolate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use `analyze_tweets` to see the sentiment of these tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.21428571428571427, 0.2857142857142857)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_tweets(chocolate_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows us the fraction of positive, negative, and neutral tweets in the collection `chocolate_tweets`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, more practically, a company like Pantene might want to get a gauge for the sentiment of people towards the brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6153846153846154, 0.0, 0.38461538461538464)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pantene_tweets = get_tweets_by_query('pantene')\n",
    "analyze_tweets(pantene_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a large, random sample of tweets is collected, these fractions of positive vs. negative tweets can help marketers understand what the general sentiment of people on social media is toward a brand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Naive Bayes Classifier with NLTK\n",
    "\n",
    "Though VADER's sentiment intensity analyzer is an easy tool to use without much preparation, it is not always good for texts other than social media. We can also manually train a new classifier using NLTK. Many times, this gives better results, since the training data can be tailored to the task at hand.\n",
    "\n",
    "In this part of the tutorial, we will introduce the ways new classifiers can be trained and tested.\n",
    "The steps are:\n",
    "\n",
    "1. Take a labeled dataset. For sentiment analysis, the labels are usually positive and negative\n",
    "2. Separate the data into train and test sets\n",
    "3. Train the classifier on the training data\n",
    "4. Calculate the accuracy on the test set\n",
    "\n",
    "NLTK contains many corpora that can be used for classification tasks. For sentiment analysis in particular, one useful corpus is `movie_reviews`. We will use this dataset for our example. \n",
    "\n",
    "A classifier included in NLTK that is commonly used for sentiment analysis tasks is `NaiveBayesClassifier`. We first import the corpus and classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's corpora are a collection of labelled files containing text. The `fileids` method retrieves a list of identifiers for all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg/cv000_29416.txt\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "file_ids = movie_reviews.fileids()\n",
    "print(file_ids[0])\n",
    "print(len(file_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 2,000 files in the movie_reviews corpus. The `categories` method is used to find out what all the possible labels for files in this corpus are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this corpus is suited for sentiment analysis, each file is labeled with either 'neg' or 'pos', which mean negative or positive sentiment. We can get all file ids that match a certain category by passing the category into the `fileids` call. For example, we can access the negative and positive files separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "neg_files = movie_reviews.fileids('neg')\n",
    "pos_files = movie_reviews.fileids('pos') \n",
    "print(len(neg_files))\n",
    "print(len(pos_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that there are an equal number of positive and negative files in this corpus. There are also methods to access the actual contents of the corpus. `words` and `sents` retrieve a list of words and of sentences in the corpus. As we can see below, a sentence is represented by a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]\n",
      "[['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.words())\n",
    "print(movie_reviews.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are more familiar with the corpus, we return to our analysis task. We first split the corpus into a train and test set, each containing an equal number of negative and positive files. For this example, we will use 1,400 files for the train set and 600 files for the test set. In a real-world application, these numbers should be tweaked to find the best ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_files = neg_files[:700]\n",
    "train_pos_files = pos_files[:700]\n",
    "test_neg_files = neg_files[700:]\n",
    "test_pos_files = pos_files[700:]\n",
    "(train_set, test_set) = ((train_neg_files + train_pos_files), (test_neg_files + test_pos_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to train a classifier on the train set. One quirk is that `NaiveBayesClassifier` expects features as a tuple of the format (`file_dict`, `category`), where `word_dict` contains all the words in one file, each mapped to True, and `category` is the file label. Currently, `train_set` and `test_set` are just a list of file ids. So, we need to convert both the train and test set into the correct form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a dataset into features that can be supplied to NaiveBayesClassifier\n",
    "def create_features(dataset):\n",
    "    result = []\n",
    "    for file_id in dataset:\n",
    "        file_dict = {}\n",
    "        for word in movie_reviews.words(fileids = [file_id]):\n",
    "            file_dict[word] = True\n",
    "        result.append((file_dict, (movie_reviews.categories(fileids = [file_id])[0])))\n",
    "    return result\n",
    "\n",
    "train_features = create_features(train_set)\n",
    "test_features = create_features(test_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can simply feed `train_features` to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = NaiveBayesClassifier.train(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nb_classifier` has now been trained on the train set. The final step is to test it. To do this, we will use the method `nltk.classify.util.accuracy`, which takes in a labelled test set, runs the classifier on the observations, and then computes the accuracy using the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.util.accuracy(nb_classifier, test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test set is 72%. We can also find out which words in the corpus were most influential in classifying the files. The function `show_most_informative_features` does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     17.8 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               insulting = True              neg : pos    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.0 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.0 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n",
      "                flawless = True              pos : neg    =      9.8 : 1.0\n",
      "                riveting = True              pos : neg    =      9.7 : 1.0\n",
      "               affecting = True              pos : neg    =      9.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would expect, the informative words usually carry a strong subjective meaning. For example, \"outstanding\" is a strong positive word, while \"idiotic\" is strongly negative. It makes sense that these kinds of words are the most influential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and reloading classifiers\n",
    "\n",
    "Training a classifier can be a long and tedious process. The quality of the training data is very important, so this process is usually heavily focused on. Additionally, the same classifier can often be used for various different programs. In these cases, it would be costly to retrain it over and over.\n",
    "\n",
    "In Python, there are ways to avoid this hassle and simply save a classifier and reload it later. We will take advantage of the `pickle` module to save our classifier. `pickle` is used to serialize and deserialize objects. Generally, objects are first \"pickled\", or serialized, into a byte stream, and later \"unpickled\", or deserialized, back into the original structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will pickle our classifier. We do this by specifying a path where it will be saved by the `dump` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_path = 'classifier.pkl'\n",
    "classifier_pickle = open(pickle_path, 'wb')\n",
    "pickle.dump(nb_classifier, classifier_pickle)\n",
    "classifier_pickle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To unpickle the classifier, we use the `load` method. This takes in the path where the object was saved and converts it back into the original structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     17.8 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               insulting = True              neg : pos    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.0 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.0 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n",
      "                flawless = True              pos : neg    =      9.8 : 1.0\n",
      "                riveting = True              pos : neg    =      9.7 : 1.0\n",
      "               affecting = True              pos : neg    =      9.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_unpickle = open(pickle_path, 'rb')\n",
    "opened_classifier = pickle.load(classifier_unpickle)\n",
    "opened_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the classifier retains the same information after saving and reloading. Once reloaded, it can be used on a new classification task as is, or it can be trained further if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating classifiers from scikit-learn\n",
    "\n",
    "Though 72% accuracy is reasonable, it isn't as high as we would like for certain tasks. There is a greater variety of classifiers that can be useful in the `scikit-learn` module. `SklearnClassifier` is an api for these classifiers built into `NLTK`. Some of the common classifiers included in this module are `MultinomialNB`, `BernoulliNB`, `LogisticRegression`, `LinearSVC`, and `SGDClassifier`. Let us load these five classifiers using`SkLearnClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "multi_nb = SklearnClassifier(MultinomialNB())\n",
    "bernoulli_nb = SklearnClassifier(BernoulliNB())\n",
    "logistic_reg = SklearnClassifier(LogisticRegression())\n",
    "linear_svc = SklearnClassifier(LinearSVC())\n",
    "sgd_classifier = SklearnClassifier(SGDClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train these classifiers on the same training data as before, and check the the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy: 0.821667\n",
      "BernoulliNB accuracy: 0.796667\n",
      "LogisticRegression accuracy: 0.883333\n",
      "Linear_SVC accuracy: 0.866667\n",
      "SGDClassifier accuracy: 0.836667\n"
     ]
    }
   ],
   "source": [
    "classifiers = {'MultinomialNB' : multi_nb, 'BernoulliNB': bernoulli_nb, 'LogisticRegression' : logistic_reg, \n",
    "               'Linear_SVC': linear_svc, 'SGDClassifier': sgd_classifier}\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.train(train_features)\n",
    "    print(\"%s accuracy: %f\" % (name, nltk.classify.util.accuracy(classifier, test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, these classifiers all have better accuracy than the `NaiveBayesClassifier`. In this case, the logistic regression classifier has the highest accuracy. In the real world, sentiment analysis is a tough question and classifiers used for this task are never perfect. Often times, one classifier is chosen for a certain task, or a few classifiers can be combined in a voting scheme that takes each into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and references\n",
    "\n",
    "This tutorial gave an overview of the sentiment analysis tools provided in the NLTK library. Some useful resources for further exploration are:\n",
    "\n",
    "1. [General sentiment analysis overview](https://callminer.com/blog/sentiment-analysis-examples-best-practices/)\n",
    "2. [Sentiment analysis training and testing](https://blog.griddynamics.com/creating-training-and-test-data-sets-and-preparing-the-data-for-twitter-stream-sentiment-analysis-of-social-movie-reviews/)\n",
    "3. [VADER documentation](http://www.nltk.org/_modules/nltk/sentiment/vader.html)\n",
    "4. [VADER algorithmic details](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf)\n",
    "5. [Examples using NLTK's NaiveBayesClassifier](http://www.nltk.org/book/ch06.html)\n",
    "6. [NLTK SklearnClassifier documentation](http://www.nltk.org/_modules/nltk/classify/scikitlearn.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
