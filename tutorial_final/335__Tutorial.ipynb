{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is very popular right now and as one of the main parts of machine learning based on learning data representations, whose models are loosely related to communication patterns and information processing in a biological neural system, for example the neural coding attempts to define the relationship between various stimuli and associated neuronal responses in the brain. [1][2][3] \n",
    "\n",
    "Neural networks become one of the major thrust areas recently in various pattern recognition, prediction, and analysis problems. Further, a multilayer perceptron(MLP) is a class of feedforward artificial neural network, which includes at least 3 layers of nodes. Apart from the input nodes, each node is a neuron which uses a non-linear action function. Also, we have some other basic network formalisms such as Convolutional networks, Recurrent networks and Boltzmann machines and some advanced formalisms such as adversarial models: GANs. \n",
    "\n",
    "MLP utilizes a supervised learning technique called backpropagation for training. [4][5] Its multiple layers and nonlinear activation distinguish MLP from a linear perceptron. It can distinguish data which is non-linear separable. [6] Moreover, MLPs are connectionist computational models and can represent any fuctions. For example, an XOR takes three perceptrons that has 6 weights, 3 threshold values and 9 total parameters. Individual perceptrons are computational equivalent of neurons. The MLP is a layered composition of many perceptrons. Also, MLPs can model Boolean functions so that individual perceptrons can act as Boolean gates and networks of perceptrons are Boolean functions. Otherwise, MLPs are Boolean machines so that they represent Boolean functions over linear boundaries, they can represent arbitrary decision boundaries and they can be used to classify data.\n",
    "![](pic2.png)\n",
    "Figure 1: Multi-Layer Perceptron XOR \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pic1.png)\n",
    "Figure 2: The MLP as a Boolean Function Over Feature Fetectors\n",
    "\n",
    "Explanation for Figure 1:\n",
    "- The input layer comprises “feature detector”: Detect if certain patterns have occurred in the input.\n",
    "- The network is a Boolean function over the feature detectors.\n",
    "- I.e. It is important for the first layer to capture relevant patterns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are something else you should know in advance. A threshold unit  comprises a set of weights and a threshold and “Fires” if the weighted sum of inputs exceeds a threshold. A “squashing” function instead of a threshold at the output will be much better. Therefore, the sigmoid “activation”(the function that acts on the weighted combination of inputs (and threshold)) replaces the threshold. Output neuron may have some other actual “activation” – Threshold, sigmoid, tanh, softplus, rectifier, etc. Also, perceptrons with sigmoidal activations actually model class probabilities. Moreover, as for the softmax output layer, one of the outputs goes to 1,the others go to 0. Parameters are weights and bias.\n",
    "![](pic6.png)\n",
    "Figure 3: “Proper” Networks: Outputs With ActiFvations\n",
    "\n",
    "![](pic7.png)\n",
    "Figure 4: Vector Activation Example: Softmax."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main body"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will write an implementation of the backpropagation algorithm for training the neural network. In this tutorial, it will not use any audodiff toolboxes such as Tensorflow, Pytorch, ect and only use numpy like libraries.\n",
    "\n",
    "The goal of this assignment is to label images of 10 handwritten digits of “zero”, “one”, ..., “nine”. It is a typical problem among MLPs. The images are 28 by 28 in size (MNIST dataset), which we will be represented as a vector x of dimension 784 by listing all the pixel values in raster scan order. The labels t are 0,1,2, ...,9 corresponding to 10 classes as written in the image. There are 3000 training cases, containing 300 examples of each of 10 classes. The can be found in the file digitstrain.txt.\n",
    "\n",
    "This typical sample can be used in many situations nowadays because the numbers in the real world are almost everywhere. For example, you give the tip every day when you do the signature on the payment pad in the restaurant or somewhere using the card during the daily life. Also, when the student does the mathematics written calculation or someone doing the internet digital verification, it has many applications. Moreover, such as vehicle license plate recognitions, credit/debit card number auto-recognitions and personal ID numbers recognitions in the country/companies/schools, this typical sample is meaningful. More advanced, it can be updated to do handwritten letters/vocabularies recognition or personal written identify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pic3.png)\n",
    "Figure 5: Demo of This Typical Problem \n",
    "\n",
    "\n",
    "![](pic4.png)\n",
    "Figure 6: Overview Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 1: Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here firstly, you must read an input file. Each line contains 785 numbers (comma delimited): the first values are between 0.0 and 1.0 correspond to the 784 pixel values (black and white images), and the last number denotes the class label: 0 corresponds to digit 0, 1 corresponds to digit 1, etc. As a warm up question, load the data firstly.\n",
    "\n",
    "For this problem you must write a function that takes a file path as an argument which contains this data. Your function must return two values (X and Y) that contains the data from the file as described. Specifically, the first return value (X) must be a matrix where the rows are individual examples of images, and the columns are individual pixels (N x 784 matrix). The second return value must be a list/array of real numbers representing the labels of the examples (rows) in X.\n",
    " \n",
    "eg:\n",
    "\n",
    "1.0,0.0,1.0,0.0,....0.0,0.25,0.0,0.0\n",
    "... 1.0,0.0,1.0,0.0,...,1.0,0.0,0.0,0.96776\n",
    "\n",
    "X=[\n",
    "\n",
    "[1.0,0.0,1.0,0.0,....0.0,0.25,0.0,0.0]\n",
    "\n",
    "... \n",
    "[1.0,0.0,1.0,0.0,...,1.0,0.0,0.0,0.96776] \n",
    "\n",
    "]\n",
    "\n",
    "Y = [5,...,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample\n",
    "\n",
    "\n",
    "# def load_data(filePath): \n",
    "#     X = []\n",
    "#     Y = []\n",
    "#     #INSERT YOUR CODE HERE \n",
    "#     return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from helpers.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filePath):\n",
    "    sample = np.loadtxt(filePath, delimiter=\",\")\n",
    "    X = sample[:, np.arange(len(sample[0])-1)]\n",
    "    Y = sample[:, len(sample[0])-1]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST CODE (IF NOTHING HAPPENS, IT MEANS EVERTHING IS GOOD!)\n",
    "# AUTOLAB_IGNORE_START\n",
    "\n",
    "# def test_problem_1():\n",
    "#     REF_XTRAIN = \"fixtures/X_expected\"\n",
    "#     REF_YTRAIN = \"fixtures/Y_expected\"\n",
    "\n",
    "#     (xtrain, ytrain) = load_data(\"data/digitstest.txt\")\n",
    "\n",
    "#     ref_xtrain = pickle.load(open(REF_XTRAIN, \"rb\"), **PICKLE_KWARGS)\n",
    "#     ref_ytrain = pickle.load(open(REF_YTRAIN, \"rb\"), **PICKLE_KWARGS)\n",
    "\n",
    "#     assert(ref_xtrain.shape == xtrain.shape)\n",
    "#     assert(ref_ytrain.shape == ytrain.shape)\n",
    "#     assert(isAllClose(ref_ytrain, ytrain))\n",
    "#     assert(isAllClose(ref_xtrain, xtrain))\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 2: Back Propagation Algorithm Without Hidden Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the backpropagation algorithm in a zero hidden layer neural network (weights between input and output nodes). The output layer should be a softmax output over 10 classes corresponding to 10 classes of handwritten digits (e.g. an architecture: 784 > 10). Your backprop code should minimize the cross-entropy entropy function for multi-class classification problem (categorical cross entropy). Using cross-entropy entropy function to calculate the loss. \n",
    "![](pic5.png)\n",
    "\n",
    "This step should be done with a full step of gradient descent, not SGD or RMSProp. For this problem you must write a function that takes as an input a matrix of X values, a list of Y values (as returned from problem 1), a weight matrix, and a learning rate and performs a single step of backpropagation. You will need to do both a forward step with the inputs, and then a backward prop to get the gradients. Return the updated weight matrix and bias in the same format as it was passed.\n",
    "\n",
    "The list of weight matrices will be a list with 1 entry where the only entry is a matrix in the format where the rows represent all of the outgoing weights for a neuron in the input layer and the columns represent the weights for the incoming neurons. A specific row column index will give you the weight for a neuron to neuron connection.\n",
    "\n",
    "The list of bias vectors will be in the form where each entry in the list is a vector with the same length as the first set of weights. (e.g. for an architecture of 784 > 10, there will be a single element list with a vector of size 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample\n",
    "\n",
    "# def update_weights_perceptron(X, Y, weights, bias, lr): \n",
    "#     #INSERT YOUR CODE HERE\n",
    "#     return updated_weights, updated_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firstly, we need a softmax on the output of forward through part.\n",
    "def softmax(a):\n",
    "    # 2-D dimension\n",
    "    return np.exp(a) / np.sum(np.exp(a), axis=1)[:, None]\n",
    "\n",
    "def forwardthrough(X, weights, bias):\n",
    "    Z = np.add(np.dot(X, weights), bias)\n",
    "    return softmax(Z)\n",
    "\n",
    "#Then we need the cross-entropy entropy function to calculate the loss/difference.\n",
    "\n",
    "def cross_entropy_loss(X, Y, weights, bias):\n",
    "    Z = forwardthrough(X, np.asarray(weights)[0], np.asarray(bias)[0])\n",
    "    A = np.zeros((Y.shape[0], 10), dtype=int)\n",
    "    for i in range(Y.shape[0]):\n",
    "        A[i][int(Y[i])] = 1\n",
    "    return Z - A\n",
    "\n",
    "def update_weights_perceptron(X, Y, weights, bias, lr):\n",
    "    diff_Z = cross_entropy_loss(X, Y, weights, bias)\n",
    "    diff_B = 1 / Y.shape[0] * np.sum(diff_Z, axis=0, keepdims=True)\n",
    "    diff_W = 1/Y.shape[0] * np.dot(X.T, diff_Z)\n",
    "    bias = np.asarray(bias)\n",
    "    bias[0] += -lr * diff_B\n",
    "    weights = np.asarray(weights)\n",
    "    weights[0] += - lr * diff_W\n",
    "    return weights.tolist(), bias.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST CODE (IF NOTHING HAPPENS, IT MEANS EVERTHING IS GOOD!)\n",
    "# AUTOLAB_IGNORE_START\n",
    "\n",
    "# def test_problem_2():\n",
    "#     STARTING_WEIGHTS_PATH = \"fixtures/start_weights_problem2\"\n",
    "#     ENDING_WEIGHTS_PATH = \"fixtures/final_weights_problem2\"\n",
    "#     STARTING_BIAS_PATH = \"fixtures/start_bias_problem2\"\n",
    "#     ENDING_BIAS_PATH = \"fixtures/final_bias_problem2\"\n",
    "#     PARAMS_PATH = \"fixtures/problem2.params.json\"\n",
    "\n",
    "#     params = json.loads(open(PARAMS_PATH, \"r\").read())\n",
    "\n",
    "#     (X, Y)= load_data(\"data/digitstrain.txt\")\n",
    "#     inputWeights = pickle.load(open(STARTING_WEIGHTS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "#     finalWeights = pickle.load(open(ENDING_WEIGHTS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "\n",
    "#     inputBias = pickle.load(open(STARTING_BIAS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "#     finalBias = pickle.load(open(ENDING_BIAS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "\n",
    "#     weightsToTest, biasToTest = update_weights_perceptron(X, Y, inputWeights, inputBias, float(params[\"LEARNING_RATE\"]))\n",
    "\n",
    "#     assert(isAllClose(finalWeights, weightsToTest))\n",
    "#     assert(isAllClose(finalBias, biasToTest))\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 3: Single Layer Neural Network With Hidden Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend your code from problem 2 to support a single layer neural network with N hidden units (e.g. an architecture: 784 > 10 > 10). These hidden units should be using sigmoid activations.\n",
    "\n",
    "For this problem you must write a function that takes as an input a matrix of X values, a list of Y values (as returned from problem 1), list of weight matrices, a list of bias vectors, a list of bias vectors, and a learning rate and performs a single step of backpropagation. You will need to do both a forward step with the inputs to get the outputs, and then a backward prop to get the gradients. Return the updated weight matrix and bias in the same format as it was passed.\n",
    "\n",
    "The list of weight matrices is a list with 2 entries where each entry in the list contains a single weight matrix as previously defined in problem 2. For a network with shape 784 > 10 > 10 the past list of weight matrices would look like this: [Matrix with shape 784x10, Matrix with shape 10x10]. **Note:** Though a hidden layer of size 10 is used as an example here, your code must be able to support a hidden layer of dimension N.\n",
    "\n",
    "The list of bias vectors will be in the form where each entry in the list is a vector with the same length as the first set of weights. (e.g. for an architecture of 784 > 10 > 10, there will be a two element list with a vector of size 10 and a vector of size 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample\n",
    "\n",
    "# def update_weights_perceptron(X, Y, weights, bias, lr): \n",
    "#     #INSERT YOUR CODE HERE\n",
    "#     return updated_weights, updated_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sigmoid “activation” replaces the threshold on the forward through part ----Activation: The function that acts on the weighted combination of inputs (and threshold) \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#Using the derivation of sigmoid on the backward through part\n",
    "\n",
    "def derivation_sigmoid(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "def cross_entropy_loss_single_layer(S, Y, weights, bias):\n",
    "    Z = forwardthrough(S, np.asarray(weights)[1], np.asarray(bias)[1])\n",
    "    A = np.zeros((Y.shape[0], 10), dtype=int)\n",
    "    for i in range(Y.shape[0]):\n",
    "        A[i][int(Y[i])] = 1\n",
    "    return Z - A\n",
    "\n",
    "\n",
    "def update_weights_single_layer(X, Y, weights, bias, lr):\n",
    "    Z = np.add(np.dot(X, np.asarray(weights)[0]), np.asarray(bias)[0])\n",
    "    S = sigmoid(Z)\n",
    "    diff_Z = cross_entropy_loss_single_layer(S, Y, weights, bias)\n",
    "    diff_B2 = 1 / Y.shape[0] * np.sum(diff_Z, axis=0, keepdims=True)\n",
    "    diff_W2 = 1 / Y.shape[0] * np.dot(S.T, diff_Z)\n",
    "    diff_S =  np.dot(diff_Z, np.asarray(weights)[1].T)\n",
    "    diff_Z1 = diff_S * derivation_sigmoid(S)\n",
    "    diff_B1 = 1 / Y.shape[0] * np.sum(diff_Z1, axis=0, keepdims=True)\n",
    "    diff_W1 = 1 / Y.shape[0] * np.dot(X.T, diff_Z1)\n",
    "    bias = np.asarray(bias)\n",
    "    bias[0] += -lr * diff_B1\n",
    "    bias[1] += -lr * diff_B2\n",
    "    weights = np.asarray(weights)\n",
    "    weights[0] += - lr * diff_W1\n",
    "    weights[1] += - lr * diff_W2\n",
    "    return weights.tolist(), bias.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST CODE (IF NOTHING HAPPENS, IT MEANS EVERTHING IS GOOD!)\n",
    "# AUTOLAB_IGNORE_START\n",
    "\n",
    "# def test_problem_3():\n",
    "#     STARTING_WEIGHTS_PATH = \"fixtures/start_weights_problem3\"\n",
    "#     ENDING_WEIGHTS_PATH = \"fixtures/final_weights_problem3\"\n",
    "#     STARTING_BIAS_PATH = \"fixtures/start_bias_problem3\"\n",
    "#     ENDING_BIAS_PATH = \"fixtures/final_bias_problem3\"\n",
    "#     PARAMS_PATH = \"fixtures/problem3.params.json\"\n",
    "\n",
    "#     params = json.loads(open(PARAMS_PATH, \"r\").read())\n",
    "\n",
    "#     (X, Y)= load_data(\"data/digitstrain.txt\")\n",
    "#     inputWeights = pickle.load(open(STARTING_WEIGHTS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "#     finalWeights = pickle.load(open(ENDING_WEIGHTS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "\n",
    "#     inputBias = pickle.load(open(STARTING_BIAS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "#     finalBias = pickle.load(open(ENDING_BIAS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "\n",
    "#     weightsToTest, biasToTest = update_weights_single_layer(X, Y, inputWeights, inputBias, float(params[\"LEARNING_RATE\"]))\n",
    "\n",
    "#     assert(isAllClose(finalWeights, weightsToTest))\n",
    "#     assert(isAllClose(finalBias, biasToTest))\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 4: 2-Layers Neural Network With Hidden Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend your code from problem 3 (use cross entropy error) and implement a 2-layer neural network, starting with a simple architecture containing N hidden units in each layer (e.g. with architecture: 784 > 10 > 10 > 10). These hidden units should be using sigmoid activations.\n",
    "\n",
    "For this problem you must write a function that takes as an input a matrix of X values, a list of Y values (as returned from problem 1), list of weight matrices, a list of bias vectors, and a learning rate and performs a single step of backpropagation. You will need to do both a forward step with the inputs to get the outputs, and then a backward prop to get the gradients. Return the updated weight matrix and bias in the same format as it was passed.\n",
    "\n",
    "The list of weight matrices is a list with 3 entries where each entry in the list contains a single weight matrix as previously defined in problem 2. For a network with shape 784 > 10 > 10 > 10 the passed list of weight matrices would look like this: [Matrix with shape 784x10, Matrix with shape 10x10, Matrix with shape 10x10]. Note: Though a hidden layer of size 10 is used as an example here, your code must be able to support a hidden layer of dimension N.\n",
    "\n",
    "The list of bias vectors will be in the form where each entry in the list is a vector with the same length as the first set of weights. (e.g. for an architecture of 784 > 10 > 10, there will be a two element list with an vector of size 10 and a vector of size 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample\n",
    "\n",
    "# def update_weights_perceptron(X, Y, weights, bias, lr): \n",
    "#     #INSERT YOUR CODE HERE\n",
    "#     return updated_weights, updated_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_double_layer(S1, Y, weights, bias):\n",
    "    Z = forwardthrough(S1, np.asarray(weights)[2], np.asarray(bias)[2])\n",
    "    A = np.zeros((Y.shape[0], 10), dtype=int)\n",
    "    for i in range(Y.shape[0]):\n",
    "        A[i][int(Y[i])] = 1\n",
    "    return Z - A\n",
    "\n",
    "def update_weights_double_layer(X, Y, weights, bias, lr):\n",
    "    Z = np.add(np.dot(X, np.asarray(weights)[0]), np.asarray(bias)[0])\n",
    "    S = sigmoid(Z)\n",
    "    Z1 = np.add(np.dot(S, np.asarray(weights)[1]), np.asarray(bias)[1])\n",
    "    S1 = sigmoid(Z1)\n",
    "    diff_Z = cross_entropy_loss_double_layer(S1, Y, weights, bias)\n",
    "    diff_B3 = 1 / Y.shape[0] * np.sum(diff_Z, axis=0, keepdims=True)\n",
    "    diff_W3 = 1 / Y.shape[0] * np.dot(S1.T, diff_Z)\n",
    "    diff_S1 = np.dot(diff_Z, np.asarray(weights)[2].T)\n",
    "    diff_Z1 = diff_S1 * derivation_sigmoid(S1)\n",
    "    #One more Layer\n",
    "    diff_B2 = 1 / Y.shape[0] * np.sum(diff_Z1, axis=0, keepdims=True)\n",
    "    diff_W2 = 1 / Y.shape[0] * np.dot(S.T, diff_Z1)\n",
    "    diff_S = np.dot(diff_Z1, np.asarray(weights)[1].T)\n",
    "    diff_Z2 = diff_S * derivation_sigmoid(S)\n",
    "    # One more Layer\n",
    "    diff_B1 = 1 / Y.shape[0] * np.sum(diff_Z2, axis=0, keepdims=True)\n",
    "    diff_W1 = 1 / Y.shape[0] * np.dot(X.T, diff_Z2)\n",
    "    #Return\n",
    "    bias = np.asarray(bias)\n",
    "    bias[0] += -lr * diff_B1\n",
    "    bias[1] += -lr * diff_B2\n",
    "    bias[2] += -lr * diff_B3\n",
    "    weights = np.asarray(weights)\n",
    "    weights[0] += - lr * diff_W1\n",
    "    weights[1] += - lr * diff_W2\n",
    "    weights[2] += - lr * diff_W3\n",
    "    return weights.tolist(), bias.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST CODE (IF NOTHING HAPPENS, IT MEANS EVERTHING IS GOOD!)\n",
    "# AUTOLAB_IGNORE_START\n",
    "# def test_problem_4():\n",
    "#     STARTING_WEIGHTS_PATH = \"fixtures/start_weights_problem4\"\n",
    "#     ENDING_WEIGHTS_PATH = \"fixtures/final_weights_problem4\"\n",
    "#     STARTING_BIAS_PATH = \"fixtures/start_bias_problem4\"\n",
    "#     ENDING_BIAS_PATH = \"fixtures/final_bias_problem4\"\n",
    "#     PARAMS_PATH = \"fixtures/problem4.params.json\"\n",
    "\n",
    "#     params = json.loads(open(PARAMS_PATH, \"r\").read())\n",
    "\n",
    "#     (X, Y)= main.load_data(\"data/digitstrain.txt\")\n",
    "#     inputWeights = pickle.load(open(STARTING_WEIGHTS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "#     finalWeights = pickle.load(open(ENDING_WEIGHTS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "\n",
    "#     inputBias = pickle.load(open(STARTING_BIAS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "#     finalBias = pickle.load(open(ENDING_BIAS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "\n",
    "#     weightsToTest, biasToTest = main.update_weights_double_layer(X, Y, inputWeights, inputBias, float(params[\"LEARNING_RATE\"]))\n",
    "\n",
    "#     assert(isAllClose(finalWeights, weightsToTest))\n",
    "#     assert(isAllClose(finalBias, biasToTest))\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 5: Different Activations Functions With Implementation Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend your code from problem 4 to implement different activations functions which will be passed as a parameter and implement momentum with your gradient descent. In this problem all activations (except the final layer which should remain a softmax) must be changed to the passed activation function. Also, The momentum value will be passed as a parameter. Your function should perform “epoch” number of epochs and return the resulting weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample\n",
    "\n",
    "# def update_weights_double_layer_act_mom(X, Y, weights, bias, lr, activation, momentum, epochs): \n",
    "#     #INSERT YOUR CODE HERE\n",
    "#     if activation == 'sigmoid':\n",
    "#         #INSERT YOUR CODE HERE\n",
    "#     if activation == 'tanh':\n",
    "#         #INSERT YOUR CODE HERE\n",
    "#     if activation == 'relu':\n",
    "#         #INSERT YOUR CODE HERE\n",
    "#     #INSERT YOUR CODE HERE\n",
    "#     return updated_weights, updated_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tanh activations for forward\n",
    "def tanh(x):\n",
    "    return np.sinh(x)/np.cosh(x)\n",
    "\n",
    "#tanh activations for backward\n",
    "def derivation_tanh(x):\n",
    "    return 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "#relu activations for forward\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "#relu activations for backward\n",
    "def derivation_relu(x):\n",
    "        return np.greater(x, 0).astype(int)\n",
    "\n",
    "def update_weights_double_layer_act_mom(X, Y, weights, bias, lr, activation, momentum, epochs):\n",
    "    deltaW1 = 0\n",
    "    deltaW2 = 0\n",
    "    deltaW3 = 0\n",
    "    for i in range(epochs):\n",
    "\n",
    "        if activation == 'sigmoid':\n",
    "            Z = np.add(np.dot(X, np.asarray(weights)[0]), np.asarray(bias)[0])\n",
    "            S = sigmoid(Z)\n",
    "            Z1 = np.add(np.dot(S, np.asarray(weights)[1]), np.asarray(bias)[1])\n",
    "            S1 = sigmoid(Z1)\n",
    "            diff_Z = cross_entropy_loss_double_layer(S1, Y, weights, bias)\n",
    "            diff_B3 = 1 / Y.shape[0] * np.sum(diff_Z, axis=0, keepdims=True)\n",
    "            diff_W3 = 1 / Y.shape[0] * np.dot(S1.T, diff_Z)\n",
    "            diff_S1 = np.dot(diff_Z, np.asarray(weights)[2].T)\n",
    "            diff_Z1 = diff_S1 * derivation_sigmoid(S1)\n",
    "            # One more Layer\n",
    "            diff_B2 = 1 / Y.shape[0] * np.sum(diff_Z1, axis=0, keepdims=True)\n",
    "            diff_W2 = 1 / Y.shape[0] * np.dot(S.T, diff_Z1)\n",
    "            diff_S = np.dot(diff_Z1, np.asarray(weights)[1].T)\n",
    "            diff_Z2 = diff_S * derivation_sigmoid(S)\n",
    "            # One more Layer\n",
    "            diff_B1 = 1 / Y.shape[0] * np.sum(diff_Z2, axis=0, keepdims=True)\n",
    "            diff_W1 = 1 / Y.shape[0] * np.dot(X.T, diff_Z2)\n",
    "            # Return\n",
    "            bias = np.asarray(bias)\n",
    "            bias[0] += -lr * diff_B1\n",
    "            bias[1] += -lr * diff_B2\n",
    "            bias[2] += -lr * diff_B3\n",
    "            weights = np.asarray(weights)\n",
    "            deltaW1 = momentum * deltaW1 - lr * diff_W1\n",
    "            weights[0] += deltaW1\n",
    "            deltaW2 = momentum * deltaW2 - lr * diff_W2\n",
    "            weights[1] += deltaW2\n",
    "            deltaW3 = momentum * deltaW3 - lr * diff_W3\n",
    "            weights[2] += deltaW3\n",
    "\n",
    "        if activation == 'tanh':\n",
    "            Z = np.add(np.dot(X, np.asarray(weights)[0]), np.asarray(bias)[0])\n",
    "            S = tanh(Z)\n",
    "            Z1 = np.add(np.dot(S, np.asarray(weights)[1]), np.asarray(bias)[1])\n",
    "            S1 = tanh(Z1)\n",
    "            diff_Z = cross_entropy_loss_double_layer(S1, Y, weights, bias)\n",
    "            diff_B3 = 1 / Y.shape[0] * np.sum(diff_Z, axis=0, keepdims=True)\n",
    "            diff_W3 = 1 / Y.shape[0] * np.dot(S1.T, diff_Z)\n",
    "            diff_S1 = np.dot(diff_Z, np.asarray(weights)[2].T)\n",
    "            diff_Z1 = diff_S1 * derivation_tanh(S1)\n",
    "            # One more Layer\n",
    "            diff_B2 = 1 / Y.shape[0] * np.sum(diff_Z1, axis=0, keepdims=True)\n",
    "            diff_W2 = 1 / Y.shape[0] * np.dot(S.T, diff_Z1)\n",
    "            diff_S = np.dot(diff_Z1, np.asarray(weights)[1].T)\n",
    "            diff_Z2 = diff_S * derivation_tanh(S)\n",
    "            # One more Layer\n",
    "            diff_B1 = 1 / Y.shape[0] * np.sum(diff_Z2, axis=0, keepdims=True)\n",
    "            diff_W1 = 1 / Y.shape[0] * np.dot(X.T, diff_Z2)\n",
    "            # Return\n",
    "            bias = np.asarray(bias)\n",
    "            bias[0] += -lr * diff_B1\n",
    "            bias[1] += -lr * diff_B2\n",
    "            bias[2] += -lr * diff_B3\n",
    "            weights = np.asarray(weights)\n",
    "            deltaW1 = momentum * deltaW1 - lr * diff_W1\n",
    "            weights[0] += deltaW1\n",
    "            deltaW2 = momentum * deltaW2 - lr * diff_W2\n",
    "            weights[1] += deltaW2\n",
    "            deltaW3 = momentum * deltaW3 - lr * diff_W3\n",
    "            weights[2] += deltaW3\n",
    "\n",
    "        if activation == 'relu':\n",
    "            Z = np.add(np.dot(X, np.asarray(weights)[0]), np.asarray(bias)[0])\n",
    "            S = relu(Z)\n",
    "            Z1 = np.add(np.dot(S, np.asarray(weights)[1]), np.asarray(bias)[1])\n",
    "            S1 = relu(Z1)\n",
    "            diff_Z = cross_entropy_loss_double_layer(S1, Y, weights, bias)\n",
    "            diff_B3 = 1 / Y.shape[0] * np.sum(diff_Z, axis=0, keepdims=True)\n",
    "            diff_W3 = 1 / Y.shape[0] * np.dot(S1.T, diff_Z)\n",
    "            diff_S1 = np.dot(diff_Z, np.asarray(weights)[2].T)\n",
    "            diff_Z1 = diff_S1 * derivation_relu(S1)\n",
    "            # One more Layer\n",
    "            diff_B2 = 1 / Y.shape[0] * np.sum(diff_Z1, axis=0, keepdims=True)\n",
    "            diff_W2 = 1 / Y.shape[0] * np.dot(S.T, diff_Z1)\n",
    "            diff_S = np.dot(diff_Z1, np.asarray(weights)[1].T)\n",
    "            diff_Z2 = diff_S * derivation_relu(S)\n",
    "            # One more Layer\n",
    "            diff_B1 = 1 / Y.shape[0] * np.sum(diff_Z2, axis=0, keepdims=True)\n",
    "            diff_W1 = 1 / Y.shape[0] * np.dot(X.T, diff_Z2)\n",
    "            # Return\n",
    "            bias = np.asarray(bias)\n",
    "            bias[0] += -lr * diff_B1\n",
    "            bias[1] += -lr * diff_B2\n",
    "            bias[2] += -lr * diff_B3\n",
    "            weights = np.asarray(weights)\n",
    "            deltaW1 = momentum * deltaW1 - lr * diff_W1\n",
    "            weights[0] += deltaW1\n",
    "            deltaW2 = momentum * deltaW2 - lr * diff_W2\n",
    "            weights[1] += deltaW2\n",
    "            deltaW3 = momentum * deltaW3 - lr * diff_W3\n",
    "            weights[2] += deltaW3\n",
    "\n",
    "    return weights.tolist(), bias.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST CODE (IF NOTHING HAPPENS, IT MEANS EVERTHING IS GOOD!)\n",
    "# AUTOLAB_IGNORE_START\n",
    "\n",
    "# def test_problem_6():\n",
    "#     STARTING_WEIGHTS_PATH = \"fixtures/start_weights_problem6\"\n",
    "#     ENDING_WEIGHTS_PATH = \"fixtures/final_weights_problem6\"\n",
    "#     STARTING_BIAS_PATH = \"fixtures/start_bias_problem6\"\n",
    "#     ENDING_BIAS_PATH = \"fixtures/final_bias_problem6\"\n",
    "#     PARAMS_PATH = \"fixtures/problem6.params.json\"\n",
    "\n",
    "#     params = json.loads(open(PARAMS_PATH, \"r\").read())\n",
    "\n",
    "#     (X, Y)= main.load_data(\"data/digitstrain.txt\")\n",
    "#     inputWeights = pickle.load(open(STARTING_WEIGHTS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "#     finalWeights = pickle.load(open(ENDING_WEIGHTS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "\n",
    "#     inputBias = pickle.load(open(STARTING_BIAS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "#     finalBias = pickle.load(open(ENDING_BIAS_PATH, \"rb\"), **PICKLE_KWARGS)\n",
    "\n",
    "#     weightsToTest, biasToTest = main.update_weights_double_layer_act_mom(X, Y, inputWeights, inputBias, float(params[\"LEARNING_RATE\"]), params[\"ACTIVATION\"], float(params[\"MOMENTUM\"]), int(params[\"EPOCH_COUNT\"]))\n",
    "\n",
    "#     assert(isAllClose(finalWeights, weightsToTest))\n",
    "#     assert(isAllClose(finalBias, biasToTest))\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial highlighted just a few elements MLPs and a typical sample of MLPs. Much more detail about the algorithm and questions on MLPs is general are available from the following links (References).\n",
    "\n",
    "[1] Bengio, Y.; Courville, A.; Vincent, P. (2013). \"Representation Learning: A Review and New Perspectives\". IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (8): 1798–1828. arXiv:1206.5538 . doi:10.1109/tpami.2013.50\n",
    "\n",
    "[2] Schmidhuber, J. (2015). \"Deep Learning in Neural Networks: An Overview\". Neural Networks. 61: 85–117. arXiv:1404.7828 . doi:10.1016/j.neunet.2014.09.003. PMID 25462637\n",
    "\n",
    "[3] Olshausen, B. A. (1996). \"Emergence of simple-cell receptive field properties by learning a sparse code for natural images\". Nature. 381 (6583): 607–609. Bibcode:1996Natur.381..607O. doi:10.1038/381607a0. PMID 8637596.\n",
    "\n",
    "[4] Rosenblatt, Frank. x. “Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms”. Spartan Books, Washington DC, 1961.\n",
    "\n",
    "[5] Rumelhart, David E., Geoffrey E. Hinton, and R. J. Williams. \"Learning Internal Representations by Error Propagation\". David E. Rumelhart, James L. McClelland, and the PDP research group. (editors), Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundation. MIT Press, 1986\n",
    "\n",
    "[6] Cybenko, G. 1989. “Approximation by superpositions of a sigmoidal function Mathematics of Control, Signals, and Systems” , 2(4), 303–314. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
