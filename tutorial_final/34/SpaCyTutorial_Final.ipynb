{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with spaCy\n",
    "### Introduction\n",
    "\n",
    "In this tutorial we will explore some of the really cool problems that Natural Language Processing tackles. We will do this by exploring the spaCy library and seeing some of the interesting things that it can do for us in our attempt to get a computer to process text in different ways. spaCy fills a gap that NLTK misses through various design descisions. Unlike NLTK, spaCy isn't aimed towards research so it makes different  uses Cython in order to speed up processes such as word tokenization and Part-of-Speech tagging. It also builds a syntactic tree for each sentence for better understanding. But as we shall see, there are many other things that this library lets us do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "\n",
    "In this tutorial, we will explore basic Natural Language Processing with [spaCy](https://spacy.io/).\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- [Installing the libraries](#Installing-the-Libraries)\n",
    "- [Creating a spaCy Doc](#Creating-a-spaCy-Doc)\n",
    "- [Looking at Tokens](#Looking-at-Tokens)\n",
    "- [Visualizers](#Visualizers)\n",
    "- [Similarities](#Similarities)\n",
    "- [Training](#Training)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intalling the Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to begin using spaCy, we can download the source packages using pip:\n",
    "    \n",
    "    $ pip install -U spacy\n",
    "    \n",
    "We can also install with conda using the following command:\n",
    "\n",
    "    $ conda install -c conda-forge spacy\n",
    "\n",
    "After finishing the installation, we now need to download the model for English.\n",
    "\n",
    "    $ python -m spacy download en\n",
    "\n",
    "spaCy also supports other languages and models that have been trained on different inputs that you can find [here](https://spacy.io/models/#available-models).\n",
    "\n",
    "After installing spaCy, make sure the following works for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have installed the library!\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the tokenizer, tagger, parser, Named Entity Recognition, and word vectors for English\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Process the sentence into a spacy document object\n",
    "doc = nlp(u'You have installed the library!')\n",
    "\n",
    "print(doc)\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also install the Wikipedia API so we can work with the articles there.\n",
    "\n",
    "    $ pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a spaCy Doc\n",
    "We'll now look at what we can do with a document in spaCy. We'll begin by using the Wikipedia API to get an article about Alan Turing. We can use the `page` command to get a `WikipediaPage` object. From this, we can pass the plain text of the article which will be easier for spaCy to work with.\n",
    "\n",
    "We then let the spaCy model for English process the document so that a syntactic tree is built. The default pipeline uses the following components as described in the picture below:\n",
    "[<img src=\"https://spacy.io/assets/img/pipeline.svg\">](https://spacy.io/assets/img/pipeline.svg)\n",
    "1. Tokenizer - Splits the document into meaningful pieces according to the Penn Treebank standard. spaCy assumes no mult-word tokens and allows merging after.\n",
    "2. Tagger - Gives Part of Speech tags to each token to give purpose to each token.\n",
    "3. Parser - Assigns dependencies between tokens.\n",
    "4. Named Entity Recognizer - Detects and labels named entities.\n",
    "\n",
    "spaCy also allows for [custom components](https://spacy.io/usage/processing-pipelines#section-custom-components) that you can also add to the pipeline, but for now we'll stick with the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "# Get the WikipediaPage article\n",
    "article_wiki = wikipedia.page('Alan Turing')\n",
    "\n",
    "# Process the article\n",
    "article_parsed = nlp(article_wiki.content)\n",
    "print(type(article_parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through words:  [Alan, Mathison, Turing,  , (;, 23, June, 1912, –, 7, June, 1954, ), was, an]\n",
      "Iterating through sentences:  [Alan Mathison Turing  , (; 23 June 1912 – 7 June 1954) was an English computer scientist, mathematician, logician, cryptanalyst, philosopher, and theoretical biologist.\n",
      ", Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general purpose computer., Turing is widely considered to be the father of theoretical computer science and artificial intelligence.\n",
      ", During the Second World War, Turing worked for the Government Code and Cypher School, (GC&CS) at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence., For a time he led Hut 8, the section which was responsible for German naval cryptanalysis., Here he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bombe method, an electromechanical machine that could find settings for the Enigma machine., Turing played a pivotal role in cracking intercepted coded messages that enabled the Allies to defeat the Nazis in many crucial engagements, including the Battle of the Atlantic, and in so doing helped win the war., Counterfactual history is difficult with respect to the effect Ultra intelligence had on the length of the war, but at the upper end it has been estimated that this work shortened the war in Europe by more than two years and saved over fourteen million lives.\n",
      ", After the war, Turing worked at the National Physical Laboratory, where he designed the ACE, among the first designs for a stored-program computer., In 1948 Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers and became interested in mathematical biology., He wrote a paper on the chemical basis of morphogenesis, and predicted oscillating chemical reactions such as the Belousov–, Zhabotinsky reaction, first observed in the 1960s.\n",
      ", Turing was prosecuted in 1952 for homosexual acts, when by the Labouchere Amendment, \"gross indecency\" was criminal in the UK.]\n"
     ]
    }
   ],
   "source": [
    "# Prints out some of the prcoessed tokens and preceived sentences.\n",
    "sents = article_parsed.sents\n",
    "wordlist = [] # <class 'spacy.tokens.token.Token'>\n",
    "sentlist = [] # <class 'spacy.tokens.span.Span'>\n",
    "\n",
    "for i in range(15):\n",
    "    wordlist.append(article_parsed[i])\n",
    "    sentlist.append(next(sents))\n",
    "\n",
    "print(\"Iterating through words: \", wordlist)\n",
    "print(\"Iterating through sentences: \", sentlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this whole preprocessing step as well as gaining of understanding of each token is very simple. In fact it takes much less lines of code and [much more efficient](https://spacy.io/usage/facts-figures) for most basic NLP tasks compared to other popular libraries such as NLTK and Stanford's CoreNLP with [comparable accuracy](https://spacy.io/usage/facts-figures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Tokens\n",
    "\n",
    "Now let's look at the different information that each of our Token objects have. The [Token](https://spacy.io/api/token) object is some word, symbol, etc. Let's look at a sample sentence and see the different information that we can get from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\tLemma\tPart of Speech\tPOS Tag\tDependency\tShape\tIn Stop Word List?\t\n",
      "Jimmy\tjimmy\tPROPN\t\tNNP\tnsubj\t\tXxxxx\tFalse\t\n",
      "wanted\twant\tVERB\t\tVBD\tROOT\t\txxxx\tFalse\t\n",
      "to\tto\tPART\t\tTO\taux\t\txx\tTrue\t\n",
      "throw\tthrow\tVERB\t\tVB\txcomp\t\txxxx\tFalse\t\n",
      "a\ta\tDET\t\tDT\tdet\t\tx\tTrue\t\n",
      "rock\trock\tNOUN\t\tNN\tdobj\t\txxxx\tFalse\t\n",
      "into\tinto\tADP\t\tIN\tprep\t\txxxx\tTrue\t\n",
      "the\tthe\tDET\t\tDT\tdet\t\txxx\tTrue\t\n",
      "pond\tpond\tNOUN\t\tNN\tpobj\t\txxxx\tFalse\t\n",
      ",\t,\tPUNCT\t\t,\tpunct\t\t,\tFalse\t\n",
      "but\tbut\tCCONJ\t\tCC\tcc\t\txxx\tTrue\t\n",
      "he\t-PRON-\tPRON\t\tPRP\tnsubj\t\txx\tTrue\t\n",
      "did\tdo\tVERB\t\tVBD\taux\t\txxx\tTrue\t\n",
      "n't\tnot\tADV\t\tRB\tneg\t\tx'x\tFalse\t\n",
      "have\thave\tVERB\t\tVB\tconj\t\txxxx\tTrue\t\n",
      "any\tany\tDET\t\tDT\tdobj\t\txxx\tTrue\t\n",
      ".\t.\tPUNCT\t\t.\tpunct\t\t.\tFalse\t\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Jimmy wanted to throw a rock into the pond, but he didn\\'t have any.')\n",
    "\n",
    "print('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t'.format('Token', 'Lemma', 'Part of Speech', 'POS Tag', 'Dependency', 'Shape', 'In Stop Word List?'))\n",
    "for token in doc:\n",
    "    print('{}\\t{}\\t{}\\t\\t{}\\t{}\\t\\t{}\\t{}\\t'.format(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy gives us a lot of information to understand what purpose each word has. It gives us a trained model to work with, that can help classify each token in one of many ways. \n",
    "From the table of tokens, we see that different words have been lemmatized, or getting the base form of the word. In addition, we have Parts of Speech, with varying specificities as well as dependencies, shapes, etc. In the code above, we only looked at a few of the many features that are available to us more can be found in the documentation of the [Token](https://spacy.io/api/token) class. However, just this information itself can take us a long way in working with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizers\n",
    "spaCy also supports dependency and entity visualization through displaCy. We demonstrate both below with the `style` argument. displaCy normally can be visualized using the `serve` function, but since we are using Jupyter, we want to use `render`, which returns markup that is rendered in the cell immediately and is easier to export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"890\" height=\"317.0\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">have</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">n't</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">eaten</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">anything</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">since</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">yesterday.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,182.0 C70,2.0 410.0,2.0 410.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,184.0 L62,172.0 78,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M190,182.0 C190,62.0 405.0,62.0 405.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M190,184.0 L182,172.0 198,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M310,182.0 C310,122.0 400.0,122.0 400.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310,184.0 L302,172.0 318,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M430,182.0 C430,122.0 520.0,122.0 520.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M520.0,184.0 L528.0,172.0 512.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M430,182.0 C430,62.0 645.0,62.0 645.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M645.0,184.0 L653.0,172.0 637.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M670,182.0 C670,122.0 760.0,122.0 760.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M760.0,184.0 L768.0,172.0 752.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity         Label     Explanation of Label\n",
      "I              nsubj     nominal subject\n",
      "have           aux       auxiliary\n",
      "n't            neg       negation modifier\n",
      "eaten          ROOT      None\n",
      "anything       dobj      direct object\n",
      "since          prep      prepositional modifier\n",
      "yesterday      pobj      object of preposition\n",
      ".              punct     punctuation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"800\" height=\"287.0\" style=\"max-width: none; height: 287.0px; color: blue; background: #d3d9e2; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">My</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">glass</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">half</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">full.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M62,152.0 62,127.0 197.0,127.0 197.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,154.0 L58,146.0 66,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M212,152.0 212,127.0 347.0,127.0 347.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M212,154.0 L208,146.0 216,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M512,152.0 512,127.0 647.0,127.0 647.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M512,154.0 L508,146.0 516,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M362,152.0 362,102.0 650.0,102.0 650.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M650.0,154.0 L654.0,146.0 646.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Visualize dependencies\n",
    "doc1 = nlp(u'I haven\\'t eaten anything since yesterday.')\n",
    "displacy.render(doc1, style='dep', jupyter=True, options={'distance': 120})\n",
    "\n",
    "# Explain what each dependency means\n",
    "print(\"{:<15}{:<10}{}\".format(\"Entity\", \"Label\", \"Explanation of Label\"))\n",
    "for tok in doc1:\n",
    "    print(\"{:<15}{:<10}{}\".format(tok.text, tok.dep_, spacy.explain(tok.dep_)))\n",
    "\n",
    "# We can also customize our visualizations\n",
    "doc2 = nlp(u'My glass is half full.')\n",
    "options2 = {'compact': True, 'bg': '#d3d9e2', 'color': 'blue', 'font': 'Arial'}\n",
    "displacy.render(doc2, style='dep', jupyter=True, options=options2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above visualization, we are able to view both part of speech information as well as how the different tokens depend on each other, which is very informative when using algorithms to deal with words. For example, a computer could look at this information and see that the word eaten is negated or that I is the nominal subject that the verb eaten is referring to. This can make it easier for a computer to figure out what the sentence means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general purpose computer. Turing is widely considered to be the father of theoretical computer science and artificial intelligence. During \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the Second World War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       ", Turing worked for \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the Government Code\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Cypher School\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", (\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    GC&CS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ") at \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Bletchley Park\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Britain\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "'s codebreaking centre that produced \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #d142f4, #f49b41); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Ultra\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " intelligence. For a time he led Hut 8, the section which was responsible for \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #d142f4, #f49b41); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    German\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " naval cryptanalysis. Here he devised a number of techniques for speeding the breaking of \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #d142f4, #f49b41); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    German\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " ciphers, including improvements to the pre-war \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #d142f4, #f49b41); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Polish\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " bombe method, an electromechanical machine that could find settings for the \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Enigma\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " machine. Turing played a pivotal role in cracking intercepted coded messages that enabled the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Allies\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " to defeat the \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #d142f4, #f49b41); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Nazis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " in many crucial engagements, including \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the Battle of the Atlantic\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       ", and in so doing helped win the war.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity                        Label\tExplanation of Label\n",
      "the Second World War          EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
      "the Government Code           EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
      "Cypher School                 ORG\tCompanies, agencies, institutions, etc.\n",
      "GC&CS                         PERSON\tPeople, including fictional\n",
      "Bletchley Park                GPE\tCountries, cities, states\n",
      "Britain                       GPE\tCountries, cities, states\n",
      "Ultra                         NORP\tNationalities or religious or political groups\n",
      "German                        NORP\tNationalities or religious or political groups\n",
      "German                        NORP\tNationalities or religious or political groups\n",
      "Polish                        NORP\tNationalities or religious or political groups\n",
      "Enigma                        PRODUCT\tObjects, vehicles, foods, etc. (not services)\n",
      "Allies                        ORG\tCompanies, agencies, institutions, etc.\n",
      "Nazis                         NORP\tNationalities or religious or political groups\n",
      "the Battle of the Atlantic    EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n"
     ]
    }
   ],
   "source": [
    "# Visualize Named entities\n",
    "rawdoc2 = \"\"\"Turing was highly influential in the development of theoretical computer science, \n",
    "providing a formalisation of the concepts of algorithm and computation with the Turing machine, \n",
    "which can be considered a model of a general purpose computer. Turing is widely considered to \n",
    "be the father of theoretical computer science and artificial intelligence. During the Second \n",
    "World War, Turing worked for the Government Code and Cypher School, (GC&CS) at Bletchley Park, \n",
    "Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the \n",
    "section which was responsible for German naval cryptanalysis. Here he devised a number of \n",
    "techniques for speeding the breaking of German ciphers, including improvements to the pre-war \n",
    "Polish bombe method, an electromechanical machine that could find settings for the Enigma \n",
    "machine. Turing played a pivotal role in cracking intercepted coded messages that enabled the \n",
    "Allies to defeat the Nazis in many crucial engagements, including the Battle of the Atlantic, \n",
    "and in so doing helped win the war.\n",
    "\"\"\".replace('\\n','')\n",
    "doc2 = nlp(rawdoc2)\n",
    "\n",
    "# Colors for certain labels\n",
    "colors2 = {'NORP': 'linear-gradient(90deg, #d142f4, #f49b41)'}\n",
    "# Options list, setting ents to None means all entities are highlighted\n",
    "options2 = {'ents': None, 'colors': colors2}\n",
    "\n",
    "displacy.render(doc2, style='ent', jupyter=True, options=options2)\n",
    "\n",
    "# Explain what each label means\n",
    "print(\"{:<30}{}\\t{}\".format(\"Entity\", \"Label\", \"Explanation of Label\"))\n",
    "for ent in doc2.ents:\n",
    "    print(\"{:<30}{}\\t{}\".format(ent.text, ent.label_, spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are different labels that are given that help explain what certain entities are. However, not all of them are necessarily accurate, as we can see by the label of 'GC&CS' as a `PERSON`. However, there is a pretty good accuracy, and it does help to have information like this in figuring out different facts from sentences that a computer can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities\n",
    "\n",
    "We can use spaCy's models to compare different objects. This allows us to do things such as finding similar sentences. The can be done with any `Doc`, `Span`, or `Token` object using the `similarity` function that calculates a semantic similarity estimate with a cosine similarity. We can see some examples below. We'll also load a larger vocabulary so that we have better estimates using the following command\n",
    "\n",
    "    $ python -m spacy download en_core_web_md\n",
    "    \n",
    "And then load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlplg = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at some words and their similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between dog and cat:  0.8016853893732596\n",
      "Similarity between dog and lion:  0.4742449314321914\n",
      "Similarity between dog and truck:  0.355217429684785\n",
      "Similarity between cat and lion:  0.5265437205262408\n"
     ]
    }
   ],
   "source": [
    "dog = nlplg('dog')\n",
    "cat = nlplg('cat')\n",
    "lion = nlplg('lion')\n",
    "truck = nlplg('truck')\n",
    "print(\"Similarity between dog and cat: \", dog.similarity(cat))\n",
    "print(\"Similarity between dog and lion: \", dog.similarity(lion))\n",
    "print(\"Similarity between dog and truck: \", dog.similarity(truck))\n",
    "print(\"Similarity between cat and lion: \", cat.similarity(lion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the numbers, this model is pretty good at seeing similar features between words. The words \"dog\" and \"cat\" are pretty similar in that they are both pets. However, dogs and lions are less similar, with dogs and trucks even less so. Cats and lions are also slightly more related, as they are both feline. These simple associations that we take for granted as humans can be trained into the spaCy model and can be used to let computers understand these relation concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare documents and bigger pieces of text. For example, the the similarity function is useful in helping with information retrieval. For example, we can compare similarities between words and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with doc1:  0.34316940126082063\n",
      "Similarity with doc2:  0.3901161487616457\n",
      "Similarity with doc3:  0.31983017325749513\n"
     ]
    }
   ],
   "source": [
    "question = nlplg('European')\n",
    "doc1 = nlplg(wikipedia.page('Lego').content)\n",
    "doc2 = nlplg(wikipedia.page('Greece').content)\n",
    "doc3 = nlplg(wikipedia.page('Bicycle').content)\n",
    "print(\"Similarity with doc1: \", question.similarity(doc1))\n",
    "print(\"Similarity with doc2: \", question.similarity(doc2))\n",
    "print(\"Similarity with doc3: \", question.similarity(doc3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from this code, European is most similar to the article about Greece, and even though we have only a word to go with, there is a much bigger similarity with the word European. With a little more effort, this can be expanded to be a stand alone search tool that can search for documents then search inside them for the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how different words are related. To start, we'll download the English vector information with the following command\n",
    "\n",
    "    $ python -m spacy download en_vectors_web_lg\n",
    "\n",
    "Now we can load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlpvec = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['obama', 'barack', 'mccain', 'clinton', 'hillary', 'palin', 'biden', 'gop', 'democrats', 'republicans', 'democrat', 'america', 'cnn', 'pelosi', 'republican', 'osama', 'iraq', 'reagan', 'george', 'congress']\n"
     ]
    }
   ],
   "source": [
    "# Uses the similarity function to find the words most related to the given word\n",
    "def most_similar_words(aword):\n",
    "    queries = [w for w in aword.vocab if w.is_lower == aword.is_lower and w.prob >= -15]\n",
    "    simrank = sorted(queries, key=lambda w: aword.similarity(w), reverse=True)\n",
    "    simset = set()\n",
    "    topsims = []\n",
    "    i = 0\n",
    "    while len(topsims) < 20:\n",
    "        curword = simrank[i].lower_\n",
    "        if curword not in simset:\n",
    "            topsims.append(curword)\n",
    "            simset.add(curword)\n",
    "        i+=1\n",
    "    return topsims\n",
    "\n",
    "print(most_similar_words(nlpvec.vocab[u'Obama']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even cooler feature that we get straight out of the box with spaCy is vector representation of words. What this means is that each word is given a multi-dimensional vector representation of its meaning, which can be created with algorithms such as [word2vec](https://en.wikipedia.org/wiki/Word2vec). We can use this information to help understand similarities and differences between different words and concepts. Here we use the cosine similarity to compare the vector embeddings of each word that can be gotten using the `vector` attribute of each `Token`. We use this to see what different combinations of words give us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman =  queen\n",
      "king - man + boy =  prince\n"
     ]
    }
   ],
   "source": [
    "# Define a cosine similarity function\n",
    "import numpy as np\n",
    "cosine = lambda v1, v2: np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Define a vector similarity function\n",
    "def most_similar_vecs(invec, usedwords):\n",
    "    aword = nlplg('vocabulary')\n",
    "    queries = [w for w in aword.vocab if w.prob >= -15 and w.has_vector and w.lower_ not in usedwords]\n",
    "    simrank = sorted(queries, key=lambda w: cosine(invec, w.vector), reverse=True)\n",
    "    return simrank[0]\n",
    "\n",
    "# Example 1: king - man + woman = queen \n",
    "king = nlplg('king')\n",
    "man = nlplg('man')\n",
    "woman = nlplg('woman')\n",
    "combo = king.vector - man.vector + woman.vector\n",
    "print(\"king - man + woman = \", most_similar_vecs(combo, ['king','man','woman']).lower_)\n",
    "\n",
    "# Example 2: king - man + boy = prince\n",
    "king = nlplg('king')\n",
    "man = nlplg('man')\n",
    "boy = nlplg('boy')\n",
    "combo = king.vector - man.vector + boy.vector\n",
    "print(\"king - man + boy = \", most_similar_vecs(combo, ['king','man','boy']).lower_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really cool result, as we can do arithmetic on the different traits of each word, to gain more understanding about the word itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "spaCy is extremely flexible, so we can easily train new things or add them into our existing models. As an example, here's a common way of training a new Entity, which can be found [here](https://spacy.io/usage/examples#new-entity-type), a simpler one of which, is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses:  29.8425466609112\n",
      "losses:  19.768894641538864\n",
      "losses:  0.6083590924211353\n",
      "losses:  1.2559197037586947\n",
      "losses:  1.999995067574418\n",
      "losses:  6.938959127561024e-06\n",
      "losses:  7.774443399531807e-10\n",
      "losses:  0.7661760759705846\n",
      "losses:  1.4613375916554925\n",
      "losses:  5.808917630956588e-07\n",
      "losses:  0.011225481546843802\n",
      "losses:  8.21031993301105e-06\n",
      "losses:  1.8904445171356201\n",
      "losses:  2.2261234293325513e-16\n",
      "losses:  1.5845715999603278\n",
      "losses:  1.649967823160589e-20\n",
      "losses:  4.826274426037046e-16\n",
      "losses:  9.086029890861538e-14\n",
      "losses:  1.37945756468473e-15\n",
      "losses:  3.7093330535683384e-14\n",
      "Entities in 'Do you like burgers?'\n",
      "FOOD burgers\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# New entity label\n",
    "LABEL = 'FOOD'\n",
    "\n",
    "# Training data\n",
    "TRAIN_DATA = [\n",
    "    (\"Burgers are very tasty.\", {\n",
    "        'entities': [(0, 7, 'FOOD')]\n",
    "    }),\n",
    "    (\"Burgers are very tasty and they are great for your bulk.\", {\n",
    "        'entities': [(0, 7, 'FOOD')]\n",
    "    }),\n",
    "    (\"Do I eat them?\", {\n",
    "        'entities': []\n",
    "    }),\n",
    "    (\"I like burgers. Do you?\", {\n",
    "        'entities': [(7, 14, 'FOOD')]\n",
    "    }),\n",
    "    (\"burgers are goood\", {\n",
    "        'entities': [(0, 7, 'FOOD')]\n",
    "    }),\n",
    "    (\"yum they are tasty\", {\n",
    "        'entities': []\n",
    "    }),\n",
    "    (\"how would I survive without burgers\", {\n",
    "        'entities': [(28, 35, 'FOOD')]\n",
    "    }),\n",
    "    (\"burgers!\", {\n",
    "        'entities': [(0, 7, 'FOOD')]\n",
    "    })\n",
    "]\n",
    "\n",
    "def train(model=None, new_model_name='animal', output_dir=None, n_iter=20):\n",
    "    \"\"\"Creates an entity recognizer and trains the entity.\"\"\"\n",
    "    \n",
    "    # Create blank model\n",
    "    nlp = spacy.blank('en')\n",
    "\n",
    "    # Add entity recognizer to the model\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner)\n",
    "    \n",
    "    # Add new entity label\n",
    "    ner.add_label(LABEL)   \n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    # Disable other pipes during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(\"losses: \", losses['ner'])\n",
    "\n",
    "    return nlp\n",
    "\n",
    "entrec = train()\n",
    "# test the trained model\n",
    "test_text = 'Do you like burgers?'\n",
    "doc = entrec(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Overall, Natural Language Processing is a very import part of data science. It allows us to understand data through the realm of words instead of just numbers. spaCy provides us a lot of cool functionality and makes it much easier than other libraries to do a lot of the tasks that we would want, such as understanding text and doing preprocessing on it. Thus, spaCy is a very useful library to learn and will hopefully prove useful to you at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "More detail about the libraries can be found at the following links.\n",
    "\n",
    "1. spaCy: https://spacy.io/\n",
    "2. Wikipedia Python API: https://wikipedia.readthedocs.io/en/latest/\n",
    "3. The spaCy tokenizer - https://explosion.ai/blog/how-spacy-works"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
