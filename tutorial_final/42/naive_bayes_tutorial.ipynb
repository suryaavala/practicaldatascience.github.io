{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Classification Using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Classification** is the problem of identifying which category a new observation belongs in, based on a training set of data containing observations whose category membership we already know. As one of the most widely used areas of machine learning, classification plays a large role in our everyday lives behind the scenes with its wide variety of applications including but not limited to ad targeting, spam detection, medical diagnosis, and image classification. \n",
    "\n",
    "Any algorithm that implements classification is known as a **classifier**. Essentially, a classification model draws some conclusion from a prior history of values, and based on a new input, it will predict the value of the outcome. \n",
    "\n",
    "Without going over too much math or probability theory, in this tutorial, we will be introducing some of the concepts behind Naive Bayes before looking at a simple example. Then, we will classify some emails as *'spam' or 'not spam'*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Naive Bayes?\n",
    "\n",
    "A **Naive Bayes classifier** is a simple probabilistic classifier. This means it is a classifier that is able to predict a probability distribution over a set of possible results/classes, rather than just returning the single most likely class that a new observation should belong to. As you can probably tell, this classifier applies **Bayes' Theorem**, which states the following: $$ P(A\\,|\\,B) = \\frac{P(B\\,|\\,A) * P(A)}{P(B)} $$ This conditional probability model can essentially be seen as: $$ posterior = \\frac{prior * likelihood}{evidence} $$\n",
    "\n",
    "As aforementioned, we are essentially using prior knowledge with some observed data to make a new prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Naive Bayes - A Simple Example\n",
    "\n",
    "Now that you have some familiarity of the background behind Naive Bayes, let's dive into a simple example. All we have to do is use the simple formula above for each outcome. We look at the evidence, and consider how likely it is to be one class or another class, and assign a label. The class with the highest probability is the one we assign to that specific combination of features. For example, let us consider the following data:\n",
    "\n",
    "| | | Round | Not Round | Sweet | Not Sweet | Red | Not Red | $\\textbf{Total}$\n",
    "|------||------|------||------|------||------|------||------|------||------|\n",
    "| $\\textbf{Apple}$ ||   4000  | 1000|   3500  | 1500|   4500  | 500|   5000  | \n",
    "| $\\textbf{Banana}$ ||   0  | 3000|   1500  | 1500|   3000  | 0 |   3000  | \n",
    "| $\\textbf{Other}$ ||   1000  | 1000|   1500  | 500|   500  | 1500|   2000  | \n",
    "| $\\textbf{Total}$ ||   5000  | 5000|   6500  | 3500|   8000  | 2000 |   10000  | \n",
    "\n",
    "This is our **training set**. Here, we have 10,000 pieces of fruit. Here, we essentially have 3 **features** or **predictors**: whether or not the fruit is round, whether or not the fruit is sweet, and whether or not the fruit is red. We have 3 possible **classes/labels**: *Apple, Banana, or Other.* \n",
    "\n",
    "Let's say that we receive a new piece of fruit that is round, sweet, and red. We want to be able to classify what type of fruit the new piece is. If we follow the formulas mentioned in the previous section, we have the following:\n",
    "\n",
    "   $$ P(Apple \\,|\\, Round, Sweet, Red) = \\frac{P(Long\\,|\\,Apple) * P (Sweet\\,|\\,Apple) * P (Sweet\\,|\\,Apple) * P(Apple)}{P(Round) * P(Sweet) * P(Red)}$$\n",
    "   \n",
    " The formula would be the same for Banana and Other, just replace Apple with the proper fruit. If you notice, the demoninator or the \"evidence\" doesn't actually rely on our class and is constant. We use this to scale the result probability to be between 0 and 1, but in the grand scheme of things we can get rid of this as long as we do it for the other classes as well. \n",
    " \n",
    " Now, all you have to do is count and do the math. Getting rid of the denominator, for the 3 classes, we would end up with the following:\n",
    " \n",
    "   $$ P(Apple \\,|\\, Round, Sweet, Red) = 0.8 * 0.7 * 0.9 * 0.5 = 0.252 $$\n",
    "   $$ P(Banana \\,|\\, Round, Sweet, Red) = 0 $$\n",
    "   $$ P(Other \\,|\\, Round, Sweet, Red) = 0.5 * 0.75 * 0.25 * 0.2 =  0.01875 $$\n",
    "  \n",
    "Since 0.252 is greater than 0.01875 and 0, we would classify a new fruit that is Round, Sweet, and Red as an Apple! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Naive Bayes - Classifying Emails and Filtering Spam\n",
    "\n",
    "Now that we've gone over the basics of Naive Bayes, let's start actually coding. We'll be using the [Enron Email dataset](http://www2.aueb.gr/users/ion/data/enron-spam/), already divided into spam or not spam [here](http://www2.aueb.gr/users/ion/data/enron-spam/). Download the dataset and put the enron directories on the same level as this notebook file. We'll use Enron1 in its pre-processed form for training, and then we will use Enron5 and Enron6 for testing to see how accurate our classifier is. Essentially, words are our features and our labels are 'spam' or 'not spam'.\n",
    "\n",
    "Before doing anything, you should manually inspect the data. You'll notice that the emails looks quite messy, and that we should definitely process our text. From previous lectures, you should know that we have many options here but we'll just do a few simple things for the sake of brevity in this tutorial. Also note that the term for **'not spam'** is **ham**, and spam is just spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anyLabel', 5172]\n",
      "['spam', 1500]\n",
      "['ham', 3672]\n",
      "['ham^anyWord', 575678]\n",
      "['ham^Subject', 3672]\n",
      "['ham^christmas', 19]\n",
      "['ham^tree', 4]\n",
      "['ham^farm', 6]\n",
      "['ham^pictures', 42]\n",
      "['ham^', 983]\n"
     ]
    }
   ],
   "source": [
    "# Text Processing\n",
    "def removePunctuation(st):\n",
    "    mapping = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    removedPunc = st.translate(mapping)\n",
    "    return removedPunc\n",
    "\n",
    "def tokenize(st):\n",
    "    regex = \"\\W+\"\n",
    "    removedPunc = removePunctuation(st)\n",
    "    splitWords = re.split(regex, st)\n",
    "    return splitWords\n",
    "\n",
    "# Returns a dictionary with the counts for every word\n",
    "def createDictionary(hashTable, words, label):\n",
    "    for word in words:\n",
    "        totalLabelTokens = label + \"^\" + \"anyWord\"\n",
    "        if totalLabelTokens in hashTable:\n",
    "            hashTable[totalLabelTokens] += 1\n",
    "        else:\n",
    "            hashTable[totalLabelTokens] = 1\n",
    "        labelPlusWord = label + \"^\" + word\n",
    "        if labelPlusWord in hashTable:\n",
    "            hashTable[labelPlusWord] += 1\n",
    "        else:\n",
    "            hashTable[labelPlusWord] = 1\n",
    "    return hashTable\n",
    "\n",
    "def trainDataset(dataset):\n",
    "    hashTable = {\"anyLabel\": 0, \"spam\": 0, \"ham\": 0}\n",
    "    for data in dataset:\n",
    "        for root, dirs, files in os.walk(data):\n",
    "            # If we're in a spam or not spam folder\n",
    "            if len(dirs) == 0:\n",
    "                subFolder = root.split('/')[1]\n",
    "                for file in files:\n",
    "                    # Latin-1 encoding to read special characters\n",
    "                    with open(os.path.join(root, file), encoding=\"latin-1\") as email:\n",
    "                        hashTable[\"anyLabel\"] += 1\n",
    "                        hashTable[subFolder] += 1\n",
    "                        unProcessedEmail = email.read()\n",
    "                        words = tokenize(unProcessedEmail)\n",
    "                        hashTable = createDictionary(hashTable, words, subFolder) \n",
    "    return hashTable\n",
    "\n",
    "trainingData = ['enron1']\n",
    "hashTable = trainDataset(trainingData)\n",
    "\n",
    "count = 0\n",
    "for key, value in hashTable.items():\n",
    "    if count == 10:\n",
    "        break\n",
    "    print([key, value])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from some of the key value pairs, we've read the data and then created a dictionary mapping every word with what label it was categorized in with a count of how many times the word occurred. All our code is doing here is accumulating the counts of every word in the training data in order to do the probability calculations in the prediction step. \n",
    "\n",
    "### Smoothing\n",
    "\n",
    "Before we get into testing, there's something else important to mention that we touched upon in class.\n",
    "\n",
    "If you notice in our equations above, we take the product of the probabilities. When we multiply many small probabilities, we may run into numerical underflow. To prevent this, we'll be using the addition of **log probabilities.** I won't go into this too much since we went over it in class, but since the log function is monotonic, we can still just simply look at the greatest value. \n",
    "\n",
    "**But what happens when one of the counts is 0? **\n",
    "\n",
    "log(0) is undefined, which would cause an error in our calculations. A common way to combat this is [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), where we just add 1 to the count, and then balance this by adding 1 to the overall vocabulary size as well. We need this to make our probability fail safe. If none of the words in the training sample appear in the test, then our model would essentially conclude that the sentence is impossible, which is obviously not the case.\n",
    "\n",
    "With that out of the way, let's get to testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct: 10812/11175=0.9675167785234899\n"
     ]
    }
   ],
   "source": [
    "def findNumUniqueWords(hashTable):\n",
    "    uniqueWords = set()\n",
    "    for key in hashTable:\n",
    "        if \"^\" in key and \"anyWord\" not in key:\n",
    "            uniqueWords.add(key.split(\"^\")[1])\n",
    "    return len(uniqueWords)\n",
    "\n",
    "# Function to grab a subset of the testing data\n",
    "# for brevity purposes. Returns a list of file\n",
    "# paths for ham emails and spam emails\n",
    "def getSubsetData(testData):\n",
    "    hamEmails = []\n",
    "    spamEmails = []\n",
    "    for data in testData:\n",
    "        for root, dirs, files in os.walk(data):\n",
    "            if (len(dirs) == 0):\n",
    "                subFolder = root.split('/')[1]\n",
    "                if subFolder == 'ham':\n",
    "                    for file in files:\n",
    "                        hamEmails.append(os.path.join(root, file))\n",
    "                else:\n",
    "                    for file in files:\n",
    "                        spamEmails.append(os.path.join(root, file))\n",
    "    return hamEmails, spamEmails\n",
    "\n",
    "def naiveBayesEstimator(hashTable, testData):\n",
    "    numCorrect = 0\n",
    "    numEmails = 0\n",
    "    uniqueWords = findNumUniqueWords(hashTable)\n",
    "    logProbDict = {}\n",
    "    hamEmails, spamEmails = getSubsetData(testData)\n",
    "    emails = hamEmails + spamEmails\n",
    "    for email in emails:\n",
    "        trueLabel = email.split('/')[1]\n",
    "        with open(email, encoding=\"latin-1\") as e:\n",
    "            numEmails += 1\n",
    "            unProcessedEmail = e.read()\n",
    "            words = tokenize(unProcessedEmail)\n",
    "            # Go through the labels in our hash table\n",
    "            for label in hashTable:\n",
    "                logProb = 0\n",
    "                sigmaValue = 0\n",
    "                # Doing the actual calculations. 2 is the number of classes (spam/not spam)\n",
    "                numerator = float(int(hashTable[label]) + 2 * float(1)/2)\n",
    "                denominator = int(hashTable['anyLabel']) + 2\n",
    "                rhs = math.log(numerator/denominator)\n",
    "                if label in ['ham', 'spam']:\n",
    "                    for word in words:\n",
    "                        key = label + \"^\" + word\n",
    "                        if key in hashTable:\n",
    "                            count = int(hashTable[key])\n",
    "                        else:\n",
    "                            count = 0\n",
    "                        # Smoothing\n",
    "                        lhsNumerator = float(count + 1)\n",
    "                        lhsDenominator = int(hashTable[label + \"^\" + \"anyWord\"]) + uniqueWords\n",
    "                        # The sum of the log probabilities\n",
    "                        sigmaValue += math.log(lhsNumerator/lhsDenominator)\n",
    "                    # Add the terms to get the final log probability for that label\n",
    "                    logProb = sigmaValue + rhs\n",
    "                    logProbDict[label] = logProb\n",
    "            # Get the label with the highest probability\n",
    "            maxProbLabel = max(logProbDict, key = logProbDict.get)\n",
    "            # Checking to see if our prediction is correct\n",
    "            if maxProbLabel == trueLabel:\n",
    "                numCorrect += 1\n",
    "    return numCorrect, numEmails\n",
    "    \n",
    "testData = ['enron5', 'enron6']\n",
    "numCorrect, numEmails = naiveBayesEstimator(hashTable, testData)\n",
    "print(\"Percent correct: \" + str(numCorrect) + \"/\" + str(numEmails) + \"=\" + str(float(numCorrect)/numEmails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Things to Note\n",
    "\n",
    "The Naive Bayes model also has an important subtlety that one should take note of:\n",
    "\n",
    "### The features might not be independent!\n",
    "\n",
    "The whole backbone of Naive Bayes classifiers is the assumption that the predictors are independent, or that presence of a feature in a class is unrelated to the presence of any other feature. In reality, the conditional independence assumption does not hold, especially for text data. The words I write in a sentence or email definitely depend on other words in the text. This is one of the reasons why it is called *Naive* Bayes.\n",
    "\n",
    "One might ask: How can Naive Bayes be a good classifier when it oversimplifies everything and one of its biggest assumptions are violated? You can read a formal study [here](http://www.aaai.org/Papers/FLAIRS/2004/Flairs04-097.pdf), but the basic gist is that Naive Bayes works not only when features are independent, but also when dependencies of features from each other are similar between features. In a practical sense, despite the fact that the main assumption is most likely being violated, Naive Bayes still performs quite well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary, Further Exploration, and Additional Resources\n",
    "\n",
    "Hopefully this tutorial has shown you how big of a role classifiers play in our lives, whether we consciously realize this or not. You can blame a classifier the next time you get spam in your inbox!\n",
    "\n",
    "To summarize, a Naive Bayes classifier uses the assumption that the features/predictors are independent, and that they follow some distribution. We went over many different types of distributions in class, and there are other specific instances of a Naive Bayes classifier such as the Multinomial Naive Bayes classifier or the Bernoulli Naive Bayes classifier, where the features following different types of distributions. We can use a Naive Bayes classifier for a broad array of problems, and despite the strong independence assumption, it still performs quite well, even against other classifiers!\n",
    "\n",
    "**Some parting thoughts:** \n",
    "1. How would our code in Part II change depending on the distribution of the features?\n",
    "2. Note that in our spam classification task, we also didn't really process the text much before training. We've learned some text processing methods in class. How might we improve the accuracy of our classifier?\n",
    "\n",
    "If you're interested in learning more about classification or the application of Naive Bayes, here are some resources I would recommend:\n",
    "\n",
    "1. [Udacity - Classification Models](https://www.udacity.com/course/classification-models--ud978): Free video lectures and assignments teaching students how to use classification models to create business insights.\n",
    "2. [Coursera - Classification in Machine Larning](https://www.coursera.org/learn/ml-classification): Course on classification and its broad array of applications.\n",
    "3. [Data Mining - Naive Bayes](https://gerardnico.com/data_mining/naive_bayes): Good overview of Naive Bayes\n",
    "4. [Comparing Classification Techniques](https://pdfs.semanticscholar.org/51c0/68c263ee197a292df5b74b58c8c55df9f9ca.pdf): Comparative study of various classification algorithms/techniques, specifically kNN, Naive Bayes, and Decision Trees\n",
    "5. [scikit-learn: Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html): Documentation for Naive Bayes in scikit-learn, a free machine learning library for Python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
