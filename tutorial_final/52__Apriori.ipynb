{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Frequent Pattern Mining\n",
    "\n",
    "Pattern mining is an important subfield for data mining and has been applied on a variety of applications. The intuition behind pattern mining is to discover interesting or useful patterns using pattern mining algorithms. Pattern mining algorithms can be applied on various types of data to discover various types of pattern such as direct/indirect associations and subgraphs. \n",
    "\n",
    "In this tutorial, we will be focusing on the problem of mining frequent patterns. And we will be implementing one of the most popular algorithms for finding frequent patterns, the Apriori algorithm. This algorithm is first designed to discover common patterns of transactions made by customer in stores. One famous example is the Walmart's beer and diaper parable. Through mining hundreds of millions transaction records, Walmart surprisingly discovered that young American male who buy diapers tends to buy beer as well. The two products seem unrelated at all. One possible reason is that raising kids can be very stressful so dads decide to buy beer to relax. The sales of beer increased significantly after the beer is repositioned next to the diapers. Although this story is very likely an urban legend, the significant impact of the Apriori algorithm has on mining frequent patterns is undeniable. \n",
    "\n",
    "In the following sections, we will introduce the Apriori algorithm and walk you through the implementation of it step by step. On top of that, we will also be optimizing the algorithm to make if more computationally efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori Algorithm\n",
    "As mentioned earlier, the Apriori algorithm is a popular data mining algorithm for mining frequent datasets and association rules. It is commonly applied on large transaction databases such as customer transactions for market basket analysis and healthcare databases for adverse drug detection reactions. \n",
    "\n",
    "Before diving into how the algorithm works, we will first introduce two important measures that are used in the Apriori algorithm, support and confidence. \n",
    "\n",
    "\n",
    "### Support\n",
    "The support of an itemset denotes the popularity of the itemset and is calculated by the proportion of the transactions in which the itemset X appears. \n",
    "\n",
    "### Confidence\n",
    "The confidence calculates the likelihood of itemset Y being purchased when itemset X is being purchased. For example, if the association rule {'A', 'B'} ==> {'C', 'D'} has a 0.8 confidence interval, it means that 80% of the transactions that contains items A and B will also contain items C and D. \n",
    "\n",
    "### Property Used\n",
    "The Apriori algorithm utilizes two important properties to reduce search space. The first property is called the anti-monotonicity property. The idea is very simple. If there are two itemsets X and Y and X is a subset of Y. Then the support of Y must be less than or equal to the support of X. In other words, the number of transactions that contain Y must not be greater than the number of transactions that contain X. The second property shares a similar idea. For any infrequent itemset, all of its supersets will be infrequent as well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go through the major steps of the Apriori algorithm with an example.\n",
    "Here is the example transaction database to help illustrate the algorithm.\n",
    "\n",
    "| Transaction ID | Apple | Bread | Cheese | Dumpling | Egg\n",
    "| :- | :-: | :-: | :-: | :-: | :-: \n",
    "|1|1|1|0|0|1\n",
    "|2|1|0|0|1|1\n",
    "|3|0|0|1|1|0\n",
    "|4|0|1|0|1|1\n",
    "|5|1|1|1|0|1\n",
    "|6|1|1|1|1|1\n",
    "\n",
    "\n",
    "1\\. Compute the frequency of all items (itemset with only one item) that have appeared in the transactions. \n",
    "\n",
    "| Itemset | Frequency \n",
    "| :- | :-: \n",
    "| Apple | 4\n",
    "| Bread |  4\n",
    "| Cheese | 3\n",
    "| Dumpling | 4\n",
    "| Egg | 5\n",
    "\n",
    "2\\. Filter out the items whose support is lower than the threshold since with the second property we know that all supersets of an infrequent itemset will also be infrequent. This will give us the single items that are purchased frequently. For example, if the min support threshold is 0.5, we would be left with\n",
    "\n",
    "| Itemset | Frequency \n",
    "| :- | :-: \n",
    "| Apple (A) | 4\n",
    "| Bread (B) |  4\n",
    "| Dumpling (D) | 4\n",
    "| Egg (E) | 5\n",
    "\n",
    "3\\. Based on the items we obtained from step 2, we then generate all possible itemsets of 2 items and record the occurrences of each itemset in the transactions.\n",
    "\n",
    "| Itemset | Frequency \n",
    "| :- | :-: \n",
    "| AB | 3\n",
    "| AD | 2\n",
    "| AE | 4\n",
    "| BD | 2\n",
    "| BE | 4\n",
    "| DE | 3\n",
    "\n",
    "4\\. Again, filter out the itemsets with support lower than the support threshold.\n",
    "\n",
    "| Itemset | Frequency \n",
    "| :- | :-: \n",
    "| AB | 3\n",
    "| AE | 4\n",
    "| BE | 4\n",
    "| DE | 3\n",
    "\n",
    "5\\. Compute the frequency table of itemsets with 3 items based on the frequent itemsets obtained from step 4.\n",
    "\n",
    "| Itemset | Frequency \n",
    "| :- | :-: \n",
    "| ABE | 3\n",
    "| BDE | 2\n",
    "\n",
    "6\\. Filter out itemsets under the min support threshold.\n",
    "\n",
    "| Itemset | Frequency \n",
    "| :- | :-: \n",
    "| ABE | 3\n",
    "\n",
    "Therefore, given 50% support threshold, the frequent common patterns are {A, B, D, E, AB, AE, BE, DE, ABE}\n",
    "\n",
    "The Apriori algorithm can be divided into two major steps.\n",
    "With k starting from 1, repeat the following steps until no new itemsets can be obtained by the self join rule.\n",
    "\n",
    "1\\. Find the frequent itemsets with k items from all transactions. \n",
    "\n",
    "2\\. Find the frequent itemsets with k+1 items by applying the self join rule on the frequent itemsets with k items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "For this tutorial, we will be using the UCI Adult Data Set, which could be found here: https://archive.ics.uci.edu/ml/datasets/adult. The dataset contains the following fields of information for over 32,000 adults.\n",
    "\n",
    "| Attribute | Type\n",
    "| :- | :-: \n",
    "| age | continuous\n",
    "| workclass | categorical\n",
    "| fnlwgt | continuous\n",
    "| education | categorical\n",
    "| ed_num | continuous\n",
    "| marital-status | categorical\n",
    "| occupation | categorical\n",
    "| relationship | categorical\n",
    "| race | categorical\n",
    "| sex | categorical\n",
    "| capital-gain | continuous\n",
    "| capital-loss | continuous\n",
    "| hrs-per-week | continuous\n",
    "| native-country | categorical\n",
    "| annual-income  | categorical\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with parsing the adult_data.csv file. The purpose of this function is to parse the data file into a more readable format. Note that the iterator is used so we can deal with larger files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse data file into a readable format\n",
    "def parseFile(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            lines = line.strip().split(',')\n",
    "            lines[0] = 'age: '+lines[0]\n",
    "            lines[1] = 'workclass:'+lines[1]\n",
    "            lines[2] = 'fnlwgt:'+lines[2]\n",
    "            lines[3] = 'education:'+lines[3]\n",
    "            lines[4] = 'ed_num:'+lines[4]\n",
    "            lines[5] = 'marital-status:'+lines[5]\n",
    "            lines[6] = 'occupation:'+lines[6]\n",
    "            lines[7] = 'relationship:'+lines[7]\n",
    "            lines[8] = 'race:'+lines[8]\n",
    "            lines[9] = 'sex:'+lines[9]\n",
    "            lines[10] = 'capital-gain:'+lines[10]\n",
    "            lines[11] = 'capital-loss:'+lines[11]\n",
    "            lines[12] = 'hrs-per-week:'+lines[12]\n",
    "            lines[13] = 'native-country:'+lines[13]\n",
    "            lines[14] = 'annual-income:'+lines[14]\n",
    "            # use iterator so we can deal with bigger files\n",
    "            yield lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age: 39', 'workclass: State-gov', 'fnlwgt: 77516', 'education: Bachelors', 'ed_num: 13', 'marital-status: Never-married', 'occupation: Adm-clerical', 'relationship: Not-in-family', 'race: White', 'sex: Male', 'capital-gain: 2174', 'capital-loss: 0', 'hrs-per-week: 40', 'native-country: United-States', 'annual-income: <=50K']\n",
      "['age: 50', 'workclass: Self-emp-not-inc', 'fnlwgt: 83311', 'education: Bachelors', 'ed_num: 13', 'marital-status: Married-civ-spouse', 'occupation: Exec-managerial', 'relationship: Husband', 'race: White', 'sex: Male', 'capital-gain: 0', 'capital-loss: 0', 'hrs-per-week: 13', 'native-country: United-States', 'annual-income: <=50K']\n",
      "['age: 38', 'workclass: Private', 'fnlwgt: 215646', 'education: HS-grad', 'ed_num: 9', 'marital-status: Divorced', 'occupation: Handlers-cleaners', 'relationship: Not-in-family', 'race: White', 'sex: Male', 'capital-gain: 0', 'capital-loss: 0', 'hrs-per-week: 40', 'native-country: United-States', 'annual-income: <=50K']\n",
      "['age: 53', 'workclass: Private', 'fnlwgt: 234721', 'education: 11th', 'ed_num: 7', 'marital-status: Married-civ-spouse', 'occupation: Handlers-cleaners', 'relationship: Husband', 'race: Black', 'sex: Male', 'capital-gain: 0', 'capital-loss: 0', 'hrs-per-week: 40', 'native-country: United-States', 'annual-income: <=50K']\n",
      "['age: 28', 'workclass: Private', 'fnlwgt: 338409', 'education: Bachelors', 'ed_num: 13', 'marital-status: Married-civ-spouse', 'occupation: Prof-specialty', 'relationship: Wife', 'race: Black', 'sex: Female', 'capital-gain: 0', 'capital-loss: 0', 'hrs-per-week: 40', 'native-country: Cuba', 'annual-income: <=50K']\n",
      "['age: 37', 'workclass: Private', 'fnlwgt: 284582', 'education: Masters', 'ed_num: 14', 'marital-status: Married-civ-spouse', 'occupation: Exec-managerial', 'relationship: Wife', 'race: White', 'sex: Female', 'capital-gain: 0', 'capital-loss: 0', 'hrs-per-week: 40', 'native-country: United-States', 'annual-income: <=50K']\n",
      "['age: 49', 'workclass: Private', 'fnlwgt: 160187', 'education: 9th', 'ed_num: 5', 'marital-status: Married-spouse-absent', 'occupation: Other-service', 'relationship: Not-in-family', 'race: Black', 'sex: Female', 'capital-gain: 0', 'capital-loss: 0', 'hrs-per-week: 16', 'native-country: Jamaica', 'annual-income: <=50K']\n",
      "['age: 52', 'workclass: Self-emp-not-inc', 'fnlwgt: 209642', 'education: HS-grad', 'ed_num: 9', 'marital-status: Married-civ-spouse', 'occupation: Exec-managerial', 'relationship: Husband', 'race: White', 'sex: Male', 'capital-gain: 0', 'capital-loss: 0', 'hrs-per-week: 45', 'native-country: United-States', 'annual-income: >50K']\n",
      "['age: 31', 'workclass: Private', 'fnlwgt: 45781', 'education: Masters', 'ed_num: 14', 'marital-status: Never-married', 'occupation: Prof-specialty', 'relationship: Not-in-family', 'race: White', 'sex: Female', 'capital-gain: 14084', 'capital-loss: 0', 'hrs-per-week: 50', 'native-country: United-States', 'annual-income: >50K']\n",
      "['age: 42', 'workclass: Private', 'fnlwgt: 159449', 'education: Bachelors', 'ed_num: 13', 'marital-status: Married-civ-spouse', 'occupation: Exec-managerial', 'relationship: Husband', 'race: White', 'sex: Male', 'capital-gain: 5178', 'capital-loss: 0', 'hrs-per-week: 40', 'native-country: United-States', 'annual-income: >50K']\n"
     ]
    }
   ],
   "source": [
    "# printing the first 10 lines of the parsed input file\n",
    "count = 0\n",
    "for line in parseFile('adult_data.csv'):\n",
    "    if count == 10:\n",
    "        break\n",
    "    print (line)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the Apriori algorithm, we first need to create a set of all 1-itemsets (one_cset) that have appeared in the transactions. The initTransactionList function will take the input file, create the one_cset, store all transactions in the transaction_list, and return both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a set of single items set that have appeared in the transaction\n",
    "# one_cset: (A, B, C, ...)\n",
    "def initTransactionList(input_file):\n",
    "    # the transaction_list stores all the transactions\n",
    "    # return the one_cset(C1) so we can start running the Apriori algorithm\n",
    "    one_cset = set()\n",
    "    transaction_list = []\n",
    "    for record in parseFile(input_file):\n",
    "        # use frozenset because it is hashable\n",
    "        transaction = frozenset(record)\n",
    "        # create the 1-item sets\n",
    "        for item in transaction:\n",
    "            one_cset.add(frozenset([item]))\n",
    "        # append each transaction into the transaction list\n",
    "        transaction_list.append(transaction)\n",
    "    return one_cset, transaction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'fnlwgt: 96975'})\n",
      "frozenset({'fnlwgt: 183930'})\n",
      "frozenset({'fnlwgt: 259014'})\n",
      "frozenset({'capital-gain: 2176'})\n",
      "frozenset({'native-country: Haiti'})\n",
      "frozenset({'fnlwgt: 267989'})\n",
      "frozenset({'fnlwgt: 188300'})\n",
      "frozenset({'fnlwgt: 388093'})\n",
      "frozenset({'fnlwgt: 309634'})\n",
      "frozenset({'capital-gain: 15024'})\n",
      "frozenset({'occupation: Adm-clerical', 'ed_num: 13', 'education: Bachelors', 'hrs-per-week: 40', 'annual-income: <=50K', 'native-country: United-States', 'race: White', 'workclass: State-gov', 'relationship: Not-in-family', 'fnlwgt: 77516', 'age: 39', 'capital-gain: 2174', 'sex: Male', 'capital-loss: 0', 'marital-status: Never-married'})\n",
      "frozenset({'relationship: Husband', 'sex: Male', 'occupation: Exec-managerial', 'ed_num: 13', 'education: Bachelors', 'annual-income: <=50K', 'native-country: United-States', 'race: White', 'workclass: Self-emp-not-inc', 'hrs-per-week: 13', 'age: 50', 'marital-status: Married-civ-spouse', 'capital-loss: 0', 'fnlwgt: 83311', 'capital-gain: 0'})\n",
      "frozenset({'ed_num: 9', 'marital-status: Divorced', 'hrs-per-week: 40', 'annual-income: <=50K', 'fnlwgt: 215646', 'native-country: United-States', 'occupation: Handlers-cleaners', 'workclass: Private', 'race: White', 'relationship: Not-in-family', 'age: 38', 'sex: Male', 'education: HS-grad', 'capital-loss: 0', 'capital-gain: 0'})\n",
      "frozenset({'relationship: Husband', 'sex: Male', 'hrs-per-week: 40', 'fnlwgt: 234721', 'race: Black', 'native-country: United-States', 'annual-income: <=50K', 'occupation: Handlers-cleaners', 'workclass: Private', 'marital-status: Married-civ-spouse', 'education: 11th', 'capital-loss: 0', 'age: 53', 'ed_num: 7', 'capital-gain: 0'})\n",
      "frozenset({'ed_num: 13', 'education: Bachelors', 'hrs-per-week: 40', 'race: Black', 'annual-income: <=50K', 'occupation: Prof-specialty', 'workclass: Private', 'age: 28', 'sex: Female', 'native-country: Cuba', 'relationship: Wife', 'fnlwgt: 338409', 'marital-status: Married-civ-spouse', 'capital-loss: 0', 'capital-gain: 0'})\n",
      "frozenset({'fnlwgt: 284582', 'occupation: Exec-managerial', 'hrs-per-week: 40', 'annual-income: <=50K', 'native-country: United-States', 'age: 37', 'education: Masters', 'workclass: Private', 'race: White', 'sex: Female', 'relationship: Wife', 'marital-status: Married-civ-spouse', 'capital-loss: 0', 'ed_num: 14', 'capital-gain: 0'})\n",
      "frozenset({'capital-loss: 0', 'hrs-per-week: 16', 'annual-income: <=50K', 'race: Black', 'fnlwgt: 160187', 'workclass: Private', 'occupation: Other-service', 'education: 9th', 'sex: Female', 'native-country: Jamaica', 'relationship: Not-in-family', 'ed_num: 5', 'age: 49', 'marital-status: Married-spouse-absent', 'capital-gain: 0'})\n",
      "frozenset({'relationship: Husband', 'sex: Male', 'ed_num: 9', 'occupation: Exec-managerial', 'native-country: United-States', 'age: 52', 'race: White', 'workclass: Self-emp-not-inc', 'annual-income: >50K', 'hrs-per-week: 45', 'marital-status: Married-civ-spouse', 'education: HS-grad', 'capital-loss: 0', 'capital-gain: 0', 'fnlwgt: 209642'})\n",
      "frozenset({'hrs-per-week: 50', 'education: Masters', 'occupation: Prof-specialty', 'workclass: Private', 'fnlwgt: 45781', 'race: White', 'sex: Female', 'capital-gain: 14084', 'relationship: Not-in-family', 'age: 31', 'annual-income: >50K', 'capital-loss: 0', 'ed_num: 14', 'native-country: United-States', 'marital-status: Never-married'})\n",
      "frozenset({'relationship: Husband', 'sex: Male', 'occupation: Exec-managerial', 'ed_num: 13', 'capital-gain: 5178', 'education: Bachelors', 'fnlwgt: 159449', 'hrs-per-week: 40', 'native-country: United-States', 'workclass: Private', 'race: White', 'marital-status: Married-civ-spouse', 'age: 42', 'capital-loss: 0', 'annual-income: >50K'})\n"
     ]
    }
   ],
   "source": [
    "# print out the first 10 items of the one_cset and transaction_list\n",
    "one_cset, transaction_list = initTransactionList('adult_data_test.csv')\n",
    "for item in list(one_cset)[:10]:\n",
    "    print(item)\n",
    "for transaction in transaction_list[:10]:\n",
    "    print(transaction)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getLset function will be used to filter out itemsets in the Cset whose support is lower than the minimum support threshold and return the Lset. The freq_set is also passed in as an argument to keep track of the global count of all itemsets. The support for each item in the current Cset can be calculated based on the local_set dictionary. Only itemsets with support greater than the minimum threshold will be added to the returned Lset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the support for each item in the itemset(cset)\n",
    "# only add the item to the lset if it meets the minimum support requirement\n",
    "def getLset(cset, transaction_list, freq_set, min_support):\n",
    "    lset = set()\n",
    "    local_set = {}\n",
    "    # C1: local_set = {'A':countA,'B':countB,...}\n",
    "    # calculate the count for each item\n",
    "    for item in cset:\n",
    "        for transaction in transaction_list:\n",
    "            if item.issubset(transaction):\n",
    "                # update global and local frequent set dictionaries\n",
    "                # global\n",
    "                if item not in freq_set:\n",
    "                    freq_set[item] = 1\n",
    "                else:\n",
    "                    freq_set[item] += 1  \n",
    "                # local\n",
    "                if item not in local_set:\n",
    "                    local_set[item] = 1\n",
    "                else:\n",
    "                    local_set[item] += 1\n",
    "    # add the item to lset if it meets the minimum support requirement\n",
    "    n = len(transaction_list)\n",
    "    for item, count in local_set.items():\n",
    "        support = count / n\n",
    "        if support >= min_support:\n",
    "            lset.add(item)\n",
    "    return lset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'annual-income: <=50K'})\n",
      "frozenset({'workclass: Private'})\n",
      "frozenset({'native-country: United-States'})\n",
      "frozenset({'race: White'})\n",
      "frozenset({'capital-loss: 0'})\n",
      "frozenset({'capital-gain: 0'})\n",
      "frozenset({'sex: Male'})\n",
      "(frozenset({'fnlwgt: 96975'}), 1)\n",
      "(frozenset({'fnlwgt: 183930'}), 1)\n",
      "(frozenset({'fnlwgt: 259014'}), 1)\n",
      "(frozenset({'capital-gain: 2176'}), 1)\n",
      "(frozenset({'native-country: Haiti'}), 1)\n",
      "(frozenset({'fnlwgt: 267989'}), 1)\n",
      "(frozenset({'fnlwgt: 188300'}), 1)\n",
      "(frozenset({'fnlwgt: 388093'}), 1)\n",
      "(frozenset({'fnlwgt: 309634'}), 1)\n",
      "(frozenset({'capital-gain: 15024'}), 9)\n"
     ]
    }
   ],
   "source": [
    "# print out the first 10 itemsets in the one_lset and the updated freq_set to have a general idea of what one_lset and freq_set should look like\n",
    "freq_set = {}\n",
    "one_lset = getLset(one_cset, transaction_list, freq_set, 0.5)\n",
    "for item in list(one_lset)[:10]:\n",
    "    print(item)\n",
    "for item in list(freq_set.items())[:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this point, we have only recorded itemsets with only one item with support above the threshold. Therefore, we need a selfJoin function to create all possible 2-element itemsets from joining the 1-element itemsets with itself. Like the getLset function, the selfJoin function will be used repeatedly in the Apriori algorithm for joining (k-1)-element itemsets with itself to create k-element itemsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join a set wtih itself(k-1-element itemsets) and return k-element itemsets\n",
    "def selfJoin(item_set, k):\n",
    "    joined_k = set()\n",
    "    for i in item_set:\n",
    "        for j in item_set:\n",
    "            union_set = i.union(j)\n",
    "            if len(union_set) == k:\n",
    "                joined_k.add(union_set)\n",
    "    return joined_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'workclass: Private', 'annual-income: <=50K'})\n",
      "frozenset({'race: White', 'annual-income: <=50K'})\n",
      "frozenset({'native-country: United-States', 'capital-gain: 0'})\n",
      "frozenset({'workclass: Private', 'capital-gain: 0'})\n",
      "frozenset({'race: White', 'capital-gain: 0'})\n",
      "frozenset({'native-country: United-States', 'annual-income: <=50K'})\n",
      "frozenset({'sex: Male', 'capital-gain: 0'})\n",
      "frozenset({'capital-loss: 0', 'workclass: Private'})\n",
      "frozenset({'native-country: United-States', 'capital-loss: 0'})\n",
      "frozenset({'capital-loss: 0', 'race: White'})\n"
     ]
    }
   ],
   "source": [
    "# print out the first 10 2-element itemsets \n",
    "two_itemsets = selfJoin(one_lset, 2)\n",
    "for item in list(two_itemsets)[:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put all the helper functions together to perform the Apriori algorithm. The algorithm follows the following steps.\n",
    "\n",
    "1\\. Initialize the one_cset and the transaction list using the initTransactionList function\n",
    "\n",
    "2\\. Filter out infrequent itemsets from the current cset to get the current lset using the getLset function\n",
    "\n",
    "3\\. Call the selfJoin function on the current lset to form the next cset.\n",
    "\n",
    "4\\. Repeat step 2 and 3 until there are no items left in the current lset. In other words, no k+1-element cset could be formed.\n",
    "\n",
    "The freq_set dictionary is used to keep track of all frequent itemsets and their counts. The n_item_set dictionary is used to keep track of all the frequent itemsets for each number of elements. With the n_item_set dictionary, we can then output frequent itemsets. On top of that, the corresponding association rules could be generated by doing a binary partition on the frequent itemsets obtained and filter out those with confidence below the minimum confidence threshold. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apriori(input_file, min_support, min_confidence):\n",
    "    # for storing global frequent itemsets\n",
    "    freq_set = {}\n",
    "    n_item_set = {}\n",
    "    items_with_support = [] # storing frequent itemsets\n",
    "    association_rules = [] # storing association rules\n",
    "    \n",
    "    one_cset, transaction_list = initTransactionList(input_file)\n",
    "    current_lset = getLset(one_cset, transaction_list, freq_set, min_support)\n",
    "    k = 1\n",
    "    empty_set = set()\n",
    "    while(current_lset != empty_set):\n",
    "        # storing the current lset into the dictionary\n",
    "        n_item_set[k] = current_lset\n",
    "        # self join to get next k+1-item cset (C_k+1)\n",
    "        next_cset = selfJoin(current_lset, k+1)\n",
    "        # Pass C_k+1 through the min support criteria to get the next_lset (L_k+1)\n",
    "        next_lset = getLset(next_cset, transaction_list, freq_set, min_support)\n",
    "        # move to the next lset\n",
    "        current_lset = next_lset\n",
    "        k += 1\n",
    "    \n",
    "    n = len(transaction_list)\n",
    "    # n_item_set is in the structure of {n: Ln set}\n",
    "    # n_item_set = {1:[A,B,C,D,E],2:[AB,AC,AD,BD,CE],3:[ABC,ABD,ACD]}\n",
    "    # go through every item in every L set, compute their support and save it in items_with_support\n",
    "    for value in n_item_set.values():\n",
    "        for item in value:\n",
    "            support = freq_set[item] / n\n",
    "            items_with_support.append((tuple(item), support))\n",
    "    # items_with_support will look like this\n",
    "    # [(A,supportA),(B,supportB),...,((A,B),supportAB),....]\n",
    "\n",
    "    for key, value in n_item_set.items():\n",
    "        for item in value:\n",
    "            subsets = chain(*[combinations(item, i + 1) for i, a in enumerate(item)])\n",
    "            subset = set()\n",
    "            for x in subsets:\n",
    "                subset.add(frozenset(x))\n",
    "            for element in subset:\n",
    "                rest = item.difference(element)\n",
    "                if item != element:\n",
    "                    confidence = freq_set[item] / freq_set[element]\n",
    "                    if confidence >= min_confidence:\n",
    "                        association_rules.append(((tuple(element), tuple(rest)), confidence))\n",
    "    return items_with_support, association_rules\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out frequent itemsets sorted by support and association rules sorted by confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printFrequentItems(items_with_support):\n",
    "    i = 1\n",
    "    for item, support in sorted(items_with_support, key=lambda line: line[1], reverse=True):\n",
    "        print ('Frequent Items %d: %s, Support: %.4f' % (i, str(item), support))\n",
    "        i += 1\n",
    "            \n",
    "def printRules(association_rules):\n",
    "    i = 1\n",
    "    for rule, confidence in sorted(association_rules, key=lambda line: line[1], reverse=True):\n",
    "        left_part = rule[0]\n",
    "        right_part = rule[1]\n",
    "        print (\"Association Rule %d: %s ==> %s, Confidence: %.4f\" % (i, str(left_part), str(right_part), confidence))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Frequent Common Patterns ==========\n",
      "Frequent Items 1: ('capital-loss: 0',), Support: 0.9533\n",
      "Frequent Items 2: ('capital-gain: 0',), Support: 0.9167\n",
      "Frequent Items 3: ('native-country: United-States',), Support: 0.8959\n",
      "Frequent Items 4: ('capital-loss: 0', 'capital-gain: 0'), Support: 0.8701\n",
      "Frequent Items 5: ('race: White',), Support: 0.8543\n",
      "Frequent Items 6: ('native-country: United-States', 'capital-loss: 0'), Support: 0.8535\n",
      "Frequent Items 7: ('native-country: United-States', 'capital-gain: 0'), Support: 0.8200\n",
      "Frequent Items 8: ('capital-loss: 0', 'race: White'), Support: 0.8129\n",
      "Frequent Items 9: ('native-country: United-States', 'race: White'), Support: 0.7869\n",
      "Frequent Items 10: ('race: White', 'capital-gain: 0'), Support: 0.7803\n",
      "Frequent Items 11: ('native-country: United-States', 'capital-loss: 0', 'capital-gain: 0'), Support: 0.7776\n",
      "Frequent Items 12: ('annual-income: <=50K',), Support: 0.7592\n",
      "Frequent Items 13: ('native-country: United-States', 'capital-loss: 0', 'race: White'), Support: 0.7478\n",
      "Frequent Items 14: ('capital-loss: 0', 'race: White', 'capital-gain: 0'), Support: 0.7390\n",
      "Frequent Items 15: ('capital-loss: 0', 'annual-income: <=50K'), Support: 0.7363\n",
      "Frequent Items 16: ('capital-gain: 0', 'annual-income: <=50K'), Support: 0.7274\n",
      "Frequent Items 17: ('native-country: United-States', 'race: White', 'capital-gain: 0'), Support: 0.7177\n",
      "Frequent Items 18: ('capital-loss: 0', 'capital-gain: 0', 'annual-income: <=50K'), Support: 0.7045\n",
      "========== Association Rules ==========\n",
      "Association Rule 1: ('annual-income: <=50K',) ==> ('capital-loss: 0',), Confidence: 0.9698\n",
      "Association Rule 2: ('capital-gain: 0', 'annual-income: <=50K') ==> ('capital-loss: 0',), Confidence: 0.9685\n",
      "Association Rule 3: ('annual-income: <=50K',) ==> ('capital-gain: 0',), Confidence: 0.9581\n",
      "Association Rule 4: ('capital-loss: 0', 'annual-income: <=50K') ==> ('capital-gain: 0',), Confidence: 0.9568\n",
      "Association Rule 5: ('native-country: United-States',) ==> ('capital-loss: 0',), Confidence: 0.9527\n",
      "Association Rule 6: ('race: White',) ==> ('capital-loss: 0',), Confidence: 0.9516\n",
      "Association Rule 7: ('native-country: United-States', 'race: White') ==> ('capital-loss: 0',), Confidence: 0.9504\n",
      "Association Rule 8: ('capital-gain: 0',) ==> ('capital-loss: 0',), Confidence: 0.9491\n",
      "Association Rule 9: ('native-country: United-States', 'capital-gain: 0') ==> ('capital-loss: 0',), Confidence: 0.9484\n",
      "Association Rule 10: ('race: White', 'capital-gain: 0') ==> ('capital-loss: 0',), Confidence: 0.9470\n",
      "Association Rule 11: ('annual-income: <=50K',) ==> ('capital-loss: 0', 'capital-gain: 0'), Confidence: 0.9280\n",
      "Association Rule 12: ('race: White',) ==> ('native-country: United-States',), Confidence: 0.9211\n",
      "Association Rule 13: ('capital-loss: 0', 'race: White') ==> ('native-country: United-States',), Confidence: 0.9199\n",
      "Association Rule 14: ('race: White', 'capital-gain: 0') ==> ('native-country: United-States',), Confidence: 0.9198\n",
      "Association Rule 15: ('native-country: United-States',) ==> ('capital-gain: 0',), Confidence: 0.9153\n",
      "Association Rule 16: ('race: White',) ==> ('capital-gain: 0',), Confidence: 0.9134\n",
      "Association Rule 17: ('capital-loss: 0',) ==> ('capital-gain: 0',), Confidence: 0.9126\n",
      "Association Rule 18: ('native-country: United-States', 'race: White') ==> ('capital-gain: 0',), Confidence: 0.9121\n",
      "Association Rule 19: ('native-country: United-States', 'capital-loss: 0') ==> ('capital-gain: 0',), Confidence: 0.9111\n",
      "Association Rule 20: ('capital-loss: 0', 'race: White') ==> ('capital-gain: 0',), Confidence: 0.9090\n",
      "Association Rule 21: ('capital-loss: 0',) ==> ('native-country: United-States',), Confidence: 0.8953\n",
      "Association Rule 22: ('capital-gain: 0',) ==> ('native-country: United-States',), Confidence: 0.8945\n",
      "Association Rule 23: ('capital-loss: 0', 'capital-gain: 0') ==> ('native-country: United-States',), Confidence: 0.8938\n",
      "Association Rule 24: ('native-country: United-States',) ==> ('race: White',), Confidence: 0.8783\n",
      "Association Rule 25: ('native-country: United-States', 'capital-loss: 0') ==> ('race: White',), Confidence: 0.8761\n",
      "Association Rule 26: ('race: White',) ==> ('native-country: United-States', 'capital-loss: 0'), Confidence: 0.8754\n",
      "Association Rule 27: ('native-country: United-States', 'capital-gain: 0') ==> ('race: White',), Confidence: 0.8753\n",
      "Association Rule 28: ('native-country: United-States',) ==> ('capital-loss: 0', 'capital-gain: 0'), Confidence: 0.8680\n",
      "Association Rule 29: ('race: White',) ==> ('capital-loss: 0', 'capital-gain: 0'), Confidence: 0.8650\n",
      "Association Rule 30: ('capital-loss: 0',) ==> ('race: White',), Confidence: 0.8527\n",
      "Association Rule 31: ('capital-gain: 0',) ==> ('race: White',), Confidence: 0.8512\n",
      "Association Rule 32: ('capital-loss: 0', 'capital-gain: 0') ==> ('race: White',), Confidence: 0.8493\n",
      "Association Rule 33: ('capital-gain: 0',) ==> ('native-country: United-States', 'capital-loss: 0'), Confidence: 0.8483\n",
      "Association Rule 34: ('race: White',) ==> ('native-country: United-States', 'capital-gain: 0'), Confidence: 0.8401\n",
      "Association Rule 35: ('native-country: United-States',) ==> ('capital-loss: 0', 'race: White'), Confidence: 0.8347\n",
      "Association Rule 36: ('capital-loss: 0',) ==> ('native-country: United-States', 'capital-gain: 0'), Confidence: 0.8157\n",
      "Association Rule 37: ('capital-loss: 0', 'capital-gain: 0') ==> ('annual-income: <=50K',), Confidence: 0.8097\n",
      "Association Rule 38: ('capital-gain: 0',) ==> ('capital-loss: 0', 'race: White'), Confidence: 0.8061\n",
      "Association Rule 39: ('native-country: United-States',) ==> ('race: White', 'capital-gain: 0'), Confidence: 0.8011\n",
      "Association Rule 40: ('capital-gain: 0',) ==> ('annual-income: <=50K',), Confidence: 0.7935\n",
      "Association Rule 41: ('capital-loss: 0',) ==> ('native-country: United-States', 'race: White'), Confidence: 0.7844\n",
      "Association Rule 42: ('capital-gain: 0',) ==> ('native-country: United-States', 'race: White'), Confidence: 0.7829\n",
      "Association Rule 43: ('capital-loss: 0',) ==> ('race: White', 'capital-gain: 0'), Confidence: 0.7751\n",
      "Association Rule 44: ('capital-loss: 0',) ==> ('annual-income: <=50K',), Confidence: 0.7723\n",
      "Association Rule 45: ('capital-gain: 0',) ==> ('capital-loss: 0', 'annual-income: <=50K'), Confidence: 0.7685\n",
      "Association Rule 46: ('capital-loss: 0',) ==> ('capital-gain: 0', 'annual-income: <=50K'), Confidence: 0.7390\n"
     ]
    }
   ],
   "source": [
    "# Print out the frequent itemsets and association rules obtained from the Apriori algorithm of the input file.\n",
    "input_file = 'adult_data.csv'\n",
    "min_support = 0.7\n",
    "min_confidence = 0.7\n",
    "frequent_items, association_rules = apriori(input_file, min_support, min_confidence)\n",
    "print(\"========== Frequent Common Patterns ==========\")\n",
    "printFrequentItems(frequent_items)\n",
    "print(\"========== Association Rules ==========\")\n",
    "printRules(association_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of the Apriori Algorithm\n",
    "\n",
    "Although the Apriori algorithm is easy to implement and understand, it could take up a huge amount of time to compute as the data scales up. The reason is that Apriori generates all possible candidates and require multiple scans through the transaction list in order to determine if the candidates meet the minimum support threshold.  \n",
    "\n",
    "However, we can significantly decrease the number of scans required and improve the runtime efficiency using the following trick. \n",
    "\n",
    "We first scan all transactions to generate L1, which contains the items, their support count, and the transaction IDs that they take place. We will use L1 to help generate L2, L2 to help generate L3,â€¦, just like what we did in the original Apriori algorithm. However, unlike scanning through all the transaction list over again and again, we only need to scan through the transactions that the item with the min support count in the n-itemset took place. For example, let (x,y) be one of the 2-itemset of C2 and x has a lower support count than y. Then we only need to scan through the transactions with ID related with x. Since we need to have both x and y in the transaction, this is simply sufficient for us to determine the correct support count with the least number of scans. By doing so, the number of scans through the transaction list can be greatly decreased.\n",
    "\n",
    "The idea is pretty straightforward and the implementation is very similar to the original one. Therefore, we won't be implementing the optimized algorithm step by step. Clear comments are provided and should be explanatory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initTransactionListOptimized(input_file):\n",
    "    L1_TID = {}\n",
    "    one_cset = set()\n",
    "    tid=0\n",
    "    transaction_list = []\n",
    "    for record in parseFile(input_file):\n",
    "        transaction = frozenset(record)\n",
    "        transaction_list.append(transaction)\n",
    "        tid += 1\n",
    "        for item in transaction:\n",
    "            one_cset.add(frozenset([item]))\n",
    "            # keeps track of the transaction IDs that each item has taken place\n",
    "            if item not in L1_TID:\n",
    "                L1_TID[item] = set([tid])\n",
    "            else:\n",
    "                L1_TID[item].add(tid)\n",
    "    return one_cset, transaction_list, L1_TID\n",
    "\n",
    "def getLsetOptimized(cset, transaction_list, L1_count, L1_TID, freq_set, min_support):\n",
    "    lset = set()\n",
    "    local_set = {}\n",
    "    for item in cset:         \n",
    "        if all(x in L1_count for x in list(item)):\n",
    "            min_item = findMinItem(item, L1_count)\n",
    "            # TID is a list of transaaction IDs for min_item\n",
    "            TID = L1_TID[min_item]\n",
    "            # only scan the transactions that has id in TID\n",
    "            for index in TID:\n",
    "                if item.issubset(transaction_list[index-1]):\n",
    "                    if item not in freq_set:\n",
    "                        freq_set[item] = 1\n",
    "                    else:\n",
    "                        freq_set[item] += 1\n",
    "                    if item not in local_set:\n",
    "                        local_set[item] = 1\n",
    "                    else:\n",
    "                        local_set[item] += 1\n",
    "    n = len(transaction_list)\n",
    "    for item, count in local_set.items():\n",
    "        support = count / n\n",
    "        if support >= min_support:\n",
    "            lset.add(item)\n",
    "            \n",
    "    return lset\n",
    "\n",
    "# split the items into single item and return the item with the minimum support\n",
    "def findMinItem(items, L1_count):\n",
    "    minimum = sys.maxsize\n",
    "    for key, value in L1_count.items():\n",
    "        if key in items and value < minimum:\n",
    "            minimum = value\n",
    "            min_key = key\n",
    "    return min_key\n",
    "\n",
    "# generate the content of the dictionary L1_count\n",
    "def getL1Count(one_cset, transaction_list, min_support):\n",
    "    L1_count = {}\n",
    "    local_set = {}\n",
    "    for item in one_cset:\n",
    "        for transaction in transaction_list:\n",
    "            if item.issubset(transaction):\n",
    "                if item not in local_set:\n",
    "                    local_set[item] = 1\n",
    "                else:\n",
    "                    local_set[item] += 1\n",
    "    n = len(transaction_list)\n",
    "    for item, count in local_set.items():\n",
    "        support = count / n\n",
    "        if support >= min_support:\n",
    "            key = [k for k in item][0]\n",
    "            L1_count[key] = count\n",
    "    return L1_count\n",
    "\n",
    "def apriori_optimized(input_file, min_support, min_confidence):    \n",
    "    freq_set = {}\n",
    "    n_item_set = {}\n",
    "    items_with_support = [] # storing frequent itemsets\n",
    "    association_rules = [] # storing association rules\n",
    "    \n",
    "    # L1_TID keeps track of the transaction IDs that each item has taken place\n",
    "    one_cset, transaction_list, L1_TID = initTransactionListOptimized(input_file)\n",
    "    # L1_count keeps track of the count of L1 itemsets\n",
    "    L1_count = getL1Count(one_cset, transaction_list, min_support)\n",
    "    current_lset = getLsetOptimized(one_cset, transaction_list, L1_count, L1_TID, freq_set, min_support)\n",
    "    k = 1\n",
    "    empty_set = set()\n",
    "    \n",
    "    while(current_lset != empty_set):\n",
    "        # storing the current lset into the dictionary\n",
    "        n_item_set[k] = current_lset\n",
    "        # self join to get next k+1-item cset (C_k+1)\n",
    "        next_cset = selfJoin(current_lset, k+1)\n",
    "        # Pass C_k+1 through the min support criteria to get the next_lset (L_k+1)\n",
    "        next_lset = getLsetOptimized(next_cset, transaction_list, L1_count, L1_TID, freq_set, min_support)\n",
    "        # move to the next lset\n",
    "        current_lset = next_lset\n",
    "        k += 1\n",
    "        \n",
    "    n = len(transaction_list)\n",
    "    for value in n_item_set.values():\n",
    "        for item in value:\n",
    "            support = freq_set[item] / n\n",
    "            items_with_support.append((tuple(item), support))\n",
    "    for key, value in n_item_set.items():\n",
    "        for item in value:\n",
    "            subsets = chain(*[combinations(item, i + 1) for i, a in enumerate(item)])\n",
    "            subset = set()\n",
    "            for x in subsets:\n",
    "                subset.add(frozenset(x))\n",
    "            for element in subset:\n",
    "                rest = item.difference(element)\n",
    "                if item != element:\n",
    "                    confidence = freq_set[item] / freq_set[element]\n",
    "                    if confidence >= min_confidence:\n",
    "                        association_rules.append(((tuple(element), tuple(rest)), confidence))\n",
    "    return items_with_support, association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Frequent Common Patterns ==========\n",
      "Frequent Items 1: ('capital-loss: 0',), Support: 0.9533\n",
      "Frequent Items 2: ('capital-gain: 0',), Support: 0.9167\n",
      "Frequent Items 3: ('native-country: United-States',), Support: 0.8959\n",
      "Frequent Items 4: ('capital-loss: 0', 'capital-gain: 0'), Support: 0.8701\n",
      "Frequent Items 5: ('race: White',), Support: 0.8543\n",
      "Frequent Items 6: ('native-country: United-States', 'capital-loss: 0'), Support: 0.8535\n",
      "Frequent Items 7: ('native-country: United-States', 'capital-gain: 0'), Support: 0.8200\n",
      "Frequent Items 8: ('capital-loss: 0', 'race: White'), Support: 0.8129\n",
      "Frequent Items 9: ('native-country: United-States', 'race: White'), Support: 0.7869\n",
      "Frequent Items 10: ('race: White', 'capital-gain: 0'), Support: 0.7803\n",
      "Frequent Items 11: ('native-country: United-States', 'capital-loss: 0', 'capital-gain: 0'), Support: 0.7776\n",
      "Frequent Items 12: ('annual-income: <=50K',), Support: 0.7592\n",
      "Frequent Items 13: ('native-country: United-States', 'capital-loss: 0', 'race: White'), Support: 0.7478\n",
      "Frequent Items 14: ('capital-loss: 0', 'race: White', 'capital-gain: 0'), Support: 0.7390\n",
      "Frequent Items 15: ('capital-loss: 0', 'annual-income: <=50K'), Support: 0.7363\n",
      "Frequent Items 16: ('capital-gain: 0', 'annual-income: <=50K'), Support: 0.7274\n",
      "Frequent Items 17: ('native-country: United-States', 'race: White', 'capital-gain: 0'), Support: 0.7177\n",
      "Frequent Items 18: ('capital-loss: 0', 'capital-gain: 0', 'annual-income: <=50K'), Support: 0.7045\n",
      "========== Association Rules ==========\n",
      "Association Rule 1: ('annual-income: <=50K',) ==> ('capital-loss: 0',), Confidence: 0.9698\n",
      "Association Rule 2: ('capital-gain: 0', 'annual-income: <=50K') ==> ('capital-loss: 0',), Confidence: 0.9685\n",
      "Association Rule 3: ('annual-income: <=50K',) ==> ('capital-gain: 0',), Confidence: 0.9581\n",
      "Association Rule 4: ('capital-loss: 0', 'annual-income: <=50K') ==> ('capital-gain: 0',), Confidence: 0.9568\n",
      "Association Rule 5: ('native-country: United-States',) ==> ('capital-loss: 0',), Confidence: 0.9527\n",
      "Association Rule 6: ('race: White',) ==> ('capital-loss: 0',), Confidence: 0.9516\n",
      "Association Rule 7: ('native-country: United-States', 'race: White') ==> ('capital-loss: 0',), Confidence: 0.9504\n",
      "Association Rule 8: ('capital-gain: 0',) ==> ('capital-loss: 0',), Confidence: 0.9491\n",
      "Association Rule 9: ('native-country: United-States', 'capital-gain: 0') ==> ('capital-loss: 0',), Confidence: 0.9484\n",
      "Association Rule 10: ('race: White', 'capital-gain: 0') ==> ('capital-loss: 0',), Confidence: 0.9470\n",
      "Association Rule 11: ('annual-income: <=50K',) ==> ('capital-loss: 0', 'capital-gain: 0'), Confidence: 0.9280\n",
      "Association Rule 12: ('race: White',) ==> ('native-country: United-States',), Confidence: 0.9211\n",
      "Association Rule 13: ('capital-loss: 0', 'race: White') ==> ('native-country: United-States',), Confidence: 0.9199\n",
      "Association Rule 14: ('race: White', 'capital-gain: 0') ==> ('native-country: United-States',), Confidence: 0.9198\n",
      "Association Rule 15: ('native-country: United-States',) ==> ('capital-gain: 0',), Confidence: 0.9153\n",
      "Association Rule 16: ('race: White',) ==> ('capital-gain: 0',), Confidence: 0.9134\n",
      "Association Rule 17: ('capital-loss: 0',) ==> ('capital-gain: 0',), Confidence: 0.9126\n",
      "Association Rule 18: ('native-country: United-States', 'race: White') ==> ('capital-gain: 0',), Confidence: 0.9121\n",
      "Association Rule 19: ('native-country: United-States', 'capital-loss: 0') ==> ('capital-gain: 0',), Confidence: 0.9111\n",
      "Association Rule 20: ('capital-loss: 0', 'race: White') ==> ('capital-gain: 0',), Confidence: 0.9090\n",
      "Association Rule 21: ('capital-loss: 0',) ==> ('native-country: United-States',), Confidence: 0.8953\n",
      "Association Rule 22: ('capital-gain: 0',) ==> ('native-country: United-States',), Confidence: 0.8945\n",
      "Association Rule 23: ('capital-loss: 0', 'capital-gain: 0') ==> ('native-country: United-States',), Confidence: 0.8938\n",
      "Association Rule 24: ('native-country: United-States',) ==> ('race: White',), Confidence: 0.8783\n",
      "Association Rule 25: ('native-country: United-States', 'capital-loss: 0') ==> ('race: White',), Confidence: 0.8761\n",
      "Association Rule 26: ('race: White',) ==> ('native-country: United-States', 'capital-loss: 0'), Confidence: 0.8754\n",
      "Association Rule 27: ('native-country: United-States', 'capital-gain: 0') ==> ('race: White',), Confidence: 0.8753\n",
      "Association Rule 28: ('native-country: United-States',) ==> ('capital-loss: 0', 'capital-gain: 0'), Confidence: 0.8680\n",
      "Association Rule 29: ('race: White',) ==> ('capital-loss: 0', 'capital-gain: 0'), Confidence: 0.8650\n",
      "Association Rule 30: ('capital-loss: 0',) ==> ('race: White',), Confidence: 0.8527\n",
      "Association Rule 31: ('capital-gain: 0',) ==> ('race: White',), Confidence: 0.8512\n",
      "Association Rule 32: ('capital-loss: 0', 'capital-gain: 0') ==> ('race: White',), Confidence: 0.8493\n",
      "Association Rule 33: ('capital-gain: 0',) ==> ('native-country: United-States', 'capital-loss: 0'), Confidence: 0.8483\n",
      "Association Rule 34: ('race: White',) ==> ('native-country: United-States', 'capital-gain: 0'), Confidence: 0.8401\n",
      "Association Rule 35: ('native-country: United-States',) ==> ('capital-loss: 0', 'race: White'), Confidence: 0.8347\n",
      "Association Rule 36: ('capital-loss: 0',) ==> ('native-country: United-States', 'capital-gain: 0'), Confidence: 0.8157\n",
      "Association Rule 37: ('capital-loss: 0', 'capital-gain: 0') ==> ('annual-income: <=50K',), Confidence: 0.8097\n",
      "Association Rule 38: ('capital-gain: 0',) ==> ('capital-loss: 0', 'race: White'), Confidence: 0.8061\n",
      "Association Rule 39: ('native-country: United-States',) ==> ('race: White', 'capital-gain: 0'), Confidence: 0.8011\n",
      "Association Rule 40: ('capital-gain: 0',) ==> ('annual-income: <=50K',), Confidence: 0.7935\n",
      "Association Rule 41: ('capital-loss: 0',) ==> ('native-country: United-States', 'race: White'), Confidence: 0.7844\n",
      "Association Rule 42: ('capital-gain: 0',) ==> ('native-country: United-States', 'race: White'), Confidence: 0.7829\n",
      "Association Rule 43: ('capital-loss: 0',) ==> ('race: White', 'capital-gain: 0'), Confidence: 0.7751\n",
      "Association Rule 44: ('capital-loss: 0',) ==> ('annual-income: <=50K',), Confidence: 0.7723\n",
      "Association Rule 45: ('capital-gain: 0',) ==> ('capital-loss: 0', 'annual-income: <=50K'), Confidence: 0.7685\n",
      "Association Rule 46: ('capital-loss: 0',) ==> ('capital-gain: 0', 'annual-income: <=50K'), Confidence: 0.7390\n"
     ]
    }
   ],
   "source": [
    "# Print out the frequent itemsets and association rules obtained from the Apriori algorithm of the input file.\n",
    "items_with_support, association_rules = apriori_optimized(input_file, min_support, min_confidence)\n",
    "print(\"========== Frequent Common Patterns ==========\")\n",
    "printFrequentItems(items_with_support)\n",
    "print(\"========== Association Rules ==========\")\n",
    "printRules(association_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other resources\n",
    "In this tutorial, we introduced the Apriori algorithm and one way to optimize it by reducing the number of required scans. If you would like to know more about the algorithm and other optimization techniques, here are some relevant resources. Hope you enjoyed this tutorial! \n",
    "\n",
    "1\\. A Method to Optimize Apriori Algorithm for Frequent Items Mining: http://ieeexplore.ieee.org/document/7064142/?reload=true\n",
    "\n",
    "2\\. The Optimization and Improvement of the Apriori Algorithm: http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732130\n",
    "\n",
    "3\\. New Approach to Optimize the Time of Association Rules Extraction: https://arxiv.org/pdf/1312.4800.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
