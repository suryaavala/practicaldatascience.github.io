{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will introduce you to the Gaussian Naive Bayes. algorithm, which is one of the simplest and most commonly used classification algorithms in machine learning. Although the traditional naive Bayes will be covered in the lecture of 15-688, we will focus on the Gaussian Naive Bayes. We will introduce the rationale and theoretical framework behind the Gaussian Naive Bayes classifier, discuss about its advantages and disadvantages, and finally walk you through an example implementation of the Gaussian Naive Bayes classifier with an example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial content\n",
    "In this tutorial, we will show how the Gaussian Naive Bayes algorithm works and how to implement a Gaussian Naive Bayes classifier with an example dataset.\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- Theoretical framework\n",
    "- Pros and Cons of Gaussian Naive Bayes classifier\n",
    "- Example application\n",
    "- References and further resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical framework\n",
    "In this section, we will introduce the theoretical frameworks of Gaussian Naive Bayes classifier:\n",
    "1. Assumption of Independent and Identically Distribution\n",
    "2. Bayes Rule\n",
    "3. Conditional Independence Assumption\n",
    "4. Gaussian Distribution Assumption\n",
    "5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Assumption of Independent and Identically Distribution\n",
    "We assume our data are drawn independent and identically distributed (i.i.d) from a joint probability distribution over feature vectors X and labels Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bayes Rule\n",
    "$$P(Y\\mid X) = \\frac{P(X\\mid Y)P(Y)}{P(X)}$$\n",
    "\n",
    "Equivalently: $$P(Y\\mid X) = \\frac{P(X\\mid Y)P(Y)}{\\sum_k P(X\\mid Y_{k})P(Y_k)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conditional Independence Assumption\n",
    "Gaussian Naive Bayes assumes that  $X_i$ and $X_j$ are conditionally independent given Y, for all iâ‰ j. (This is why we call it \"Naive\").\n",
    "\n",
    "$$P(X_1...X_n\\mid Y) =\\prod_i P(X_i\\mid Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gaussian Distribution Assumption\n",
    "Gaussian Naive Bayes assumes that the values of each numerical attributes are normally distributed. (This is why we call it \"Gaussian\").\n",
    "\n",
    " $$P(X_i\\mid Y_k) \\sim {\\mathcal {N}}(\\mu_{ik} ,\\sigma_{ik} ^{2})$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Summary\n",
    "To predict the label, we will choose the most probable class label given X = {$X_1$...$X_n$}.\n",
    "\n",
    "$$\\DeclareMathOperator*{\\argmax}{argmax} \\widehat{y} = \\argmax_y P(Y = y | X = x)\\ $$\n",
    "\n",
    "Using the Bayes Rule, we can rewrite the formula as follows:\n",
    "\n",
    "$$\\DeclareMathOperator*{\\argmax}{argmax} \\widehat{y} = \\argmax_y \\frac{P(X_1...X_n| Y=y)P(Y = y)}{P(X_1...X_n)} \\\\=\\argmax_y  P(X_1...X_n| Y=y)P(Y = y)$$\n",
    "\n",
    "Using the Conditional Independence Assumption, we can rewrite the formula as follows:\n",
    "\n",
    "$$\\DeclareMathOperator*{\\argmax}{argmax} \\widehat{y} = \\argmax_y \\prod_i P(X_i| Y=y)P(Y = y)$$\n",
    "\n",
    "Using the Gaussian Distribution Assumption, we can compute $P(X_i| Y=y)$ as follows:\n",
    "\n",
    "$$ P(X_i|Y=y_k) = {{\\frac {1}{\\sqrt {2\\pi \\sigma_{ik} ^{2}}}}e^{-{\\frac {(x-\\mu_{ik} )^{2}}{2\\sigma_{ik} ^{2}}}},}$$\n",
    "where $\\mu_{ik}$ and $\\sigma_{ik}$ are the mean and standard deviation of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Gaussian Naive Bayes algorithm\n",
    "This section will briefly discuss the advantages and disadvantages of the Gaussian Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "Gaussian Naive Bayes classifier is a simple and powerful algorithm to make predictions, especially when dataset is small. It has following advantages:\n",
    "1. __Gaussian Naive Bayes can handle continuous numeric attributes.__ <br>\n",
    "> When an numerical attribute is continuous, it is impossible to count the frequency. In this case, we would calculate the probability densities using probability density functions. Gaussian Naive Bayes assumes that each numerical attribute is normally distributed. Thus we can easily calculate the probability densities of the normal distribution.\n",
    "\n",
    "2. __Gaussian Naive Bayes can handle missing data.__ <br>\n",
    "> If a data instance has a missing value for an attribute, it can be ignored, because Gaussian Naive Bayes handles attributes seperately while calculating probabilities.\n",
    "\n",
    "3. __Gaussian Naive Bayes performs well even with small datasets.__ <br>\n",
    "> Gaussian Naive Bayes does not need too much data to learn about the probabilistic relationship between an certain attribute and the predicted attribute. Furthermore, it is less likely to overfit the training data with a smaller sample size.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disdvantages\n",
    "Although Gaussian Naive Bayes algorithm usually perform well, in some cases, somehow it might perform poorly, if used inappropriately. Gaussian Naive Bayes algorithm has following disadvantages:\n",
    "1. __Gaussian Naive Bayes makes very strong assumptions.__ <br>\n",
    "> Due to the Conditional Independence assumption and Gaussian Distribution assumption, Gaussian Naive Bayes may not work when any two attributes are highly correlated given a class value or when any attribute's distribution is highly skewed. However, sometimes the predicted result is surprisingly robust when the assumptions are somewhat violated.\n",
    "\n",
    "2. __Gaussian Naive Bayes is not good at handling large datasets__ <br>\n",
    "> Underfitting might occurs when Gaussian Naive Bayes is trained on a large dataset or when the data distribution is uneven.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example application\n",
    "In this section, we will walk through the Gaussian Naive Bayes algorithm with the Blood Transfusion Service Center Data Set from the donor database of Blood Transfusion Service Center in Hsin-Chu City in Taiwan. The dataset is composed of 748 donor data, each one includes 4 attributes: R (Recency - months since last donation), F (Frequency - total number of donation), M (Monetary - total blood donated in c.c.), T (Time - months since first donation). The predicted attribute is a binary indicator representing whether he/she donated blood in March 2007 (1 stand for donating blood; 0 stands for not donating blood).\n",
    "\n",
    "Our implementation is broken into the following steps:\n",
    "\n",
    "1. Import data\n",
    "2. Split dataset\n",
    "3. Seperate data by class\n",
    "4. Summarize attribute distributions for each instance\n",
    "5. Calculate probabilities for each instance\n",
    "6. Classify\n",
    "7. Evaluate the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import data\n",
    "First, we need to load the data in csv format using the reader function in the csv module. We remove the header row and convert the data into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "def loadDataset(filename):\n",
    "    with open(filename, 'rb') as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        next(csvfile)\n",
    "        dataset = list(lines)\n",
    "    dataset = np.array(dataset)\n",
    "    dataset = dataset.astype('float')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the loadDataset function to import the Blood Transfusion Service Center dataset. We can test it by printing the number of instances and the number of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Instances: 748\n",
      "Number of Attributes: 5\n"
     ]
    }
   ],
   "source": [
    "dataset = loadDataset('transfusion.csv')\n",
    "print ('Number of Instances: ' + repr(len(dataset)))\n",
    "print ('Number of Attributes: ' + repr(len(dataset[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split dataset\n",
    "Next, we need to split the data into a training dataset and a test dataset randomly with a given split ratio, by using the random module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def splitDataset(dataset, split):\n",
    "    trainingSet = []\n",
    "    testSet = list(dataset)\n",
    "    trainingSize = int(len(dataset) * split)\n",
    "    while len(trainingSet) < trainingSize:\n",
    "        i = random.randrange(len(testSet))\n",
    "        trainingSet.append(testSet.pop(i))\n",
    "    trainingSet = np.array(trainingSet)\n",
    "    testSet = np.array(testSet)\n",
    "    return trainingSet, testSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the splitDataset function to split the loaded data into a training dataset and a test dataset, with a ratio of 67% training and 33% test. We can test it by printing the size of each splitted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 501\n",
      "Test: 247\n"
     ]
    }
   ],
   "source": [
    "train, test = splitDataset(dataset, 0.67)\n",
    "print('Train: ' + repr(len(train)))\n",
    "print('Test: ' + repr(len(test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Seperate data by class\n",
    "Next, in order to summarize statistics for each class, we need to seperate the training dataset instances by class value first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seperate(dataset):\n",
    "    yes = []\n",
    "    no = []\n",
    "    for row in dataset:\n",
    "        if row[-1] == 1:\n",
    "            yes.append(row)\n",
    "        else :\n",
    "            no.append(row)\n",
    "    yes = np.array(yes)\n",
    "    no = np.array(no)\n",
    "    return (no, yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the function with some sample data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: array([[ 2, 12,  0],\n",
      "       [ 4, 14,  0]])\n",
      "Yes: array([[ 1, 11,  1],\n",
      "       [ 3, 13,  1]])\n"
     ]
    }
   ],
   "source": [
    "sampleData = np.array([[1,11,1],[2,12,0],[3,13,1],[4,14,0]])\n",
    "no, yes = seperate(sampleData)\n",
    "print('No: ' + repr(no))\n",
    "print('Yes: ' + repr(yes)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Summarize attribute distributions for each class\n",
    "Now we need to summarize statistics for each class, by calculating the mean and standard deviation for each attribute in the class, because Gaussian Naive Bayes model has assumed that each attribute in each class is accorded with Gaussian Distribution, as known as Normal Distribution. We use the builtin function in the numpy module to calculate the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def summarize(dataset):\n",
    "    instances = seperate(dataset)\n",
    "    summary = {}\n",
    "    for k, i in enumerate(instances):\n",
    "        summary_i = []\n",
    "        for atr in i.T[:-1]:\n",
    "            mean = atr.mean()\n",
    "            std = atr.std(ddof = 1)\n",
    "            summary_i.append((mean, std))\n",
    "        summary[k] = summary_i\n",
    "    return summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the function with some sample data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: {0: [(3.0, 1.4142135623730951), (13.0, 1.4142135623730951)], 1: [(2.0, 1.4142135623730951), (12.0, 1.4142135623730951)]}\n"
     ]
    }
   ],
   "source": [
    "sampleData = np.array([[1,11,1],[2,12,0],[3,13,1],[4,14,0]])\n",
    "summary = summarize(sampleData)\n",
    "print('Summary: ' + repr(summary)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take a look at the summary statistics for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: {0: [(10.771653543307087, 8.791014230790875), (4.6955380577427821, 4.4893682636981813), (1173.8845144356956, 1122.3420659245455), (33.490813648293965, 24.094726320120291)], 1: [(5.625, 5.094176111217676), (7.6083333333333334, 7.8497399553709277), (1902.0833333333333, 1962.4349888427319), (32.333333333333336, 23.650229033422345)]}\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(train)\n",
    "print('Summary: ' + repr(summary)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate probabilities for each class\n",
    "Now with the summary statistics for each class, we can calculate the probability of a class and the probability of a data instance belonging to a given class.\n",
    "\n",
    "Calculating the probability of a class is fairly easy, so we will not get into details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probY(trainingSet):\n",
    "    prob_yes = float(sum(trainingSet[:,-1]))/len(trainingSet)\n",
    "    prob_no = 1 - prob_yes\n",
    "    return {0: prob_no, 1: prob_yes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the probability of a data instance belonging to a given class, by multiplying together the probibilities of all of the attribute values for the data instance. Therefore, for each class value, we come up with a probability of a data instance belonging to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def probXGivenY(testSet, summary):\n",
    "    prob_xgiveny = []\n",
    "    for x in testSet:\n",
    "        prob = {}\n",
    "        for k in summary:\n",
    "            prob_k = 1;\n",
    "            for i, xi in enumerate(x[:-1]):\n",
    "                meani = summary[k][i][0]\n",
    "                stdi = summary[k][i][1]\n",
    "                prob_ki = (1/(((2*np.pi)**0.5)*stdi)) * np.exp((-(xi-meani)**2)/(2*(stdi**2))) \n",
    "                prob_k *= prob_ki\n",
    "            prob[k] = prob_k\n",
    "        prob_xgiveny.append(prob)\n",
    "    return prob_xgiveny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the function with some sample data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Y: {0: 0.5, 1: 0.5}\n",
      "Probability of X given Y: [{0: 0.010769639650924319, 1: 0.00088402585592600919}]\n"
     ]
    }
   ],
   "source": [
    "sampleTrain = np.array([[1,11,1],[2,12,0],[3,13,1],[4,14,0]])\n",
    "summary = summarize(sampleTrain)\n",
    "x = [[5,15,1]]\n",
    "prob_Y = probY(sampleTrain)\n",
    "prob_XGivenY = probXGivenY(x, summary)\n",
    "print('Probability of Y: ' + repr(prob_Y))\n",
    "print('Probability of X given Y: ' + repr(prob_XGivenY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Classify\n",
    "Now that we can calculate the probability of a data instance belonging to each class, we are able to choose the largest probability and take its corresponding class value as prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(probXGivenY, probY):\n",
    "    ypred = []\n",
    "    for pxy in probXGivenY:\n",
    "        prob_no = pxy[0] * probY[0]\n",
    "        prob_yes = pxy[1] * probY[1]\n",
    "        if prob_no > prob_yes:\n",
    "            ypred.append(0.0)\n",
    "        else:\n",
    "            ypred.append(1.0)\n",
    "        \n",
    "    ypred = np.array(ypred)\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the function with some sample data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: array([ 0.])\n"
     ]
    }
   ],
   "source": [
    "y_pred = classify(prob_XGivenY,prob_Y)\n",
    "print ('Prediction: '+ repr(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluate the accuracy\n",
    "Now that we have the result of classification prediction, we can compare the predictions with the class value in the test dataset. Moreover, we can evaluate the classification accuracy by calculating a ratio of the total number of correct predictions out of all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAccuracy(ypred, testSet):\n",
    "    count = 0.0\n",
    "    for i in range(len(ypred)):\n",
    "        if ypred[i] == testSet[i,-1]:\n",
    "            count += 1.0\n",
    "    return count/len(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the function with some sample data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.666666666667\n"
     ]
    }
   ],
   "source": [
    "testSet = np.array([[1,1], [2,0], [3,1]])\n",
    "predictions = np.array([1, 1, 1])\n",
    "accuracy = getAccuracy(predictions, testSet)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting together\n",
    "We now have all the functions and we need tie them together. Below is the complete example application of the Gaussian Naive Bayes algorithm implemented in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 501\n",
      "Test: 247\n",
      "Accuracy: 0.7692307692307693\n",
      "[ 1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  1.  1.  0.]\n",
      "[ 1.  1.  1.  1.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  1.  1.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "#prepare data\n",
    "split = 0.67\n",
    "dataset = loadDataset('transfusion.csv')\n",
    "train, test = splitDataset(dataset, split)\n",
    "print('Train: ' + repr(len(train)))\n",
    "print('Test: ' + repr(len(test))) \n",
    "\n",
    "#train data\n",
    "summary = summarize(train)\n",
    "prob_Y = probY(train)\n",
    "\n",
    "#generate predictions\n",
    "prob_XGivenY = probXGivenY(test, summary)\n",
    "ypred = classify(prob_XGivenY, prob_Y)\n",
    "accuracy = getAccuracy(ypred, test)\n",
    "\n",
    "print('Accuracy: ' + repr(accuracy))\n",
    "\n",
    "#example outputs\n",
    "print(ypred[0:18])\n",
    "print(test[0:18,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References and further resources\n",
    "1. Blood Transfusion Service Center Data Set: https://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center\n",
    "2. Wikipedia: Naive Bayes classifier. https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "3. Wikipedia: Normal distribution: https://en.wikipedia.org/wiki/Normal_distribution\n",
    "4. Better Naive Bayes: 12 Tips To Get The Most From The Naive Bayes Algorithm. https://machinelearningmastery.com/better-naive-bayes/\n",
    "5. Tom M. Mitchell (1997). Machine Learning. Chapter 6."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
