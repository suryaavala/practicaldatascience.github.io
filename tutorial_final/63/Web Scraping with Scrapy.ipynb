{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Scrapy\n",
    "## Introduction\n",
    "This tutorial provides an introduction to data collection through web scraping using the Python library Scrapy. \n",
    "Data science is dependent on having data to analyze, and with the abundance of data available on the internet, web scraping&mdash;fetching and extracting data from webpages&mdash;has become a popular and effective way to collect that data. Scrapy provides an convenient API for programmatically extracting data from connected web pages in relatively few lines of code.\n",
    "\n",
    "![Scrapy](https://scrapy.org/img/scrapylogo.png)\n",
    "\n",
    "Scrapy is a Python library that deals with web pages, so readers who are new to Python 3 or are unfamiliar with the structure of web pages may want to first go through [the official Python Tutorial](https://docs.python.org/3/tutorial/) or brush up on [HTML](https://developer.mozilla.org/en-US/docs/Learn/HTML/Introduction_to_HTML) and [CSS](https://developer.mozilla.org/en-US/docs/Learn/CSS/Introduction_to_CSS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Installation](#Installation)\n",
    "2. [Getting Started with Spiders](#Getting-Started-with-Spiders)\n",
    "3. [Running Spiders](#Running-Spiders)\n",
    "4. [Following Links](#Following-Links)\n",
    "5. [Making Requests and Using APIs](#Making-Requests-and-Using-APIs)\n",
    "6. [Summary and Additional Resources](#Summary-and-Additional-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "The recommended way to install Scrapy for users who use Anaconda is to open an Anaconda prompt and run the command \n",
    "    \n",
    "<code>$ conda install -c conda-forge scrapy</code>\n",
    "\n",
    "You can also install using pip by running\n",
    "\n",
    "<code>$ pip install Scrapy</code>\n",
    "\n",
    "If you have any issues while installing, you can refer to the [platform-specific instructions](https://doc.scrapy.org/en/latest/intro/install.html#platform-specific-installation-notes) in Scrapy's documentation.\n",
    "\n",
    "After installation, the below imports should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Spiders\n",
    "The main component of a Scrapy web scraper is the **Spider**. Spiders are user defined classes that extend [`scrapy.Spider`](https://doc.scrapy.org/en/latest/topics/spiders.html#scrapy-spider) and have methods specifying which web requests to make, which pages and links to follow, and what data on each page to extract. Here's a simple example spider that scrapes and prints a few tweets from a page on [Twitter](https://twitter.com/BarackObama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class SimpleExampleSpider(scrapy.Spider):\n",
    "    name = \"simple example\"\n",
    "    start_urls = ['https://twitter.com/BarackObama']\n",
    "\n",
    "    def parse(self, response):\n",
    "        tweets = dict()\n",
    "        for tweet in response.css('div.content'):\n",
    "            time = tweet.css('span._timestamp::text').extract_first()\n",
    "            text = tweet.css('p.tweet-text::text').extract()\n",
    "            tweets[time] = ''.join(text)\n",
    "            if self.settings['FEED_URI'] == None: print(time + ': ' + ''.join(text) + '\\n')\n",
    "        yield tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `SimpleExampleSpider` defines some required attributes and methods. The `name` attribute is a unique identifier for our spider, `start_urls` is a list of urls to start scraping from, and `parse()` is the method that parses the page and extracts the data we want.\n",
    "\n",
    "When the spider is run, HTTP requests are made to each of the urls in `start_urls`, and `parse()` is used as a callback method: it's called after each request with the HTTP response as an argument to extract data from the response. \n",
    "\n",
    "A convenient way to do this is with [CSS selectors](https://www.w3.org/TR/selectors/) and the `css()` method. In the above example, `response.css('div.content')` returns all &lt;div&gt; tags in the page that have class `content`. The next lines then select the &lt;span&gt; and &lt;p&gt; tags with the `_timestamp` and `tweet-text` classes, respectively, and select the text inside the tags with the `::text` selector. \n",
    "\n",
    "The `extract_first()` and `extract()` methods get the text from the [`Selector`](https://doc.scrapy.org/en/latest/topics/selectors.html#selector-objects) object returned by the `css()` method, with `extract_first()` returning only the first piece of text found inside the tag and `extract()` returning a list of all the text found inside the tag.\n",
    "\n",
    "CSS selectors offer many other ways to select elements in a page. Here are some of the more commonly used selectors.\n",
    "\n",
    "For an HTML element of type `E`:\n",
    "\n",
    "|||\n",
    "|:---|:---|\n",
    "|`E.myclass`|Specifies an element of class `myclass`|\n",
    "|`E#myid`|Specifies an element with id `myid`|\n",
    "|`E[attr]`|Specifies an element with attribute `attr`|\n",
    "|`E[attr=val]`|Specifies an element with attribute `attr` having value `val`|\n",
    "|`E C`|Specifies an element of type `C` that is a descendant of an element of type `E`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Spiders\n",
    "Scrapy offers two main ways to run crawlers: a command line interface and a Python API. In this tutorial, we'll be using the API to run crawlers from Python scripts, but you can visit [this page](https://doc.scrapy.org/en/latest/topics/commands.html) for more information about the command line tools. \n",
    "\n",
    "Spiders can be run in Python by creating a [`CrawlerProcess`](https://doc.scrapy.org/en/latest/topics/api.html#scrapy.crawler.CrawlerProcess), which launches a Twisted [`reactor`](https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html) to handle network communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mar 25: Incredible to have a Chicago team in the Final Four. I’ll take that over an intact bracket any day! Congratulations to everybody  - let’s keep it going!\n",
      "\n",
      "Mar 24: Michelle and I are so inspired by all the young people who made today’s marches happen. Keep at it. You’re leading us forward. Nothing can stand in the way of millions of voices calling for change.\n",
      "\n",
      "Mar 19: Our most important task as a nation is to make sure all our young people can achieve their dreams. We’ve started this work with , but there’s so much more all of us have to do—government, private sector, academia & community leaders—to change the odds for our kids.\n",
      "\n",
      "Mar 19: In Singapore with young people who are advocating for education, empowering young women, and getting involved all over Southeast Asia with a profoundly optimistic commitment to building the world they want to see.\n",
      "\n",
      "Mar 15: 41: I like the competition. And the loyalty to the home team. - 44\n",
      "\n",
      "Mar 15: Congrats to  and Sister Jean for a last-second upset - I had faith in my pick!\n",
      "\n",
      "Mar 14: Just because I have more time to watch games doesn’t mean my picks will be better, but here are my brackets this year: \n",
      "\n",
      "Mar 14: Have fun out there among the stars.\n",
      "\n",
      "Mar 12: Four years ago,  and I had the privilege to host Lt. Cmdr. Dan Cnossen and his fellow Paralympians and Olympians at the White House. Today, we’re so proud of him for winning gold and silver - while still representing the red, white, and blue.\n",
      "\n",
      "Feb 27: I got my start holding community meetings in Chicago, so it was fun to be home for one tonight. Michelle and I want the world to come together on the South Side at a place built with local ideas, values, and hopes. That’s the  and Presidential Center.\n",
      "\n",
      "Feb 22: Young people have helped lead all our great movements. How inspiring to see it again in so many smart, fearless students standing up for their right to be safe; marching and organizing to remake the world as it should be. We've been waiting for you. And we've got your backs.\n",
      "\n",
      "Feb 21: Billy Graham was a humble servant who prayed for so many - and who, with wisdom and grace, gave hope and guidance to generations of Americans.\n",
      "\n",
      "Feb 15: We are grieving with Parkland. But we are not powerless. Caring for our kids is our first job. And until we can honestly say that we're doing enough to keep them safe from harm, including long overdue, common-sense gun safety laws that most Americans want, then we have to change.\n",
      "\n",
      "Feb 14: Happy Valentine’s Day, . You make every day and every place better.\n",
      "\n",
      "Feb 14: Happy  to my one and only, . To celebrate the occasion, I’m dedicating a little Valentine’s Day playlist to you!  \n",
      "\n",
      "Jan 15: Dr. King was 26 when the Montgomery bus boycott began. He started small, rallying others who believed their efforts mattered, pressing on through challenges and doubts to change our world for the better. A permanent inspiration for the rest of us to keep pushing towards justice.\n",
      "\n",
      "29 Dec 2017: All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\n",
      "\n",
      "29 Dec 2017: Ten-year-old Jahkil Jackson is on a mission to help homeless people in Chicago. He created kits full of socks, toiletries, and food for those in need. Just this week, Jahkil reached his goal to give away 5,000 “blessing bags.” That’s a story from 2017.\n",
      "\n",
      "29 Dec 2017: Chris Long gave his paychecks from the first six games of the NFL season to fund scholarships in Charlottesville, VA. He wanted to do more, so he decided to give away an entire season’s salary. That’s a story from 2017.\n",
      "\n",
      "29 Dec 2017: Kat Creech, a wedding planner in Houston, turned a postponed wedding into a volunteer opportunity for Hurricane Harvey victims. Thirty wedding guests became an organization of hundreds of volunteers. That’s a story from 2017.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crawler = CrawlerProcess({\n",
    "    'LOG_ENABLED': False,\n",
    "})\n",
    "\n",
    "crawler.crawl(SimpleExampleSpider)\n",
    "crawler.start() # blocks until done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we instantiate a `CrawlerProcess` with settings for the crawler, call `crawl()` with our spider, and `start()` the crawler. \n",
    "\n",
    "The constructor for `CrawlerProcess` takes in a `dict` of settings. A full list of the various settings can be found [here](https://doc.scrapy.org/en/latest/topics/settings.html). Some of the more important settings include `FEED_URI` and `FEED_FORMAT`, which can be used to specify an output file and format for your spider. Calling the constructor like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = CrawlerProcess({\n",
    "    'LOG_ENABLED': False,\n",
    "    'FEED_URI': 'out.json',\n",
    "    'FEED_FORMAT': 'json',\n",
    "})\n",
    "\n",
    "crawler.crawl(SimpleExampleSpider)\n",
    "crawler.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "would put the spider output in json format in a file called out.json. Settings for the export feed (output of the crawler) can be found on [this page](https://doc.scrapy.org/en/latest/topics/feed-exports.html#feed-exports).\n",
    "\n",
    "One thing to note is that a crawler cannot be run more than once in the lifetime of a process. This is because the Twisted `reactor` that `CrawlerProcess` uses is started when `crawler.start()` is called and is stopped after the crawler finishes, and a `reactor`, once stopped, cannot be restarted. Although this isn't an issue when running a Python script for a crawler,  Jupyter Notebooks like this one will not be able to run consecutive cells with crawlers without restarting the kernel. You will need to restart the kernel and run cells containing `CrawlerProcess`es one at a time if you want to run the example code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following Links\n",
    "The example spider we've used so far has shown us how to extract data from a single page, but in order to extract data from multiple pages on a site, we'll need to follow links. Following links allows our spiders to deal with pagination and enables data collection from any page connected to one of the start urls. With Scrapy, following links is as simple as calling `response.follow()`. The next example scrapes data from IMDb about the [top 250 movies](http://www.imdb.com/search/title?groups=top_250&sort=user_rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class IMDbSpider(scrapy.Spider):\n",
    "    name = 'IMDb'\n",
    "    start_urls = ['http://www.imdb.com/search/title?groups=top_250&sort=user_rating']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for movie_link in response.css('h3.lister-item-header a::attr(href)').extract():\n",
    "            yield response.follow(movie_link, callback=self.parse_movie)\n",
    "        \n",
    "        next_link = response.css('div.desc a.lister-page-next::attr(href)').extract_first()\n",
    "        yield response.follow(next_link, callback=self.parse)\n",
    "    \n",
    "    def parse_movie(self, response):\n",
    "        name = response.css('h1[itemprop=name]::text').extract_first()\n",
    "        date = response.css('a meta[itemprop=datePublished]::attr(content)').extract_first()\n",
    "        rating = response.css('span[itemprop=ratingValue]::text').extract_first()\n",
    "        rating_count = response.css('span.small[itemprop=ratingCount]::text').extract_first()\n",
    "        genres = response.css('span.itemprop[itemprop=genre]::text').extract()\n",
    "        content_rating = response.css('div.subtext meta[itemprop=contentRating]::attr(content)').extract_first()\n",
    "        length = response.css('time[itemprop=duration]::text').extract_first()\n",
    "        storyline = ''.join(response.css('div[itemprop=description] p::text').extract())\n",
    "        keywords = response.css('span[itemprop=keywords]::text').extract()\n",
    "        yield {\n",
    "            'name': name.strip(),\n",
    "            'date': date.strip(),\n",
    "            'rating': rating.strip(),\n",
    "            'rating_count': rating_count.strip(),\n",
    "            'genres': genres,\n",
    "            'content_rating': content_rating.strip(),\n",
    "            'length': length.strip(),\n",
    "            'storyline': storyline.strip(),\n",
    "            'keywords': keywords,\n",
    "        }\n",
    "        \n",
    "crawler = CrawlerProcess({\n",
    "    'LOG_ENABLED': False,\n",
    "    'FEED_URI': 'imdb_top_250.json',\n",
    "    'FEED_FORMAT': 'json',\n",
    "})\n",
    "\n",
    "crawler.crawl(IMDbSpider)\n",
    "crawler.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here instead of directly parsing the response passed to `parse()`, we iterate through the links to movies found in the page and call `response.follow()` on each link, which we extract using the `a::attr(href)` selector. The `follow()` method then makes an HTTP request in the same way requests are made to start the spider, with the `callback` keyword argument specifying a method to parse the response. After following all the links to movies on the page, we follow the link to the next page of the listing, and the callback `parse()` is called again to recursively parse the rest of the pages.\n",
    "\n",
    "The output of the above example, a file named 'imdb_top_250.json', should look something like this:\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"name\": \"The Matrix\",\n",
    "        \"date\": \"1999-03-31\",\n",
    "        \"rating\": \"8.7\",\n",
    "        \"rating_count\": \"1,388,432\",\n",
    "        \"genres\": [\n",
    "            \"Action\",\n",
    "            \"Sci-Fi\"\n",
    "        ],\n",
    "        \"content_rating\": \"R\",\n",
    "        \"length\": \"2h 16min\",\n",
    "        \"storyline\": \"Thomas A. Anderson is a man living two lives. By day he is an average computer programmer and by night a hacker known as Neo. Neo has always questioned his reality, but the truth is far beyond his imagination. Neo finds himself targeted by the police when he is contacted by Morpheus, a legendary computer hacker branded a terrorist by the government. Morpheus awakens Neo to the real world, a ravaged wasteland where most of humanity have been captured by a race of machines that live off of the humans' body heat and electrochemical energy and who imprison their minds within an artificial reality known as the Matrix. As a rebel against the machines, Neo must return to the Matrix and confront the agents: super-powerful computer programs devoted to snuffing out Neo and the entire human rebellion.\",\n",
    "        \"keywords\": [\n",
    "            \"artificial reality\",\n",
    "            \"simulated reality\",\n",
    "            \"post apocalypse\",\n",
    "            \"questioning reality\",\n",
    "            \"war with machines\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"One Flew Over the Cuckoo's Nest\",\n",
    "        \"date\": \"1975-11-19\",\n",
    "        \"rating\": \"8.7\",\n",
    "        \"rating_count\": \"771,037\",\n",
    "        \"genres\": [\n",
    "            \"Drama\"\n",
    "        ],\n",
    "        \"content_rating\": \"R\",\n",
    "        \"length\": \"2h 13min\",\n",
    "        \"storyline\": \"McMurphy has a criminal past and has once again gotten himself into trouble and is sentenced by the court. To escape labor duties in prison, McMurphy pleads insanity and is sent to a ward for the mentally unstable. Once here, McMurphy both endures and stands witness to the abuse and degradation of the oppressive Nurse Ratched, who gains superiority and power through the flaws of the other inmates. McMurphy and the other inmates band together to make a rebellious stance against the atrocious Nurse.\",\n",
    "        \"keywords\": [\n",
    "            \"mental institution\",\n",
    "            \"escape\",\n",
    "            \"evil woman\",\n",
    "            \"psychiatric examination\",\n",
    "            \"mental illness\"\n",
    "        ]\n",
    "    },\n",
    "    ....\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Requests and Using APIs\n",
    "So far, we've seen how requests are made to the targets in `start_urls` and to links passed as arguments to `response.follow()`, but Scrapy also allows us to make custom requests with more options. The [`scrapy.http.Request`](https://doc.scrapy.org/en/latest/topics/request-response.html#request-objects) object has attributes and methods that can be used to attach data to each request. This data can be in the form of request headers, cookies, or a Python `dict` and is stored in the `headers`, `cookies`, and `meta` attributes, respectively, attached to the `Request` object.\n",
    "\n",
    "To use requests with data attached, we'll need to make requests a little differently from before. Instead of using `start_urls` and `response.follow()` to automatically make requests, we'll need to create our own request objects. To demonstrate the differences, we'll modify the `IMDbSpider` from the previous section to manually create `Request` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " class ModifiedIMDbSpider(IMDbSpider): # extends IMDbSpider to inherit parse_movie method\n",
    "    name = 'modifiedIMDb'\n",
    "    \n",
    "    # start_urls = ['http://www.imdb.com/search/title?groups=top_250&sort=user_rating']\n",
    "    # becomes this method\n",
    "    def start_requests(self):\n",
    "        urls = ['http://www.imdb.com/search/title?groups=top_250&sort=user_rating']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url, callback=self.parse)\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for movie_link in response.css('h3.lister-item-header a::attr(href)').extract():\n",
    "            # yield response.follow(movie_link, callback=self.parse_movie)\n",
    "            # becomes\n",
    "            link = response.urljoin(movie_link)\n",
    "            yield scrapy.Request(link, callback=self.parse_movie)\n",
    "        \n",
    "        next_link = response.css('div.desc a.lister-page-next::attr(href)').extract_first()\n",
    "        # yield response.follow(next_link, callback=self.parse)\n",
    "        # becomes\n",
    "        next_link = response.urljoin(next_link)\n",
    "        yield scrapy.Request(next_link, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key difference between calling `response.follow()` and manually creating `Request`s is that the url passed to the Constructor should be the result of a call to `response.urljoin()`. The `urljoin()` method converts relative urls to absolute urls because `Request`s are not able to resolve relative urls.\n",
    "\n",
    "Running the `ModifiedIMDbSpider` with these modifications should produce the exact same output as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how to create `Requests`, we can add headers to our requests and supplement our web scraping with calls to a web API. The next example uses the Yelp API to search for businesses and scrapes each business' page for reviews. It includes an API key in the HTTP request headers for requests made to API endpoints and uses the `meta` attribute of `Request` and `Response` objects to pass information between the two parse methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import json\n",
    "\n",
    "class YelpSpider(scrapy.Spider):\n",
    "    name = 'yelp'\n",
    "    url = 'https://api.yelp.com/v3/businesses/search?categories=restaurants&location='\n",
    "    offset = 0\n",
    "\n",
    "    def start_requests(self):\n",
    "        self.url += self.location\n",
    "        self.headers = {'Authorization': 'Bearer %s' % self.api_key,}\n",
    "        yield scrapy.Request(self.url, headers=self.headers, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        response_json = json.loads(response.text) # Yelp API returns a json response\n",
    "        total = response_json['total']\n",
    "        businesses = response_json['businesses']\n",
    "        self.offset += len(businesses)\n",
    "        for business in businesses:\n",
    "            business_request = scrapy.Request(business['url'], callback=self.parse_business)\n",
    "            business_request.meta['info'] = business # store the API response in the request\n",
    "            business_request.meta['reviews'] = []\n",
    "            yield business_request\n",
    "        if self.offset < self.max_results:\n",
    "            yield scrapy.Request(self.url + '&offset=' + str(self.offset), \n",
    "                             headers=self.headers, callback=self.parse)\n",
    "\n",
    "    def parse_business(self, response):\n",
    "        reviews = []\n",
    "        for review in response.css('div.review-content'):\n",
    "            rating = review.css('div.i-stars.rating-large::attr(title)').extract_first()[:3]\n",
    "            date = review.css('span.rating-qualifier::text').extract_first()\n",
    "            text = ''.join(review.css('p::text').extract())\n",
    "            reviews.append({\n",
    "                'rating': rating,\n",
    "                'date': date,\n",
    "                'text': text\n",
    "            })\n",
    "            \n",
    "        business_info = response.meta['info'] # extract the API response for the business\n",
    "        yield {'info': business_info, 'reviews': reviews,} # output both the API response and the scraped reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the spider below, we pass in the location, API key, and a limit on the number of results as arguments. These arguments are converted to attributes of our spider by the default `__init__()` method of `scrapy.Spider`. More information on passing arguments to spiders can be found [here](https://doc.scrapy.org/en/latest/topics/spiders.html#spider-arguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_key(path):\n",
    "    with open(path, 'r') as file:\n",
    "        return file.read().replace('\\n','')\n",
    "\n",
    "api_key = get_key('api_key.txt')\n",
    "\n",
    "crawler = CrawlerProcess({\n",
    "    'LOG_ENABLED': False,\n",
    "    'FEED_URI': 'yelp.json',\n",
    "    'FEED_FORMAT': 'json',\n",
    "})\n",
    "\n",
    "crawler.crawl(YelpSpider, location='Polish Hill, Pittsburgh', api_key=api_key, max_results=60)\n",
    "crawler.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above code, located in 'yelp.json', should be something like this:\n",
    "```\n",
    "    [\n",
    "        {\n",
    "        \"info\": {\n",
    "            \"id\": \"mount-everest-sushi-pittsburgh\",\n",
    "            \"name\": \"Mount Everest Sushi\",\n",
    "            \"image_url\": \"https://s3-media4.fl.yelpcdn.com/bphoto/Vx_hhEFamDFpnTF-lnG_aQ/o.jpg\",\n",
    "            \"is_closed\": false,\n",
    "            \"url\": \"https://www.yelp.com/biz/mount-everest-sushi-pittsburgh?adjust_creative=XWzn1hOw6xktvH04lFxiXA&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=XWzn1hOw6xktvH04lFxiXA\",\n",
    "            \"review_count\": 125,\n",
    "            \"categories\": [\n",
    "                {\n",
    "                    \"alias\": \"asianfusion\",\n",
    "                    \"title\": \"Asian Fusion\"\n",
    "                },\n",
    "    ...\n",
    "        },\n",
    "        \"reviews\": [\n",
    "            {\n",
    "                \"rating\": \"5.0\",\n",
    "                \"date\": \"\\n        3/2/2018\\n            \",\n",
    "                \"text\": \"I called to order and as busy as they were (everyone was scrambling around trying to get orders done), they had my order done in the ten minutes from when I called to the time it took me to walk over there! It's pretty impressive considering their phones were ringing off the hook order after order.I ordered 2 house special poké bowls and an order of sushi tacos (which came with 2 tacos) and miso soup. They've raised their prices a bit since I've been there last and compared to the menu photos here on yelp so it's a little more pricey (I think my total came to around $41) but still, you get a ton of sashimi and it tastes super fresh. They were all packed with sashimi, especially the sushi tacos, it was amazing. Delicious too!\"\n",
    "            },\n",
    "            {\n",
    "                \"rating\": \"4.0\",\n",
    "                \"date\": \"\\n        3/27/2018\\n    \",\n",
    "                \"text\": \"Don't be fooled by the small/sketchy appearance when you first walk in. They actually have a pretty decent seating area upstairs. You order your food at the counter in the entrance and can then head upstairs to enjoyI had the salmon poke bowl, and I thought it was delicious!\"\n",
    "            },\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Additional Resources\n",
    "This tutorial covered the basics of web scraping with Python scripts using Scrapy. You can learn more about Scrapy, or topics related to web scraping in general, from these links:\n",
    "* [Scrapy's official documentation](https://doc.scrapy.org/en/latest/index.html) \n",
    "* [CSS selectors](https://www.w3.org/TR/selectors/)\n",
    "* [XPath expressions](https://www.w3.org/TR/xpath/), a powerful alternative to CSS selectors in Scrapy\n",
    "* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), another library for parsing web pages\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
