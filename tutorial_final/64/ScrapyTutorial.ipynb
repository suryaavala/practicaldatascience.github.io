{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will be an introduction to the popular python library Scrapy, which is used primarly for web scraping. Scrapy gives freedom to the programmer to choose to use APIs for gathering data or creating their own general purpose web crawler.\n",
    "Scrapy also gives tools for requesting source HTML and parsing through data which will all be covered in the tutorial.\n",
    "\n",
    "What is a web crawler? Web crawlers collect information of a website (i.e. URL, web page content, meta tag information), add available links to the queue of links to go to next, and recursively goes to the next website. They are key components of Web search engines, Web archiving, and data mining on various statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Web Crawler Pipeline](https://photos-6.dropbox.com/t/2/AACswTUCLUi0yyL30XTVeKUS14ivT_VjVnfcAjfL4ZzxeQ/12/136140040/png/32x32/1/_/1/2/AAEAAQAAAAAAAAf4AAAAJGQ1YWYzOWQ1LWJjNDItNDU0Ni05ZWQwLThmMTNiNDI4ODYwNg.png/EP2prGgYzzkgAigC/y4lK_d-PcmUrHehYzQGGtb7e7XAZjy6WHhOo6xmUUcY?preserve_transparency=1&size=1024x768&size_mode=3)\n",
    "\n",
    "Not included in the diagram is how to decide what scheduling is used to pick the next URL to request from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the libraries\n",
    "\n",
    "To install Scrapy using `pip`, run:\n",
    "\n",
    "    $ pip install Scrapy\n",
    "    \n",
    "Alternatively, you can install through `conda` by running:\n",
    "\n",
    "    $ conda install -c conda-forge scrapy\n",
    "    \n",
    "After installing, make sure the following commands work for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import Request, Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request and Response\n",
    "\n",
    "In previous homeworks we used the third party library requests for requesting source HTML from websites. Shifting to the functionality provided by Scrapy allows us to parse through the data faster and easier. \n",
    "\n",
    "These requests and responses will __only be used in context of a Spider__ and cannot be used as stand alone as the Spider internally executes the request and gives us the response like a black box. Therefore any code in this section will have no actual output. To fetch HTML source outside of this context we must use the Scrapy shell.\n",
    "\n",
    "Let's get started with the basic request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "request = Request(url='http://www.example.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy has the ability to pass a callback function on requests like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    return Request(url='http://www.example.com',\n",
    "                   callback=self.parse_logger)\n",
    "\n",
    "def parse_logger(self, response):\n",
    "    self.logger.info('Visited %s', response.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be applied for the errback function argument for error handling. There are also additional parameters which can be found in documentation that are extremely useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Request` has an extremely helpful subclass, `FormRequest`, with functionality for dealing with HTML forms. Form data can be automatically filled in and used for login purposes.\n",
    "It has a new argument for its constructor:\n",
    "* __formdata__ (dict or iterable of tuples) - Contains HTML Form data to be assigned to the body of the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    return FormRequest(url='https://s3.andrew.cmu.edu/sio/',\n",
    "                       formdata={'AndrewID': 'yourandrewid', 'Password': 'yourpassword'},\n",
    "                       callback=self.login_attempt)\n",
    "\n",
    "def login_attempt(self, response):\n",
    "    if 'Authentication Failed' in response.body :\n",
    "        self.logger.info('Failed login.')\n",
    "    else :\n",
    "        self.logger.info('Successful login.')\n",
    "        # continue scrapping otherwise...\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the Spider makes the request, the Downloader executes and generates a `Response`. The most important fields are: \n",
    "\n",
    "* __url__ (String): url of the response.\n",
    "* __headers__ (dict): headers of the response; dict values can be strings or lists depending on how many values a header has.\n",
    "* __body__ (str): text body of the response.\n",
    "\n",
    "Generally you would want to cast response from `Response` to one of its subclasses, as the base `Response` class is meant to be only used for binary data. Most often `TextResponse` is used to allow encoding capabilities. Otherwise you can make your own subclass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relearning Parsing\n",
    "\n",
    "Scrapy also has its own methods from extracting data from HTML source. Previously in the course we have used BeautifulSoup, but this has the downside of being slow. Web scraping is a very time-intensive so we will relearn parsing through Scrapy for better performance.\n",
    "\n",
    "Scrapy extracts data via so called \"selectors\" because they select certain parts of the HTML document specified either by XPath or CSS expressions. XPath is a language for selected nodes in XML and CSS is a language for applying styles to HTML documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing selector from text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body = '<html><body><span>Data Science is not Statistics</span></body></html>'\n",
    "selector = Selector(text=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing Selector from a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = HtmlResponse(url='http://example.com', body=body, encoding='utf-8')\n",
    "selector = Selector(response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complicated body for parsing\n",
    "body = '''<html>\n",
    "           <head>\n",
    "            <base href='http://scrapytutorial.com/' />\n",
    "            <title>Parsing is fun!</title>\n",
    "           </head>\n",
    "           <body>\n",
    "            <div id='images'>\n",
    "             <a href='image1.html'>Name: Spiderup <br /><img src='spider_thumb.jpg' /></a>\n",
    "             <a href='image2.html'>Name: Pipeline <br /><img src='pipeline_thumb.jpg' /></a>\n",
    "             <a href='image3.html'>Name: Crawling <br /><img src='crawlin_thumb.jpg' /></a>\n",
    "             <a href='image4.html'>Name: Crawlers <br /><img src='crawler_thumb.jpg' /></a>\n",
    "             <a href='image5.html'>Name: Crawfish <br /><img src='crawfish_thumb.jpg' /></a>\n",
    "             <a href='extra.html'> Name: notimage <br /><gif src='gotem.gif /></a>\n",
    "            </div>\n",
    "           </body>\n",
    "          </html>'''\n",
    "response = HtmlResponse(url='http://scrapytutorial.com', body=body, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying responses using XPath and CSS has two convenience shortcuts: response.xpath() and response.css(). The arguments for both are their respective path expressions. \n",
    "\n",
    "For a quick review of XPath, below are the most common and important expressions (taken from https://www.w3schools.com/xml/xpath_syntax.asp):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| __Expression__ | Description                                  |\n",
    "|--------------|--------------------------------------------|\n",
    "| __nodename__ | Selects all nodes with the name \"nodename\" |\n",
    "| __/__        | Selects from the root node                 |\n",
    "| __//__       | Selects nodes in the document from the current node that match the selection no matter where they are                              |\n",
    "| __.__        | Selects the current node                   |\n",
    "| __..__       | Selects the parent of the current node     |\n",
    "| __@__        | Selects attributes                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful tool are predicates, which selects nodes that contain a specific value. These are enclosed in square brackets and are powerful tools (i.e. finding last of an element or elements with specific tags). One such use which is seen later is contains(), which is used to find entries with a specific substring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick review of CSS, below are the most common and important expressions (taken from https://www.w3schools.com/cssref/css_selectors.asp):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| __Selector__ | Description                                  |\n",
    "|--------------|--------------------------------------------|\n",
    "| __.class__   | Selects all elements with specified class  |\n",
    "| __#id__      | Selects the element with specified id      |\n",
    "| __*__        | Selects all elements                       |\n",
    "| __element__  | Selects all `<`element`>` elements             |\n",
    "| __element1 element2__  | Selects all `<`element1`>` elements inside `<`element2`>` elements |\n",
    "| __[attribute=value]__  | Selects all elements with attribute=\"value\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='//title' data='<title>Parsing is fun!</title>'>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".xpath() and .css() both output a `SelectorList` which can in turn be recursively upon each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='@src' data='spider_thumb.jpg'>,\n",
       " <Selector xpath='@src' data='pipeline_thumb.jpg'>,\n",
       " <Selector xpath='@src' data='crawlin_thumb.jpg'>,\n",
       " <Selector xpath='@src' data='crawler_thumb.jpg'>,\n",
       " <Selector xpath='@src' data='crawfish_thumb.jpg'>]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.css('img').xpath('@src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".extract() extracts the text data for each `Selector` in `SelectorList`. Some example code follows to get more familiar with the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//a[contains(@href, \"image\")]/@href').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spider_thumb.jpg',\n",
       " 'pipeline_thumb.jpg',\n",
       " 'crawlin_thumb.jpg',\n",
       " 'crawler_thumb.jpg',\n",
       " 'crawfish_thumb.jpg']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//a[contains(@href, \"image\")]/img/@src').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, `Selectors` have a re() method that takes in regular expressions. However, it cannot be used recursively further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crawlin_thumb.jpg', 'crawler_thumb.jpg', 'crawfish_thumb.jpg']\n"
     ]
    }
   ],
   "source": [
    "craw_stuff = response.xpath('//a[contains(@href, \"image\")]/img/@src').re(r'(craw\\s*.*)')\n",
    "print(craw_stuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing your first spider\n",
    "\n",
    "Now that we have the knowledge to make `Requests` and parse through the `Responses`, we can now write a basic spider. \n",
    "\n",
    "The Spider subclass spider.Spider requires the following attributes and/or functions:\n",
    "* __name__: Unique string id for the spider within the current directory\n",
    "* __start_urls__: An iterable of class String consisting of URLs to start from.\n",
    "* __start_requests()__: Returns an iterable of the class scrapy.Request (e.g. list or a generator function) from which the Spider crawls from.\n",
    "* __parse()__: Handles the response downloaded for each request made. The response parameter is an instance of scrapy.Response which holds the page content. We can simply print information of the URL or save it elsewhere\n",
    "\n",
    "Either start_requests() or start_urls can be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicSpider(scrapy.Spider):\n",
    "    name = \"basic\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        #Getting the page number\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-' + str(page) + '.html'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file '+ filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This spider does the bare minimum by only parsing the starting URLs and saving the HTML source of each one. Our next step should be making the Spider go to the next page and store some specific information.\n",
    "\n",
    "In the following example we want to store information about each author so we find the author link and follow the corresponding link. After requesting, the name, birthdate, and bio fields are easy to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = 'author'\n",
    "\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # follow links to author pages\n",
    "        for href in response.css('.author + a::attr(href)'):\n",
    "            yield response.follow(href, self.parse_author)\n",
    "\n",
    "        # follow pagination links\n",
    "        for href in response.css('li.next a::attr(href)'):\n",
    "            yield response.follow(href, self.parse)\n",
    "\n",
    "    def parse_author(self, response):\n",
    "        def extract_with_css(query):\n",
    "            return response.css(query).extract_first().strip()\n",
    "\n",
    "        yield {\n",
    "            'name': extract_with_css('h3.author-title::text'),\n",
    "            'birthdate': extract_with_css('.author-born-date::text'),\n",
    "            'bio': extract_with_css('.author-description::text'),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I run the Spider?\n",
    "\n",
    "To actually run the `Spiders` that we've made, we have to use the Scrapy shell. We run it through the command line via the command:\n",
    "\n",
    "    scrapy crawl <name>\n",
    "\n",
    "in the same folder as the spider we wish to run, where `<name>` is the unique id of the spider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More with Scrapy shell\n",
    "\n",
    "With the Scrapy shell we can also allow command line arguments when crawlying by using the  `-a` option :\n",
    "\n",
    "    scrapy crawl <name> -a field=attribute\n",
    "    \n",
    "If we run `scrapy crawl quotes -a tag=love`, QuotesSpider will now have a tag field with it initialized to \"love\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        url = 'http://quotes.toscrape.com/'\n",
    "        tag = getattr(self, 'tag', None)\n",
    "        if tag is not None:\n",
    "            url = url + 'tag/' + tag\n",
    "        yield scrapy.Request(url, self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'author': quote.css('small.author::text').extract_first(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').extract_first()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This tutorial only covered the basics of Scrapy, but should be a good starting foundation to build complex Spiders. A very helpful resource when learning Scrapy is just interacting with the Scrapy shell, which is the only way to use `Request` and `Response` outside of the context of a `Spider` (https://doc.scrapy.org/en/latest/topics/shell.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the side of parsing, we all know that parsing can be very difficult and tedious even with the Scrapy wrapper functions. It is very common to rely on outside libraries inside parse() for specific uses. For example, to extract microdata items we can use the library Extruct (https://github.com/scrapinghub/extruct). \n",
    "\n",
    "Additional pieces of data can be stored as the values of some parameters in URLs listed in a page such as its users. Instead of using regex, we can rely on the w3lib library (https://github.com/scrapinghub/w3lib). You should always be on the lookout for libraries that make parsing easier and not rely solely on the functions given by Scrapy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
