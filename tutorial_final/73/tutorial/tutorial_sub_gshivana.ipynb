{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science with SiLK/ELK - Solr/Elastic Search-Logstash-Kibana stack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logstash\n",
    "import logging\n",
    "import pysolr\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction:\n",
    "\n",
    "\n",
    "Searching and Sorting are the most fundamental problems of computer science and also our day-to-day life. Whether it is to look out when will the next season of our favorite Netflix series be hosted or find the score of our favorite baseball team we start searching. Well to have our search algorithm work efficiently and fetch the data we want, we  have to sort documents, snippets, articles and web pages so that only relevant data are at the top of the hit list.\n",
    "All this is the background for a search engine and there are a lot of search engines in the market ( Yes! there are search engines other than our beloved Google and most hated Internet Explorer!). Out of them Apache Lucene is one famous free and open-source information retrieval software library originally written completely in Java.[1]\n",
    "\n",
    "\n",
    "## 1.1 Motivation:\n",
    "There are a number of problems that the current technology is facing with respect to searching and sorting.\n",
    "The motivation here is to address two of them quoted below using a well designed elastic search engine-solr logstash and banana dashboards.\n",
    "\n",
    "### 1.1.1 Faceted Search using Solr: \n",
    "The use case here is simple. Let us consider an online shopping scenario. An important requirement here is to help online shoppers to  filter data ( items in the shopping catalog ) based on multiple features. For example, say I have to buy a car. I would love to search based on the color, mileage, 4 wheel drive, horse power etc. These are the features and the search engine has to automatically give me sorted results based on these multiple results. This is faceted search. Apache Solr provides a nice way to integrate faceted search. I will be walking through the steps of creating such an experience using a simple dataset having details about the sales of digital wearables equipments using Solr.\n",
    "\n",
    "### 1.1.2 Time Series Analysis using ELK Stack:\n",
    "This is a fairly complex use case when compared to the faceted search. Time series analysis is highly ubiquitous since ages. Now, there are different use cases along the same line. Be it real time stock analysis, transport data prediction, or  Stats telemetry, all these include hundreds and thousands of metrics to be collected every minute. The challenge is how to store, analyze and visualize them. This is exactly where Data Science comes into picture. The SiLK and ELK stack can be used in an efficient way to solve this problem. I will be walking through a real stock analysis example to how can the ELK stack be used as an \"eye\" for such complex data.\n",
    "\n",
    "## 1.2 Ingredients\n",
    "\n",
    "### 1.2.1 Apache Solr:\n",
    "According to wikipedia page[2], Solr is an open source enterprise search platform, written in Java, from the Apache Lucene project. Its major features include full-text search, hit highlighting, faceted search, real-time indexing, dynamic clustering, database integration, NoSQL features and rich document (e.g., Word, PDF) handling. In our use case we can think Solr-Lucene combo as the back end storage engine that also provides elastic search capability and it is cool as it is open source!\n",
    "\n",
    "### 1.2.2 Elastic Search:\n",
    "Elasticsearch is a search engine based on Lucene. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.[10] Elastic search can be thought as a cousin of Apache Solr as both are enterprise search engines built on top of Lucene.\n",
    "\n",
    "### 1.2.3 Logstash: \n",
    "Logstash is a tool to collect, process, and forward events and log messages[3] For our use case, we can treat logstash as kind of middle-man in the SiLK pipeline who has input plugins to collect the logs (sys-logs), provides some nice filters to modify and annotate [3] them and finally give out the annotated data to the output channels. Logstash can also collect data from various databases like MySQL, MongoDB, WildFly etc.\n",
    "\n",
    "### 1.2.4 Kibana:\n",
    "Kibana is an open source data visualization plugin for Elasticsearch.[11] It is the eye for data that we have hosted on top of Elastic search/ Solr. It provides rich representation of data so that the analysis can be made in an efficient way.\n",
    "Kibana completes the whole stack. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation Guidelines:\n",
    "\n",
    "### 2.1 Apache Solr Installation: \n",
    "[5] is from the official Solr page that is the best guide to install Solr in a simple way.(They provide guidelines to host Solr on Windows machines which I tried. But I felt it is better and easier to host it on Linux based machines because of lots of dependencies on other tools like curl etc that are required to build the whole stack.)  Briefing out the installation steps with screenshots in the following cell.\n",
    "( I will be demonstrating the installation steps on Ubuntu 16.04 for the tutorial )\n",
    "\n",
    "#### 2.1.1 System Requirements:\n",
    "As solr is a platform running on Lucene which is built on top of Java, we need to have the latest JVM\n",
    "Further requriement is as mentioned in [6]\n",
    "\n",
    "#### 2.1.2 Download and Installation Steps:\n",
    "1. Download the latest version here. \n",
    "http://apache.claz.org/lucene/solr/7.2.1 \n",
    "2. Unpack the tar\n",
    "\n",
    "\n",
    "\n",
    "#### 2.1.3 PySolr- A Python Wrapper for Solr Installation:\n",
    "Pysolr can be directly downloaded from Conda-forge cloud as follows:\n",
    "\n",
    "VirtualBox:/tmp$ conda install -c conda-forge pysolr\n",
    "\n",
    "### 2.2 Logstash Installation on Anaconda:\n",
    "If pip is not installed on Anaconda,\n",
    "1. Navigate to Conda home path \n",
    "\n",
    "2. Install pip\n",
    "VirtualBox:/tmp$ conda install pip\n",
    "\n",
    "3. Pip install python-logstash\n",
    "VirtualBox:/tmp$ pip install python-logstash\n",
    "\n",
    "\n",
    "### 2.1.4 Installing Elastic Search:\n",
    "\n",
    "1. First step is to check the version of Java. If the machine doesn't have Java, we can download as follows:\n",
    "\n",
    "sudo add-apt-repository -y ppa:webupd8team/java\n",
    "sudo apt-get update\n",
    "sudo apt-get -y install oracle-java8-installer\n",
    "java -version\n",
    "\n",
    "2. Download Elastic search from the website \n",
    "\n",
    "https://www.elastic.co/downloads/elasticsearch\n",
    "dpkg -i elasticsearch-6.2.3.deb\n",
    "sudo systemctl enable elasticsearch.service\n",
    "\n",
    "sudo service elasticsearch start\n",
    "\n",
    "\n",
    "To test if it is installed, we can fire a simple curl as follows\n",
    "\n",
    "curl -XGET 'localhost:9200'\n",
    "\n",
    "\n",
    "irtualBox:~/Desktop/elk$ sudo dpkg -i elasticsearch-6.2.3.deb \n",
    "Selecting previously unselected package elasticsearch.\n",
    "(Reading database ... 220574 files and directories currently installed.)\n",
    "Preparing to unpack elasticsearch-6.2.3.deb ...\n",
    "Creating elasticsearch group... OK\n",
    "Creating elasticsearch user... OK\n",
    "Unpacking elasticsearch (6.2.3) ...\n",
    "Setting up elasticsearch (6.2.3) ...\n",
    "Processing triggers for systemd (229-4ubuntu21.2) ...\n",
    "Processing triggers for ureadahead (0.100.0-19) ...\n",
    "VirtualBox:~/Desktop/elk$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The output shall be this if elastic search is installed properly\n",
    "\n",
    "VirtualBox:~/Desktop/elk$ curl -XGET 'localhost:9200'\n",
    "{\n",
    "  \"name\" : \"Fx7F0m6\",\n",
    "  \"cluster_name\" : \"elasticsearch\",\n",
    "  \"cluster_uuid\" : \"Jz7sepLTQyqi6DzCMFG_Eg\",\n",
    "  \"version\" : {\n",
    "    \"number\" : \"6.2.3\",\n",
    "    \"build_hash\" : \"c59ff00\",\n",
    "    \"build_date\" : \"2018-03-13T10:06:29.741383Z\",\n",
    "    \"build_snapshot\" : false,\n",
    "    \"lucene_version\" : \"7.2.1\",\n",
    "    \"minimum_wire_compatibility_version\" : \"5.6.0\",\n",
    "    \"minimum_index_compatibility_version\" : \"5.0.0\"\n",
    "  },\n",
    "  \"tagline\" : \"You Know, for Search\"\n",
    "}\n",
    "\n",
    "Thats it! Elastic starts running at the assigned port.\n",
    "\n",
    "\n",
    "#### 2.1.5 Installing Kibana:\n",
    "\n",
    "1. Download the Kibana from the official website\n",
    "https://www.elastic.co/downloads/kibana\n",
    "sudo dpkg -i kibana-6.2.3-amd64.deb\n",
    "\n",
    "VirtualBox:~/Desktop/elk$ sudo dpkg -i kibana-6.2.3-amd64.deb \n",
    "Selecting previously unselected package kibana.\n",
    "(Reading database ... 177484 files and directories currently installed.)\n",
    "Preparing to unpack kibana-6.2.3-amd64.deb ...\n",
    "Unpacking kibana (6.2.3) ...\n",
    "Setting up kibana (6.2.3) ...\n",
    "Processing triggers for systemd (229-4ubuntu21.1) ...\n",
    "Processing triggers for ureadahead (0.100.0-19) ..\n",
    "\n",
    "\n",
    "2. We have to take care of two things in the kibana Yaml Configuration file:\n",
    "  1. The URL of the elastic search. For the demo we are using the local host\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kibana_1.png](kibana_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Fun Part: Actual Working of the Stack\n",
    "\n",
    "## 3.1 Running Solr and creating Cores:\n",
    "For the demonstration purpose, creating a core called 'wearables' as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Creating Cores:\n",
    "@-VirtualBox:~/Desktop/solr-7.2.1$ bin/solr create -c wearables\n",
    "\n",
    "WARNING: Using _default configset. Data driven schema functionality is enabled by default, which is\n",
    "         NOT RECOMMENDED for production use.\n",
    "\n",
    "         To turn it off:\n",
    "            curl http://localhost:7574/solr/wearables/config -d '{\"set-user-property\": {\"update.autoCreateFields\":\"false\"}}'\n",
    "Created collection 'wearables' with 1 shard(s), 1 replica(s) with config-set 'wearables'\n",
    "\n",
    "@-VirtualBox:~/Desktop/solr-7.2.1$ \n",
    "\n",
    "#### 3.1.2 Posting CSV Files:\n",
    "Posting a CSV file on the newly created core wearables\n",
    "\n",
    "@-VirtualBox:~/Desktop/solr-7.2.1$ bin/post -c wearables Wearable1.csv\n",
    "\n",
    "/usr/lib/jvm/java-8-oracle/bin/java -classpath /home//Desktop/solr-7.2.1/dist/solr-core-7.2.1.jar -Dauto=yes -Dc=wearables -Ddata=files org.apache.solr.util.SimplePostTool Wearable1.csv\n",
    "SimplePostTool version 5.0.0\n",
    "Posting files to [base] url http://localhost:8983/solr/wearables/update...\n",
    "Entering auto mode. File endings considered are xml,json,jsonl,csv,pdf,doc,docx,ppt,pptx,xls,xlsx,odt,odp,ods,ott,otp,ots,rtf,htm,html,txt,log\n",
    "POSTing file Wearable1.csv (text/csv) to [base]\n",
    "1 files indexed.\n",
    "COMMITting Solr index changes to http://localhost:8983/solr/wearables/update...\n",
    "Time spent: 0:00:01.454\n",
    "\n",
    "@-VirtualBox:~/Desktop/solr-7.2.1$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Solr in Action with PySolr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#https://pypi.python.org/pypi/pysolr/3.6.0 [8]\n",
    "#http://www.confusedcoders.com/bigdata/solr/indexing-csv-data-in-solr-via-python-pysolr\n",
    "\n",
    "#This creates a connection to the solr server running on the local host with port 8983.\n",
    "#test2 is the core name that we created in above step.\n",
    "#The timeout is an optional field\n",
    "\n",
    "solr = pysolr.Solr('http://localhost:8983/solr/wearables', timeout=10)\n",
    "\n",
    "#Reading a CSV file to Solr\n",
    "\n",
    "#solr search similar to lucene search\n",
    "results = solr.search(q='*:*')\n",
    "print(\"Saw {0} result(s).\".format(len(results)))\n",
    "\n",
    "#Each result is a pysolr object\n",
    "print(\"results \",str(results))\n",
    "\n",
    "#Printing the name of some items \n",
    "for result in results:\n",
    "    print(\"Item Name '{0}'.\".format(result['Name']))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1: Pagination in Solr\n",
    "Now, in the previous example, we might wonder why only 10 results are shown ( the CSV file has more than 10 records )\n",
    "[9] explains the issue. Basically Solr is NOT a database. It tries to retrieve the top results but not all documents which can be viewed as a typical behaviour of a search engine. \n",
    "However, we can have more rows printed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pagination in Solr\n",
    "results = solr.search(q='*:*',rows=20)    \n",
    "for result in results:\n",
    "    print(\"Item Name '{0}'.\".format(result['Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 More Filtering in Solr:\n",
    "More filtering can be added pretty easily and queries can be launched as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find all the wearables manufactured by Casio\n",
    "#https://stackoverflow.com/questions/9532395/pysolr-filter-search\n",
    "filters = ['Company.Name:Casio, Category:Medical']\n",
    "results = solr.search(q=\"*:*\",fq=filters,rows=20)    \n",
    "for result in results:\n",
    "    print(\"Item Name '{0}'.\".format(result['Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is a basic search where Solr retrieves 20 wearables manufactured by Casio OR ( not AND ) categorized as Medical\n",
    "\n",
    "### 3.2.3 Parial Match in Solr:\n",
    "Requires a \"*\" in the filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "results = solr.search(q=\"Company.Name:Cas*\",rows=20)    \n",
    "for result in results:\n",
    "    print(\"Item Name '{0}'.\".format(result['Company.Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Time to See Fuzzy Matches:\n",
    "\n",
    "This is where Solr stands out. The fuzzy matches are based on Edit distances. That is number of replacement,deletion or insertion required to match the pattern with a string is edit distance. We can do fuzzy matches in a very simple and convenient way in Solr.\n",
    "First, let us look at a scenario where we have to Company.Name having an edit distance of 1 with \"Cas\" can be done something as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/16655933/fuzzy-search-in-solr\n",
    "results = solr.search(q=\"Company.Name:Cas~1\",rows=20)    \n",
    "for result in results:\n",
    "    print(\"Item Name '{0}'.\".format(result['Company.Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solr retrieves 'Sas Safety' as Sas is 1 edit distance away from Cas\n",
    "\n",
    "### 3.2.5 More Interesting Fuzzy Matches:\n",
    "Suppose we are interested in watches only out all different wearables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = solr.search(q=\"Name:watches~1\",rows=20)    \n",
    "for result in results:\n",
    "    print(\"Item Name '{0}'.\".format(result['Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Name '['Barska GB12166 Fitness Watch with Heart Rate Monitor']'.\n",
      "Item Name '['Belkin GS5 Sport Fit Armband, Black F8M918B1C00']'.\n",
      "Item Name '['Bowflex EZ Pro Strapless Heart Rate Monitor Watch, Black']'.\n",
      "Item Name '['Casio G Shock Watch Solar Atom (gwm500a-1)']'.\n",
      "Item Name '['Casio WS220 Solar Runner Digital Wrist Watch']'.\n",
      "Item Name '['Coleman G7HD-SWIM POV 1080p 5 Megapixel Goggles Camcorder ELBG7HDSWIM']'.\n",
      "Item Name '['Ekho Fit-18 Heart Rate Monitor 12-2042']'.\n",
      "Item Name '[\"Ekho FiT-9 Women's Heart Rate Monitor 12-2041\"]'.\n",
      "Item Name '['Fitbit Flex Cordless Activity/Sleep Tracker - Black']'.\n",
      "Item Name '['G1 Smartwatch /w Bluetooth Hands-Free Solution Large Black']'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#https://stackoverflow.com/questions/18704339/how-to-use-facets-with-pysolr-cant-seem-to-get-facet-results-to-show\n",
    "params = {\n",
    "  'facet': 'on',\n",
    "  'facet.query':['facet.field', \"Body.Location\",\n",
    "          'facet.field.value', \"Torso\", \n",
    "          'facet.field', \"Company.Name\",\n",
    "          'facet.field.value', \"Jawbone\",\n",
    "   ],\n",
    "  'rows': '10',\n",
    "}\n",
    "\n",
    "results = solr.search( q=\"*:*\",**params)\n",
    "for result in results:\n",
    "    print(\"Item Name '{0}'.\".format(result['Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Analysis With ELK Stack\n",
    "\n",
    "For this use case, we need to install Logstash. We have already installed Elastic Search and Kibana.\n",
    "Just to check they are working fine, we can perform the following,\n",
    "\n",
    "ps -eaf | gerp kibana\n",
    "ps -eaf | grep elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![ek_working.png](ek_working.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Logstash\n",
    "1. Download and unpack the logstash package the same way we did for Elastic search and Kibana\n",
    "2. Navigate to the directory\n",
    "    cd /usr/share/logstash/bin\n",
    "    sudo ./logstash -f FILE_PATH for the conf file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logstash_loading_firstdata.png](logstash_loading_firstdata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Logstash Conf File\n",
    "\n",
    "It has three components\n",
    "1. Input: Defines where the data being fetched\n",
    "Provide the path for the input file. In our case, it is the path for the csv file\n",
    "\n",
    "2. Filter: \n",
    "This is where all the fun happens with Logstash. We can filter the unstructured data using GROK- a pattern matching library (please find the link for a good tutorial on the same in the following cells)\n",
    "\n",
    "3. Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/minsuk-heo/BigData/tree/master/ch06\n",
    "#The conf file for logstash to get the stock data on to the elastic search and also provide a pipeline to Kibana\n",
    "input {\n",
    "  file {\n",
    "    path => \"/home/Desktop/gasg/elk/stock.csv\"\n",
    "    start_position => \"beginning\"\n",
    "    sincedb_path => \"/dev/null\"    \n",
    "  }\n",
    "}\n",
    "\n",
    "filter {\n",
    "  csv {\n",
    "      separator => \",\"\n",
    "      columns => [\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"Adj Close\"]\n",
    "  }\n",
    "  mutate {convert => [\"Open\", \"float\"]}\n",
    "  mutate {convert => [\"High\", \"float\"]}\n",
    "  mutate {convert => [\"Low\", \"float\"]}\n",
    "  mutate {convert => [\"Close\", \"float\"]}\n",
    "}\n",
    "output {  \n",
    "    elasticsearch {\n",
    "        hosts => \"localhost\"\n",
    "        index => \"stock\"\n",
    "    }\n",
    "    stdout {}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Kibana: The last leg!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kibana_logs.png](kibana_logs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kibana_frontend.png](kibana_frontend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Input CSV File for the ELK stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ugly lokking.png](ugly lokking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kibana_stock1.png](kibana_stock1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the Stocks data indexed by the date field can be viewed as both Json Format and tables on Kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![timestamps.png](timestamps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kibana3.png](kibana3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### We Can Also Perform Lucene Syntax Based Search in Kibana\n",
    "close:[30 TO 35] shows all the dates when the closing value of the stocks were between 30-35 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![close-30-35.png](close-30-35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Options Provided by Kibana:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![visuals.png](visuals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![weekly.png](weekly.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![both_closr_high.png](both_closr_high.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kibana Dashboards\n",
    "Finally, We can have a nice neat dashboard that can have collection of all the graphs and visualizations on a single page as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dashboard.png](dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "#### ELK and SiLK Face off! \n",
    "\n",
    "In this part, I would like to report some of the pros and cons of both stacks based on my experience installing and setting them up. \n",
    "\n",
    "Documentation/Time spent in bringing up the stack:\n",
    "ELK is easier to set-up and work on. Period! I spent a lot of time finding the support for Solr and SiLk. With all due respect to the authors of installation guide for Solr ( which is beautifully written ) but they lack the support. Especially, when it comes to the SiLk. Kibana doesn't support Solr out of the box. We need to have a fork of Kibana called Banana-UI for the same. The banana-UI lacks support too. \n",
    "On the other hand, even logstash doesn't gel well with Solr. We need a logstash plugin called solr-http-plugin to push logs from logstash to Solr. Again takes a lot of effort to integrate all the three to bring up SiLK.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary Tutorials and Blogs:\n",
    "\n",
    "The following are some amazing guidelines and tutorials to create complex and more useful applications with the help of Solr,Elasticsearch Logstash and Kibana\n",
    "\n",
    "Logstash requires Grok which is a clean and nice pattern matching tool to parse unstructured data and process them so that they are indexed. Indexed data are efficiently queryable. Naturally, Grok is perfect for processing syslogs for the second kind of usecases we looked at in this tutorial. The following link provides a good start for using Grok.\n",
    "\n",
    " https://logz.io/blog/logstash-grok/\n",
    "\n",
    "Elastic search is an excellent distributed search engine with a lots of knobs available to tune up the way we can spread large data-set. Elastic search sharding can be acheived by a few configurations and it the setup depends on the requirement and usecases. In the below git hub, the author thoroughly explains sharding, filebeat( that helps putting data to logstash from multiple sources) and other aspects aroudn ELK stack.\n",
    "\n",
    "https://github.com/minsuk-heo/BigData/\n",
    "\n",
    "ELK over Amazon AWS: The following set of Youtube videos shall be a good place to start with if the requirement is to setup ELK stack over Amazon cloud.\n",
    "\n",
    "https://www.youtube.com/watch?v=ge8uHdmtb1M&t=218s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations and references\n",
    "1. https://en.wikipedia.org/wiki/Apache_Lucene\n",
    "2. https://en.wikipedia.org/wiki/Apache_Solr\n",
    "3. https://wikitech.wikimedia.org/wiki/Logstash\n",
    "4. https://github.com/lucidworks/banana\n",
    "5. https://lucene.apache.org/solr/guide/7_2/solr-tutorial.html\n",
    "6. https://lucene.apache.org/solr/7_2_1/SYSTEM_REQUIREMENTS.html\n",
    "7. https://github.com/logpai/loghub/tree/master/Linux\n",
    "8. https://pypi.python.org/pypi/pysolr/3.6.0\n",
    "9. https://stackoverflow.com/questions/6385168/get-all-the-results-from-solr-without-10-as-limit\n",
    "10. https://en.wikipedia.org/wiki/Elasticsearch\n",
    "11. https://en.wikipedia.org/wiki/Kibana\n",
    "12. http://solr-vs-elasticsearch.com/\n",
    "13. https://github.com/minsuk-heo/BigData/tree/master/ch06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
