{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi! my friend, if you are reading this tutorial, I am sure you will be satisfied when finish your reading. Why? Because this tutorial will walk you through algorithm that help Google and Yahoo and Baidu and all other great search engine companies to success. Sounds familiar? Yes, you are right, this tutorial is related to PageRank algorithm which you leanrt in the class, but more than that, this tutorial will introduce to you another widely used powerful algorithm which also play a essential role in Google---HITS. Getting to know both PageRank and HITS, you can proudly anounce to everyone that you are familiar with the most important link search algorithms in the world. I might be a little bit exaggerate, but trust me, you won't regret learning this algorithm as a complementary algorithm with PageRank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I will walk you through HITS step by step, beginning with the background of HITS, all the way to its principle, and its split step by step algorithm tutorial and a comparison between HITS and pagerank at the end. Note that in this tutorial, I will use Python 3 to realize the algorithm. And if you want to realize the algorithm by yourself, you need to install [python-graph-core](https://anaconda.org/rhishi/python-graph-core) first.\n",
    "\n",
    "Just a reminder of Pagerank algorithm, you can always check the pagerank algorithm in the class note:http://www.datasciencecourse.org/notes/graphs/. I simply put a very good illustration picture here to help you quickly remember the core of the pagerank agorithm:\n",
    "[<img src=\"http://7xoujr.com1.z0.glb.clouddn.com/concept.png\">](http://7xoujr.com1.z0.glb.clouddn.com/concept.png)\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "\n",
    "- [HITS backgrounds](#HITS-backgrounds)\n",
    "- [HITS algorithm principle](#HITS-algorithm-principle)\n",
    "- [Step into HITS](#Step-into-HITS)\n",
    "- [HITS algorithm & pagerank algorithm](#HITS-algorithm-&-pagerank-algorithm)\n",
    "- [Summary and references](#Summary-and-references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HITS backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1999, [Jon Kleinberg](http://www.cs.cornell.edu/home/kleinber/) proposed the HITS algorithm. HITS also aims at a more accurate search and is still an excellent algorithm today. It is currently used by the [Teoma search engine](https://en.wikipedia.org/wiki/Teoma) as a link analysis algorithm.\n",
    "\n",
    "The full name of the HITS algorithm is Hyperlink-Induced Topic Search. In the HITS algorithm, each page is given two attributes: the hub attribute and the authority attribute. At the same time, web pages are divided into two types: hub pages and authority pages. Hub means the center, so the hub page refers to those pages that contain a lot of links to the authoritative page, such as some portal sites; authority pages refer to those pages that contain substantive content. The purpose of the HITS algorithm is to return a high-quality authority page to the user when the user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HITS algorithm principle\n",
    "\n",
    "Many algorithms are based on some assumptions, and the HITS algorithm is no exception. The HITS algorithm is based on the following two assumptions:\n",
    "\n",
    "- A high quality authority page will be pointed to by many high quality hub pages.\n",
    "\n",
    "- A high quality hub page points to many high quality authority pages.\n",
    "\n",
    "And what is \"high quality\"? This is determined by the hub value and the authority value of each page. Its determination method is:\n",
    "\n",
    "- The page's hub value equals to the sum of the authority values of all pages it points to.\n",
    "\n",
    "- The page's authority value is equal to the sum of the hub values of all pages pointing to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple example helps you understand HITS\n",
    "\n",
    "In order to let you have a quickly understanding of the HITS algorithm, a simple example is given below:\n",
    "\n",
    "\n",
    "[<img src=\"http://7xoujr.com1.z0.glb.clouddn.com/hits1.png\">](http://7xoujr.com1.z0.glb.clouddn.com/hits1.png)\n",
    "\n",
    "\n",
    "There are 3 pages in the map, which form a directed graph. We set each web page to have both an initial hub and authority value of 1. We use  $h(p)$ as hub value of page  $p$ , and  $a(p)$  as authority value of page  $p$ . Then we have:\n",
    "\n",
    "$$h(1)=h(2)=h(3)=1,a(1)=a(2)=a(3)=1$$\n",
    "\n",
    "The calculation process of the HITS algorithm is an iterative process. In the first iteration, we have:\n",
    "\n",
    "$a(1)=0,a(2)=0,a(3)=h(1)+h(2)=2$  (No pages point to Page 1 and Page 2)\n",
    "\n",
    "$h(1)=a(3)=2,h(2)=a(3)=2,h(3)=0$  (Page 3 does not point to any page)\n",
    "\n",
    "It is evident that Web page 3 is a relatively good authority page, and Web page 1 and Web page 2 are relatively good hub pages. In fact, the iteration here can also be ended, because it is apparent if continues, the values of  $a(3)$,$h(1)$  and  $h(2)$  would increase continuously, and this won't change the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more details about HITS\n",
    "\n",
    "\n",
    "Unlike the PageRank algorithm, the HITS algorithm runs after the user searches, so the set of objects which HITS need to process is much smaller.\n",
    "\n",
    "First of all, we need to determine this set. The relationship between pages in the entire Internet can be abstracted as a directed graph  $G=(V,E)$ . When a search request is generated (the keyword may be set as  $σ$ ), we can take all pages containing the keyword  $σ$  and compose to the initial set as  $Qσ$ . Then we can run our HITS algorithm on this set. However, what if this set can be very large that it contains millions of pages? This is not a very proper size right? So, according to research, we need to find a smaller set  Sσ  that satisfies these conditions:\n",
    "\n",
    "- $Sσ$  is small enough.\n",
    "- $Sσ$  contains many pages related to the query.\n",
    "- $Sσ$  contains many high quality authority pages.\n",
    "\n",
    "So how to find  $Sσ$ ? We assume that the user enters a keyword search and the search engine uses a text-based engine to search. We take the top  $t$(usually 200) pages as the root set  $Rσ$ . This set satisfies the first two conditions we mentioned above, but not so good under the third condition.\n",
    "\n",
    "So we need to extend  $Rσ$ . It is generally believed that a keyword-related high quality web page, even if it is not in  $Rσ$ , is likely to have some pages pointing to it in  $Rσ$ . Based on this, we extend the process of  $Rσ$  as follows ([from the paper of Jon Kleinberg](http://delab.csd.auth.gr/~dimitris/courses/ir_spring07/papers/JACM_46_5_1999_Authoritative%20sources%20in%20a%20hyperlinked%20environment.pdf)):\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Subgraph(σ, ψ, t, d) \n",
    "　　σ: a query string. \n",
    "　　ψ: a text-based search engine. \n",
    "　　t, d: natural numbers.\n",
    "\n",
    "　　Let Rσ denote the top t results of ψ on σ. \n",
    "　　Set Sσ := Rσ\n",
    "　　For each page p ∈ Rσ \n",
    "　　　　Let Γ+(p) denote the set of all pages p points to. \n",
    "　　　　Let Γ−(p) denote the set of all pages pointing to p. \n",
    "　　　　Add all pages in Γ+(p) to Sσ. \n",
    "　　　　If |Γ−(p)|≤d, then \n",
    "　　　　　　Add all pages in Γ−(p) to Sσ. \n",
    "　　　　Else \n",
    "　　　　　　Add an arbitrary set of d pages from Γ−(p) to Sσ. \n",
    "　　End \n",
    "　　Return Sσ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning we set  $Sσ=Rσ$ . Then we add all the web pages pointed by the web pages in  $Rσ$  to  $Sσ$ , and add a certain number of web pages pointing to the web pages in  $Rσ$  set (in  $Rσ$ , each web page can add up to  d  web pages pointing to it) to  $Sσ$  set. In order to ensure the proper size of  $Sσ$  set,  $d$  can not be too large, generally set to about 50. In general, the size of the set after the expansion is between 1000 to 5000 pages, which satisfies the three conditions I mentioned above.\n",
    "\n",
    "Next, we also need to deal with  $Sσ$ . We should delete all links between pages under the same domain (a domain name refers to a website), because usually these links are just for people to switch between different pages under the same site, such as navigation links within the site. In HITS algorithm, these links in the same website are not contributing too much to reflect the transitive relation between the hub value and the authority value. So we remove these links in the Sσ and new set is  $Gσ$  now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step into HITS\n",
    "\n",
    "In this part, I will walk you through detailed algorithm step by step tutorial. First, you should import necessary libraries for realizing HITS:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pygraph.classes.digraph import digraph \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initialize the digraph and the initial authority and hub set, we can start calculating the hub and authority values. First, the initial hub value  $h(p)$  of each page is set to 1,And the initial authority value  $a(p)$  is also set to 1:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for node in graph.nodes():\n",
    "    hub[node] = 1 \n",
    "    authority[node] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start the iterative calculation process ( $n$  is the total number of web pages in  $Gσ$  set). First, let's calculate the authority value of each page. In each iteration, the process should be like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$∀p,a(p)=\\sum_{i=1}^{n}=h(i)$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for node in dg.nodes(): \n",
    "    for page in dg.incidents(node): # traversal all the incoming pages \n",
    "        authority[node] += hub[page] # just as the formula we gave above \n",
    "        norm += pow(authority[node], 2) # this is for standardized after each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, every round of iterations needs to be standardized, which means we should make sure  \n",
    "$$\\sum_{i=1}^{n}a(i)^2 = 1$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "norm = sqrt(norm) \n",
    "for node in dg.nodes(): \n",
    "    authority[node] /= norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to keep an record of the absolute difference between two iterations:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "   change += abs(tmp[node] - authority[node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then similar process will happen to calculate hub value,also need to standardized everytime after iteration:\n",
    "\n",
    "$$∀p,h(p)=\\sum_{i=1}^{n}=a(i)$$\n",
    "\n",
    "$$\\sum_{i=1}^{n}h(i)^2 = 1$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for node in self.graph.nodes(): \n",
    "    for neighbor_page in self.graph.neighbors(node): # traversal all the outgoing pages\n",
    "        self.hub[node] += self.authority[neighbor_page] \n",
    "        norm += pow(self.hub[node], 2) \n",
    "        norm = math.sqrt(norm) \n",
    "        for node in self.graph.nodes(): \n",
    "            self.hub[node] /= norm \n",
    "            change += abs(tmp[node] - self.hub[node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when does the iteration end? We can set an upper limit of iterations  k  to control, or set a threshold, when the change is less than the threshold, that's when the iteration ends:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "max_iterations = k \n",
    "min_delta = threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can use a flag to measure whether or not we should stop iteration. The flag would be set true if the change we kept a record is less than the initial threshould(min_delta) we set. That't when we stop:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if change < min_delta: \n",
    "    flag = True break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then just return to the user with dozens of pages in which the authority value is relative high.\n",
    "Well, taht's all about the principle of the HITS algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HITS algorithm & pagerank algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I gave you a whole executable HITS algorithm realization and compare to a corresponding pagerank algorithm with the same input graph. The input graph is like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygraph.classes.digraph import digraph\n",
    "import math\n",
    "\n",
    "\n",
    "dg = digraph()\n",
    "\n",
    "dg.add_nodes([\"A\", \"B\", \"C\", \"D\", \"E\"])\n",
    "\n",
    "dg.add_edge((\"A\", \"C\"))\n",
    "dg.add_edge((\"A\", \"D\"))\n",
    "dg.add_edge((\"B\", \"D\"))\n",
    "dg.add_edge((\"C\", \"E\"))\n",
    "dg.add_edge((\"D\", \"E\"))\n",
    "dg.add_edge((\"B\", \"E\"))\n",
    "dg.add_edge((\"E\", \"A\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the whole program with comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HITSIterator:\n",
    "    '''calclulating the hub and authority value in a graph'''\n",
    "\n",
    "    def __init__(self, dg):\n",
    "        self.max_iterations = 100  # maximum iteration times\n",
    "        self.min_delta = 0.0001  # parameter which tells us should the iteration ends or not\n",
    "        self.graph = dg \n",
    "        self.hub = {}\n",
    "        self.authority = {}\n",
    "        for node in self.graph.nodes():\n",
    "            self.hub[node] = 1\n",
    "            self.authority[node] = 1\n",
    "\n",
    "    def hits(self):\n",
    "            \"\"\"\n",
    "            calclulate the authority and hub value in each page\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            if not self.graph:\n",
    "                return\n",
    "\n",
    "            flag = False\n",
    "            for i in range(self.max_iterations):\n",
    "                change = 0.0  # make a record of the change value in each iteration\n",
    "                norm = 0  # normalized parameter\n",
    "                tmp = {}\n",
    "                # calclulate the authority value in each page \n",
    "                tmp = self.authority.copy()\n",
    "                for node in self.graph.nodes():\n",
    "                    self.authority[node] = 0\n",
    "                    for incident_page in self.graph.incidents(node):  # traversal all the incoming pages \n",
    "                        self.authority[node] += self.hub[incident_page]\n",
    "                    norm += pow(self.authority[node], 2)\n",
    "                # normalization\n",
    "                norm = math.sqrt(norm)\n",
    "                for node in self.graph.nodes():\n",
    "                    self.authority[node] /= norm\n",
    "                    change += abs(tmp[node] - self.authority[node])\n",
    "\n",
    "                # cal every page's hub value\n",
    "                norm = 0\n",
    "                tmp = self.hub.copy()\n",
    "                for node in self.graph.nodes():\n",
    "                    self.hub[node] = 0\n",
    "                    for neighbor_page in self.graph.neighbors(node):  # traversal all the outgoing paged\n",
    "                        self.hub[node] += self.authority[neighbor_page]\n",
    "                    norm += pow(self.hub[node], 2)\n",
    "                # normalization\n",
    "                norm = math.sqrt(norm)\n",
    "                for node in self.graph.nodes():\n",
    "                    self.hub[node] /= norm\n",
    "                    change += abs(tmp[node] - self.hub[node])\n",
    "\n",
    "\n",
    "                if change < self.min_delta:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag:\n",
    "                print(\"finished in %s iterations!\" % (i + 1))\n",
    "            else:\n",
    "                print(\"finished out of 100 iterations!\")\n",
    "\n",
    "                \n",
    "            max_a = max(self.authority.items(), key=lambda x: x[1])\n",
    "            max_b = max(self.hub.items(), key=lambda x: x[1])\n",
    "            print(\"The best authority page: \", max_a)\n",
    "            print(\"The best hub page: \", max_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished in 9 iterations!\n",
      "The best authority page:  ('E', 0.7886751345855355)\n",
      "The best hub page:  ('B', 0.7071067811721405)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "   \n",
    "    hits = HITSIterator(dg)\n",
    "    hits.hits()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the pagerank algorithm and its output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = digraph()\n",
    "\n",
    "dg.add_nodes([\"A\", \"B\", \"C\", \"D\", \"E\"])\n",
    "\n",
    "dg.add_edge((\"A\", \"C\"))\n",
    "dg.add_edge((\"A\", \"D\"))\n",
    "dg.add_edge((\"B\", \"D\"))\n",
    "dg.add_edge((\"C\", \"E\"))\n",
    "dg.add_edge((\"D\", \"E\"))\n",
    "dg.add_edge((\"B\", \"E\"))\n",
    "dg.add_edge((\"E\", \"A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished in 22 iterations!\n",
      "The final page rank is\n",
      " {'A': 0.3081253850448491, 'B': 0.030000000000000006, 'C': 0.16095328864406086, 'D': 0.17370328864406087, 'E': 0.3272080906949035}\n"
     ]
    }
   ],
   "source": [
    "class PRIterator:\n",
    "\n",
    "    def __init__(self, dg):\n",
    "        self.damping_factor = 0.85  # α\n",
    "        self.max_iterations = 100  # iteration times\n",
    "        self.min_delta = 0.00001  # ϵ used to determine whether the iteration should stop \n",
    "        self.graph = dg\n",
    "\n",
    "    def page_rank(self):\n",
    "        #  change the graph, all the nodes has no outgoing link are now changed to has outgoing to themselves\n",
    "        for node in self.graph.nodes():\n",
    "            if len(self.graph.neighbors(node)) == 0:\n",
    "                for node2 in self.graph.nodes():\n",
    "                    digraph.add_edge(self.graph, (node, node2))\n",
    "\n",
    "        nodes = self.graph.nodes()\n",
    "        graph_size = len(nodes)\n",
    "\n",
    "        if graph_size == 0:\n",
    "            return {}\n",
    "        page_rank = dict.fromkeys(nodes, 1.0 / graph_size)  # initialize the PR value\n",
    "        damping_value = (1.0 - self.damping_factor) / graph_size  #  (1−α)/N in the pagerank formula\n",
    "\n",
    "        flag = False\n",
    "        for i in range(self.max_iterations):\n",
    "            change = 0\n",
    "            for node in nodes:\n",
    "                rank = 0\n",
    "                for incident_page in self.graph.incidents(node):  # traversal all the incoming pages\n",
    "                    rank += self.damping_factor * (page_rank[incident_page] / len(self.graph.neighbors(incident_page)))\n",
    "                rank += damping_value\n",
    "                change += abs(page_rank[node] - rank)  # absolute diffrence \n",
    "                page_rank[node] = rank\n",
    "\n",
    "\n",
    "            if change < self.min_delta:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            print(\"finished in %s iterations!\" % (i+1))\n",
    "        else:\n",
    "            print(\"finished out of 100 iterations!\")\n",
    "        return page_rank\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "   \n",
    "    pr = PRIterator(dg)\n",
    "    page_ranks = pr.page_rank()\n",
    "\n",
    "\n",
    "    print(\"The final page rank is\\n\", page_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see using the HITS, the best authority page is E; In pagerank, the most important page is E. The result is similar. But actually, if we apply these two algorithms on the actual internet with actual content, we will find that HITS algorithm is closely related to the query request entered by the user, while PageRank is independent of the query request. Therefore, the HITS algorithm can be used as a similarity evaluation criterion alone, and PageRank must be combined with content similarity calculation to evaluate the relevance of the webpage\n",
    "\n",
    "\n",
    "- And from the two algorithm structure we can tell, HITS algorithm has a small number of computational objects. It only needs to calculate the link relationship between web pages in the extended set. PageRank is a global algorithm that processes all Internet page nodes\n",
    "\n",
    "\n",
    "- What's more, in the calculation of the HITS algorithm, two scores need to be calculated for each page, while PageRank only needs to calculate one score; in the search engine field, the importance of the Authorities weight calculated by the HITS algorithm is more emphasized, but in many application HITS algorithms In other areas, the Hub score also plays an important role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and references\n",
    "\n",
    "\n",
    "This tutorial walked you through the detailed principle and prove of HITS algorithm, as well as the code realization. This tutorial is based on Jon Kleinberg's paper: [Authoritative Sources in a Hyperlinked Environment](http://delab.csd.auth.gr/~dimitris/courses/ir_spring07/papers/JACM_46_5_1999_Authoritative%20sources%20in%20a%20hyperlinked%20environment.pdf)\n",
    "\n",
    "Much more detail about the HITS algorithm more professional explanation and some improved HITS algorithm and very comprehensive comparison between pagerank and HITS algorithm are available from the following links:\n",
    "\n",
    "HITS wikipedia: https://en.wikipedia.org/wiki/HITS_algorithm\n",
    "\n",
    "HITS in lecture of Cornell: http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture4/lecture4.html\n",
    "\n",
    "Comparison between pagerank and HITS algorithm: http://www.cis.hut.fi/Opinnot/T-61.6020/2008/pagerank_hits.pdf\n",
    "\n",
    "A Method for Accelerating the HITS Algorithm: https://arxiv.org/abs/0909.0572"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
