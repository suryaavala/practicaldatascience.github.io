{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Introduction to Tensorflow and demonstrating use of tensorflow in an example of image classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Contents_:\n",
    "* [Objective](#first-bullet)\n",
    "* [What is  Tensorflow?](#second-bullet)\n",
    "* [Why should we care about?](#third-bullet)\n",
    "* [What are Tensorflow API's](#fifth-bullet)\n",
    "* [Application: Image classifier using Tensorflow](#sixth-bullet)\n",
    "* [Summary](#seventh-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial aims at introducing readers to what is Tensorflow and how to get it on your local machine.It also aims at helping readers understand the Tensorflow API from high level and build deep learning application by demonstartaing an example of Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Tensorflow?<a class=\"anchor\" id=\"#second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow™ is an open source software library for numerical computation using data flow graphs. In the graphs, the nodes are the mathamatical operations that are performed and the edges are the multidimensional arrays, also called as tensors.\n",
    "\n",
    "It was originally developed by Research and development team at Google;s Machine Intellegence department for purpose of conducting machine learning algorithms.Although the system happened to build so generally that it can be applied to varied application in Machine learning domain.\n",
    "\n",
    "There have been similar kind of framework available originally before Google realeasing it.For example: Theano,Torch,Caffe etc.But the claim that Google puts is the flexibility of deployement on distributed system that Tensorflow have is'nt available with other possible options.\n",
    "\n",
    "\n",
    "If you want to browse through the graph that is generated by tensorflow, go here:\n",
    "<img src=\"https://www.tensorflow.org/versions/master/images/graph_vis_animation.gif\">\n",
    "\n",
    "ImageSource: https://www.tensorflow.org/versions/master/images/graph_vis_animation.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why should we care? <a class=\"anchor\" id=\"third-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google released tensorflow and various calling methods for pretrained tensorflow models but it did not really release any of the training models to tweek around.These pre-trained Deep Learning model are trained at high volumnious data which enables the impressive pattern recognition capabilities.  To be able to create something comparable, one would need to have Google's data and Google's compute resources which is highly difficult to process.\n",
    "\n",
    "In nutshell,it allows machine learning researcher to experiment with deep neural nets  _more easily_. Here, \"more easily\" means it takes less time to construct artificial neural networks, less time to train them (this is arguable, as is always the case when speed is concerned), and less time to deploy them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow API's <a class=\"anchor\" id=\"fourth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google opensourced theird Object detection API and few other API to help machine learning community to use.\n",
    "These APIs were relaeased to save time and to do things that require computationally extensive resources.\n",
    "\n",
    "Object Detection API:\n",
    "https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html\n",
    "\n",
    "The API has described 5 majour training models developed by them and which have been pre-trained on  COCO dataset(Common Objects in Context) to provide the object detection mechanism.This is a dataset of 300k images of most commonly found objects. The dataset is huge as a result it results in really impressive way of identifying the objects in a image.\n",
    "\n",
    "Examples of objects includes:\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*z3w1pldwnhF3pvMnxVfrtw.png\">\n",
    "\n",
    "Image Source: https://towardsdatascience.com/is-google-tensorflow-object-detection-api-the-easiest-way-to-implement-image-recognition-a8bd1f500ea0\n",
    "\n",
    "\n",
    "The models that are provided to us are as given below:\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*-EyxSs2OiyWm-E6MSpSJiA.png\">\n",
    "\n",
    "Image Source: https://cdn-images-1.medium.com/max/800/1*-EyxSs2OiyWm-E6MSpSJiA.png\n",
    "\n",
    "Here maP is Mean average precision (accuracy metric) on a train example of detecting a box.It’s a good combined measure for how sensitive the network is to objects of interest and how well it avoids false alarms.\n",
    "\n",
    "More information can be found here:\n",
    "<a href=\"https://github.com/tensorflow/models/blob/477ed41e7e4e8a8443bc633846eb01e2182dc68a/object_detection/g3doc/detection_model_zoo.md\">link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Image classifier using Tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement:\n",
    "The below application basically binary classifies image (image of trump or image of obama) by face detection and tensor flow pre-trained model.Our objective is to build and train a model in such a way that after training our model could identify \n",
    "1. Cartoon Images of Obama and Trump\n",
    "2. Distorted Images(morphed images) of them\n",
    "3. Younger version of Trump and Obama.\n",
    "\n",
    "##### Dataset:\n",
    "The dataset for originality purposes have been made by collecting various images from google pictures.\n",
    "The dataset has 150 images of Barack Obama(original) and 150 images of Donald Trump(original).(Training Data)\n",
    "The test dataset has 15 images including Cartoon images, Young versions of Obama and Trump,distorted images for testing purposes.\n",
    "\n",
    "###### _Note: No image in training dataset has any distorted/cartoon/young images of either of two._.This is done so as to see how well is the model able to predict it which image corrosponds to which president.\n",
    "\n",
    "##### Model:\n",
    "We would be using the _Inceptionv3 Model_ (pre-trained) provided by Tensorflow\n",
    "\n",
    "### Steps to follow:\n",
    "* [Performance of tensorflow API on our image:](#Performance of tensorflow API on our image)\n",
    "* [Import neccesary files](#first-bullet)\n",
    "* [Preprocessing: Defining some neccesary variables to use](#second-bullet)\n",
    "* [Preprocessing: Defining a dictionary with all the neccesary paths](#third-bullet)\n",
    "* [Preprocessing: Lets download the model first](#fourth-bullet)\n",
    "* [Preprocessing: Lets draw the graph now](#fifth-bullet)\n",
    "* [Training and Validation: Lets build the data and split it now](#sixth-bullet)\n",
    "* [Training and Validation: Lets retrain the model on our new dataset](#seventh-bullet)\n",
    "* [Testing times!](#eight-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance of tensorflow API on our image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran a test image that contained both presidents in it on the available Tensorflow pre-trained Object detection API(Trained on COCO dataset)\n",
    "\n",
    "The result was that the image classified only them as a **Persons** and not on their specific names:\n",
    "This could be the reason as COCO dataset do not have any pre-traiend knowledge on how trump or obama looks like.\n",
    "Below are obtained result:\n",
    "<img src=\"https://image.ibb.co/hPEaWn/api_test.png\" alt=\"api_test\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we retrain the model on new categories as per our own customized dataset:\n",
    "\n",
    "In the process:\n",
    "**We use the pre trained model and then re-train it on our new customized data set to generate a graph and labels file which can then be used together for testing purposes.**\n",
    "\n",
    "<img src=\"https://4.bp.blogspot.com/-TMOLlkJBxms/Vt3HQXpE2cI/AAAAAAAAA8E/7X7XRFOY6Xo/s1600/image03.png\">\n",
    "\n",
    "Image Source:https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import neccesary files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we start by importing packages that we would be needing \n",
    "1. Basic Packages: Like regrex,datetime,hashlib,sys,tarfile(for untaring the downloaded file),numpy\n",
    "2. Tensor flow packages: Also we add packages that we would be using from tensorflow packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "import os\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.python.util import compat\n",
    "import my_utility1 as um"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing: Defining some neccesary variables to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some variables that would be needing in the process, in this we basically define some of the key variables that we use eg:\n",
    "    Model Url: Specifies the URl from which we would be downloading the model\n",
    "    BottleNeck\n",
    "    \n",
    "    \n",
    "These are all parameters that are tied to the particular model architecture and hence for us these are standard characteristic paramter for _Inception v3_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_url=\"http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\"\n",
    "bottleneck_name = \"pool_3/_reshape:0\"\n",
    "bottleneck_size=2048\n",
    "w=h=299\n",
    "d=3\n",
    "jpg_tensor_name=\"DecodeJpeg/contents:0\"\n",
    "resize_name = \"ResizeBilinear:0\"\n",
    "max_count=2 ** 27 - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Preprocessing: Defining a dictionary with all the neccesary paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*bottleneck_dir*\n",
    "Bottleneck dir is place where we would like bottlenecks to store. Bottlenecks are all the images and converted txt format.Each image is converted to text file and thereafter stored in bottleneck directory.\n",
    " \n",
    "*how_many_training_steps:*\n",
    "We can step size our training process(I have set it 400) you can change it according to the iteration you would want to make to better train it.400 is just done out of speed.Default is 4000.\n",
    " \n",
    "*image_dir:*\n",
    "Lets go further and make a dictionary with different paramters  with some of the paramter that needs path where to find theimages to train on defined by _image_dir_.\n",
    " \n",
    "*model_dir*: \n",
    "Path where our model is downloaded (that we would be doing in subsequemt steps.\n",
    "\n",
    "*output_graph*: \n",
    "Takes the path of file where we want our model to store the generated output after training.\n",
    " \n",
    "*output_labels*:\n",
    "Takes the path of file where we want our model to store the labels that it generates after training.\n",
    "\n",
    "*summaries_dir*:\n",
    "Takes the path of place we want to store our summary reports that it prints while training the models.\n",
    "\n",
    "_Note:I have used these paths, you can go ahead and change these paths as per your convience._\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS={}\n",
    "FLAGS[\"bottleneck_dir\"]='/tmp/bottleneck'\n",
    "FLAGS[\"eval_step_interval\"]=10\n",
    "FLAGS[\"final_tensor_name\"]='final_result' \n",
    "FLAGS[\"flip_left_right\"]=False \n",
    "FLAGS[\"how_many_training_steps\"]=4000\n",
    "FLAGS[\"image_dir\"]='C:\\\\Users\\\\Anupam\\\\Documents\\\\SPRING 2018\\\\MINI4\\\\PDS\\\\Tutorial-adewan\\\\train' \n",
    "FLAGS[\"learning_rate\"]=0.01 \n",
    "FLAGS[\"model_dir\"]='/tmp/imagenet' \n",
    "FLAGS[\"output_graph\"]='retrained_graph.pb' \n",
    "FLAGS[\"output_labels\"]='retrained_labels.txt' \n",
    "FLAGS[\"print_misclassified_test_images\"]=False \n",
    "FLAGS[\"random_brightness\"]=0 \n",
    "FLAGS[\"random_crop\"]=0 \n",
    "FLAGS[\"random_scale\"]=0 \n",
    "FLAGS[\"summaries_dir\"]='/tmp/retrain_logs' \n",
    "FLAGS[\"test_batch_size\"]=-1 \n",
    "FLAGS[\"testing_percentage\"]=10 \n",
    "FLAGS[\"train_batch_size\"]=100 \n",
    "FLAGS[\"validation_batch_size\"]=100 \n",
    "FLAGS[\"validation_percentage\"]=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Preprocessing: Lets download the model first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after we have our variables in place, our first task is to go ahead and download the model \n",
    "As here we are using Inception v3 model we will go ahead and download the model from :\n",
    "http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "After our python code download the modle we will tell it where to store as we have specified it in above FLAGS = dictionary with key **\"model_dir\"**.If we have run once our model is not downloaded again and we simply use the odwnloaded model.\n",
    "\n",
    "_Note: These are helper functions and will be used when we start our training proces later in subsequent steps_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _progress(count, block_size, total_size):\n",
    "    sys.stdout.write('\\r>> Downloading %s %.1f%%' %\n",
    "                       (filename,\n",
    "                        float(count * block_size) / float(total_size) * 100.0))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def download_model(FLAGS):\n",
    "    des_dir = FLAGS[\"model_dir\"]\n",
    "    URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "    if not os.path.exists(des_dir):\n",
    "        os.makedirs(des_dir)\n",
    "    fpath = os.path.join(des_dir, \"inception-2015-12-05.tgz\")\n",
    "    if not os.path.exists(fpath):\n",
    "        filepath, _ = urllib.request.urlretrieve(URL,fpath,_progress)\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    tarfile.open(fpath, 'r:gz').extractall(des_dir)\n",
    "    print(\"Download Sucessful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Preprocessing: Lets draw the graph now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After we have our model downloaded and extracted lets draw the graph \n",
    "We create a graph from saved GraphDef file and returns a Graph object.This graph defination file happens to be present at the location we downloaded our model at _\"classify_image_graph_def.pb\"_.\n",
    "\n",
    "It takes this graph file reads into a string and take various tensors from the file.It finally returns graph holding the trained inception network and various tensors we'll be manipulating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_graph():\n",
    "    lst=[bottleneck_name, jpg_tensor_name,resize_name]\n",
    "    with tf.Graph().as_default() as g:\n",
    "            m_fname =\"/tmp/imagenet\\classify_image_graph_def.pb\"\n",
    "            with gfile.FastGFile(m_fname, 'rb') as f:\n",
    "                gd = tf.GraphDef()\n",
    "                gd.ParseFromString(f.read())\n",
    "                btlnk_tens, jpeg_tens, resized_tens = (tf.import_graph_def(gd, name='', return_elements=lst))\n",
    "    print(\"Graph Drawn!\")\n",
    "    return g, btlnk_tens, jpeg_tens, resized_tens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Training and Validation: Lets build the data and split it now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two functions make_data and split data, we read train data and read the associated labels.\n",
    "Our train data images should be structured in the following folder structure:\n",
    "\n",
    "    -Parent Folder:\n",
    "    \n",
    "        -Trump(class1)folder\n",
    "        \n",
    "            ->images of trump(training data)\n",
    "    \n",
    "        -Obama(class2)folder\n",
    "        \n",
    "            ->images of obama(training data)\n",
    "            \n",
    "The function make_Data basically constructs the whole data by using the percentage that we had set for training, validation,testing puposes and returns a dictionary of following format:\n",
    "\n",
    "{\n",
    "        \"dir\":\\\\directory path\\\\\n",
    "        \"trianing\":\n",
    "            [\\\\list\\\\of\\\\all\\\\paths\\\\of\\\\training\\\\image\\\\]\n",
    "        \"testing\":\n",
    "            [\\\\list\\\\of\\\\all\\\\paths\\\\of\\\\training\\\\image\\\\that were split\\\\as training\\\\]\n",
    "        \"validation\":\n",
    "             [\\\\list of paths\\\\of\\\\image\\\\segregrated\\\\as\\\\validation\\\\data]\n",
    "}\n",
    "\n",
    "_Split percentage Used_:  \n",
    "- Train data: 80%\n",
    "- Validation Data: 10%\n",
    "- Test Data: 10%(10% out of training data only)\n",
    "\n",
    "For Split Data I used reference from Tensorflow:https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(image_dir,valid_per,test_per,file_lst):\n",
    "    training_images = []\n",
    "    testing_images = []\n",
    "    validation_images = []\n",
    "    for file_name in file_lst:\n",
    "        base_name = os.path.basename(file_name)\n",
    "        hash_name = re.sub(r'_nohash_.*$', '', file_name)\n",
    "        hash_name_hashed = hashlib.sha1(compat.as_bytes(hash_name)).hexdigest()\n",
    "        MAX_NUM_IMAGES_PER_CLASS = 2 ** 27 - 1  # ~134M\n",
    "        percentage_hash = ((int(hash_name_hashed, 16) %\n",
    "                          (MAX_NUM_IMAGES_PER_CLASS + 1)) *\n",
    "                         (100.0 / MAX_NUM_IMAGES_PER_CLASS))\n",
    "        if percentage_hash < valid_per:\n",
    "            validation_images.append(base_name)\n",
    "        elif percentage_hash < (test_per + valid_per):\n",
    "            testing_images.append(base_name)\n",
    "            training_images.append(base_name)\n",
    "    return{'dir': image_dir,'training': training_images,'testing': testing_images,'validation': validation_images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data(FLAGS):\n",
    "    res_dict={}\n",
    "    image_dir = FLAGS[\"image_dir\"]\n",
    "    valid_per = FLAGS[\"validation_percentage\"]\n",
    "    test_per = FLAGS[\"testing_percentage\"]\n",
    "    sub_dirs = [x[0] for x in gfile.Walk(image_dir)]\n",
    "    sub_dirs = sub_dirs[1:]\n",
    "    for i in sub_dirs:\n",
    "        f_lst = []\n",
    "        file_glob = os.path.join(image_dir, i, '*.jpg')\n",
    "        f_lst.extend(gfile.Glob(file_glob))\n",
    "        label_name = re.sub(r'[^a-z0-9]+', ' ', i.lower())\n",
    "        label_name = label_name.split(\" \")[-1]\n",
    "        res_dict[label_name]=split_data(label_name,valid_per,test_per,f_lst)\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Validation: Lets retrain the model on our new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally we have our model in place,our graph drawn, and our data splitted into validation data and train data.\n",
    "We now use these methods in our tensorflow_classify() function which basically trains on batch of data  in the step size that we mentioned in FLAGS dictionary(4000 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Sucessful!\n",
      "Graph Drawn!\n",
      "Model trained and validated!\n",
      "Test accuracy pridicted\n",
      "INFO:tensorflow:Froze 2 variables.\n",
      "Converted 2 variables to const ops.\n",
      "Trained Graph created\n",
      "Labels created!\n",
      "Training task ended\n"
     ]
    }
   ],
   "source": [
    "def tenorflow_classify(FLAGS):\n",
    "    res_dict={}\n",
    "    download_model(FLAGS)\n",
    "    tupl = draw_graph()\n",
    "    gr,bottleneck,jpeg_tensor,resize_tensor=tupl\n",
    "    res_dict = make_data(FLAGS)\n",
    "    no_of_classes = len(res_dict.keys())\n",
    "    steps = FLAGS[\"how_many_training_steps\"]\n",
    "    with tf.Session(graph=gr) as sess:\n",
    "        (ts,ce,bi,gt,ft) = um.training_step(no_of_classes,FLAGS[\"final_tensor_name\"],bottleneck)\n",
    "        evalu, pred = um.evaluation_step(ft, gt)\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(FLAGS[\"summaries_dir\"] + '/train', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(steps):\n",
    "            (train_b,train_gt, _) = um.get_random_cached_bottlenecks(sess, res_dict, FLAGS[\"train_batch_size\"], 'training',FLAGS[\"bottleneck_dir\"], FLAGS[\"image_dir\"], jpeg_tensor,bottleneck)\n",
    "            dict1 ={bi: train_b,gt: train_gt}\n",
    "            train_summary, _ = sess.run([merged, ts],feed_dict=dict1)\n",
    "            train_writer.add_summary(train_summary, i)\n",
    "            validation_writer = tf.summary.FileWriter(FLAGS[\"summaries_dir\"] + '/validation')\n",
    "            is_last_step = (i + 1 == steps)\n",
    "            if (i % FLAGS[\"eval_step_interval\"]) == 0 or is_last_step:\n",
    "                train_accuracy, cross_entropy_value = sess.run([evalu, ce],feed_dict=dict1)\n",
    "                validation_b, validation_gt, _ = um.get_random_cached_bottlenecks(sess, res_dict, FLAGS[\"validation_batch_size\"], 'validation',FLAGS[\"bottleneck_dir\"], FLAGS[\"image_dir\"], jpeg_tensor,bottleneck)\n",
    "                dict2 ={bi: validation_b,gt: validation_gt}\n",
    "                validation_summary, validation_accuracy = sess.run([merged, evalu],feed_dict=dict2)\n",
    "                validation_writer.add_summary(validation_summary, i)\n",
    "        print(\"Model trained and validated!\")\n",
    "        test_b, test_gt, test_f = (um.get_random_cached_bottlenecks(sess, res_dict,FLAGS[\"test_batch_size\"],'testing', FLAGS[\"bottleneck_dir\"], FLAGS[\"image_dir\"], jpeg_tensor,bottleneck))\n",
    "        dict3={bi:test_b,gt:test_gt}\n",
    "        test_accuracy, predictions = sess.run([evalu, pred],feed_dict=dict3)\n",
    "        print(\"Test accuracy pridicted\")\n",
    "        output_graph_def = graph_util.convert_variables_to_constants(sess, gr.as_graph_def(), [FLAGS[\"final_tensor_name\"]])\n",
    "        with gfile.FastGFile(FLAGS[\"output_graph\"], 'wb') as f:\n",
    "              f.write(output_graph_def.SerializeToString())\n",
    "        print(\"Trained Graph created\")\n",
    "        with gfile.FastGFile(FLAGS[\"output_labels\"], 'w') as f:\n",
    "              f.write('\\n'.join(res_dict.keys()) + '\\n')\n",
    "        print(\"Labels created!\")\n",
    "        print(\"Training task ended\")\n",
    "tenorflow_classify(FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Testing times!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification returns a retrained.graph and a retrained_labels.txt file where we have our new graoh created and a new labels text file created that consist of the the labels for classification.\n",
    "Now we test it on our test data.\n",
    "Here we take the input layer and output layer and get the set of operation from the graph\n",
    "We then run the session instance on the operation obtained and produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import my_utility1 as mu1\n",
    "import PIL\n",
    "from PIL import ImageFont\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "def test_my_classifier(image_to_test_path,path_to_label_file,path_to_graph):\n",
    "    im1 = Image.open(image_to_test_path)\n",
    "    #font = ImageFont.truetype(\"sans-serif.ttf\", 16)\n",
    "    FLAGS2={}\n",
    "    FLAGS2[\"image\"]=image_to_test_path\n",
    "    FLAGS2[\"labels\"]=path_to_label_file\n",
    "    FLAGS2[\"graph\"]=path_to_graph\n",
    "    FLAGS2[\"input_layer\"]=\"Mul\"\n",
    "    FLAGS2[\"output_layer\"]=\"final_result\"\n",
    "    FLAGS2[\"num_top_predictions\"]=2\n",
    "    graph = mu1.load_graph(FLAGS2[\"graph\"])\n",
    "    t = mu1.read_tensor_from_image_file(\n",
    "      FLAGS2[\"image\"],\n",
    "      input_height=299,\n",
    "      input_width=299,\n",
    "      input_mean=0,\n",
    "      input_std=255)\n",
    "    input_name = \"import/\" + FLAGS2[\"input_layer\"]\n",
    "    output_name = \"import/\" + FLAGS2[\"output_layer\"]\n",
    "    input_operation = graph.get_operation_by_name(input_name)\n",
    "    output_operation = graph.get_operation_by_name(output_name)\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        results = sess.run(output_operation.outputs[0], {\n",
    "        input_operation.outputs[0]: t\n",
    "        })\n",
    "    results = np.squeeze(results)\n",
    "\n",
    "    top_k = results.argsort()[-5:][::-1]\n",
    "    labels = mu1.load_labels(FLAGS2[\"labels\"])\n",
    "    result_str=\"\"\n",
    "#     for i in top_k:\n",
    "#         result_str=result_str +\"  Probablity of having\"+ labels[i]+ \"is\" + results[i]\n",
    "    if(results[0]>results[1]):\n",
    "        prob=results[0]\n",
    "        label=labels[0]\n",
    "    elif(results[0]<results[1]):\n",
    "        prob=results[1]\n",
    "        label=labels[1]\n",
    "    i = prob\n",
    "    i *= 100\n",
    "    FLAGS2={}\n",
    "#     if i >= 50:\n",
    "        #strr =\"Recognised as {0} with {1} percentage chances!\".format(label,i)\n",
    "    strr=\"{0}:{1}\".format(label, round(i,2))\n",
    "#     else:\n",
    "#         return None  \n",
    "    draw = ImageDraw.Draw(im1)\n",
    "    f = ImageFont.truetype(\"c:/windows/fonts/arial.ttf\", 25)\n",
    "    draw.text((70, 20),strr,(255,0,0),font=f)\n",
    "    draw = ImageDraw.Draw(im1)\n",
    "    return im1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "lst_of_test_imgs = glob.glob('C:/Users/Anupam/Documents/SPRING 2018/MINI4/PDS/Tutorial-adewan/test/*.jpg')\n",
    "output=\"C:/Users/Anupam/Documents/SPRING 2018/MINI4/PDS/Tutorial-adewan/test/output\"\n",
    "mapp={}\n",
    "for img in lst_of_test_imgs:\n",
    "    im = test_my_classifier(img,\"retrained_labels.txt\",\"retrained_graph.pb\")\n",
    "    if im is not None:\n",
    "        parts = img.split(\"\\\\\")\n",
    "        im.save(output+\"/\"+parts[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test sample 1: Pic when both are together:\n",
    "##### VERDICT: Performs Good!\n",
    "\n",
    "<img src=\"https://image.ibb.co/dT8kWn/both_together.jpg\" alt=\"both together\" border=\"0\" />\n",
    "\n",
    "Trump Probability is maximum among the two.\n",
    "\n",
    "### Test Sample 2:Cartoon Pic of Obama:\n",
    "##### VERDICT: Performs Good!\n",
    "<img src=\"https://image.ibb.co/hQx7cS/cartoon_obama.jpg\" alt=\"cartoon_obama\" border=\"0\">\n",
    "\n",
    "Our Model predicts Obama\n",
    "\n",
    "Observe Closely the probablity percentage dropped drastically from 95'ish to 60-70'ish\n",
    "\n",
    "\n",
    "### Test Sample 3:Cartoon Pic of Trump:\n",
    "##### VERDICT: Performs Good!\n",
    "<img src=\"https://image.ibb.co/jTdDHS/cartoon_trump.jpg\" alt=\"cartoon_trump\" border=\"0\">\n",
    "\n",
    "<img src=\"https://image.ibb.co/dwWdj7/cartoon_trump2.jpg\" alt=\"cartoon_trump2\" border=\"0\">\n",
    "\n",
    "\n",
    "Our Model predicts Trump\n",
    "\n",
    "### Test Sample 4: Fake Pic of Obama:\n",
    "##### VERDICT: Performs Good!\n",
    "\n",
    "<img src=\"https://image.ibb.co/hwUncS/fake_obama.jpg\" alt=\"fake_obama\" border=\"0\">\n",
    "\n",
    "<img src=\"https://image.ibb.co/n7ftHS/fake_obama3.jpg\" alt=\"fake_obama3\" border=\"0\">\n",
    "\n",
    "Our Model predicts Obama\n",
    "\n",
    "### Test Sample 5:Fake Pic of Trump:\n",
    "##### VERDICT: Performs Good!\n",
    "\n",
    "<img src=\"https://image.ibb.co/fSOxcS/fake_trump.jpg\" alt=\"fake_trump\" border=\"0\">\n",
    "\n",
    "Our Model predicts Trump\n",
    "\n",
    "### Test Sample 6: Morphed Clinton-Trump Images 1:\n",
    "##### VERDICTPerforms Good!\n",
    "\n",
    "<img src=\"https://image.ibb.co/dNOxcS/morphed_trump.jpg\" alt=\"morphed_trump\" border=\"0\">\n",
    "\n",
    "Our Model predicts Trump\n",
    "\n",
    "### Test Sample 7: Morphed Obama-Trump Images 2:\n",
    "##### VERDICT:Not so Good.\n",
    "<img src=\"https://image.ibb.co/b6oVxS/trum_obama_morph.jpg\" alt=\"trum_obama_morph\" border=\"0\">\n",
    "<img src=\"https://image.ibb.co/cWSOHS/trum_obama_morph2.jpg\" alt=\"trum_obama_morph2\" border=\"0\">\n",
    "\n",
    "\n",
    "Our Model predicts Obama(low accuracy)\n",
    "\n",
    "Observe Closely the probablity percentage dropped drastically from 95'ish to 60-70'ish\n",
    "\n",
    "### Test Sample 8: Young Pic of Trump\n",
    "##### VERDICT:Terrible.\n",
    "<img src=\"https://image.ibb.co/i4ULxS/young_trump.jpg\" alt=\"young_trump\" border=\"0\">\n",
    "\n",
    "Our Model predicts Obama\n",
    "\n",
    "### Test Sample 9: Young Pic of Obama\n",
    "##### VERDICT:Terrible.\n",
    "<img src=\"https://image.ibb.co/k0qAxS/young_obama.jpg\" alt=\"young_obama\" border=\"0\">\n",
    "Our Model predicts Trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion we find that there are instances when the model predicts accuratly.But certain instances have a drawback like(morphed images,young times images) where it does not do quite a good job.The reason that could be the case is that the training data created by ownself is of low range (300 images) in total and as the train data has no knowledge of how they looked in earlier times.It makes it difficult for prediction to be correct.\n",
    "\n",
    "But in nutshell we find tensorflow to do a good job in overall classification of pictures into trump and obama classes by retraining the pre-trained model on new dataset and joining a new layer above it in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.https://www.datacamp.com/community/tutorials/tensorflow-tutorial\n",
    "\n",
    "2.https://developers.googleblog.com/2018/01/announcing-tensorflow-15.html\n",
    "\n",
    "3.https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0\n",
    "\n",
    "4.https://www.tensorflow.org/api_docs/\n",
    "\n",
    "5.https://www.tensorflow.org/tutorials/\n",
    "\n",
    "6.https://medium.com/autonomous-agents/how-to-teach-logic-to-your-neuralnetworks-116215c71a49\n",
    "\n",
    "7.https://github.com/tensorflow/models/tree/master/research/object_detection\n",
    "\n",
    "8.https://github.com/ryanwebber/tensorflow-image-classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Please Note that the tar file do not consist of the train and test dataset please use below link to get the images dataset:\n",
    "    <a href=\"https://github.com/anupam-dewan/Tensorflow-Image-Classification-trump-or-Obama/tree/master/Tutorial-adewan\">link</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
