{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will introduce you to the basics of Scrapy, a web crawling framework for a developer to write code to and create a spider, which define how a certain site (or a group of sites) will be scraped. The biggest feature is that it is built on Twisted, an asynchronous networking library, which increasees Scrapy's performance in regards to other scraping frameworks. Before we begin, let's answer some questions that some of you may have initially.\n",
    "\n",
    "![alt text](https://www.22nds.com/wp-content/uploads/2017/07/scrapy-e1501276846765.png \"Scrappy\")\n",
    "\n",
    "What is **aysnchronous**? \n",
    "\n",
    "To put simply, if you have a phone book and needed to call everyone, calling everyone one by one is **synchronous**. If you have a phone book, calling multiple phone numbers at the same time, **asynchronous**.\n",
    "\n",
    "\n",
    "What is the difference between **Scrapy** and **Beautiful Soup**?\n",
    "\n",
    "As mentioned before, Scrapy is a web scraper framework. In Scrapy, we input a root URL to start crawling, then specify how many URLs you want to crawl and parse. \n",
    "\n",
    "**On the other hand...**\n",
    "\n",
    "BeautifulSoup is a tool to quickly extract valid data from web pages. It is also very friendly for beginners as learning it is very easy. However, in most cases, BeautifulSoup alone can not get the job done, you need use another package such as urlib2 or requests to help you download the web page and then you can use BeautifulSoup to parse the HTML source code. It also only fetches the contents of the URL that you give and then stops. It does not crawl unless you manually put it inside an infinite loop with certain criteria.\n",
    "\n",
    "In conclusion, the difference lies in that Beautiful Soup is a library while Scrapy is a complete framework-- we can use Beautiful Soup to create something similar to Scrapy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial content\n",
    "\n",
    "In this tutorial, we will show how to do some basic web scraping with the Scrapy frame work. \n",
    "\n",
    "While we have learned how to scrape data from the web using Beautiful Soup in class, this tutorial will show you an alternative, way to scrape data that is much quicker. We will first be scraping the /r/funny, https://www.reddit.com/r/funny/  page on reddit and converting the up/down votes and content into a .csv file. \n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "Overview of Scrapy\n",
    "Write your first Web Scraping code with Scrapy\n",
    "Set up your system\n",
    "Scraping Reddit: Fast Experimenting with Scrapy Shell\n",
    "Writing Custom Scrapy Spiders\n",
    "Case Studies using Scrapy\n",
    "Scraping an E-Commerce site\n",
    "Scraping Techcrunch: Create your own RSS Feed Reader\n",
    "\n",
    "- [Installing Scrapy](#Installing-Scrapy)\n",
    "- [Scraping  Reddit: Intro to Scrapy using Scrapy Shell](#Scrapy-Shell)\n",
    "- [Writing Custom Spiders](#Writing-Custom-Spiders)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use Scrapy, we must first install Scrapy onto our computer. Scrapy supports both versions of Python 2 and 3. If you’re using Anaconda, you can install the package from the conda-forge channel, which has up-to-date packages for Linux, Windows and OS X.\n",
    "\n",
    "To install using `conda`:\n",
    "\n",
    "    conda install -c conda-forge scrapy\n",
    "\n",
    "For Linux and OS x users, you can also use `pip`.\n",
    "\n",
    "To install using `pip`:\n",
    "\n",
    "    pip install scrapy\n",
    "    \n",
    "**NOTE**: I ran into some erros after installing scrapy, specifically:\n",
    "\n",
    "    ImportError: libiconv.so.2: cannot open shared object file: No such file or directory\n",
    "    \n",
    "If this is the case, make sure you run:\n",
    "\n",
    "    conda update --all\n",
    "\n",
    "and errors such as these should be fixed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the tutorial, we will primarily be using terminal command line to get use to the syntax of Scrapy. To get use to this syntax, we will be scraping the /r/datascience page on reddit. Create a new folder on your computer and start scrapy shell in that folder. To start the scrapy shell in your command line type:\n",
    "\n",
    "    scrapy shell\n",
    "    \n",
    "If Scrapy just wrote a bunch of stuff, don't worry, you are on the right track. In order to get information from /r/datascience on Reddit you will have to first run a crawler on it. A crawler is a program that browses web sites and downloads content. Sometimes crawlers are also referred as spiders.\n",
    "\n",
    "To run the scrapy crawler, type the following into the shell:\n",
    "\n",
    "    fetch(\"https://www.reddit.com/r/datascience/\")\n",
    "    \n",
    "It should return something like this:![alt text](https://image.ibb.co/fKkEX7/1.png \"Scrappy\")\n",
    "\n",
    "When you crawl something with scrapy it returns a “response” object that contains the downloaded information. If done correctly, you should see that page of /r/datascience scraped onto your computer locally. Compare it with the actual /r/datascience page.\n",
    "\n",
    "If you type print(response.text) into your terminal you should see the elements that make up the webpage.\n",
    "\n",
    "For now, we simply just want to collect \n",
    "* Title of each post\n",
    "* Number of votes it has\n",
    "* Number of comments\n",
    "* Time of post creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Title of each post\n",
    "Similar to Beautiful Soup, Scrapy provides ways to extract information from HTML based on css selectors like class, id etc. Let’s find the css selector for title, right click on any post’s title and select “Inspect” or “Inspect Element”:\n",
    "\n",
    "![alt text](https://image.ibb.co/fA3Uzn/2.png \"Scrappy\")\n",
    "\n",
    "You should see that the p tag has a class title. The title can then be extracted by typing the following:\n",
    "\n",
    "    response.css(\".title::text\").extract()\n",
    "    \n",
    "Here response.css(..) is a function which helps pull content based on the css class/selector inserted. The ‘.’ before title is used because .title is the css class. We use ::text in order to only extract the text content of the matching elements. Otherwise, the html tags would be scraped as well. The following two images show an example:\n",
    "\n",
    "   ![alt text](https://image.ibb.co/c5Rk5S/3.png \"Scrappy\")\n",
    "\n",
    "This similar idea can be applied to the extraction of the number of votes, number of comments, time of post creation.\n",
    "\n",
    "### Extracting Number of votes\n",
    "\n",
    "For extracting number of votes, we also simply Inspect Element of the page. From inspecting elements, we see that .score.unvoted corresponds directly to the vote count we see on the page. After find this class, we then type:\n",
    "\n",
    "    response.css(\".score.unvoted::text\").extract()\n",
    "    \n",
    "### Extracting time of post creation\n",
    "\n",
    "For the extraction of time of post creation, we cannot simply extract the text, as the text displays how long ago the post was created and not the time. As a result we must use another method.\n",
    "\n",
    "Take a look at the documentation first in regards to the attr method and see if you can apply this to extracting time of post creation. If not follow below:\n",
    "\n",
    "   ![alt text](https://image.ibb.co/jTa1en/4.png \"Scrappy\")\n",
    "\n",
    "Since we need to extract the attribute contents in the title, we must use the attr method as follows:\n",
    "\n",
    "    response.css(\"time::attr(title)\").extract()\n",
    "\n",
    "The .attr(attributename) is used to get the value of the specified attribute of the matching element.\n",
    "\n",
    "### Extracting Number of comments\n",
    "\n",
    "Extracting the number of comments should be relatively easy based on what we learn so far and will be left as an excercise to the readers. \n",
    "\n",
    "As a recap, so far we have used:\n",
    "\n",
    "* response – An object that the scrapy crawler returns. This object contains all the information about the downloaded content.\n",
    "* response.css(..) – Matches the element with the given CSS selectors.\n",
    "* extract_first(..) – Extracts the “first” element that matches the given criteria.\n",
    "* extract(..) – Extracts “all” the elements that match the given criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Custom Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have stated, a spider/scraper is a program that pulls content from web from a given URL. Normally, you also would need to write code to convert the extracted data to a structured format and store it in a reusable format like CSV, JSON,etc as we have seen in Beautiful soup. However, that is very tedious and is a lot of code. Fortunately for us, Scrapy does most of the work for us, as it comes with most of the functionalities to scrape and convert to a usable format.\n",
    "\n",
    "Before we can start using Custom Spiders, we must initialize spider to work in Jupyter Notebook. Because Scrapy is a framework and is usually not used in Jupyter Notebook, there are some preset configurations we must apply before we write our Custom Spiders. First, we need to allow Jupyter to input into terminal. This is done by imporitng InteractiveSHell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must import scrapy and its web crawler from scrapy to scrape websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we import json and create a simple pipeline that writes all found items to a JSON file, where each line contains one JSON element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('quoteresult.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://www.reddit.com/r/datascience/',\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
    "        'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
    "        'FEED_URI': 'result.json'                        # Used for pipeline 2\n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        titles = response.css('.title.may-blank::text').extract()\n",
    "        votes = response.css('.score.unvoted::text').extract()\n",
    "        times = response.css('time::attr(title)').extract()\n",
    "        comments = response.css('.comments::text').extract()\n",
    "       \n",
    "        #Give the extracted content row wise\n",
    "        for item in zip(titles,votes,times,comments):\n",
    "            #create a dictionary to store the scraped info\n",
    "            scraped_info = {\n",
    "                'title' : item[0],\n",
    "                'vote' : item[1],\n",
    "                'created_at' : item[2],\n",
    "                'comments' : item[3],\n",
    "            }\n",
    "\n",
    "            #yield or give the scraped info to scrapy\n",
    "            yield scraped_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-31 23:57:47 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\n",
      "2018-03-31 23:57:47 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 18:21:58) - [GCC 7.2.0], pyOpenSSL 17.5.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.1, Platform Linux-4.13.0-37-generic-x86_64-with-debian-stretch-sid\n",
      "2018-03-31 23:57:47 [scrapy.crawler] INFO: Overridden settings: {'FEED_FORMAT': 'json', 'FEED_URI': 'result.json', 'LOG_LEVEL': 30, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7fca206816d8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d8e819c2a04b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQuotesSpider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stop_after_crawl)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shutdown'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_dns_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \"\"\"\n\u001b[1;32m   1221\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mReactorBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(QuotesSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 andy 0 Mar 31 23:57 quoteresult.jl\r\n"
     ]
    }
   ],
   "source": [
    "ll quoteresult.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 2 quoteresult.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>created_at</th>\n",
       "      <th>title</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77 comments</td>\n",
       "      <td>Sat Aug 6 11:43:42 2011 UTC</td>\n",
       "      <td>Weekly 'Entering &amp; Transitioning' Thread. Ques...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21 comments</td>\n",
       "      <td>Sun Mar 25 17:09:02 2018 UTC</td>\n",
       "      <td>How much math is really needed for DS?</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 comment</td>\n",
       "      <td>Sat Mar 31 17:58:33 2018 UTC</td>\n",
       "      <td>Autodidacts, how do you attack a textbook on y...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3 comments</td>\n",
       "      <td>Sun Apr 1 01:41:02 2018 UTC</td>\n",
       "      <td>Validate significance of classification of unb...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 comment</td>\n",
       "      <td>Sat Mar 31 14:58:40 2018 UTC</td>\n",
       "      <td>Data Science Intermediate Projects</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2 comments</td>\n",
       "      <td>last edited 11 hours ago</td>\n",
       "      <td>Create internal blog from Notebooks</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>comment</td>\n",
       "      <td>Sat Mar 31 20:33:52 2018 UTC</td>\n",
       "      <td>Best courses to take if I hope to be a Data Sc...</td>\n",
       "      <td>•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>comment</td>\n",
       "      <td>Sat Mar 31 23:38:08 2018 UTC</td>\n",
       "      <td>Word Embeddings : Word2Vec and Latent Semantic...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>comment</td>\n",
       "      <td>Sun Apr 1 03:20:42 2018 UTC</td>\n",
       "      <td>Before Sunrise Text Classification: Who Said It?</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50 comments</td>\n",
       "      <td>Sat Mar 31 20:56:01 2018 UTC</td>\n",
       "      <td>What is the easiest way for one to store a dat...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7 comments</td>\n",
       "      <td>Sat Mar 31 16:50:46 2018 UTC</td>\n",
       "      <td>What are the best job boards for posting a pos...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2 comments</td>\n",
       "      <td>Fri Mar 30 22:12:18 2018 UTC</td>\n",
       "      <td>A xlsx and html rendering library for renderin...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7 comments</td>\n",
       "      <td>Sat Mar 31 12:36:38 2018 UTC</td>\n",
       "      <td>New to python/data analysis, can't delimit a f...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11 comments</td>\n",
       "      <td>Fri Mar 30 20:04:58 2018 UTC</td>\n",
       "      <td>Categorical features in dataset</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>comment</td>\n",
       "      <td>Fri Mar 30 23:52:58 2018 UTC</td>\n",
       "      <td>Using DataFlow to build a fully-managed pipeli...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1 comment</td>\n",
       "      <td>Sat Mar 31 09:31:31 2018 UTC</td>\n",
       "      <td>Yet another Titanic kaggle solve</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>comment</td>\n",
       "      <td>Fri Mar 30 12:47:56 2018 UTC</td>\n",
       "      <td>Data Science + Studio Production @ Netflix</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14 comments</td>\n",
       "      <td>Fri Mar 30 17:50:35 2018 UTC</td>\n",
       "      <td>I'm working on a new skating app and need help...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>comment</td>\n",
       "      <td>Fri Mar 30 17:42:54 2018 UTC</td>\n",
       "      <td>How can I just send my data dashboards via iPh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>comment</td>\n",
       "      <td>Fri Mar 30 14:04:10 2018 UTC</td>\n",
       "      <td>Information age notebook. This notebook label ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4 comments</td>\n",
       "      <td>Sat Mar 31 04:59:19 2018 UTC</td>\n",
       "      <td>Accademic Journals Concerning Data Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10 comments</td>\n",
       "      <td>Sat Mar 31 02:40:59 2018 UTC</td>\n",
       "      <td>In this article, I explain about Deep Learning...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25 comments</td>\n",
       "      <td>Fri Mar 30 22:25:48 2018 UTC</td>\n",
       "      <td>I'm a data analyst. I've been learning Python ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2 comments</td>\n",
       "      <td>last edited 1 day ago</td>\n",
       "      <td>Dynamic Hierarchical Topic Modelling</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2 comments</td>\n",
       "      <td>Thu Mar 29 20:10:11 2018 UTC</td>\n",
       "      <td>forecasting using price as input</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7 comments</td>\n",
       "      <td>Fri Mar 30 06:57:03 2018 UTC</td>\n",
       "      <td>What is more profitable for an enterprise to h...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       comments                    created_at  \\\n",
       "0   77 comments   Sat Aug 6 11:43:42 2011 UTC   \n",
       "1   21 comments  Sun Mar 25 17:09:02 2018 UTC   \n",
       "2     1 comment  Sat Mar 31 17:58:33 2018 UTC   \n",
       "3    3 comments   Sun Apr 1 01:41:02 2018 UTC   \n",
       "4     1 comment  Sat Mar 31 14:58:40 2018 UTC   \n",
       "5    2 comments      last edited 11 hours ago   \n",
       "6       comment  Sat Mar 31 20:33:52 2018 UTC   \n",
       "7       comment  Sat Mar 31 23:38:08 2018 UTC   \n",
       "8       comment   Sun Apr 1 03:20:42 2018 UTC   \n",
       "9   50 comments  Sat Mar 31 20:56:01 2018 UTC   \n",
       "10   7 comments  Sat Mar 31 16:50:46 2018 UTC   \n",
       "11   2 comments  Fri Mar 30 22:12:18 2018 UTC   \n",
       "12   7 comments  Sat Mar 31 12:36:38 2018 UTC   \n",
       "13  11 comments  Fri Mar 30 20:04:58 2018 UTC   \n",
       "14      comment  Fri Mar 30 23:52:58 2018 UTC   \n",
       "15    1 comment  Sat Mar 31 09:31:31 2018 UTC   \n",
       "16      comment  Fri Mar 30 12:47:56 2018 UTC   \n",
       "17  14 comments  Fri Mar 30 17:50:35 2018 UTC   \n",
       "18      comment  Fri Mar 30 17:42:54 2018 UTC   \n",
       "19      comment  Fri Mar 30 14:04:10 2018 UTC   \n",
       "20   4 comments  Sat Mar 31 04:59:19 2018 UTC   \n",
       "21  10 comments  Sat Mar 31 02:40:59 2018 UTC   \n",
       "22  25 comments  Fri Mar 30 22:25:48 2018 UTC   \n",
       "23   2 comments         last edited 1 day ago   \n",
       "24   2 comments  Thu Mar 29 20:10:11 2018 UTC   \n",
       "25   7 comments  Fri Mar 30 06:57:03 2018 UTC   \n",
       "\n",
       "                                                title vote  \n",
       "0   Weekly 'Entering & Transitioning' Thread. Ques...   13  \n",
       "1              How much math is really needed for DS?   39  \n",
       "2   Autodidacts, how do you attack a textbook on y...    4  \n",
       "3   Validate significance of classification of unb...   17  \n",
       "4                  Data Science Intermediate Projects    4  \n",
       "5                 Create internal blog from Notebooks    2  \n",
       "6   Best courses to take if I hope to be a Data Sc...    •  \n",
       "7   Word Embeddings : Word2Vec and Latent Semantic...    3  \n",
       "8    Before Sunrise Text Classification: Who Said It?    3  \n",
       "9   What is the easiest way for one to store a dat...   61  \n",
       "10  What are the best job boards for posting a pos...    4  \n",
       "11  A xlsx and html rendering library for renderin...   30  \n",
       "12  New to python/data analysis, can't delimit a f...    6  \n",
       "13                    Categorical features in dataset    0  \n",
       "14  Using DataFlow to build a fully-managed pipeli...   28  \n",
       "15                   Yet another Titanic kaggle solve   10  \n",
       "16         Data Science + Studio Production @ Netflix   10  \n",
       "17  I'm working on a new skating app and need help...   13  \n",
       "18  How can I just send my data dashboards via iPh...    0  \n",
       "19  Information age notebook. This notebook label ...    0  \n",
       "20         Accademic Journals Concerning Data Science    1  \n",
       "21  In this article, I explain about Deep Learning...  125  \n",
       "22  I'm a data analyst. I've been learning Python ...    9  \n",
       "23               Dynamic Hierarchical Topic Modelling    3  \n",
       "24                   forecasting using price as input    5  \n",
       "25  What is more profitable for an enterprise to h...    2  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dfjson = pd.read_json('result.json')\n",
    "dfjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}